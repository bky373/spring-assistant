"url","content"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/index.html","Spring AI: The Spring AI project aims to streamline the development of applications that incorporate artificial intelligence functionality without unnecessary complexity. The project draws inspiration from notable Python projects, such as LangChain and LlamaIndex, but Spring AI is not a direct port of those projects. The project was founded with the belief that the next wave of Generative AI applications will not be only for Python developers but will be ubiquitous across many programming languages. At its core, Spring AI addresses the fundamental challenge of AI integration: Connecting your enterprise Data and APIs with the AI Models . Spring AI provides abstractions that serve as the foundation for developing AI applications. These abstractions have multiple implementations, enabling easy component swapping with minimal code changes. Spring AI provides the following features: Support for all major Model providers such as OpenAI, Microsoft, Amazon, Google, and Hugging Face. Supported Model types are Chat, Text to Image, Audio Transcription, Text to Speech, Moderation, and more on the way. Portable API across AI providers for all models. Both synchronous and stream API options are supported. Dropping down to access model specific features is also supported. Mapping of AI Model output to POJOs. Support for all major Vector Database providers such as Apache Cassandra, Azure Vector Search, Chroma, Milvus, MongoDB Atlas, Neo4j, Oracle, PostgreSQL/PGVector, PineCone, Qdrant, Redis, and Weaviate. Portable API across Vector Store providers, including a novel SQL-like metadata filter API that is also portable. Function calling. Spring Boot Auto Configuration and Starters for AI Models and Vector Stores. ETL framework for Data Engineering. This feature set lets you implement common use cases such as “Q&A over your documentation” or “Chat with your documentation.” The concepts section(concepts.html) provides a high-level overview of AI concepts and their representation in Spring AI. The Getting Started(getting-started.html) section shows you how to create your first AI application. Subsequent sections delve into each component and common use cases with a code-focused approach. AI Concepts(concepts.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/concepts.html","AI Concepts: This section describes core concepts that Spring AI uses. We recommend reading it closely to understand the ideas behind how Spring AI is implemented. Models: AI models are algorithms designed to process and generate information, often mimicking human cognitive functions. By learning patterns and insights from large datasets, these models can make predictions, text, images, or other outputs, enhancing various applications across industries. There are many different types of AI models, each suited for a specific use case. While ChatGPT and its generative AI capabilities have captivated users through text input and output, many models and companies offer diverse inputs and outputs. Before ChatGPT, many people were fascinated by text-to-image generation models such as Midjourney and Stable Diffusion. The following table categorizes several models based on their input and output types: Spring AI currently supports models that process input and output as language, image, and audio. The last row in the previous table, which accepts text as input and outputs numbers, is more commonly known as embedding text and represents the internal data structures used in an AI model. Spring AI has support for embeddings to enable more advanced use cases. What sets models like GPT apart is their pre-trained nature, as indicated by the ""P"" in GPT—Chat Generative Pre-trained Transformer. This pre-training feature transforms AI into a general developer tool that does not require an extensive machine learning or model training background. Prompts: Prompts serve as the foundation for the language-based inputs that guide an AI model to produce specific outputs. For those familiar with ChatGPT, a prompt might seem like merely the text entered into a dialog box that is sent to the API. However, it encompasses much more than that. In many AI Models, the text for the prompt is not just a simple string. ChatGPT’s API has multiple text inputs within a prompt, with each text input being assigned a role. For example, there is the system role, which tells the model how to behave and sets the context for the interaction. There is also the user role, which is typically the input from the user. Crafting effective prompts is both an art and a science. ChatGPT was designed for human conversations. This is quite a departure from using something like SQL to ""ask a question"". One must communicate with the AI model akin to conversing with another person. Such is the importance of this interaction style that the term ""Prompt Engineering"" has emerged as its own discipline. There is a burgeoning collection of techniques that improve the effectiveness of prompts. Investing time in crafting a prompt can drastically improve the resulting output. Sharing prompts has become a communal practice, and there is active academic research being done on this subject. As an example of how counter-intuitive it can be to create an effective prompt (for example, contrasting with SQL), a recent research paper(https://arxiv.org/abs/2205.11916) found that one of the most effective prompts you can use starts with the phrase, “Take a deep breath and work on this step by step.” That should give you an indication of why language is so important. We do not yet fully understand how to make the most effective use of previous iterations of this technology, such as ChatGPT 3.5, let alone new versions that are being developed. Prompt Templates: Creating effective prompts involves establishing the context of the request and substituting parts of the request with values specific to the user’s input. This process uses traditional text-based template engines for prompt creation and management. Spring AI employs the OSS library StringTemplate(https://www.stringtemplate.org/) for this purpose. For instance, consider the simple prompt template: Tell me a {adjective} joke about {content}. In Spring AI, prompt templates can be likened to the ""View"" in Spring MVC architecture. A model object, typically a java.util.Map , is provided to populate placeholders within the template. The ""rendered"" string becomes the content of the prompt supplied to the AI model. There is considerable variability in the specific data format of the prompt sent to the model. Initially starting as simple strings, prompts have evolved to include multiple messages, where each string in each message represents a distinct role for the model. Embeddings: Embeddings are numerical representations of text, images, or videos that capture relationships between inputs. Embeddings work by converting text, image, and video into arrays of floating point numbers, called vectors. These vectors are designed to capture the meaning of the text, images, and videos. The length of the embedding array is called the vector’s dimensionality. By calculating the numerical distance between the vector representations of two pieces of text, an application can determine the similarity between the objects used to generate the embedding vectors. As a Java developer exploring AI, it’s not necessary to comprehend the intricate mathematical theories or the specific implementations behind these vector representations. A basic understanding of their role and function within AI systems suffices, particularly when you’re integrating AI functionalities into your applications. Embeddings are particularly relevant in practical applications like the Retrieval Augmented Generation (RAG) pattern. They enable the representation of data as points in a semantic space, which is akin to the 2-D space of Euclidean geometry, but in higher dimensions. This means just like how points on a plane in Euclidean geometry can be close or far based on their coordinates, in a semantic space, the proximity of points reflects the similarity in meaning. Sentences about similar topics are positioned closer in this multi-dimensional space, much like points lying close to each other on a graph. This proximity aids in tasks like text classification, semantic search, and even product recommendations, as it allows the AI to discern and group related concepts based on their ""location"" in this expanded semantic landscape. You can think of this semantic space as a vector. Tokens: Tokens serve as the building blocks of how an AI model works. On input, models convert words to tokens. On output, they convert tokens back to words. In English, one token roughly corresponds to 75% of a word. For reference, Shakespeare’s complete works, totaling around 900,000 words, translate to approximately 1.2 million tokens. Perhaps more important is that Tokens = Money. In the context of hosted AI models, your charges are determined by the number of tokens used. Both input and output contribute to the overall token count. Also, models are subject to token limits, which restrict the amount of text processed in a single API call. This threshold is often referred to as the ""context window"". The model does not process any text that exceeds this limit. For instance, ChatGPT3 has a 4K token limit, while GPT4 offers varying options, such as 8K, 16K, and 32K. Anthropic’s Claude AI model features a 100K token limit, and Meta’s recent research yielded a 1M token limit model. To summarize the collected works of Shakespeare with GPT4, you need to devise software engineering strategies to chop up the data and present the data within the model’s context window limits. The Spring AI project helps you with this task. Structured Output: The output of AI models traditionally arrives as a java.lang.String , even if you ask for the reply to be in JSON. It may be a correct JSON, but it is not a JSON data structure. It is just a string. Also, asking “for JSON” as part of the prompt is not 100% accurate. This intricacy has led to the emergence of a specialized field involving the creation of prompts to yield the intended output, followed by converting the resulting simple string into a usable data structure for application integration. The Structured output conversion(api/structured-output-converter.html#_structuredoutputconverter) employs meticulously crafted prompts, often necessitating multiple interactions with the model to achieve the desired formatting. Bringing Your Data & APIs to the AI Model: How can you equip the AI model with information on which it has not been trained? Note that the GPT 3.5/4.0 dataset extends only until September 2021. Consequently, the model says that it does not know the answer to questions that require knowledge beyond that date. An interesting bit of trivia is that this dataset is around 650GB. Three techniques exist for customizing the AI model to incorporate your data: Fine Tuning : This traditional machine learning technique involves tailoring the model and changing its internal weighting. However, it is a challenging process for machine learning experts and extremely resource-intensive for models like GPT due to their size. Additionally, some models might not offer this option. Prompt Stuffing : A more practical alternative involves embedding your data within the prompt provided to the model. Given a model’s token limits, techniques are required to present relevant data within the model’s context window. This approach is colloquially referred to as “stuffing the prompt.” The Spring AI library helps you implement solutions based on the “stuffing the prompt” technique otherwise known as Retrieval Augmented Generation (RAG)(#concept-rag) . Function Calling(#concept-fc) : This technique allows registering custom, user functions that connect the large language models to the APIs of external systems. Spring AI greatly simplifies code you need to write to support function calling(api/functions.html) . Retrieval Augmented Generation: A technique termed Retrieval Augmented Generation (RAG) has emerged to address the challenge of incorporating relevant data into prompts for accurate AI model responses. The approach involves a batch processing style programming model, where the job reads unstructured data from your documents, transforms it, and then writes it into a vector database. At a high level, this is an ETL (Extract, Transform and Load) pipeline. The vector database is used in the retrieval part of RAG technique. As part of loading the unstructured data into the vector database, one of the most important transformations is to split the original document into smaller pieces. The procedure of splitting the original document into smaller pieces has two important steps: Split the document into parts while preserving the semantic boundaries of the content. For example, for a document with paragraphs and tables, one should avoid splitting the document in the middle of a paragraph or table. For code, avoid splitting the code in the middle of a method’s implementation. Split the document’s parts further into parts whose size is a small percentage of the AI Model’s token limit. The next phase in RAG is processing user input. When a user’s question is to be answered by an AI model, the question and all the “similar” document pieces are placed into the prompt that is sent to the AI model. This is the reason to use a vector database. It is very good at finding similar content. The ETL Pipeline(api/etl-pipeline.html) provides further information about orchestrating the flow of extracting data from data sources and storing it in a structured vector store, ensuring data is in the optimal format for retrieval when passing it to the AI model. The ChatClient - RAG(api/chatclient.html#_retrieval_augmented_generation) explains how to use the QuestionAnswerAdvisor to enable the RAG capability in your application. Function Calling: Large Language Models (LLMs) are frozen after training, leading to stale knowledge, and they are unable to access or modify external data. The Function Calling(api/functions.html) mechanism addresses these shortcomings. It allows you to register your own functions to connect the large language models to the APIs of external systems. These systems can provide LLMs with real-time data and perform data processing actions on their behalf. Spring AI greatly simplifies code you need to write to support function invocation. It handles the function invocation conversation for you. You can provide your function as a @Bean and then provide the bean name of the function in your prompt options to activate that function. Additionally, you can define and reference multiple functions in a single prompt. Perform a chat request sending along function definition information. The latter provides the name , description (e.g. explaining when the Model should call the function), and input parameters (e.g. the function’s input parameters schema). When the Model decides to call the function, it will call the function with the input parameters and return the output to the model. Spring AI handles this conversation for you. It dispatches the function call to the appropriate function and returns the result to the model. The Model can perform multiple function calls to retrieve all the information it needs. Once all information needed is acquired, the Model will generate a response. Follow the Function Calling(api/functions.html) documentation for further information on how to use this feature with different AI models. Evaluating AI responses: Effectively evaluating the output of an AI system in response to user requests is very important to ensuring the accuracy and usefulness of the final application. Several emerging techniques enable the use of the pre-trained model itself for this purpose. This evaluation process involves analyzing whether the generated response aligns with the user’s intent and the context of the query. Metrics such as relevance, coherence, and factual correctness are used to gauge the quality of the AI-generated response. One approach involves presenting both the user’s request and the AI model’s response to the model, querying whether the response aligns with the provided data. Furthermore, leveraging the information stored in the vector database as supplementary data can enhance the evaluation process, aiding in the determination of response relevance. The Spring AI project provides an Evaluator API which currently gives access to basic strategies to evaluate model responses. Follow the Evaluation Testing(api/testing.html) documentation for further information. Overview(index.html) Getting Started(getting-started.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/getting-started.html","Getting Started: This section offers jumping off points for how to get started using Spring AI. You should follow the steps in each of the following section according to your needs. Spring AI supports Spring Boot 3.2.x and 3.3.x Spring Initializr: Head on over to start.spring.io(https://start.spring.io/) and select the AI Models and Vector Stores that you want to use in your new applications. Add Milestone and Snapshot Repositories: If you prefer to add the dependency snippets by hand, follow the directions in the following sections. To use the Milestone and Snapshot version, you need to add references to the Spring Milestone and/or Snapshot repositories in your build file. For Maven, add the following repository definitions as needed: <repositories> <repository> <id>spring-milestones</id> <name>Spring Milestones</name> <url>https://repo.spring.io/milestone</url> <snapshots> <enabled>false</enabled> </snapshots> </repository> <repository> <id>spring-snapshots</id> <name>Spring Snapshots</name> <url>https://repo.spring.io/snapshot</url> <releases> <enabled>false</enabled> </releases> </repository> </repositories> For Gradle, add the following repository definitions as needed: repositories { mavenCentral() maven { url 'https://repo.spring.io/milestone' } maven { url 'https://repo.spring.io/snapshot' } } Dependency Management: The Spring AI Bill of Materials (BOM) declares the recommended versions of all the dependencies used by a given release of Spring AI. Using the BOM from your application’s build script avoids the need for you to specify and maintain the dependency versions yourself. Instead, the version of the BOM you’re using determines the utilized dependency versions. It also ensures that you’re using supported and tested versions of the dependencies by default, unless you choose to override them. If you’re a Maven user, you can use the BOM by adding the following to your pom.xml file - <dependencyManagement> <dependencies> <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bom</artifactId> <version>1.0.0-SNAPSHOT</version> <type>pom</type> <scope>import</scope> </dependency> </dependencies> </dependencyManagement> Gradle users can also use the Spring AI BOM by leveraging Gradle (5.0+) native support for declaring dependency constraints using a Maven BOM. This is implemented by adding a 'platform' dependency handler method to the dependencies section of your Gradle build script. As shown in the snippet below this can then be followed by version-less declarations of the Starter Dependencies for the one or more spring-ai modules you wish to use, e.g. spring-ai-openai. dependencies { implementation platform(""org.springframework.ai:spring-ai-bom:1.0.0-SNAPSHOT"") // Replace the following with the starter dependencies of specific modules you wish to use implementation 'org.springframework.ai:spring-ai-openai' } Add dependencies for specific components: Each of the following sections in the documentation shows which dependencies you need to add to your project build system. Chat Models(api/chatmodel.html) Embeddings Models(api/embeddings.html) Image Generation Models(api/imageclient.html) Transcription Models(api/audio/transcriptions.html) Text-To-Speech (TTS) Models(api/audio/speech.html) Vector Databases(api/vectordbs.html) Sample Projects: You can clone these projects on GitHub to get started. Flight Booking Assistant: github.com/tzolov/playground-flight-booking(https://github.com/tzolov/playground-flight-booking) AI-powered system that has access to terms and conditions (Retrieval Augmented Generation, RAG), access tools (Java methods) to perform actions (Function Calling) and uses an LLM to interact with the user OpenAI: github.com/rd-1-2022/ai-openai-helloworld(https://github.com/rd-1-2022/ai-openai-helloworld) Azure OpenAI: github.com/rd-1-2022/ai-azure-openai-helloworld(https://github.com/rd-1-2022/ai-azure-openai-helloworld) github.com/Azure-Samples/spring-ai-azure-workshop(https://github.com/Azure-Samples/spring-ai-azure-workshop) AI Concepts(concepts.html) Spring AI API(api/index.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/index.html","Spring AI API: Introduction: The Spring AI API covers a wide range of functionalities. Each major feature is detailed in its own dedicated section. To provide an overview, the following key functionalities are available: AI Model API: Portable Model API across AI providers for Chat , Text to Image , Audio Transcription , Text to Speech , and Embedding models. Both synchronous and stream API options are supported. Dropping down to access model specific features is also supported. With support for AI Models from OpenAI, Microsoft, Amazon, Google, Amazon Bedrock, Hugging Face and more. Vector Store API: Portable Vector Store API across multiple providers, including a novel SQL-like metadata filter API that is also portable. Support for 14 vector databases are available. Function Calling API: Function calling . Spring AI makes it easy to have the AI model invoke your POJO java.util.Function object. Check the Spring AI Function Calling(functions.html) documentation. Auto Configuration: Spring Boot Auto Configuration and Starters for AI Models and Vector Stores. ETL Data Engineering: ETL framework for Data Engineering. This provides the basis of loading data into a vector database, helping implement the Retrieval Augmented Generation pattern that enables you to bring your data to the AI model to incorporate into its response. Feedback and Contributions: The project’s GitHub discussions(https://github.com/spring-projects/spring-ai/discussions) is a great place to send feedback. Getting Started(../getting-started.html) Chat Client API(chatclient.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chatclient.html","Chat Client API: The ChatClient offers a fluent API for communicating with an AI Model. It supports both a synchronous and streaming programming model. The fluent API has methods for building up the constituent parts of a Prompt(prompt.html#_prompt) that is passed to the AI model as input. The Prompt contains the instructional text to guide the AI model’s output and behavior. From the API point of view, prompts consist of a collection of messages. The AI model processes two main types of messages: user messages, which are direct inputs from the user, and system messages, which are generated by the system to guide the conversation. These messages often contain placeholders that are substituted at runtime based on user input to customize the response of the AI model to the user input. There are also Prompt options that can be specified, such as the name of the AI Model to use and the temperature setting that controls the randomness or creativity of the generated output. Creating a ChatClient: The ChatClient is created using a ChatClient.Builder object. You can obtain an autoconfigured ChatClient.Builder instance for any ChatModel(chatmodel.html) Spring Boot autoconfiguration or create one programmatically. Using an autoconfigured ChatClient.Builder: In the most simple use case, Spring AI provides Spring Boot autoconfiguration, creating a prototype ChatClient.Builder bean for you to inject into your class. Here is a simple example of retrieving a String response to a simple user request. @RestController class MyController { private final ChatClient chatClient; public MyController(ChatClient.Builder chatClientBuilder) { this.chatClient = chatClientBuilder.build(); } @GetMapping(""/ai"") String generation(String userInput) { return this.chatClient.prompt() .user(userInput) .call() .content(); } } In this simple example, the user input sets the contents of the user message. The call() method sends a request to the AI model, and the content() method returns the AI model’s response as a String . Create a ChatClient programmatically: You can disable the ChatClient.Builder autoconfiguration by setting the property spring.ai.chat.client.enabled=false . This is useful if multiple chat models are used together. Then, create a ChatClient.Builder instance programmatically for every ChatModel you need: ChatModel myChatModel = ... // usually autowired ChatClient.Builder builder = ChatClient.builder(myChatModel); // or create a ChatClient with the default builder settings: ChatClient chatClient = ChatClient.create(myChatModel); ChatClient Fluent API: The ChatClient fluent API allows you to create a prompt in three distinct ways using an overloaded prompt method to initiate the fluent API: prompt() : This method with no arguments lets you start using the fluent API, allowing you to build up user, system, and other parts of the prompt. prompt(Prompt prompt) : This method accepts a Prompt argument, letting you pass in a Prompt instance that you have created using the Prompt’s non-fluent APIs. prompt(String content) : This is a convenience method similar to the previous overload. It takes the user’s text content. ChatClient Responses: The ChatClient API offers several ways to format the response from the AI Model using the fluent API. Returning a ChatResponse: The response from the AI model is a rich structure defined by the type ChatResponse(chatmodel.html#ChatResponse) . It includes metadata about how the response was generated and can also contain multiple responses, known as Generation(chatmodel.html#Generation) s, each with its own metadata. The metadata includes the number of tokens (each token is approximately 3/4 of a word) used to create the response. This information is important because hosted AI models charge based on the number of tokens used per request. An example to return the ChatResponse object that contains the metadata is shown below by invoking chatResponse() after the call() method. ChatResponse chatResponse = chatClient.prompt() .user(""Tell me a joke"") .call() .chatResponse(); Returning an Entity: You often want to return an entity class that is mapped from the returned String . The entity() method provides this functionality. For example, given the Java record: record ActorFilms(String actor, List<String> movies) {} You can easily map the AI model’s output to this record using the entity() method, as shown below: ActorFilms actorFilms = chatClient.prompt() .user(""Generate the filmography for a random actor."") .call() .entity(ActorFilms.class); There is also an overloaded entity method with the signature entity(ParameterizedTypeReference<T> type) that lets you specify types such as generic Lists: List<ActorFilms> actorFilms = chatClient.prompt() .user(""Generate the filmography of 5 movies for Tom Hanks and Bill Murray."") .call() .entity(new ParameterizedTypeReference<List<ActorFilms>>() {}); Streaming Responses: The stream() method lets you get an asynchronous response as shown below: Flux<String> output = chatClient.prompt() .user(""Tell me a joke"") .stream() .content(); You can also stream the ChatResponse using the method Flux<ChatResponse> chatResponse() . In the future, we will offer a convenience method that will let you return a Java entity with the reactive stream() method. In the meantime, you should use the Structured Output Converter(structured-output-converter.html#StructuredOutputConverter) to convert the aggregated response explicity as shown below. This also demonstrates the use of parameters in the fluent API that will be discussed in more detail in a later section of the documentation. var converter = new BeanOutputConverter<>(new ParameterizedTypeReference<List<ActorsFilms>>() {}); Flux<String> flux = this.chatClient.prompt() .user(u -> u.text("""""" Generate the filmography for a random actor. {format} """""") .param(""format"", converter.getFormat())) .stream() .content(); String content = flux.collectList().block().stream().collect(Collectors.joining()); List<ActorFilms> actorFilms = converter.convert(content); call() return values: After specifying the call() method on ChatClient , there are a few different options for the response type. String content() : returns the String content of the response ChatResponse chatResponse() : returns the ChatResponse object that contains multiple generations and also metadata about the response, for example how many token were used to create the response. entity() to return a Java type entity(ParameterizedTypeReference<T> type) : used to return a Collection of entity types. entity(Class<T> type) : used to return a specific entity type. entity(StructuredOutputConverter<T> structuredOutputConverter) : used to specify an instance of a StructuredOutputConverter to convert a String to an entity type. You can also invoke the stream() method instead of call() . stream() return values: After specifying the stream() method on ChatClient , there are a few options for the response type: Flux<String> content() : Returns a Flux of the string being generated by the AI model. Flux<ChatResponse> chatResponse() : Returns a Flux of the ChatResponse object, which contains additional metadata about the response. Using Defaults: Creating a ChatClient with a default system text in an @Configuration class simplifies runtime code. By setting defaults, you only need to specify the user text when calling ChatClient , eliminating the need to set a system text for each request in your runtime code path. Default System Text: In the following example, we will configure the system text to always reply in a pirate’s voice. To avoid repeating the system text in runtime code, we will create a ChatClient instance in a @Configuration class. @Configuration class Config { @Bean ChatClient chatClient(ChatClient.Builder builder) { return builder.defaultSystem(""You are a friendly chat bot that answers question in the voice of a Pirate"") .build(); } } and a @RestController to invoke it: @RestController class AIController { private final ChatClient chatClient; AIController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(""/ai/simple"") public Map<String, String> completion(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""completion"", chatClient.prompt().user(message).call().content()); } } When calling the application endpoint via curl, the result is: ❯ curl localhost:8080/ai/simple {""generation"":""Why did the pirate go to the comedy club? To hear some arrr-rated jokes! Arrr, matey!""} Default System Text with parameters: In the following example, we will use a placeholder in the system text to specify the voice of the completion at runtime instead of design time. @Configuration class Config { @Bean ChatClient chatClient(ChatClient.Builder builder) { return builder.defaultSystem(""You are a friendly chat bot that answers question in the voice of a {voice}"") .build(); } } @RestController class AIController { private final ChatClient chatClient; AIController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(""/ai"") Map<String, String> completion(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message, String voice) { return Map.of(""completion"", chatClient.prompt() .system(sp -> sp.param(""voice"", voice)) .user(message) .call() .content()); } } When calling the application endpoint via httpie, the result is: http localhost:8080/ai voice=='Robert DeNiro' { ""completion"": ""You talkin' to me? Okay, here's a joke for ya: Why couldn't the bicycle stand up by itself? Because it was two tired! Classic, right?"" } Other defaults: At the ChatClient.Builder level, you can specify the default prompt configuration. defaultOptions(ChatOptions chatOptions) : Pass in either portable options defined in the ChatOptions class or model-specific options such as those in OpenAiChatOptions . For more information on model-specific ChatOptions implementations, refer to the JavaDocs. defaultFunction(String name, String description, java.util.function.Function<I, O> function) : The name is used to refer to the function in user text. The description explains the function’s purpose and helps the AI model choose the correct function for an accurate response. The function argument is a Java function instance that the model will execute when necessary. defaultFunctions(String…​ functionNames) : The bean names of `java.util.Function`s defined in the application context. defaultUser(String text) , defaultUser(Resource text) , defaultUser(Consumer<UserSpec> userSpecConsumer) : These methods let you define the user text. The Consumer<UserSpec> allows you to use a lambda to specify the user text and any default parameters. defaultAdvisors(Advisor…​ advisor) : Advisors allow modification of the data used to create the Prompt . The QuestionAnswerAdvisor implementation enables the pattern of Retrieval Augmented Generation by appending the prompt with context information related to the user text. defaultAdvisors(Consumer<AdvisorSpec> advisorSpecConsumer) : This method allows you to define a Consumer to configure multiple advisors using the AdvisorSpec . Advisors can modify the data used to create the final Prompt . The Consumer<AdvisorSpec> lets you specify a lambda to add advisors, such as QuestionAnswerAdvisor , which supports Retrieval Augmented Generation by appending the prompt with relevant context information based on the user text. You can override these defaults at runtime using the corresponding methods without the default prefix. options(ChatOptions chatOptions) function(String name, String description, java.util.function.Function<I, O> function) functions(String…​ functionNames) user(String text) , user(Resource text) , user(Consumer<UserSpec> userSpecConsumer) advisors(Advisor…​ advisor) advisors(Consumer<AdvisorSpec> advisorSpecConsumer) Advisors: The Advisors API(advisors.html) provides a flexible and powerful way to intercept, modify, and enhance AI-driven interactions in your Spring applications. A common pattern when calling an AI model with user text is to append or augment the prompt with contextual data. This contextual data can be of different types. Common types include: Your own data : This is data the AI model hasn’t been trained on. Even if the model has seen similar data, the appended contextual data takes precedence in generating the response. Conversational history : The chat model’s API is stateless. If you tell the AI model your name, it won’t remember it in subsequent interactions. Conversational history must be sent with each request to ensure previous interactions are considered when generating a response. Advisor Configuration in ChatClient: The ChatClient fluent API provides an AdvisorSpec interface for configuring advisors. This interface offers methods to add parameters, set multiple parameters at once, and add one or more advisors to the chain. interface AdvisorSpec { AdvisorSpec param(String k, Object v); AdvisorSpec params(Map<String, Object> p); AdvisorSpec advisors(Advisor... advisors); AdvisorSpec advisors(List<Advisor> advisors); } The order in which advisors are added to the chain is crucial, as it determines the sequence of their execution. Each advisor modifies the prompt or the context in some way, and the changes made by one advisor are passed on to the next in the chain. ChatClient.builder(chatModel) .build() .prompt() .advisors( new MessageChatMemoryAdvisor(chatMemory), new QuestionAnswerAdvisor(vectorStore, SearchRequest.defaults()) ) .user(userText) .call() .content(); In this configuration, the MessageChatMemoryAdvisor will be executed first, adding the conversation history to the prompt. Then, the QuestionAnswerAdvisor will perform its search based on the user’s question and the added conversation history, potentially providing more relevant results. Retrieval Augmented Generation: A vector database stores data that the AI model is unaware of. When a user question is sent to the AI model, a QuestionAnswerAdvisor queries the vector database for documents related to the user question. The response from the vector database is appended to the user text to provide context for the AI model to generate a response. Assuming you have already loaded data into a VectorStore , you can perform Retrieval Augmented Generation (RAG) by providing an instance of QuestionAnswerAdvisor to the ChatClient . ChatResponse response = ChatClient.builder(chatModel) .build().prompt() .advisors(new QuestionAnswerAdvisor(vectorStore, SearchRequest.defaults())) .user(userText) .call() .chatResponse(); In this example, the SearchRequest.defaults() will perform a similarity search over all documents in the Vector Database. To restrict the types of documents that are searched, the SearchRequest takes an SQL like filter expression that is portable across all VectorStores . Dynamic Filter Expressions: Update the SearchRequest filter expression at runtime using the FILTER_EXPRESSION advisor context parameter: ChatClient chatClient = ChatClient.builder(chatModel) .defaultAdvisors(new QuestionAnswerAdvisor(vectorStore, SearchRequest.defaults())) .build(); // Update filter expression at runtime String content = chatClient.prompt() .user(""Please answer my question XYZ"") .advisors(a -> a.param(QuestionAnswerAdvisor.FILTER_EXPRESSION, ""type == 'Spring'"")) .call() .content(); The FILTER_EXPRESSION parameter allows you to dynamically filter the search results based on the provided expression. Chat Memory: The interface ChatMemory represents a storage for chat conversation history. It provides methods to add messages to a conversation, retrieve messages from a conversation, and clear the conversation history. There are currently two implementations, InMemoryChatMemory and CassandraChatMemory , that provide storage for chat conversation history, in-memory and persisted with time-to-live , correspondingly. To create a CassandraChatMemory with time-to-live : CassandraChatMemory.create(CassandraChatMemoryConfig.builder().withTimeToLive(Duration.ofDays(1)).build()); The following advisor implementations use the ChatMemory interface to advice the prompt with conversation history which differ in the details of how the memory is added to the prompt MessageChatMemoryAdvisor : Memory is retrieved and added as a collection of messages to the prompt PromptChatMemoryAdvisor : Memory is retrieved and added into the prompt’s system text. VectorStoreChatMemoryAdvisor : The constructor VectorStoreChatMemoryAdvisor(VectorStore vectorStore, String defaultConversationId, int chatHistoryWindowSize) lets you specify the VectorStore to retrieve the chat history from, the unique conversation ID, the size of the chat history to be retrieved in token size. A sample @Service implementation that uses several advisors is shown below. import static org.springframework.ai.chat.client.advisor.AbstractChatMemoryAdvisor.CHAT_MEMORY_CONVERSATION_ID_KEY; import static org.springframework.ai.chat.client.advisor.AbstractChatMemoryAdvisor.CHAT_MEMORY_RETRIEVE_SIZE_KEY; @Service public class CustomerSupportAssistant { private final ChatClient chatClient; public CustomerSupportAssistant(ChatClient.Builder builder, VectorStore vectorStore, ChatMemory chatMemory) { this.chatClient = builder .defaultSystem("""""" You are a customer chat support agent of an airline named ""Funnair""."", Respond in a friendly, helpful, and joyful manner. Before providing information about a booking or cancelling a booking, you MUST always get the following information from the user: booking number, customer first name and last name. Before changing a booking you MUST ensure it is permitted by the terms. If there is a charge for the change, you MUST ask the user to consent before proceeding. """""") .defaultAdvisors( new PromptChatMemoryAdvisor(chatMemory), // new MessageChatMemoryAdvisor(chatMemory), // CHAT MEMORY new QuestionAnswerAdvisor(vectorStore, SearchRequest.defaults()), // RAG new LoggingAdvisor()) .defaultFunctions(""getBookingDetails"", ""changeBooking"", ""cancelBooking"") // FUNCTION CALLING .build(); } public Flux<String> chat(String chatId, String userMessageContent) { return this.chatClient.prompt() .user(userMessageContent) .advisors(a -> a .param(CHAT_MEMORY_CONVERSATION_ID_KEY, chatId) .param(CHAT_MEMORY_RETRIEVE_SIZE_KEY, 100)) .stream().content(); } } Logging: The SimpleLoggerAdvisor is an advisor that logs the request and response data of the ChatClient . This can be useful for debugging and monitoring your AI interactions. Spring AI supports observability for LLM and vector store interactions. Refer to the Observability(../observabilty/index.html) guide for more information. To enable logging, add the SimpleLoggerAdvisor to the advisor chain when creating your ChatClient. It’s recommended to add it toward the end of the chain: ChatResponse response = ChatClient.create(chatModel).prompt() .advisors(new SimpleLoggerAdvisor()) .user(""Tell me a joke?"") .call() .chatResponse(); To see the logs, set the logging level for the advisor package to DEBUG : logging.level.org.springframework.ai.chat.client.advisor=DEBUG Add this to your application.properties or application.yaml file. You can customize what data from AdvisedRequest and ChatResponse is logged by using the following constructor: SimpleLoggerAdvisor( Function<AdvisedRequest, String> requestToString, Function<ChatResponse, String> responseToString ) Example usage: javaCopySimpleLoggerAdvisor customLogger = new SimpleLoggerAdvisor( request -> ""Custom request: "" + request.userText, response -> ""Custom response: "" + response.getResult() ); This allows you to tailor the logged information to your specific needs. Be cautious about logging sensitive information in production environments. Spring AI API(index.html) Advisors(advisors.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/advisors.html","Advisors API: The Spring AI Advisors API provides a flexible and powerful way to intercept, modify, and enhance AI-driven interactions in your Spring applications. By leveraging the Advisors API, developers can create more sophisticated, reusable, and maintainable AI components. The key benefits include encapsulating recurring Generative AI patterns, transforming data sent to and from Language Models (LLMs), and providing portability across various models and use cases. You can configure existing advisors using the ChatClient API(chatclient.html#_advisor_configuration_in_chatclient) as shown in the following example: var chatClient = ChatClient.builder(chatModel) .defaultAdvisors( new MessageChatMemoryAdvisor(chatMemory), // chat-memory advisor new QuestionAnswerAdvisor(vectorStore, SearchRequest.defaults()) // RAG advisor ) .build(); String response = chatClient.prompt() // Set advisor parameters at runtime .advisors(advisor -> advisor.param(""chat_memory_conversation_id"", ""678"") .param(""chat_memory_response_size"", 100)) .user(userText) .call() .content(); It is recommend to register the advisors at build time using builder’s defaultAdvisors() method. Advisors also participate in the Observability stack, so you can view metrics and traces related to their execution. Core Components: The API consists of CallAroundAdvisor and CallAroundAdvisorChain for non-streaming scenarios, and StreamAroundAdvisor and StreamAroundAdvisorChain for streaming scenarios. It also includes AdvisedRequest to represent the unsealed Prompt request, AdvisedResponse for the Chat Completion response. Both hold an advise-context to share state across the advisor chain. The nextAroundCall() and the nextAroundStream() are the key advisor methods, typically performing actions such as examining the unsealed Prompt data, customizing and augmenting the Prompt data, invoking the next entity in the advisor chain, optionally blocking the request, examining the chat completion response, and throwing exceptions to indicate processing errors. In addition the getOrder() method determines advisor order in the chain, while getName() provides a unique advisor name. The Advisor Chain, created by the Spring AI framework, allows sequential invocation of multiple advisors ordered by their getOrder() values. The lower values are executed first. The last advisor, added automatically, sends the request to the LLM. Following flow diagram illustrates the interaction between the advisor chain and the Chat Model: The Spring AI framework creates an AdvisedRequest from user’s Prompt along with an empty AdvisorContext object. Each advisor in the chain processes the request, potentially modifying it. Alternatively, it can choose to block the request by not making the call to invoke the next entity. In the latter case, the advisor is responsible for filling out the response. The final advisor, provided by the framework, sends the request to the Chat Model . The Chat Model’s response is then passed back through the advisor chain and converted into AdvisedResponse . Later includes the shared AdvisorContext instance. Each advisor can process or modify the response. The final AdvisedResponse is returned to the client by extracting the ChatCompletion . Advisor Order: The execution order of advisors in the chain is determined by the getOrder() method. Key points to understand: Advisors with lower order values are executed first. The advisor chain operates as a stack: The first advisor in the chain is the last to process the request. It is also the first to process the response. To control execution order: Set the order close to Ordered.HIGHEST_PRECEDENCE to ensure an advisor is executed first in the chain (last for request processing, first for response processing). Set the order close to Ordered.LOWEST_PRECEDENCE to ensure an advisor is executed last in the chain (first for request processing, last for response processing). Higher values are interpreted as lower priority. If multiple advisors have the same order value, their execution order is not guaranteed. The seeming contradiction between order and execution sequence is due to the stack-like nature of the advisor chain: * An advisor with the highest precedence (lowest order value) is added to the bottom of the stack. * It will be the last to process the request as the stack unwinds. * It will be the first to process the response as the stack rewinds. As a reminder, here are the semantics of the Spring Ordered interface: public interface Ordered { /** * Constant for the highest precedence value. * @see java.lang.Integer#MIN_VALUE */ int HIGHEST_PRECEDENCE = Integer.MIN_VALUE; /** * Constant for the lowest precedence value. * @see java.lang.Integer#MAX_VALUE */ int LOWEST_PRECEDENCE = Integer.MAX_VALUE; /** * Get the order value of this object. * <p>Higher values are interpreted as lower priority. As a consequence, * the object with the lowest value has the highest priority (somewhat * analogous to Servlet {@code load-on-startup} values). * <p>Same order values will result in arbitrary sort positions for the * affected objects. * @return the order value * @see #HIGHEST_PRECEDENCE * @see #LOWEST_PRECEDENCE */ int getOrder(); } For use cases that need to be first in the chain on both the input and output sides: Use separate advisors for each side. Configure them with different order values. Use the advisor context to share state between them. API Overview: The main Advisor interfaces are located in the package org.springframework.ai.chat.client.advisor.api . Here are the key interfaces you’ll encounter when creating your own advisor: public interface Advisor extends Ordered { String getName(); } The two sub-interfaces for synchronous and reactive Advisors are public interface CallAroundAdvisor extends Advisor { /** * Around advice that wraps the ChatModel#call(Prompt) method. * @param advisedRequest the advised request * @param chain the advisor chain * @return the response */ AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain); } and public interface StreamAroundAdvisor extends Advisor { /** * Around advice that wraps the invocation of the advised request. * @param advisedRequest the advised request * @param chain the chain of advisors to execute * @return the result of the advised request */ Flux<AdvisedResponse> aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain); } To continue the chain of Advice, use CallAroundAdvisorChain and StreamAroundAdvisorChain in your Advice implementation: The interfaces are public interface CallAroundAdvisorChain { AdvisedResponse nextAroundCall(AdvisedRequest advisedRequest); } and public interface StreamAroundAdvisorChain { Flux<AdvisedResponse> nextAroundStream(AdvisedRequest advisedRequest); } Implementing an Advisor: To create an advisor, implement either CallAroundAdvisor or StreamAroundAdvisor (or both). The key method to implement is nextAroundCall() for non-streaming or nextAroundStream() for streaming advisors. Examples: We will provide few hands-on examples to illustrate how to implement advisors for observing and augmenting use-cases. Logging Advisor: We can implement a simple logging advisor that logs the AdvisedRequest before and the AdvisedResponse after the call to the next advisor in the chain. Note that the advisor only observes the request and response and does not modify them. This implementation support both non-streaming and streaming scenarios. public class SimpleLoggerAdvisor implements CallAroundAdvisor, StreamAroundAdvisor { private static final Logger logger = LoggerFactory.getLogger(SimpleLoggerAdvisor.class); @Override public String getName() { (1) return this.getClass().getSimpleName(); } @Override public int getOrder() { (2) return 0; } @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { logger.debug(""BEFORE: {}"", advisedRequest); AdvisedResponse advisedResponse = chain.nextAroundCall(advisedRequest); logger.debug(""AFTER: {}"", advisedResponse); return advisedResponse; } @Override public Flux<AdvisedResponse> aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { logger.debug(""BEFORE: {}"", advisedRequest); Flux<AdvisedResponse> advisedResponses = chain.nextAroundStream(advisedRequest); return new MessageAggregator().aggregateAdvisedResponse(advisedResponses, advisedResponse -> logger.debug(""AFTER: {}"", advisedResponse)); (3) } } 1 Provides a unique name for the advisor. 2 You can control the order of execution by setting the order value. Lower values execute first. 3 The MessageAggregator is a utility class that aggregates the Flux responses into a single AdvisedResponse. This can be useful for logging or other processing that observe the entire response rather than individual items in the stream. Note that you can not alter the response in the MessageAggregator as it is a read-only operation. Re-Reading (Re2) Advisor: The "" Re-Reading Improves Reasoning in Large Language Models(https://arxiv.org/pdf/2309.06275) "" article introduces a technique called Re-Reading (Re2) that improves the reasoning capabilities of Large Language Models. The Re2 technique requires augmenting the input prompt like this: {Input_Query} Read the question again: {Input_Query} Implementing an advisor that applies the Re2 technique to the user’s input query can be done like this: public class ReReadingAdvisor implements CallAroundAdvisor, StreamAroundAdvisor { private AdvisedRequest before(AdvisedRequest advisedRequest) { (1) Map<String, Object> advisedUserParams = new HashMap<>(advisedRequest.userParams()); advisedUserParams.put(""re2_input_query"", advisedRequest.userText()); return AdvisedRequest.from(advisedRequest) .withUserText("""""" {re2_input_query} Read the question again: {re2_input_query} """""") .withUserParams(advisedUserParams) .build(); } @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { (2) return chain.nextAroundCall(this.before(advisedRequest)); } @Override public Flux<AdvisedResponse> aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { (3) return chain.nextAroundStream(this.before(advisedRequest)); } @Override public int getOrder() { (4) return 0; } @Override public String getName() { (5) return this.getClass().getSimpleName(); } } 1 The before method augments the user’s input query applying the Re-Reading technique. 2 The aroundCall method intercepts the non-streaming request and applies the Re-Reading technique. 3 The aroundStream method intercepts the streaming request and applies the Re-Reading technique. 4 You can control the order of execution by setting the order value. Lower values execute first. 5 Provides a unique name for the advisor. Spring AI built-in Advisors: You can also explore the built-in advisors provided by the Spring AI framework. For example the MessageChatMemoryAdvisor , PromptChatMemoryAdvisor and VectorStoreChatMemoryAdvisor advisors provide different strategies the conversation chat history in a chat memory store and the QuestionAnswerAdvisor uses a vector store to provide question-answering capabilities (e.g. implements the RAG pattern). The SafeGuardAdvisor is another, simple, built-in advisor that can be used to prevent the model from generating harmful or inappropriate content. Streaming vs Non-Streaming: Non-streaming advisors work with complete requests and responses. Streaming advisors handle requests and responses as continuous streams, using reactive programming concepts (e.g., Flux for responses). @Override public Flux<AdvisedResponse> aroundStream(AdvisedRequest advisedRequest, StreamAroundAdvisorChain chain) { return Mono.just(advisedRequest) .publishOn(Schedulers.boundedElastic()) .map(request -> { // This can be executed by blocking and non-blocking Threads. // Advisor before next section }) .flatMapMany(request -> chain.nextAroundStream(request)) .map(response -> { // Advisor after next section }); } Best Practices: Keep advisors focused on specific tasks for better modularity. Use the adviseContext to share state between advisors when necessary. Implement both streaming and non-streaming versions of your advisor for maximum flexibility. Carefully consider the order of advisors in your chain to ensure proper data flow. Backward Compatibility: The AdvisedRequest class is moved to a new package. While the RequestResponseAdvisor interface is still available it is marked as deprecated and will be removed around the M3 release. It is recommended to use the new CallAroundAdvisor and StreamAroundAdvisor interfaces for new implementations. Breaking API Changes: The Spring AI Advisor Chain underwent significant changes from version 1.0 M2 to 1.0 M3. Here are the key modifications: Advisor Interfaces: In 1.0 M2, there were separate RequestAdvisor and ResponseAdvisor interfaces. RequestAdvisor was invoked before the ChatModel.call and ChatModel.stream methods. ResponseAdvisor was called after these methods. In 1.0 M3, these interfaces have been replaced with: CallAroundAdvisor StreamAroundAdvisor The StreamResponseMode , previously part of ResponseAdvisor , has been removed. Context Map Handling: In 1.0 M2: The context map was a separate method argument. The map was mutable and passed along the chain. In 1.0 M3: The context map is now part of the AdvisedRequest and AdvisedResponse records. The map is immutable. To update the context, use the updateContext method, which creates a new unmodifiable map with the updated contents. Example of updating the context in 1.0 M3: @Override public AdvisedResponse aroundCall(AdvisedRequest advisedRequest, CallAroundAdvisorChain chain) { this.advisedRequest = advisedRequest.updateContext(context -> { context.put(""aroundCallBefore"" + getName(), ""AROUND_CALL_BEFORE "" + getName()); // Add multiple key-value pairs context.put(""lastBefore"", getName()); // Add a single key-value pair return context; }); // Method implementation continues... } Chat Client API(chatclient.html) Chat Model API(chatmodel.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chatmodel.html","Chat Model API: The Chat Model API offers developers the ability to integrate AI-powered chat completion capabilities into their applications. It leverages pre-trained language models, such as GPT (Generative Pre-trained Transformer), to generate human-like responses to user inputs in natural language. The API typically works by sending a prompt or partial conversation to the AI model, which then generates a completion or continuation of the conversation based on its training data and understanding of natural language patterns. The completed response is then returned to the application, which can present it to the user or use it for further processing. The Spring AI Chat Model API is designed to be a simple and portable interface for interacting with various AI Models(../concepts.html#_models) , allowing developers to switch between different models with minimal code changes. This design aligns with Spring’s philosophy of modularity and interchangeability. Also with the help of companion classes like Prompt for input encapsulation and ChatResponse for output handling, the Chat Model API unifies the communication with AI Models. It manages the complexity of request preparation and response parsing, offering a direct and simplified API interaction. API Overview: This section provides a guide to the Spring AI Chat Model API interface and associated classes. ChatModel: Here is the ChatModel(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat//model/ChatModel.java) interface definition: public interface ChatModel extends Model<Prompt, ChatResponse> { default String call(String message) {...} @Override ChatResponse call(Prompt prompt); } The call() method with a String parameter simplifies initial use, avoiding the complexities of the more sophisticated Prompt and ChatResponse classes. In real-world applications, it is more common to use the call() method that takes a Prompt instance and returns a ChatResponse . StreamingChatModel: Here is the StreamingChatModel(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/model/StreamingChatModel.java) interface definition: public interface StreamingChatModel extends StreamingModel<Prompt, ChatResponse> { default Flux<String> stream(String message) {...} @Override Flux<ChatResponse> stream(Prompt prompt); } The stream() method takes a String or Prompt parameter similar to ChatModel but it streams the responses using the reactive Flux API. Prompt: The Prompt(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/Prompt.java) is a ModelRequest that encapsulates a list of Message(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/messages/Message.java) objects and optional model request options. The following listing shows a truncated version of the Prompt class, excluding constructors and other utility methods: public class Prompt implements ModelRequest<List<Message>> { private final List<Message> messages; private ChatOptions modelOptions; @Override public ChatOptions getOptions() {...} @Override public List<Message> getInstructions() {...} // constructors and utility methods omitted } Message: The Message interface encapsulates a Prompt textual content, a collection of metadata attributes, and a categorization known as MessageType . The interface is defined as follows: public interface Content { String getContent(); Map<String, Object> getMetadata(); } public interface Message extends Content { MessageType getMessageType(); } The multimodal message types implement also the MediaContent interface providing a list of Media content objects. public interface MediaContent extends Content { Collection<Media> getMedia(); } The Message interface has various implementations that correspond to the categories of messages that an AI model can process: The chat completion endpoint, distinguish between message categories based on conversational roles, effectively mapped by the MessageType . For instance, OpenAI recognizes message categories for distinct conversational roles such as system , user , function , or assistant . While the term MessageType might imply a specific message format, in this context it effectively designates the role a message plays in the dialogue. For AI models that do not use specific roles, the UserMessage implementation acts as a standard category, typically representing user-generated inquiries or instructions. To understand the practical application and the relationship between Prompt and Message , especially in the context of these roles or message categories, see the detailed explanations in the Prompts(prompt.html) section. Chat Options: Represents the options that can be passed to the AI model. The ChatOptions class is a subclass of ModelOptions and is used to define few portable options that can be passed to the AI model. The ChatOptions class is defined as follows: public interface ChatOptions extends ModelOptions { String getModel(); Float getFrequencyPenalty(); Integer getMaxTokens(); Float getPresencePenalty(); List<String> getStopSequences(); Float getTemperature(); Integer getTopK(); Float getTopP(); ChatOptions copy(); } Additionally, every model specific ChatModel/StreamingChatModel implementation can have its own options that can be passed to the AI model. For example, the OpenAI Chat Completion model has its own options like logitBias , seed , and user . This is a powerful feature that allows developers to use model-specific options when starting the application and then override them at runtime using the Prompt request: ChatResponse: The structure of the ChatResponse class is as follows: public class ChatResponse implements ModelResponse<Generation> { private final ChatResponseMetadata chatResponseMetadata; private final List<Generation> generations; @Override public ChatResponseMetadata getMetadata() {...} @Override public List<Generation> getResults() {...} // other methods omitted } The ChatResponse(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/model/ChatResponse.java) class holds the AI Model’s output, with each Generation instance containing one of potentially multiple outputs resulting from a single prompt. The ChatResponse class also carries a ChatResponseMetadata metadata about the AI Model’s response. Generation: Finally, the Generation(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/model/Generation.java) class extends from the ModelResult to represent the model output (assistant message) and related metadata: public class Generation implements ModelResult<AssistantMessage> { private final AssistantMessage assistantMessage; private ChatGenerationMetadata chatGenerationMetadata; @Override public AssistantMessage getOutput() {...} @Override public ChatGenerationMetadata getMetadata() {...} // other methods omitted } Available Implementations: The ChatModel and StreamingChatModel implementations are provided for the following Model providers: OpenAI Chat Completion(chat/openai-chat.html) (streaming, multi-modality & function-calling support) Microsoft Azure Open AI Chat Completion(chat/azure-openai-chat.html) (streaming & function-calling support) Ollama Chat Completion(chat/ollama-chat.html) (streaming, multi-modality & function-calling support) Hugging Face Chat Completion(chat/huggingface.html) (no streaming support) Google Vertex AI PaLM2 Chat Completion(chat/vertexai-palm2-chat.html) (no streaming support) Google Vertex AI Gemini Chat Completion(chat/vertexai-gemini-chat.html) (streaming, multi-modality & function-calling support) Amazon Bedrock(bedrock.html) Cohere Chat Completion(chat/bedrock/bedrock-cohere.html) Llama Chat Completion(chat/bedrock/bedrock-llama.html) Titan Chat Completion(chat/bedrock/bedrock-titan.html) Anthropic Chat Completion(chat/bedrock/bedrock-anthropic.html) Jurassic2 Chat Completion(chat/bedrock/bedrock-jurassic2.html) Mistral AI Chat Completion(chat/mistralai-chat.html) (streaming & function-calling support) Anthropic Chat Completion(chat/anthropic-chat.html) (streaming & function-calling support) Chat Model API: The Spring AI Chat Model API is built on top of the Spring AI Generic Model API providing Chat specific abstractions and implementations. The following class diagram illustrates the main classes and interfaces of the Spring AI Chat Model API. Advisors(advisors.html) Amazon Bedrock(bedrock-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/bedrock-chat.html","Amazon Bedrock: Amazon Bedrock(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) is a managed service that provides foundation models from various AI providers, available through a unified API. Spring AI supports all the Chat and Embedding AI models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) available through Amazon Bedrock by implementing the Spring interfaces ChatModel , StreamingChatModel , and EmbeddingModel . Additionally, Spring AI provides Spring Auto-Configurations and Boot Starters for all clients, making it easy to bootstrap and configure for the Bedrock models. Getting Started: There are a few steps to get started Add the Spring Boot starter for Bedrock to your project. Obtain AWS credentials: If you don’t have an AWS account and AWS CLI configured yet, this video guide can help you configure it: AWS CLI & SDK Setup in Less Than 4 Minutes!(https://youtu.be/gswVHTrRX8I?si=buaY7aeI0l3-bBVb) . You should be able to obtain your access and security keys. Enable the Models to use: Go to Amazon Bedrock(https://us-east-1.console.aws.amazon.com/bedrock/home) and from the Model Access(https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess) menu on the left, configure access to the models you are going to use. Project Dependencies: Then add the Spring Boot Starter dependency to your project’s Maven pom.xml build file: <dependency> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> <groupId>org.springframework.ai</groupId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Connect to AWS Bedrock: Use the BedrockAwsConnectionProperties to configure AWS credentials and region: spring.ai.bedrock.aws.region=us-east-1 spring.ai.bedrock.aws.access-key=YOUR_ACCESS_KEY spring.ai.bedrock.aws.secret-key=YOUR_SECRET_KEY spring.ai.bedrock.aws.timeout=10m The region property is compulsory. AWS credentials are resolved in the following order: Spring-AI Bedrock spring.ai.bedrock.aws.access-key and spring.ai.bedrock.aws.secret-key properties. Java System Properties - aws.accessKeyId and aws.secretAccessKey . Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY . Web Identity Token credentials from system properties or environment variables. Credential profiles file at the default location ( ~/.aws/credentials ) shared by all AWS SDKs and the AWS CLI. Credentials delivered through the Amazon EC2 container service if the AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variable is set and the security manager has permission to access the variable. Instance profile credentials delivered through the Amazon EC2 metadata service or set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. AWS region is resolved in the following order: Spring-AI Bedrock spring.ai.bedrock.aws.region property. Java System Properties - aws.region . Environment Variables - AWS_REGION . Credential profiles file at the default location ( ~/.aws/credentials ) shared by all AWS SDKs and the AWS CLI. Instance profile region delivered through the Amazon EC2 metadata service. In addition to the standard Spring-AI Bedrock credentials and region properties configuration, Spring-AI provides support for custom AwsCredentialsProvider and AwsRegionProvider beans. For example, using Spring-AI and Spring Cloud for Amazon Web Services(https://spring.io/projects/spring-cloud-aws) at the same time. Spring-AI is compatible with Spring Cloud for Amazon Web Services credential configuration. Enable selected Bedrock model: By default, all models are disabled. You have to enable the chosen Bedrock models explicitly using the spring.ai.bedrock.<model>.<chat|embedding>.enabled=true property. Here are the supported <model> and <chat|embedding> combinations: Model Chat Chat Streaming Embedding llama Yes Yes No jurassic2 Yes No No cohere Yes Yes Yes anthropic 2 Yes Yes No anthropic 3 Yes Yes No jurassic2 (WIP) Yes No No titan Yes Yes Yes (however, no batch support) For example, to enable the Bedrock Llama chat model, you need to set spring.ai.bedrock.llama.chat.enabled=true . Next, you can use the spring.ai.bedrock.<model>.<chat|embedding>.* properties to configure each model as provided. For more information, refer to the documentation below for each supported model. Spring AI Bedrock Anthropic 2 Chat(chat/bedrock/bedrock-anthropic.html) : spring.ai.bedrock.anthropic.chat.enabled=true Spring AI Bedrock Anthropic 3 Chat(chat/bedrock/bedrock-anthropic3.html) : spring.ai.bedrock.anthropic.chat.enabled=true Spring AI Bedrock Llama Chat(chat/bedrock/bedrock-llama.html) : spring.ai.bedrock.llama.chat.enabled=true Spring AI Bedrock Cohere Chat(chat/bedrock/bedrock-cohere.html) : spring.ai.bedrock.cohere.chat.enabled=true Spring AI Bedrock Cohere Embeddings(embeddings/bedrock-cohere-embedding.html) : spring.ai.bedrock.cohere.embedding.enabled=true Spring AI Bedrock Titan Chat(chat/bedrock/bedrock-titan.html) : spring.ai.bedrock.titan.chat.enabled=true Spring AI Bedrock Titan Embeddings(embeddings/bedrock-titan-embedding.html) : spring.ai.bedrock.titan.embedding.enabled=true Spring AI Bedrock Ai21 Jurassic2 Chat(chat/bedrock/bedrock-jurassic2.html) : spring.ai.bedrock.jurassic2.chat.enabled=true Chat Model API(chatmodel.html) Anthropic3(chat/bedrock/bedrock-anthropic3.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/bedrock/bedrock-anthropic3.html","Bedrock Anthropic 3: Anthropic Claude(https://www.anthropic.com/) is a family of foundational AI models that can be used in a variety of applications. The Claude model has the following high level features 200k Token Context Window: Claude boasts a generous token capacity of 200,000, making it ideal for handling extensive information in applications like technical documentation, codebase, and literary works. Supported Tasks: Claude’s versatility spans tasks such as summarization, Q&A, trend forecasting, and document comparisons, enabling a wide range of applications from dialogues to content generation. AI Safety Features: Built on Anthropic’s safety research, Claude prioritizes helpfulness, honesty, and harmlessness in its interactions, reducing brand risk and ensuring responsible AI behavior. The AWS Bedrock Anthropic Model Page(https://aws.amazon.com/bedrock/claude) and Amazon Bedrock User Guide(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) contains detailed information on how to use the AWS hosted model. Anthropic’s Claude 2 and 3 models are also available directly on the Anthropic’s own cloud platform. Spring AI provides dedicated Anthropic Claude(../anthropic-chat.html) client to access it. Prerequisites: Refer to the Spring AI documentation on Amazon Bedrock(../../bedrock.html) for setting up API access. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Add the spring-ai-bedrock-ai-spring-boot-starter dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Enable Anthropic Chat: By default the Anthropic model is disabled. To enable it set the spring.ai.bedrock.anthropic3.chat.enabled property to true . Exporting environment variable is one way to set this configuration property: export SPRING_AI_BEDROCK_ANTHROPIC3_CHAT_ENABLED=true Chat Properties: The prefix spring.ai.bedrock.aws is the property prefix to configure the connection to AWS Bedrock. Property Description Default spring.ai.bedrock.aws.region AWS region to use. us-east-1 spring.ai.bedrock.aws.timeout AWS timeout to use. 5m spring.ai.bedrock.aws.access-key AWS access key. - spring.ai.bedrock.aws.secret-key AWS secret key. - The prefix spring.ai.bedrock.anthropic3.chat is the property prefix that configures the chat model implementation for Claude. Property Description Default spring.ai.bedrock.anthropic3.chat.enabled Enable Bedrock Anthropic chat model. Disabled by default false spring.ai.bedrock.anthropic3.chat.model The model id to use. Supports the anthropic.claude-3-sonnet-20240229-v1:0 , anthropic.claude-3-haiku-20240307-v1:0 and the legacy anthropic.claude-v2 , anthropic.claude-v2:1 and anthropic.claude-instant-v1 models for both synchronous and streaming responses. anthropic.claude-3-sonnet-20240229-v1:0 spring.ai.bedrock.anthropic3.chat.options.temperature Controls the randomness of the output. Values can range over [0.0,1.0] 0.8 spring.ai.bedrock.anthropic3.chat.options.top-p The maximum cumulative probability of tokens to consider when sampling. AWS Bedrock default spring.ai.bedrock.anthropic3.chat.options.top-k Specify the number of token choices the generative uses to generate the next token. AWS Bedrock default spring.ai.bedrock.anthropic3.chat.options.stop-sequences Configure up to four sequences that the generative recognizes. After a stop sequence, the generative stops generating further tokens. The returned text doesn’t contain the stop sequence. 10 spring.ai.bedrock.anthropic3.chat.options.anthropic-version The version of the generative to use. bedrock-2023-05-31 spring.ai.bedrock.anthropic3.chat.options.max-tokens Specify the maximum number of tokens to use in the generated response. Note that the models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate. We recommend a limit of 4,000 tokens for optimal performance. 500 Look at the AnthropicChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic3/api/Anthropic3ChatBedrockApi.java) for other model IDs. Supported values are: anthropic.claude-instant-v1 , anthropic.claude-v2 and anthropic.claude-v2:1 . Model ID values can also be found in the AWS Bedrock documentation for base model IDs(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) . All properties prefixed with spring.ai.bedrock.anthropic3.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The Anthropic3ChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic3/Anthropic3ChatOptions.java) provides model configurations, such as temperature, topK, topP, etc. On start-up, the default options can be configured with the BedrockAnthropicChatModel(api, options) constructor or the spring.ai.bedrock.anthropic3.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", Anthropic3ChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific AnthropicChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic3/Anthropic3ChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Multimodal: Multimodality refers to a model’s ability to simultaneously understand and process information from various sources, including text, images, audio, and other data formats. This paradigm represents a significant advancement in AI models. Currently, Anthropic Claude 3 supports the base64 source type for images , and the image/jpeg , image/png , image/gif , and image/webp media types. Check the Vision guide(https://docs.anthropic.com/claude/docs/vision) for more information. Spring AI’s Message interface supports multimodal AI models by introducing the Media type. This type contains data and information about media attachments in messages, using Spring’s org.springframework.util.MimeType and a java.lang.Object for the raw media data. Below is a simple code example extracted from Anthropic3ChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/test/java/org/springframework/ai/anthropic3/Anthropic3ChatModelIT.java) , demonstrating the combination of user text with an image. byte[] imageData = new ClassPathResource(""/test.png"").getContentAsByteArray(); var userMessage = new UserMessage(""Explain what do you see o this picture?"", List.of(new Media(MimeTypeUtils.IMAGE_PNG, imageData))); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage))); assertThat(response.getResult().getOutput().getContent()).contains(""bananas"", ""apple"", ""basket""); It takes as an input the test.png image: along with the text message ""Explain what do you see on this picture?"", and generates a response something like: The image shows a close-up view of a wire fruit basket containing several pieces of fruit. The basket appears to be made of thin metal wires formed into a round shape with an elevated handle. Inside the basket, there are a few yellow bananas and a couple of red apples or possibly tomatoes. The vibrant colors of the fruit contrast nicely against the metallic tones of the wire basket. The shallow depth of field in the photograph puts the focus squarely on the fruit in the foreground, while the basket handle extending upwards is slightly blurred, creating a pleasing bokeh effect in the background. The composition and lighting give the image a clean, minimalist aesthetic that highlights the natural beauty and freshness of the fruit displayed in this elegant wire basket. Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-bedrock-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Anthropic chat model: spring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=1000ms spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.bedrock.anthropic3.chat.enabled=true spring.ai.bedrock.anthropic3.chat.options.temperature=0.8 spring.ai.bedrock.anthropic3.chat.options.top-k=15 replace the regions , access-key and secret-key with your AWS credentials. This will create a BedrockAnthropicChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final BedrockAnthropic3ChatModel chatModel; @Autowired public ChatController(BedrockAnthropic3ChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The BedrockAnthropic3ChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic3/BedrockAnthropic3ChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level Anthropic3ChatBedrockApi Client(#low-level-api) to connect to the Bedrock Anthropic service. Add the spring-ai-bedrock dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an BedrockAnthropic3ChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic3/BedrockAnthropic3ChatModel.java) and use it for text generations: Anthropic3ChatBedrockApi anthropicApi = new Anthropic3ChatBedrockApi( AnthropicChatBedrockApi.AnthropicModel.CLAUDE_V3_SONNET.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper(), Duration.ofMillis(1000L)); BedrockAnthropic3ChatModel chatModel = new BedrockAnthropic3ChatModel(anthropicApi, AnthropicChatOptions.builder() .withTemperature(0.6) .withTopK(10) .withTopP(0.8) .withMaxTokensToSample(100) .withAnthropicVersion(AnthropicChatBedrockApi.DEFAULT_ANTHROPIC_VERSION) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); Low-level Anthropic3ChatBedrockApi Client: The Anthropic3ChatBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic3/api/Anthropic3ChatBedrockApi.java) provides is lightweight Java client on top of AWS Bedrock Anthropic Claude models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html) . Client supports the anthropic.claude-3-opus-20240229-v1:0 , anthropic.claude-3-sonnet-20240229-v1:0 , anthropic.claude-3-haiku-20240307-v1:0 and the legacy anthropic.claude-v2 , anthropic.claude-v2:1 and anthropic.claude-instant-v1 models for both synchronous (e.g. chatCompletion() ) and streaming (e.g. chatCompletionStream() ) responses. Here is a simple snippet how to use the api programmatically: Anthropic3ChatBedrockApi anthropicChatApi = new Anthropic3ChatBedrockApi( AnthropicModel.CLAUDE_V2.id(), Region.EU_CENTRAL_1.id(), Duration.ofMillis(1000L)); AnthropicChatRequest request = AnthropicChatRequest .builder(String.format(Anthropic3ChatBedrockApi.PROMPT_TEMPLATE, ""Name 3 famous pirates"")) .withTemperature(0.8) .withMaxTokensToSample(300) .withTopK(10) .build(); // Sync request AnthropicChatResponse response = anthropicChatApi.chatCompletion(request); // Streaming request Flux<AnthropicChatResponse> responseStream = anthropicChatApi.chatCompletionStream(request); List<AnthropicChatResponse> responses = responseStream.collectList().block(); Follow the Anthropic3ChatBedrockApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic3/api/Anthropic3ChatBedrockApi.java) 's JavaDoc for further information. Amazon Bedrock(../../bedrock-chat.html) Anthropic2(bedrock-anthropic.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/bedrock/bedrock-anthropic.html","Bedrock Anthropic 2 Chat: The Anthropic 2 Chat API is deprecated and replaced by the new Anthropic Claude 3 Message API. Please use the Anthropic Claude 3 Message API(bedrock-anthropic3.html) for new projects. Anthropic’s Claude(https://www.anthropic.com/product) is an AI assistant based on Anthropic’s research into training helpful, honest, and harmless AI systems. The Claude model has the following high level features 200k Token Context Window: Claude boasts a generous token capacity of 200,000, making it ideal for handling extensive information in applications like technical documentation, codebase, and literary works. Supported Tasks: Claude’s versatility spans tasks such as summarization, Q&A, trend forecasting, and document comparisons, enabling a wide range of applications from dialogues to content generation. AI Safety Features: Built on Anthropic’s safety research, Claude prioritizes helpfulness, honesty, and harmlessness in its interactions, reducing brand risk and ensuring responsible AI behavior. The AWS Bedrock Anthropic Model Page(https://aws.amazon.com/bedrock/claude) and Amazon Bedrock User Guide(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) contains detailed information on how to use the AWS hosted model. Anthropic’s Claude 2 and 3 models are also available directly on the Anthropic’s own cloud platform. Spring AI provides dedicated Anthropic Claude(../anthropic-chat.html) client to access it. Prerequisites: Refer to the Spring AI documentation on Amazon Bedrock(../../bedrock.html) for setting up API access. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Add the spring-ai-bedrock-ai-spring-boot-starter dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Enable Anthropic Chat: By default the Anthropic model is disabled. To enable it set the spring.ai.bedrock.anthropic.chat.enabled property to true . Exporting environment variable is one way to set this configuration property: export SPRING_AI_BEDROCK_ANTHROPIC_CHAT_ENABLED=true Chat Properties: The prefix spring.ai.bedrock.aws is the property prefix to configure the connection to AWS Bedrock. Property Description Default spring.ai.bedrock.aws.region AWS region to use. us-east-1 spring.ai.bedrock.aws.timeout AWS timeout to use. 5m spring.ai.bedrock.aws.access-key AWS access key. - spring.ai.bedrock.aws.secret-key AWS secret key. - The prefix spring.ai.bedrock.anthropic.chat is the property prefix that configures the chat model implementation for Claude. Property Description Default spring.ai.bedrock.anthropic.chat.enabled Enable Bedrock Anthropic chat model. Disabled by default false spring.ai.bedrock.anthropic.chat.model The model id to use. See the AnthropicChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic/api/AnthropicChatBedrockApi.java) for the supported models. anthropic.claude-v2 spring.ai.bedrock.anthropic.chat.options.temperature Controls the randomness of the output. Values can range over [0.0,1.0] 0.8 spring.ai.bedrock.anthropic.chat.options.topP The maximum cumulative probability of tokens to consider when sampling. AWS Bedrock default spring.ai.bedrock.anthropic.chat.options.topK Specify the number of token choices the generative uses to generate the next token. AWS Bedrock default spring.ai.bedrock.anthropic.chat.options.stopSequences Configure up to four sequences that the generative recognizes. After a stop sequence, the generative stops generating further tokens. The returned text doesn’t contain the stop sequence. 10 spring.ai.bedrock.anthropic.chat.options.anthropicVersion The version of the generative to use. bedrock-2023-05-31 spring.ai.bedrock.anthropic.chat.options.maxTokensToSample Specify the maximum number of tokens to use in the generated response. Note that the models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate. We recommend a limit of 4,000 tokens for optimal performance. 500 Look at the AnthropicChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic/api/AnthropicChatBedrockApi.java) for other model IDs. Supported values are: anthropic.claude-instant-v1 , anthropic.claude-v2 and anthropic.claude-v2:1 . Model ID values can also be found in the AWS Bedrock documentation for base model IDs(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) . All properties prefixed with spring.ai.bedrock.anthropic.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The AnthropicChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic/AnthropicChatOptions.java) provides model configurations, such as temperature, topK, topP, etc. On start-up, the default options can be configured with the BedrockAnthropicChatModel(api, options) constructor or the spring.ai.bedrock.anthropic.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", AnthropicChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific AnthropicChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic/AnthropicChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-bedrock-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Anthropic chat model: spring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=1000ms spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.bedrock.anthropic.chat.enabled=true spring.ai.bedrock.anthropic.chat.options.temperature=0.8 spring.ai.bedrock.anthropic.chat.options.top-k=15 replace the regions , access-key and secret-key with your AWS credentials. This will create a BedrockAnthropicChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final BedrockAnthropicChatModel chatModel; @Autowired public ChatController(BedrockAnthropicChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The BedrockAnthropicChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic/BedrockAnthropicChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level AnthropicChatBedrockApi Client(#low-level-api) to connect to the Bedrock Anthropic service. Add the spring-ai-bedrock dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an BedrockAnthropicChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic/BedrockAnthropicChatModel.java) and use it for text generations: AnthropicChatBedrockApi anthropicApi = new AnthropicChatBedrockApi( AnthropicChatBedrockApi.AnthropicModel.CLAUDE_V2.id(), EnvironmentVariableCredentialsProvider.create(), Region.EU_CENTRAL_1.id(), new ObjectMapper(), Duration.ofMillis(1000L)); BedrockAnthropicChatModel chatModel = new BedrockAnthropicChatModel(anthropicApi, AnthropicChatOptions.builder() .withTemperature(0.6) .withTopK(10) .withTopP(0.8) .withMaxTokensToSample(100) .withAnthropicVersion(AnthropicChatBedrockApi.DEFAULT_ANTHROPIC_VERSION) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); Low-level AnthropicChatBedrockApi Client: The AnthropicChatBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic/api/AnthropicChatBedrockApi.java) provides is lightweight Java client on top of AWS Bedrock Anthropic Claude models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html) . Following class diagram illustrates the AnthropicChatBedrockApi interface and building blocks: Client supports the anthropic.claude-instant-v1 , anthropic.claude-v2 and anthropic.claude-v2:1 models for both synchronous (e.g. chatCompletion() ) and streaming (e.g. chatCompletionStream() ) responses. Here is a simple snippet how to use the api programmatically: AnthropicChatBedrockApi anthropicChatApi = new AnthropicChatBedrockApi( AnthropicModel.CLAUDE_V2.id(), Region.EU_CENTRAL_1.id(), Duration.ofMillis(1000L)); AnthropicChatRequest request = AnthropicChatRequest .builder(String.format(AnthropicChatBedrockApi.PROMPT_TEMPLATE, ""Name 3 famous pirates"")) .withTemperature(0.8) .withMaxTokensToSample(300) .withTopK(10) .build(); // Sync request AnthropicChatResponse response = anthropicChatApi.chatCompletion(request); // Streaming request Flux<AnthropicChatResponse> responseStream = anthropicChatApi.chatCompletionStream(request); List<AnthropicChatResponse> responses = responseStream.collectList().block(); Follow the AnthropicChatBedrockApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/anthropic/api/AnthropicChatBedrockApi.java) 's JavaDoc for further information. Anthropic3(bedrock-anthropic3.html) Llama(bedrock-llama.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/bedrock/bedrock-llama.html","Llama Chat: Meta’s Llama Chat(https://ai.meta.com/llama/) is part of the Llama collection of large language models. It excels in dialogue-based applications with a parameter scale ranging from 7 billion to 70 billion. Leveraging public datasets and over 1 million human annotations, Llama Chat offers context-aware dialogues. Trained on 2 trillion tokens from public data sources, Llama-Chat provides extensive knowledge for insightful conversations. Rigorous testing, including over 1,000 hours of red-teaming and annotation, ensures both performance and safety, making it a reliable choice for AI-driven dialogues. The AWS Llama Model Page(https://aws.amazon.com/bedrock/llama/) and Amazon Bedrock User Guide(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) contains detailed information on how to use the AWS hosted model. Prerequisites: Refer to the Spring AI documentation on Amazon Bedrock(../../bedrock.html) for setting up API access. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Add the spring-ai-bedrock-ai-spring-boot-starter dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Enable Llama Chat Support: By default the Bedrock Llama model is disabled. To enable it set the spring.ai.bedrock.llama.chat.enabled property to true . Exporting environment variable is one way to set this configuration property: export SPRING_AI_BEDROCK_LLAMA_CHAT_ENABLED=true Chat Properties: The prefix spring.ai.bedrock.aws is the property prefix to configure the connection to AWS Bedrock. Property Description Default spring.ai.bedrock.aws.region AWS region to use. us-east-1 spring.ai.bedrock.aws.timeout AWS timeout to use. 5m spring.ai.bedrock.aws.access-key AWS access key. - spring.ai.bedrock.aws.secret-key AWS secret key. - The prefix spring.ai.bedrock.llama.chat is the property prefix that configures the chat model implementation for Llama. Property Description Default spring.ai.bedrock.llama.chat.enabled Enable or disable support for Llama false spring.ai.bedrock.llama.chat.model The model id to use (See Below) meta.llama3-70b-instruct-v1:0 spring.ai.bedrock.llama.chat.options.temperature Controls the randomness of the output. Values can range over [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied, while a value closer to 0.0 will typically result in less surprising responses from the model. This value specifies default to be used by the backend while making the call to the model. 0.7 spring.ai.bedrock.llama.chat.options.top-p The maximum cumulative probability of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Nucleus sampling considers the smallest set of tokens whose probability sum is at least topP. AWS Bedrock default spring.ai.bedrock.llama.chat.options.max-gen-len Specify the maximum number of tokens to use in the generated response. The model truncates the response once the generated text exceeds maxGenLen. 300 Look at LlamaChatBedrockApi#LlamaChatModel(https://github.com/spring-projects/spring-ai/blob/4ba9a3cd689b9fd3a3805f540debe398a079c6ef/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/llama/api/LlamaChatBedrockApi.java#L164) for other model IDs. The other value supported is meta.llama2-13b-chat-v1 . Model ID values can also be found in the AWS Bedrock documentation for base model IDs(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) . All properties prefixed with spring.ai.bedrock.llama.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The BedrockLlChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/llama/BedrockLlamaChatOptions.java) provides model configurations, such as temperature, topK, topP, etc. On start-up, the default options can be configured with the BedrockLlamaChatModel(api, options) constructor or the spring.ai.bedrock.llama.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", BedrockLlamaChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific BedrockLlamaChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/llama/BedrockLlamaChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-bedrock-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Anthropic chat model: spring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=1000ms spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.bedrock.llama.chat.enabled=true spring.ai.bedrock.llama.chat.options.temperature=0.8 replace the regions , access-key and secret-key with your AWS credentials. This will create a BedrockLlamaChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final BedrockLlamaChatModel chatModel; @Autowired public ChatController(BedrockLlamaChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The BedrockLlamaChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/llama/BedrockLlamaChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level LlamaChatBedrockApi Client(#low-level-api) to connect to the Bedrock Anthropic service. Add the spring-ai-bedrock dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an BedrockLlamaChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/llama/BedrockLlamaChatModel.java) and use it for text generations: LlamaChatBedrockApi api = new LlamaChatBedrockApi(LlamaChatModel.LLAMA2_70B_CHAT_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper(), Duration.ofMillis(1000L)); BedrockLlamaChatModel chatModel = new BedrockLlamaChatModel(api, BedrockLlamaChatOptions.builder() .withTemperature(0.5) .withMaxGenLen(100) .withTopP(0.9).build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); Low-level LlamaChatBedrockApi Client: LlamaChatBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/llama/api/LlamaChatBedrockApi.java) provides is lightweight Java client on top of AWS Bedrock Meta Llama 2 and Llama 2 Chat models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html) . Following class diagram illustrates the LlamaChatBedrockApi interface and building blocks: The LlamaChatBedrockApi supports the meta.llama3-8b-instruct-v1:0 , meta.llama3-70b-instruct-v1:0 , meta.llama2-13b-chat-v1 and meta.llama2-70b-chat-v1 models for both synchronous (e.g. chatCompletion() ) and streaming (e.g. chatCompletionStream() ) responses. Here is a simple snippet how to use the api programmatically: LlamaChatBedrockApi llamaChatApi = new LlamaChatBedrockApi( LlamaChatModel.LLAMA3_70B_INSTRUCT_V1.id(), Region.US_EAST_1.id(), Duration.ofMillis(1000L)); LlamaChatRequest request = LlamaChatRequest.builder(""Hello, my name is"") .withTemperature(0.9) .withTopP(0.9) .withMaxGenLen(20) .build(); LlamaChatResponse response = llamaChatApi.chatCompletion(request); // Streaming response Flux<LlamaChatResponse> responseStream = llamaChatApi.chatCompletionStream(request); List<LlamaChatResponse> responses = responseStream.collectList().block(); Follow the LlamaChatBedrockApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/llama/api/LlamaChatBedrockApi.java) 's JavaDoc for further information. Anthropic2(bedrock-anthropic.html) Cohere(bedrock-cohere.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/bedrock/bedrock-cohere.html","Cohere Chat: Provides Bedrock Cohere chat model. Integrate generative AI capabilities into essential apps and workflows that improve business outcomes. The AWS Bedrock Cohere Model Page(https://aws.amazon.com/bedrock/cohere-command-embed/) and Amazon Bedrock User Guide(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) contains detailed information on how to use the AWS hosted model. Prerequisites: Refer to the Spring AI documentation on Amazon Bedrock(../../bedrock.html) for setting up API access. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Add the spring-ai-bedrock-ai-spring-boot-starter dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Enable Cohere Chat Support: By default the Cohere model is disabled. To enable it set the spring.ai.bedrock.cohere.chat.enabled property to true . Exporting environment variable is one way to set this configuration property: export SPRING_AI_BEDROCK_COHERE_CHAT_ENABLED=true Chat Properties: The prefix spring.ai.bedrock.aws is the property prefix to configure the connection to AWS Bedrock. Property Description Default spring.ai.bedrock.aws.region AWS region to use. us-east-1 spring.ai.bedrock.aws.timeout AWS timeout to use. 5m spring.ai.bedrock.aws.access-key AWS access key. - spring.ai.bedrock.aws.secret-key AWS secret key. - The prefix spring.ai.bedrock.cohere.chat is the property prefix that configures the chat model implementation for Cohere. Property Description Default spring.ai.bedrock.cohere.chat.enabled Enable or disable support for Cohere false spring.ai.bedrock.cohere.chat.model The model id to use. See the CohereChatModel(https://github.com/spring-projects/spring-ai/blob/4ba9a3cd689b9fd3a3805f540debe398a079c6ef/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereChatBedrockApi.java#L326C14-L326C29) for the supported models. cohere.command-text-v14 spring.ai.bedrock.cohere.chat.options.temperature Controls the randomness of the output. Values can range over [0.0,1.0] 0.7 spring.ai.bedrock.cohere.chat.options.topP The maximum cumulative probability of tokens to consider when sampling. AWS Bedrock default spring.ai.bedrock.cohere.chat.options.topK Specify the number of token choices the model uses to generate the next token AWS Bedrock default spring.ai.bedrock.cohere.chat.options.maxTokens Specify the maximum number of tokens to use in the generated response. AWS Bedrock default spring.ai.bedrock.cohere.chat.options.stopSequences Configure up to four sequences that the model recognizes. AWS Bedrock default spring.ai.bedrock.cohere.chat.options.returnLikelihoods The token likelihoods are returned with the response. AWS Bedrock default spring.ai.bedrock.cohere.chat.options.numGenerations The maximum number of generations that the model should return. AWS Bedrock default spring.ai.bedrock.cohere.chat.options.logitBias Prevents the model from generating unwanted tokens or incentivize the model to include desired tokens. AWS Bedrock default spring.ai.bedrock.cohere.chat.options.truncate Specifies how the API handles inputs longer than the maximum token length AWS Bedrock default Look at the CohereChatModel(https://github.com/spring-projects/spring-ai/blob/4ba9a3cd689b9fd3a3805f540debe398a079c6ef/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereChatBedrockApi.java#L326C14-L326C29) for other model IDs. Supported values are: cohere.command-light-text-v14 and cohere.command-text-v14 . Model ID values can also be found in the AWS Bedrock documentation for base model IDs(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) . All properties prefixed with spring.ai.bedrock.cohere.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The BedrockCohereChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereChatOptions.java) provides model configurations, such as temperature, topK, topP, etc. On start-up, the default options can be configured with the BedrockCohereChatModel(api, options) constructor or the spring.ai.bedrock.cohere.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", BedrockCohereChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific BedrockCohereChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-bedrock-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Cohere chat model: spring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=1000ms spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.bedrock.cohere.chat.enabled=true spring.ai.bedrock.cohere.chat.options.temperature=0.8 replace the regions , access-key and secret-key with your AWS credentials. This will create a BedrockCohereChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final BedrockCohereChatModel chatModel; @Autowired public ChatController(BedrockCohereChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The BedrockCohereChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level CohereChatBedrockApi Client(#low-level-api) to connect to the Bedrock Cohere service. Add the spring-ai-bedrock dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an BedrockCohereChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereChatModel.java) and use it for text generations: CohereChatBedrockApi api = new CohereChatBedrockApi(CohereChatModel.COHERE_COMMAND_V14.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper(), Duration.ofMillis(1000L)); BedrockCohereChatModel chatModel = new BedrockCohereChatModel(api, BedrockCohereChatOptions.builder() .withTemperature(0.6) .withTopK(10) .withTopP(0.5) .withMaxTokens(678) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); Low-level CohereChatBedrockApi Client: The CohereChatBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereChatBedrockApi.java) provides is lightweight Java client on top of AWS Bedrock Cohere Command models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command.html) . Following class diagram illustrates the CohereChatBedrockApi interface and building blocks: The CohereChatBedrockApi supports the cohere.command-light-text-v14 and cohere.command-text-v14 models for both synchronous (e.g. chatCompletion() ) and streaming (e.g. chatCompletionStream() ) requests. Here is a simple snippet how to use the api programmatically: CohereChatBedrockApi cohereChatApi = new CohereChatBedrockApi( CohereChatModel.COHERE_COMMAND_V14.id(), Region.US_EAST_1.id(), Duration.ofMillis(1000L)); var request = CohereChatRequest .builder(""What is the capital of Bulgaria and what is the size? What it the national anthem?"") .withStream(false) .withTemperature(0.5) .withTopP(0.8) .withTopK(15) .withMaxTokens(100) .withStopSequences(List.of(""END"")) .withReturnLikelihoods(CohereChatRequest.ReturnLikelihoods.ALL) .withNumGenerations(3) .withLogitBias(null) .withTruncate(Truncate.NONE) .build(); CohereChatResponse response = cohereChatApi.chatCompletion(request); var request = CohereChatRequest .builder(""What is the capital of Bulgaria and what is the size? What it the national anthem?"") .withStream(true) .withTemperature(0.5) .withTopP(0.8) .withTopK(15) .withMaxTokens(100) .withStopSequences(List.of(""END"")) .withReturnLikelihoods(CohereChatRequest.ReturnLikelihoods.ALL) .withNumGenerations(3) .withLogitBias(null) .withTruncate(Truncate.NONE) .build(); Flux<CohereChatResponse.Generation> responseStream = cohereChatApi.chatCompletionStream(request); List<CohereChatResponse.Generation> responses = responseStream.collectList().block(); Llama(bedrock-llama.html) Titan(bedrock-titan.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/bedrock/bedrock-titan.html","Titan Chat: Amazon Titan(https://aws.amazon.com/bedrock/titan/) foundation models (FMs) provide customers with a breadth of high-performing image, multimodal embeddings, and text model choices, via a fully managed API. Amazon Titan models are created by AWS and pretrained on large datasets, making them powerful, general-purpose models built to support a variety of use cases, while also supporting the responsible use of AI. Use them as is or privately customize them with your own data. The AWS Bedrock Titan Model Page(https://aws.amazon.com/bedrock/titan/) and Amazon Bedrock User Guide(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) contains detailed information on how to use the AWS hosted model. Prerequisites: Refer to the Spring AI documentation on Amazon Bedrock(../../bedrock.html) for setting up API access. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Add the spring-ai-bedrock-ai-spring-boot-starter dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Enable Titan Chat: By default the Titan model is disabled. To enable it set the spring.ai.bedrock.titan.chat.enabled property to true . Exporting environment variable is one way to set this configuration property: export SPRING_AI_BEDROCK_TITAN_CHAT_ENABLED=true Chat Properties: The prefix spring.ai.bedrock.aws is the property prefix to configure the connection to AWS Bedrock. Property Description Default spring.ai.bedrock.aws.region AWS region to use. us-east-1 spring.ai.bedrock.aws.timeout AWS timeout to use. 5m spring.ai.bedrock.aws.access-key AWS access key. - spring.ai.bedrock.aws.secret-key AWS secret key. - The prefix spring.ai.bedrock.titan.chat is the property prefix that configures the chat model implementation for Titan. Property Description Default spring.ai.bedrock.titan.chat.enabled Enable Bedrock Titan chat model. Disabled by default false spring.ai.bedrock.titan.chat.model The model id to use. See the TitanChatBedrockApi#TitanChatModel(https://github.com/spring-projects/spring-ai/blob/4839a6175cd1ec89498b97d3efb6647022c3c7cb/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/TitanChatBedrockApi.java#L220) for the supported models. amazon.titan-text-lite-v1 spring.ai.bedrock.titan.chat.options.temperature Controls the randomness of the output. Values can range over [0.0,1.0] 0.7 spring.ai.bedrock.titan.chat.options.topP The maximum cumulative probability of tokens to consider when sampling. AWS Bedrock default spring.ai.bedrock.titan.chat.options.stopSequences Configure up to four sequences that the generative recognizes. After a stop sequence, the generative stops generating further tokens. The returned text doesn’t contain the stop sequence. AWS Bedrock default spring.ai.bedrock.titan.chat.options.maxTokenCount Specify the maximum number of tokens to use in the generated response. Note that the models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate. We recommend a limit of 4,000 tokens for optimal performance. AWS Bedrock default Look at the TitanChatBedrockApi#TitanChatModel(https://github.com/spring-projects/spring-ai/blob/4839a6175cd1ec89498b97d3efb6647022c3c7cb/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/TitanChatBedrockApi.java#L220) for other model IDs. Supported values are: amazon.titan-text-lite-v1 , amazon.titan-text-express-v1 and amazon.titan-text-premier-v1:0 . Model ID values can also be found in the AWS Bedrock documentation for base model IDs(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) . All properties prefixed with spring.ai.bedrock.titan.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The BedrockTitanChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanChatOptions.java) provides model configurations, such as temperature, topP, etc. On start-up, the default options can be configured with the BedrockTitanChatModel(api, options) constructor or the spring.ai.bedrock.titan.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", BedrockTitanChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific BedrockTitanChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-bedrock-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Titan chat model: spring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=1000ms spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.bedrock.titan.chat.enabled=true spring.ai.bedrock.titan.chat.options.temperature=0.8 replace the regions , access-key and secret-key with your AWS credentials. This will create a BedrockTitanChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final BedrockTitanChatModel chatModel; @Autowired public ChatController(BedrockTitanChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The BedrockTitanChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level TitanChatBedrockApi Client(#low-level-api) to connect to the Bedrock Titanic service. Add the spring-ai-bedrock dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an BedrockTitanChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanChatModel.java) and use it for text generations: TitanChatBedrockApi titanApi = new TitanChatBedrockApi( TitanChatModel.TITAN_TEXT_EXPRESS_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper(), Duration.ofMillis(1000L)); BedrockTitanChatModel chatModel = new BedrockTitanChatModel(titanApi, BedrockTitanChatOptions.builder() .withTemperature(0.6) .withTopP(0.8) .withMaxTokenCount(100) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); Low-level TitanChatBedrockApi Client: The TitanChatBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/TitanChatBedrockApi.java) provides is lightweight Java client on top of AWS Bedrock Bedrock Titan models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html) . Following class diagram illustrates the TitanChatBedrockApi interface and building blocks: Client supports the amazon.titan-text-lite-v1 and amazon.titan-text-express-v1 models for both synchronous (e.g. chatCompletion() ) and streaming (e.g. chatCompletionStream() ) responses. Here is a simple snippet how to use the api programmatically: TitanChatBedrockApi titanBedrockApi = new TitanChatBedrockApi(TitanChatCompletionModel.TITAN_TEXT_EXPRESS_V1.id(), Region.EU_CENTRAL_1.id(), Duration.ofMillis(1000L)); TitanChatRequest titanChatRequest = TitanChatRequest.builder(""Give me the names of 3 famous pirates?"") .withTemperature(0.5) .withTopP(0.9) .withMaxTokenCount(100) .withStopSequences(List.of(""|"")) .build(); TitanChatResponse response = titanBedrockApi.chatCompletion(titanChatRequest); Flux<TitanChatResponseChunk> response = titanBedrockApi.chatCompletionStream(titanChatRequest); List<TitanChatResponseChunk> results = response.collectList().block(); Follow the TitanChatBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/TitanChatBedrockApi.java) 's JavaDoc for further information. Cohere(bedrock-cohere.html) Jurassic2(bedrock-jurassic2.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/bedrock/bedrock-jurassic2.html","Jurassic-2 Chat: AI21 Labs Jurassic on Amazon Bedrock(https://aws.amazon.com/bedrock/jurassic/) Jurassic is AI21 Labs’ family of reliable FMs for the enterprise, powering sophisticated language generation tasks – such as question answering, text generation, search, and summarization – across thousands of live applications. Prerequisites: Refer to the Spring AI documentation on Amazon Bedrock(../../bedrock.html) for setting up API access. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Add the spring-ai-bedrock-ai-spring-boot-starter dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Enable Jurassic-2: By default the Bedrock Jurassic-2 model is disabled. To enable it set the spring.ai.bedrock.jurassic2.chat.enabled property to true . Exporting environment variable is one way to set this configuration property: export SPRING_AI_BEDROCK_JURASSIC2_CHAT_ENABLED=true Chat Properties: The prefix spring.ai.bedrock.aws is the property prefix to configure the connection to AWS Bedrock. Property Description Default spring.ai.bedrock.aws.region AWS region to use. us-east-1 spring.ai.bedrock.aws.timeout AWS timeout to use. 5m spring.ai.bedrock.aws.access-key AWS access key. - spring.ai.bedrock.aws.secret-key AWS secret key. - The prefix spring.ai.bedrock.jurassic2.chat is the property prefix that configures the chat model implementation for Jurassic-2. Property Description Default spring.ai.bedrock.jurassic2.chat.enabled Enable or disable support for Jurassic-2 false spring.ai.bedrock.jurassic2.chat.model The model id to use (See Below) ai21.j2-mid-v1 spring.ai.bedrock.jurassic2.chat.options.temperature Controls the randomness of the output. Values can range over [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied, while a value closer to 0.0 will typically result in less surprising responses from the model. This value specifies default to be used by the backend while making the call to the model. 0.7 spring.ai.bedrock.jurassic2.chat.options.top-p The maximum cumulative probability of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Nucleus sampling considers the smallest set of tokens whose probability sum is at least topP. AWS Bedrock default spring.ai.bedrock.jurassic2.chat.options.max-tokens Specify the maximum number of tokens to use in the generated response. The model truncates the response once the generated text exceeds maxTokens. 500 Look at Ai21Jurassic2ChatBedrockApi#Ai21Jurassic2ChatModel(https://github.com/spring-projects/spring-ai/blob/4ba9a3cd689b9fd3a3805f540debe398a079c6ef/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/jurassic2/api/Ai21Jurassic2ChatBedrockApi.java#L164) for other model IDs. The other value supported is ai21.j2-ultra-v1 . Model ID values can also be found in the AWS Bedrock documentation for base model IDs(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) . All properties prefixed with spring.ai.bedrock.jurassic2.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The BedrockAi21Jurassic2ChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/jurassic2/BedrockAi21Jurassic2ChatOptions.java) provides model configurations, such as temperature, topP, maxTokens, etc. On start-up, the default options can be configured with the BedrockAi21Jurassic2ChatModel(api, options) constructor or the spring.ai.bedrock.jurassic2.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", BedrockAi21Jurassic2ChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific BedrockAi21Jurassic2ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/jurassic2/BedrockAi21Jurassic2ChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-bedrock-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Jurassic-2 chat model: spring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.timeout=1000ms spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.bedrock.jurassic2.chat.enabled=true spring.ai.bedrock.jurassic2.chat.options.temperature=0.8 replace the regions , access-key and secret-key with your AWS credentials. This will create a BedrockAi21Jurassic2ChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final BedrockAi21Jurassic2ChatModel chatModel; @Autowired public ChatController(BedrockAi21Jurassic2ChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } } Manual Configuration: The BedrockAi21Jurassic2ChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/jurassic2/BedrockAi21Jurassic2ChatModel.java) implements the ChatModel uses the Low-level Client(#low-level-api) to connect to the Bedrock Jurassic-2 service. Add the spring-ai-bedrock dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an BedrockAi21Jurassic2ChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/jurassic2/BedrockAi21Jurassic2ChatModel.java) and use it for text generations: Ai21Jurassic2ChatBedrockApi api = new Ai21Jurassic2ChatBedrockApi(Ai21Jurassic2ChatModel.AI21_J2_MID_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper(), Duration.ofMillis(1000L)); BedrockAi21Jurassic2ChatModel chatModel = new BedrockAi21Jurassic2ChatModel(api, BedrockAi21Jurassic2ChatOptions.builder() .withTemperature(0.5) .withMaxTokens(100) .withTopP(0.9).build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); Low-level Client: Ai21Jurassic2ChatBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/jurassic2/api/Ai21Jurassic2ChatBedrockApi.java) provides a lightweight Java client on top of AWS Bedrock Jurassic-2 and Jurassic-2 Chat models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html) . The Ai21Jurassic2ChatBedrockApi supports the ai21.j2-mid-v1 and ai21.j2-ultra-v1 models and only support synchronous ( chatCompletion() ). Here is a simple snippet on how to use the API programmatically: Ai21Jurassic2ChatBedrockApi jurassic2ChatApi = new Ai21Jurassic2ChatBedrockApi( Ai21Jurassic2ChatModel.AI21_J2_MID_V1.id(), Region.US_EAST_1.id(), Duration.ofMillis(1000L)); Ai21Jurassic2ChatRequest request = Ai21Jurassic2ChatRequest.builder(""Hello, my name is"") .withTemperature(0.9) .withTopP(0.9) .withMaxTokens(20) .build(); Ai21Jurassic2ChatResponse response = jurassic2ChatApi.chatCompletion(request); Follow the Ai21Jurassic2ChatBedrockApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/jurassic2/api/Ai21Jurassic2ChatBedrockApi.java) 's JavaDoc for further information. Titan(bedrock-titan.html) Anthropic 3(../anthropic-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/anthropic-chat.html","Anthropic 3 Chat: Anthropic Claude(https://www.anthropic.com/) is a family of foundational AI models that can be used in a variety of applications. For developers and businesses, you can leverage the API access and build directly on top of Anthropic’s AI infrastructure(https://www.anthropic.com/api) . Spring AI supports the Anthropic Messaging API(https://docs.anthropic.com/claude/reference/messages_post) for sync and streaming text generations. Anthropic’s Claude models are also available through Amazon Bedrock. Spring AI provides dedicated Amazon Bedrock Anthropic(bedrock/bedrock-anthropic.html) client implementations as well. Prerequisites: You will need to create an API key on Anthropic portal. Create an account at Anthropic API dashboard(https://console.anthropic.com/dashboard) and generate the api key on the Get API Keys(https://console.anthropic.com/settings/keys) page. The Spring AI project defines a configuration property named spring.ai.anthropic.api-key that you should set to the value of the API Key obtained from anthropic.com. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_ANTHROPIC_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Anthropic Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-anthropic-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-anthropic-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the Anthropic chat model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should NOT trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty currently the retry policies are not applicable for the streaming API. Connection Properties: The prefix spring.ai.anthropic is used as the property prefix that lets you connect to Anthropic. Property Description Default spring.ai.anthropic.base-url The URL to connect to api.anthropic.com(https://api.anthropic.com) spring.ai.anthropic.version Anthropic API version 2023-06-01 spring.ai.anthropic.api-key The API Key - spring.ai.anthropic.beta-version Enables new/experimental features. If set to max-tokens-3-5-sonnet-2024-07-15 the output tokens limit is increased from 4096 to 8192 tokens (for claude-3-5-sonnet only). tools-2024-04-04 Configuration Properties: The prefix spring.ai.anthropic.chat is the property prefix that lets you configure the chat model implementation for Anthropic. Property Description Default spring.ai.anthropic.chat.enabled Enable Anthropic chat model. true spring.ai.anthropic.chat.options.model This is the Anthropic Chat model to use. Supports: claude-3-5-sonnet-20240620 , claude-3-opus-20240229 , claude-3-sonnet-20240229 , claude-3-haiku-20240307 and the legacy claude-2.1 , claude-2.0 and claude-instant-1.2 models. claude-3-opus-20240229 spring.ai.anthropic.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.8 spring.ai.anthropic.chat.options.max-tokens The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. 500 spring.ai.anthropic.chat.options.stop-sequence Custom text sequences that will cause the model to stop generating. Our models will normally stop when they have naturally completed their turn, which will result in a response stop_reason of ""end_turn"". If you want the model to stop generating when it encounters custom strings of text, you can use the stop_sequences parameter. If the model encounters one of the custom sequences, the response stop_reason value will be ""stop_sequence"" and the response stop_sequence value will contain the matched stop sequence. - spring.ai.anthropic.chat.options.top-p Use nucleus sampling. In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both. Recommended for advanced use cases only. You usually only need to use temperature. - spring.ai.anthropic.chat.options.top-k Only sample from the top K options for each subsequent token. Used to remove ""long tail"" low probability responses. Learn more technical details here. Recommended for advanced use cases only. You usually only need to use temperature. - spring.ai.anthropic.chat.options.functions List of functions, identified by their names, to enable for function calling in a single prompt requests. Functions with those names must exist in the functionCallbacks registry. - spring.ai.anthropic.chat.options.functionCallbacks Tool Function Callbacks to register with the ChatModel. - spring.ai.anthropic.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false All properties prefixed with spring.ai.anthropic.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The AnthropicChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/AnthropicChatOptions.java) provides model configurations, such as the model to use, the temperature, the max token count, etc. On start-up, the default options can be configured with the AnthropicChatModel(api, options) constructor or the spring.ai.anthropic.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", AnthropicChatOptions.builder() .withModel(""claude-2.1"") .withTemperature(0.4) .build() )); In addition to the model specific AnthropicChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/AnthropicChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Function Calling: You can register custom Java functions with the AnthropicChatModel and have the Anthropic Claude model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This is a powerful technique to connect the LLM capabilities with external tools and APIs. Read more about Anthropic Function Calling(functions/anthropic-chat-functions.html) . Multimodal: Multimodality refers to a model’s ability to simultaneously understand and process information from various sources, including text, images, audio, and other data formats. This paradigm represents a significant advancement in AI models. Currently, Anthropic Claude 3 supports the base64 source type for images , and the image/jpeg , image/png , image/gif , and image/webp media types. Check the Vision guide(https://docs.anthropic.com/claude/docs/vision) for more information. Spring AI’s Message interface supports multimodal AI models by introducing the Media type. This type contains data and information about media attachments in messages, using Spring’s org.springframework.util.MimeType and a java.lang.Object for the raw media data. Below is a simple code example extracted from AnthropicChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/test/java/org/springframework/ai/anthropic/AnthropicChatModelIT.java) , demonstrating the combination of user text with an image. byte[] imageData = new ClassPathResource(""/multimodal.test.png"").getContentAsByteArray(); var userMessage = new UserMessage(""Explain what do you see on this picture?"", List.of(new Media(MimeTypeUtils.IMAGE_PNG, imageData))); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage))); logger.info(response.getResult().getOutput().getContent()); It takes as an input the multimodal.test.png image: along with the text message ""Explain what do you see on this picture?"", and generates a response something like: The image shows a close-up view of a wire fruit basket containing several pieces of fruit. The basket appears to be made of thin metal wires formed into a round shape with an elevated handle. Inside the basket, there are a few yellow bananas and a couple of red apples or possibly tomatoes. The vibrant colors of the fruit contrast nicely against the metallic tones of the wire basket. The shallow depth of field in the photograph puts the focus squarely on the fruit in the foreground, while the basket handle extending upwards is slightly blurred, creating a pleasing bokeh effect in the background. The composition and lighting give the image a clean, minimalist aesthetic that highlights the natural beauty and freshness of the fruit displayed in this elegant wire basket. Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-anthropic-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Anthropic chat model: spring.ai.anthropic.api-key=YOUR_API_KEY spring.ai.anthropic.chat.options.model=claude-3-5-sonnet-20240620 spring.ai.anthropic.chat.options.temperature=0.7 spring.ai.anthropic.chat.options.max-tokens=450 replace the api-key with your Anthropic credentials. This will create a AnthropicChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final AnthropicChatModel chatModel; @Autowired public ChatController(AnthropicChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The AnthropicChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/AnthropicChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level AnthropicApi Client(#low-level-api) to connect to the Anthropic service. Add the spring-ai-anthropic dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-anthropic</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-anthropic' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a AnthropicChatModel and use it for text generations: var anthropicApi = new AnthropicApi(System.getenv(""ANTHROPIC_API_KEY"")); var chatModel = new AnthropicChatModel(anthropicApi, AnthropicChatOptions.builder() .withModel(""claude-3-opus-20240229"") .withTemperature(0.4) .withMaxTokens(200) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); The AnthropicChatOptions provides the configuration information for the chat requests. The AnthropicChatOptions.Builder is fluent options builder. Low-level AnthropicApi Client: The AnthropicApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/AnthropicApi.java) provides is lightweight Java client for Anthropic Message API(https://docs.anthropic.com/claude/reference/messages_post) . Following class diagram illustrates the AnthropicApi chat interfaces and building blocks: Here is a simple snippet how to use the api programmatically: AnthropicApi anthropicApi = new AnthropicApi(System.getenv(""ANTHROPIC_API_KEY"")); AnthropicMessage chatCompletionMessage = new AnthropicMessage( List.of(new ContentBlock(""Tell me a Joke?"")), Role.USER); // Sync request ResponseEntity<ChatCompletionResponse> response = anthropicApi .chatCompletionEntity(new ChatCompletionRequest(AnthropicApi.ChatModel.CLAUDE_3_OPUS.getValue(), List.of(chatCompletionMessage), null, 100, 0.8, false)); // Streaming request Flux<StreamResponse> response = anthropicApi .chatCompletionStream(new ChatCompletionRequest(AnthropicApi.ChatModel.CLAUDE_3_OPUS.getValue(), List.of(chatCompletionMessage), null, 100, 0.8, true)); Follow the AnthropicApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/main/java/org/springframework/ai/anthropic/api/AnthropicApi.java) 's JavaDoc for further information. Low-level API Examples: The AnthropicApiIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/test/java/org/springframework/ai/anthropic/chat/api/AnthropicApiIT.java) test provides some general examples how to use the lightweight library. Jurassic2(bedrock/bedrock-jurassic2.html) Function Calling(functions/anthropic-chat-functions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/functions/anthropic-chat-functions.html","Anthropic Function Calling: Starting of Jul 1th, 2024, streaming function calling and Tool use is supporetd. You can register custom Java functions with the AnthropicChatModel and have the Anthropic models intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This allows you to connect the LLM capabilities with external tools and APIs. The claude-3-5-sonnet-20240620 , claude-3-opus , claude-3-sonnet and claude-3-haiku models are trained to detect when a function should be called(https://docs.anthropic.com/claude/docs/tool-use#tool-use-best-practices-and-limitations) and to respond with JSON that adheres to the function signature. The Anthropic API does not call the function directly; instead, the model generates JSON that you can use to call the function in your code and return the result back to the model to complete the conversation. Spring AI provides flexible and user-friendly ways to register and call custom functions. In general, the custom functions need to provide a function name , description , and the function call signature (as JSON schema) to let the model know what arguments the function expects. The description helps the model to understand when to call the function. As a developer, you need to implement a function that takes the function call arguments sent from the AI model, and responds with the result back to the model. Your function can in turn invoke other 3rd party services to provide the results. Spring AI makes this as easy as defining a @Bean definition that returns a java.util.Function and supplying the bean name as an option when invoking the ChatModel . Under the hood, Spring wraps your POJO (the function) with the appropriate adapter code that enables interaction with the AI Model, saving you from writing tedious boilerplate code. The basis of the underlying infrastructure is the FunctionCallback.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallback.java) interface and the companion FunctionCallbackWrapper.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallbackWrapper.java) utility class to simplify the implementation and registration of Java callback functions. How it works: Suppose we want the AI model to respond with information that it does not have, for example the current temperature at a given location. We can provide the AI model with metadata about our own functions that it can use to retrieve that information as it processes your prompt. For example, if during the processing of a prompt, the AI Model determines that it needs additional information about the temperature in a given location, it will start a server side generated request/response interaction. The AI Model invokes a client side function. The AI Model provides method invocation details as JSON and it is the responsibility of the client to execute that function and return the response. Spring AI greatly simplifies the code you need to write to support function invocation. It brokers the function invocation conversation for you. You can simply provide your function definition as a @Bean and then provide the bean name of the function in your prompt options. You can also reference multiple function bean names in your prompt. Quick Start: Let’s create a chatbot that answer questions by calling our own function. To support the response of the chatbot, we will register our own function that takes a location and returns the current weather in that location. When the response to the prompt to the model needs to answer a question such as ""What’s the weather like in Boston?"" the AI model will invoke the client providing the location value as an argument to be passed to the function. This RPC-like data is passed as JSON. Our function can some SaaS based weather service API and returns the weather response back to the model to complete the conversation. In this example we will use a simple implementation named MockWeatherService that hard codes the temperature for various locations. The following MockWeatherService.java represents the weather service API: public class MockWeatherService implements Function<Request, Response> { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } Registering Functions as Beans: With the AnthropicChatModel Auto-Configuration(../anthropic-chat.html#_auto_configuration) you have multiple ways to register custom functions as beans in the Spring context. We start with describing the most POJO friendly options. Plain Java Functions: In this approach you define @Beans in your application context as you would any other Spring managed object. Internally, Spring AI ChatModel will create an instance of a FunctionCallbackWrapper wrapper that adds the logic for it being invoked via the AI model. The name of the @Bean is passed as a ChatOption . @Configuration static class Config { @Bean @Description(""Get the weather in location"") // function description public Function<MockWeatherService.Request, MockWeatherService.Response> weatherFunction1() { return new MockWeatherService(); } ... } The @Description annotation is optional and provides a function description (2) that helps the model understand when to call the function. It is an important property to set to help the AI model determine what client side function to invoke. Another option to provide the description of the function is to use the @JsonClassDescription annotation on the MockWeatherService.Request to provide the function description: @Configuration static class Config { @Bean public Function<Request, Response> currentWeather3() { // (1) bean name as function name. return new MockWeatherService(); } ... } @JsonClassDescription(""Get the weather in location"") // (2) function description public record Request(String location, Unit unit) {} It is a best practice to annotate the request object with information such that the generated JSON schema of that function is as descriptive as possible to help the AI model pick the correct function to invoke. The FunctionCallWithFunctionBeanIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/anthropic/tool/FunctionCallWithFunctionBeanIT.java.java) demonstrates this approach. FunctionCallback Wrapper: Another way to register a function is to create a FunctionCallbackWrapper wrapper like this: @Configuration static class Config { @Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") // (1) function name .withDescription(""Get the weather in location"") // (2) function description .build(); } ... } It wraps the 3rd party MockWeatherService function and registers it as a CurrentWeather function with the AnthropicChatModel . It also provides a description (2) and an optional response converter (3) to convert the response into a text as expected by the model. By default, the response converter does a JSON serialization of the Response object. The FunctionCallbackWrapper internally resolves the function call signature based on the MockWeatherService.Request class. Specifying functions in Chat Options: To let the model know and call your CurrentWeather function you need to enable it in your prompt requests: AnthropicChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in Paris?""); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), AnthropicChatOptions.builder().withFunction(""CurrentWeather"").build())); // (1) Enable the function logger.info(""Response: {}"", response); Above user question will trigger 3 calls to CurrentWeather function (one for each city) and produce the final response. Register/Call Functions with Prompt Options: In addition to the auto-configuration you can register callback functions, dynamically, with your Prompt requests: AnthropicChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in Paris?""); var promptOptions = AnthropicChatOptions.builder() .withFunctionCallbacks(List.of(new FunctionCallbackWrapper<>( ""CurrentWeather"", // name ""Get the weather in location"", // function description new MockWeatherService()))) // function code .build(); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), promptOptions)); The in-prompt registered functions are enabled by default for the duration of this request. This approach allows to dynamically chose different functions to be called based on the user input. The FunctionCallWithPromptFunctionIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/anthropic/tool/FunctionCallWithPromptFunctionIT.java) integration test provides a complete example of how to register a function with the AnthropicChatModel and use it in a prompt request. Anthropic 3(../anthropic-chat.html) Azure OpenAI(../azure-openai-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/azure-openai-chat.html","Azure OpenAI Chat: Azure’s OpenAI offering, powered by ChatGPT, extends beyond traditional OpenAI capabilities, delivering AI-driven text generation with enhanced functionality. Azure offers additional AI safety and responsible AI features, as highlighted in their recent update here(https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-new-ai-safety-amp-responsible-ai-features-in-azure/ba-p/3983686) . Azure offers Java developers the opportunity to leverage AI’s full potential by integrating it with an array of Azure services, which includes AI-related resources such as Vector Stores on Azure. Prerequisites: The Azure OpenAI client offers three options to connect: using an Azure API key or using an OpenAI API Key, or using Microsoft Entra ID. Azure API Key & Endpoint: Obtain your Azure OpenAI endpoint and api-key from the Azure OpenAI Service section on the Azure Portal(https://portal.azure.com) . Spring AI defines two configuration properties: spring.ai.azure.openai.api-key : Set this to the value of the API Key obtained from Azure. spring.ai.azure.openai.endpoint : Set this to the endpoint URL obtained when provisioning your model in Azure. You can set these configuration properties by exporting environment variables: export SPRING_AI_AZURE_OPENAI_API_KEY=<INSERT AZURE KEY HERE> export SPRING_AI_AZURE_OPENAI_ENDPOINT=<INSERT ENDPOINT URL HERE> OpenAI Key: To authenticate with the OpenAI service (not Azure), provide an OpenAI API key. This will automatically set the endpoint to api.openai.com/v1(https://api.openai.com/v1) . When using this approach, set the spring.ai.azure.openai.chat.options.deployment-name property to the name of the OpenAI model(https://platform.openai.com/docs/models) you wish to use. export SPRING_AI_AZURE_OPENAI_OPENAI_API_KEY=<INSERT OPENAI KEY HERE> Microsoft Entra ID: To authenticate using Microsoft Entra ID (formerly Azure Active Directory), create a TokenCredential bean in your configuration. If this bean is available, an OpenAIClient instance will be created using the token credentials. bd === Deployment Name To use Azure AI applications, you need to create an Azure AI Deployment through the Azure AI Portal(https://oai.azure.com/portal) . In Azure, each client must specify a Deployment Name to connect to the Azure OpenAI service. It’s important to note that the Deployment Name is different from the model you choose to deploy. For example, a deployment named 'MyAiDeployment' could be configured to use either the GPT 3.5 Turbo model or the GPT 4.0 model. To get started, follow these steps to create a deployment with the default settings: Deployment Name: `gpt-4o` Model Name: `gpt-4o` This Azure configuration aligns with the default configurations of the Spring Boot Azure AI Starter and its Autoconfiguration feature. If you use a different Deployment Name, make sure to update the configuration property accordingly: spring.ai.azure.openai.chat.options.deployment-name=<my deployment name> The different deployment structures of Azure OpenAI and OpenAI leads to a property in the Azure OpenAI client library named deploymentOrModelName . This is because in OpenAI there is no Deployment Name , only a Model Name . The property spring.ai.azure.openai.chat.options.model has been renamed to spring.ai.azure.openai.chat.options.deployment-name . If you decide to connect to OpenAI instead of Azure OpeanAI , by setting the spring.ai.azure.openai.openai-api-key=<Your OpenAI Key> property, then the spring.ai.azure.openai.chat.options.deployment-name is treathed as an OpenAI model(https://platform.openai.com/docs/models) name. Access the OpenAI Model: You can configure the client to use directly OpenAI instead of the Azure OpenAI deployed models. For this you need to set the spring.ai.azure.openai.openai-api-key=<Your OpenAI Key> instead of spring.ai.azure.openai.api-key=<Yur Azure OpenAi Key> . Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure OpenAI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-azure-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: The prefix spring.ai.azure.openai is the property prefix to configure the connection to Azure OpenAI. Property Description Default spring.ai.azure.openai.api-key The Key from Azure AI OpenAI Keys and Endpoint section under Resource Management - spring.ai.azure.openai.endpoint The endpoint from the Azure AI OpenAI Keys and Endpoint section under Resource Management - spring.ai.azure.openai.openai-api-key (non Azure) OpenAI API key. Used to authenticate with the OpenAI service, instead of Azure OpenAI. This automatically sets the endpoint to api.openai.com/v1(https://api.openai.com/v1) . Use either api-key or openai-api-key property. With this configuraiton the spring.ai.azure.openai.chat.options.deployment-name is threated as an OpenAi Model(https://platform.openai.com/docs/models) name. - The prefix spring.ai.azure.openai.chat is the property prefix that configures the ChatModel implementation for Azure OpenAI. Property Description Default spring.ai.azure.openai.chat.enabled Enable Azure OpenAI chat model. true spring.ai.azure.openai.chat.options.deployment-name In use with Azure, this refers to the ""Deployment Name"" of your model, which you can find at oai.azure.com/portal(https://oai.azure.com/portal) . It’s important to note that within an Azure OpenAI deployment, the ""Deployment Name"" is distinct from the model itself. The confusion around these terms stems from the intention to make the Azure OpenAI client library compatible with the original OpenAI endpoint. The deployment structures offered by Azure OpenAI and Sam Altman’s OpenAI differ significantly. Deployments model name to provide as part of this completions request. gpt-4o spring.ai.azure.openai.chat.options.maxTokens The maximum number of tokens to generate. - spring.ai.azure.openai.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.7 spring.ai.azure.openai.chat.options.topP An alternative to sampling with temperature called nucleus sampling. This value causes the model to consider the results of tokens with the provided probability mass. - spring.ai.azure.openai.chat.options.logitBias A map between GPT token IDs and bias scores that influences the probability of specific tokens appearing in a completions response. Token IDs are computed via external tokenizer tools, while bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias score varies by model. - spring.ai.azure.openai.chat.options.user An identifier for the caller or end user of the operation. This may be used for tracking or rate-limiting purposes. - spring.ai.azure.openai.chat.options.n The number of chat completions choices that should be generated for a chat completions response. - spring.ai.azure.openai.chat.options.stop A collection of textual sequences that will end completions generation. - spring.ai.azure.openai.chat.options.presencePenalty A value that influences the probability of generated tokens appearing based on their existing presence in generated text. Positive values will make tokens less likely to appear when they already exist and increase the model’s likelihood to output new topics. - spring.ai.azure.openai.chat.options.responseFormat An object specifying the format that the model must output. Using AzureOpenAiResponseFormat.JSON enables JSON mode, which guarantees the message the model generates is valid JSON. Using AzureOpenAiResponseFormat.TEXT enables TEXT mode. - spring.ai.azure.openai.chat.options.frequencyPenalty A value that influences the probability of generated tokens appearing based on their cumulative frequency in generated text. Positive values will make tokens less likely to appear as their frequency increases and decrease the likelihood of the model repeating the same statements verbatim. - spring.ai.azure.openai.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false All properties prefixed with spring.ai.azure.openai.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The AzureOpenAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/main/java/org/springframework/ai/azure/openai/AzureOpenAiChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the AzureOpenAiChatModel(api, options) constructor or the spring.ai.azure.openai.chat.options.* properties. At runtime you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", AzureOpenAiChatOptions.builder() .withModel(""gpt-4-o"") .withTemperature(0.4) .build() )); In addition to the model specific AzureOpenAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/main/java/org/springframework/ai/azure/openai/AzureOpenAiChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Function Calling: You can register custom Java functions with the AzureOpenAiChatModel and have the model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This is a powerful technique to connect the LLM capabilities with external tools and APIs. Read more about Azure OpenAI Function Calling(functions/azure-open-ai-chat-functions.html) . Multimodal: Multimodality refers to a model’s ability to simultaneously understand and process information from various sources, including text, images, audio, and other data formats. Presently, the Azure OpenAI gpt-4o model offers multimodal support. The Azure OpenAI can incorporate a list of base64-encoded images or image urls with the message. Spring AI’s Message(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/messages/Message.java) interface facilitates multimodal AI models by introducing the Media(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/messages/Media.java) type. This type encompasses data and details regarding media attachments in messages, utilizing Spring’s org.springframework.util.MimeType and a java.lang.Object for the raw media data. Below is a code example excerpted from OpenAiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java#L293) , illustrating the fusion of user text with an image using the the GPT_4_O model. URL url = new URL(""https://docs.spring.io/spring-ai/reference/_images/multimodal.test.png""); String response = ChatClient.create(chatModel).prompt() .options(AzureOpenAiChatOptions.builder().withDeploymentName(""gpt4o"").build()) .user(u -> u.text(""Explain what do you see on this picture?"").media(MimeTypeUtils.IMAGE_PNG, url)) .call() .content(); you can pass multiple images as well. It takes as an input the multimodal.test.png image: along with the text message ""Explain what do you see on this picture?"", and generates a response like this: This is an image of a fruit bowl with a simple design. The bowl is made of metal with curved wire edges that create an open structure, allowing the fruit to be visible from all angles. Inside the bowl, there are two yellow bananas resting on top of what appears to be a red apple. The bananas are slightly overripe, as indicated by the brown spots on their peels. The bowl has a metal ring at the top, likely to serve as a handle for carrying. The bowl is placed on a flat surface with a neutral-colored background that provides a clear view of the fruit inside. You can also pass in a classpath resource instead of a URL as shown in the example below Resource resource = new ClassPathResource(""multimodality/multimodal.test.png""); String response = ChatClient.create(chatModel).prompt() .options(AzureOpenAiChatOptions.builder() .withDeploymentName(""gpt-4o"").build()) .user(u -> u.text(""Explain what do you see on this picture?"") .media(MimeTypeUtils.IMAGE_PNG, resource)) .call() .content(); Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-azure-openai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the OpenAi chat model: spring.ai.azure.openai.api-key=YOUR_API_KEY spring.ai.azure.openai.endpoint=YOUR_ENDPOINT spring.ai.azure.openai.chat.options.deployment-name=gpt-4o spring.ai.azure.openai.chat.options.temperature=0.7 replace the api-key and endpoint with your Azure OpenAI credentials. This will create a AzureOpenAiChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final AzureOpenAiChatModel chatModel; @Autowired public ChatController(AzureOpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The AzureOpenAiChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/main/java/org/springframework/ai/azure/openai/AzureOpenAiChatModel.java) implements the ChatModel and StreamingChatModel and uses the Azure OpenAI Java Client(https://learn.microsoft.com/en-us/java/api/overview/azure/ai-openai-readme?view=azure-java-preview) . To enable it, add the spring-ai-azure-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-openai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-azure-openai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-azure-openai dependency also provide the access to the AzureOpenAiChatModel . For more information about the AzureOpenAiChatModel refer to the Azure OpenAI Chat(../chat/azure-openai-chat.html) section. Next, create an AzureOpenAiChatModel instance and use it to generate text responses: var openAIClient = new OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(""AZURE_OPENAI_API_KEY""))) .endpoint(System.getenv(""AZURE_OPENAI_ENDPOINT"")) .buildClient(); var openAIChatOptions = AzureOpenAiChatOptions.builder() .withDeploymentName(""gpt-4o"") .withTemperature(0.4) .withMaxTokens(200) .build(); var chatModel = new AzureOpenAiChatModel(openAIClient, openAIChatOptions); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); the gpt-4o is actually the Deployment Name as presented in the Azure AI Portal. Function Calling(functions/anthropic-chat-functions.html) Function Calling(functions/azure-open-ai-chat-functions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/functions/azure-open-ai-chat-functions.html","Azure OpenAI Function Calling: Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with. You can register custom Java functions with the AzureOpenAiChatModel and have the model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This allows you to connect the LLM capabilities with external tools and APIs. The Azure models are trained to detect when a function should be called and to respond with JSON that adheres to the function signature. The Azure OpenAI API does not call the function directly; instead, the model generates JSON that you can use to call the function in your code and return the result back to the model to complete the conversation. Spring AI provides flexible and user-friendly ways to register and call custom functions. In general, the custom functions need to provide a function name , description , and the function call signature (as JSON schema) to let the model know what arguments the function expects. The description helps the model to understand when to call the function. As a developer, you need to implement a function that takes the function call arguments sent from the AI model, and responds with the result back to the model. Your function can in turn invoke other 3rd party services to provide the results. Spring AI makes this as easy as defining a @Bean definition that returns a java.util.Function and supplying the bean name as an option when invoking the ChatModel . Under the hood, Spring wraps your POJO (the function) with the appropriate adapter code that enables interaction with the AI Model, saving you from writing tedious boilerplate code. The basis of the underlying infrastructure is the FunctionCallback.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallback.java) interface and the companion FunctionCallbackWrapper.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallbackWrapper.java) utility class to simplify the implementation and registration of Java callback functions. How it works: Suppose we want the AI model to respond with information that it does not have, for example the current temperature at a given location. We can provide the AI model with metadata about our own functions that it can use to retrieve that information as it processes your prompt. For example, if during the processing of a prompt, the AI Model determines that it needs additional information about the temperature in a given location, it will start a server side generated request/response interaction. The AI Model invokes a client side function. The AI Model provides method invocation details as JSON and it is the responsibility of the client to execute that function and return the response. Spring AI greatly simplifies the code you need to write to support function invocation. It brokers the function invocation conversation for you. You can simply provide your function definition as a @Bean and then provide the bean name of the function in your prompt options. You can also reference multiple function bean names in your prompt. Quick Start: Let’s create a chatbot that answer questions by calling our own function. To support the response of the chatbot, we will register our own function that takes a location and returns the current weather in that location. When the response to the prompt to the model needs to answer a question such as ""What’s the weather like in Boston?"" the AI model will invoke the client providing the location value as an argument to be passed to the function. This RPC-like data is passed as JSON. Our function can some SaaS based weather service API and returns the weather response back to the model to complete the conversation. In this example we will use a simple implementation named MockWeatherService that hard codes the temperature for various locations. The following MockWeatherService.java represents the weather service API: public class MockWeatherService implements Function<Request, Response> { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } Registering Functions as Beans: With the AzureOpenAiChatModelAuto-Configuration(../azure-openai-chat.html#_auto_configuration) you have multiple ways to register custom functions as beans in the Spring context. We start with describing the most POJO friendly options. Plain Java Functions: In this approach you define @Beans in your application context as you would any other Spring managed object. Internally, Spring AI ChatModel will create an instance of a FunctionCallbackWrapper wrapper that adds the logic for it being invoked via the AI model. The name of the @Bean is passed as a ChatOption . @Configuration static class Config { @Bean @Description(""Get the weather in location"") // function description public Function<MockWeatherService.Request, MockWeatherService.Response> weatherFunction1() { return new MockWeatherService(); } ... } The @Description annotation is optional and provides a function description (2) that helps the model understand when to call the function. It is an important property to set to help the AI model determine what client side function to invoke. Another option to provide the description of the function is to use the @JsonClassDescription annotation on the MockWeatherService.Request to provide the function description: @Configuration static class Config { @Bean public Function<Request, Response> currentWeather3() { // (1) bean name as function name. return new MockWeatherService(); } ... } @JsonClassDescription(""Get the weather in location"") // (2) function description public record Request(String location, Unit unit) {} It is a best practice to annotate the request object with information such that the generated JSON schema of that function is as descriptive as possible to help the AI model pick the correct function to invoke. The FunctionCallWithFunctionBeanIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/azure/tool/FunctionCallWithFunctionBeanIT.java) demonstrates this approach. FunctionCallback Wrapper: Another way to register a function is to create a FunctionCallbackWrapper wrapper like this: @Configuration static class Config { @Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") // (1) function name .withDescription(""Get the current weather in a given location"") // (2) function description .build(); } ... } It wraps the 3rd party MockWeatherService function and registers it as a CurrentWeather function with the AzureAiChatModel and provides a description (2). The default response converter does a JSON serialization of the Response object. The FunctionCallbackWrapper internally resolves the function call signature based on the MockWeatherService.Request class and internally generates an JSON schema for the function call. Specifying functions in Chat Options: To let the model know and call your CurrentWeather function you need to enable it in your prompt requests: AzureOpenAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris?""); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), AzureOpenAiChatOptions.builder().withFunction(""CurrentWeather"").build())); // (1) Enable the function logger.info(""Response: {}"", response); Above user question will trigger 3 calls to CurrentWeather function (one for each city) and the final response will be something like this: Here is the current weather for the requested cities: - San Francisco, CA: 30.0°C - Tokyo, Japan: 10.0°C - Paris, France: 15.0°C The FunctionCallWithFunctionWrapperIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/azure/tool/FunctionCallWithFunctionWrapperIT.java) test demo this approach. Register/Call Functions with Prompt Options: In addition to the auto-configuration you can register callback functions, dynamically, with your Prompt requests: AzureOpenAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris? Use Multi-turn function calling.""); var promptOptions = AzureOpenAiChatOptions.builder() .withFunctionCallbacks(List.of(FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") .withDescription(""Get the weather in location"") .build())) .build(); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), promptOptions)); The in-prompt registered functions are enabled by default for the duration of this request. This approach allows to dynamically chose different functions to be called based on the user input. The FunctionCallWithPromptFunctionIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/azure/tool/FunctionCallWithPromptFunctionIT.java) integration test provides a complete example of how to register a function with the AzureOpenAiChatModel and use it in a prompt request. Azure OpenAI(../azure-openai-chat.html) Google VertexAI(../google-vertexai.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/google-vertexai.html","Google VertexAI API: VertexAI API(https://cloud.google.com/vertex-ai/docs/reference) provides high-quality custom machine learning models with minimal machine learning expertise and effort. Spring AI provides integration with VertexAI API through the following clients: VertexAI PaLM2 Chat(vertexai-palm2-chat.html) VertexAI Gemini Chat(vertexai-gemini-chat.html) Function Calling(functions/azure-open-ai-chat-functions.html) VertexAI PaLM2(vertexai-palm2-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/vertexai-palm2-chat.html","VertexAI PaLM2 Chat: WARNING: As of February 15, 2024, the PaLM2 API for use with Google AI services and tooling is deprecated(https://ai.google.dev/palm_docs/deprecation) . In 6 months, the PaLM API will be decommissioned, meaning that users won’t be able to use a PaLM model in a prompt, tune a new PaLM model, or run inference on PaLM-tuned models. The Generative Language(https://developers.generativeai.google/api/rest/generativelanguage) PaLM API allows developers to build generative AI applications using the PaLM model. Large Language Models (LLMs) are a powerful, versatile type of machine learning model that enables computers to comprehend and generate natural language through a series of prompts. The PaLM API is based on Google’s next generation LLM, PaLM. It excels at a variety of different tasks like code generation, reasoning, and writing. You can use the PaLM API to build generative AI applications for use cases like content generation, dialogue agents, summarization and classification systems, and more. Based on the Models REST API(https://developers.generativeai.google/api/rest/generativelanguage/models) . Prerequisites: To access the PaLM2 REST API you need to obtain an access API KEY form makersuite(https://makersuite.google.com/app/apikey) . Currently the PaLM API it is not available outside US, but you can use VPN for testing. The Spring AI project defines a configuration property named spring.ai.vertex.ai.api-key that you should set to the value of the API Key obtained. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_VERTEX_AI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the VertexAI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-palm2-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-palm2-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: The prefix spring.ai.vertex.ai is used as the property prefix that lets you connect to VertexAI. Property Description Default spring.ai.vertex.ai.ai.base-url The URL to connect to generativelanguage.googleapis.com/v1beta3(https://generativelanguage.googleapis.com/v1beta3) spring.ai.vertex.ai.api-key The API Key - The prefix spring.ai.vertex.ai.chat is the property prefix that lets you configure the chat model implementation for VertexAI Chat. Property Description Default spring.ai.vertex.ai.chat.enabled Enable Vertex AI PaLM API chat model. true spring.ai.vertex.ai.chat.model This is the Vertex Chat model(https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat) to use chat-bison-001 spring.ai.vertex.ai.chat.options.temperature Controls the randomness of the output. Values can range over [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied, while a value closer to 0.0 will typically result in less surprising responses from the generative. This value specifies default to be used by the backend while making the call to the generative. 0.7 spring.ai.vertex.ai.chat.options.topK The maximum number of tokens to consider when sampling. The generative uses combined Top-k and nucleus sampling. Top-k sampling considers the set of topK most probable tokens. 20 spring.ai.vertex.ai.chat.options.topP The maximum cumulative probability of tokens to consider when sampling. The generative uses combined Top-k and nucleus sampling. Nucleus sampling considers the smallest set of tokens whose probability sum is at least topP. - spring.ai.vertex.ai.chat.options.candidateCount The number of generated response messages to return. This value must be between [1, 8], inclusive. Defaults to 1. 1 All properties prefixed with spring.ai.vertex.ai.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The VertexAiPaLm2ChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-palm2/src/main/java/org/springframework/ai/vertexai/palm2/VertexAiPaLm2ChatOptions.java) provides model configurations, such as the temperature, the topK, etc. On start-up, the default options can be configured with the VertexAiPaLm2ChatModel(api, options) constructor or the spring.ai.vertex.ai.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", VertexAiPaLm2ChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific VertexAiPaLm2ChatOptions you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-vertex-ai-palm2-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the VertexAi chat model: spring.ai.vertex.ai.api-key=YOUR_API_KEY spring.ai.vertex.ai.chat.model=chat-bison-001 spring.ai.vertex.ai.chat.options.temperature=0.5 replace the api-key with your VertexAI credentials. This will create a VertexAiPaLm2ChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final VertexAiPaLm2ChatModel chatModel; @Autowired public ChatController(VertexAiPaLm2ChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The VertexAiPaLm2ChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/vertexai/paml2/VertexAiPaLm2ChatModel.java) implements the ChatModel and uses the Low-level VertexAiPaLm2Api Client(#low-level-api) to connect to the VertexAI service. Add the spring-ai-vertex-ai-palm2 dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-palm2</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-palm' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a VertexAiPaLm2ChatModel and use it for text generations: VertexAiPaLm2Api vertexAiApi = new VertexAiPaLm2Api(< YOUR PALM_API_KEY>); var chatModel = new VertexAiPaLm2ChatModel(vertexAiApi, VertexAiPaLm2ChatOptions.builder() .withTemperature(0.4) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); The VertexAiPaLm2ChatOptions provides the configuration information for the chat requests. The VertexAiPaLm2ChatOptions.Builder is fluent options builder. Low-level VertexAiPaLm2Api Client: The VertexAiPaLm2Api(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-aipalm2/src/main/java/org/springframework/ai/vertexai/palm2/api/VertexAiPaLm2Api.java) provides is lightweight Java client for VertexAiPaLm2Api Chat API. Following class diagram illustrates the VertexAiPaLm2Api chat interfaces and building blocks: Here is a simple snippet how to use the api programmatically: VertexAiPaLm2Api vertexAiApi = new VertexAiPaLm2Api(< YOUR PALM_API_KEY>); // Generate var prompt = new MessagePrompt(List.of(new Message(""0"", ""Hello, how are you?""))); GenerateMessageRequest request = new GenerateMessageRequest(prompt); GenerateMessageResponse response = vertexAiApi.generateMessage(request); // Embed text Embedding embedding = vertexAiApi.embedText(""Hello, how are you?""); // Batch embedding List<Embedding> embeddings = vertexAiApi.batchEmbedText(List.of(""Hello, how are you?"", ""I am fine, thank you!"")); Google VertexAI(google-vertexai.html) VertexAI Gemini(vertexai-gemini-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/vertexai-gemini-chat.html","VertexAI Gemini Chat: The Vertex AI Gemini API(https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview) allows developers to build generative AI applications using the Gemini model. The Vertex AI Gemini API supports multimodal prompts as input and output text or code. A multimodal model is a model that is capable of processing information from multiple modalities, including images, videos, and text. For example, you can send the model a photo of a plate of cookies and ask it to give you a recipe for those cookies. Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision and Gemini 1.0 Pro models. For specifications of the Vertex AI Gemini API models, see Model information(https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#gemini-models) . Gemini API Reference(https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini) Prerequisites: Install the gcloud(https://cloud.google.com/sdk/docs/install) CLI, apropriate for you OS. Authenticate by running the following command. Replace PROJECT_ID with your Google Cloud project ID and ACCOUNT with your Google Cloud username. gcloud config set project <PROJECT_ID> && gcloud auth application-default login <ACCOUNT> Auto-configuration: Spring AI provides Spring Boot auto-configuration for the VertexAI Gemini Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-gemini-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-gemini-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: The prefix spring.ai.vertex.ai.gemini is used as the property prefix that lets you connect to VertexAI. Property Description Default spring.ai.vertex.ai.gemini.projectId Google Cloud Platform project ID - spring.ai.vertex.ai.gemini.location Region - spring.ai.vertex.ai.gemini.credentialsUri URI to Vertex AI Gemini credentials. When provided it is used to create an a GoogleCredentials instance to authenticate the VertexAI . - spring.ai.vertex.ai.gemini.apiEndpoint Vertex AI Gemini API endpoint. - spring.ai.vertex.ai.gemini.scopes - spring.ai.vertex.ai.gemini.transport API transport. GRPC or REST. GRPC The prefix spring.ai.vertex.ai.gemini.chat is the property prefix that lets you configure the chat model implementation for VertexAI Gemini Chat. Property Description Default spring.ai.vertex.ai.gemini.chat.options.model Supported Vertex AI Gemini Chat model(https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini) to use include the (1.0 ) gemini-pro , gemini-pro-vision (deprecated) and the new gemini-1.5-pro-001 , gemini-1.5-flash-001 models. gemini-1.5-pro-001 spring.ai.vertex.ai.gemini.chat.options.responseMimeType Output response mimetype of the generated candidate text. text/plain : (default) Text output or application/json : JSON response. spring.ai.vertex.ai.gemini.chat.options.googleSearchRetrieval Use Google search Grounding feature true or false , default false . spring.ai.vertex.ai.gemini.chat.options.temperature Controls the randomness of the output. Values can range over [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied, while a value closer to 0.0 will typically result in less surprising responses from the generative. This value specifies default to be used by the backend while making the call to the generative. 0.8 spring.ai.vertex.ai.gemini.chat.options.topK The maximum number of tokens to consider when sampling. The generative uses combined Top-k and nucleus sampling. Top-k sampling considers the set of topK most probable tokens. - spring.ai.vertex.ai.gemini.chat.options.topP The maximum cumulative probability of tokens to consider when sampling. The generative uses combined Top-k and nucleus sampling. Nucleus sampling considers the smallest set of tokens whose probability sum is at least topP. - spring.ai.vertex.ai.gemini.chat.options.candidateCount The number of generated response messages to return. This value must be between [1, 8], inclusive. Defaults to 1. - spring.ai.vertex.ai.gemini.chat.options.candidateCount The number of generated response messages to return. This value must be between [1, 8], inclusive. Defaults to 1. - spring.ai.vertex.ai.gemini.chat.options.maxOutputTokens The maximum number of tokens to generate. - spring.ai.vertex.ai.gemini.chat.options.frequencyPenalty - spring.ai.vertex.ai.gemini.chat.options.presencePenalty - spring.ai.vertex.ai.gemini.chat.options.functions List of functions, identified by their names, to enable for function calling in a single prompt requests. Functions with those names must exist in the functionCallbacks registry. - spring.ai.vertex.ai.gemini.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false All properties prefixed with spring.ai.vertex.ai.gemini.chat.options can be overridden at runtime by adding a request specific Runtime options(#chat-options) to the Prompt call. Runtime options: The VertexAiGeminiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/main/java/org/springframework/ai/vertexai/gemini/VertexAiGeminiChatOptions.java) provides model configurations, such as the temperature, the topK, etc. On start-up, the default options can be configured with the VertexAiGeminiChatModel(api, options) constructor or the spring.ai.vertex.ai.chat.options.* properties. At runtime you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", VertexAiGeminiChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific VertexAiGeminiChatOptions you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Function Calling: As of 30th of April 2023, the Vertex AI Gemini Pro model has significantly degraded the support for function calling! While the feature is still available, it is not recommended for production use. Apparently the Gemini Pro can not handle anymore the function name correctly. The parallel function calling is gone as well. You can register custom Java functions with the VertexAiGeminiChatModel and have the Gemini Pro model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This is a powerful technique to connect the LLM capabilities with external tools and APIs. Read more about Vertex AI Gemini Function Calling(functions/vertexai-gemini-chat-functions.html) . Multimodal: Multimodality refers to a model’s ability to simultaneously understand and process information from various sources, including text, images, audio, and other data formats. This paradigm represents a significant advancement in AI models. Google’s Gemini AI models support this capability by comprehending and integrating text, code, audio, images, and video. For more details, refer to the blog post Introducing Gemini(https://blog.google/technology/ai/google-gemini-ai/#introducing-gemini) . Spring AI’s Message interface supports multimodal AI models by introducing the Media type. This type contains data and information about media attachments in messages, using Spring’s org.springframework.util.MimeType and a java.lang.Object for the raw media data. Below is a simple code example extracted from VertexAiGeminiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/test/java/org/springframework/ai/vertexai/gemini/VertexAiGeminiChatModelIT.java) , demonstrating the combination of user text with an image. byte[] data = new ClassPathResource(""/vertex-test.png"").getContentAsByteArray(); var userMessage = new UserMessage(""Explain what do you see o this picture?"", List.of(new Media(MimeTypeUtils.IMAGE_PNG, data))); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage))); Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-vertex-ai-gemini-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the VertexAi chat model: spring.ai.vertex.ai.gemini.project-id=PROJECT_ID spring.ai.vertex.ai.gemini.location=LOCATION spring.ai.vertex.ai.gemini.chat.options.model=vertex-pro-vision spring.ai.vertex.ai.gemini.chat.options.temperature=0.5 replace the api-key with your VertexAI credentials. This will create a VertexAiGeminiChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final VertexAiGeminiChatModel chatModel; @Autowired public ChatController(VertexAiGeminiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The VertexAiGeminiChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/main/java/org/springframework/ai/vertexai/gemini/VertexAiGeminiChatModel.java) implements the ChatModel and uses the VertexAI to connect to the Vertex AI Gemini service. Add the spring-ai-vertex-ai-gemini dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-gemini</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-gemini' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a VertexAiGeminiChatModel and use it for text generations: VertexAI vertexApi = new VertexAI(projectId, location); var chatModel = new VertexAiGeminiChatModel(vertexApi, VertexAiGeminiChatOptions.builder() .withModel(ChatModel.GEMINI_PRO_1_5_PRO) .withTemperature(0.4) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); The VertexAiGeminiChatOptions provides the configuration information for the chat requests. The VertexAiGeminiChatOptions.Builder is fluent options builder. Low-level Java Client: Following class diagram illustrates the Vertex AI Gemini native Java API: VertexAI PaLM2(vertexai-palm2-chat.html) Function Calling(functions/vertexai-gemini-chat-functions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/functions/vertexai-gemini-chat-functions.html","Gemini Function Calling: WARNING: Apparently the Gemini Pro can not handle anymore the function name correctly. The parallel function calling is gone as well. Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with. You can register custom Java functions with the VertexAiGeminiChatModel and have the Gemini Pro model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This allows you to connect the LLM capabilities with external tools and APIs. The VertexAI Gemini Pro model is trained to detect when a function should be called and to respond with JSON that adheres to the function signature. The VertexAI Gemini API does not call the function directly; instead, the model generates JSON that you can use to call the function in your code and return the result back to the model to complete the conversation. Spring AI provides flexible and user-friendly ways to register and call custom functions. In general, the custom functions need to provide a function name , description , and the function call signature (as Open API schema) to let the model know what arguments the function expects. The description helps the model to understand when to call the function. As a developer, you need to implement a function that takes the function call arguments sent from the AI model, and responds with the result back to the model. Your function can in turn invoke other 3rd party services to provide the results. Spring AI makes this as easy as defining a @Bean definition that returns a java.util.Function and supplying the bean name as an option when invoking the ChatModel . Under the hood, Spring wraps your POJO (the function) with the appropriate adapter code that enables interaction with the AI Model, saving you from writing tedious boilerplate code. The basis of the underlying infrastructure is the FunctionCallback.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallback.java) interface and the companion FunctionCallbackWrapper.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallbackWrapper.java) utility class to simplify the implementation and registration of Java callback functions. How it works: Suppose we want the AI model to respond with information that it does not have, for example the current temperature at a given location. We can provide the AI model with metadata about our own functions that it can use to retrieve that information as it processes your prompt. For example, if during the processing of a prompt, the AI Model determines that it needs additional information about the temperature in a given location, it will start a server side generated request/response interaction. The AI Model invokes a client side function. The AI Model provides method invocation details as JSON and it is the responsibility of the client to execute that function and return the response. Spring AI greatly simplifies the code you need to write to support function invocation. It brokers the function invocation conversation for you. You can simply provide your function definition as a @Bean and then provide the bean name of the function in your prompt options. You can also reference multiple function bean names in your prompt. Quick Start: Let’s create a chatbot that answer questions by calling our own function. To support the response of the chatbot, we will register our own function that takes a location and returns the current weather in that location. When the response to the prompt to the model needs to answer a question such as ""What’s the weather like in Boston?"" the AI model will invoke the client providing the location value as an argument to be passed to the function. This RPC-like data is passed as JSON. Our function can some SaaS based weather service API and returns the weather response back to the model to complete the conversation. In this example we will use a simple implementation named MockWeatherService that hard codes the temperature for various locations. The following MockWeatherService.java represents the weather service API: public class MockWeatherService implements Function<Request, Response> { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } Registering Functions as Beans: With the VertexAiGeminiChatModel Auto-Configuration(../vertexai-gemini-chat.html#_auto_configuration) you have multiple ways to register custom functions as beans in the Spring context. We start with describing the most POJO friendly options. Plain Java Functions: In this approach you define @Beans in your application context as you would any other Spring managed object. Internally, Spring AI ChatModel will create an instance of a FunctionCallbackWrapper wrapper that adds the logic for it being invoked via the AI model. The name of the @Bean is passed as a ChatOption . @Configuration static class Config { @Bean @Description(""Get the weather in location"") // function description public Function<MockWeatherService.Request, MockWeatherService.Response> weatherFunction1() { return new MockWeatherService(); } ... } The @Description annotation is optional and provides a function description (2) that helps the model understand when to call the function. It is an important property to set to help the AI model determine what client side function to invoke. Another option to provide the description of the function is to use the @JsonClassDescription annotation on the MockWeatherService.Request to provide the function description: @Configuration static class Config { @Bean public Function<Request, Response> currentWeather3() { // (1) bean name as function name. return new MockWeatherService(); } ... } @JsonClassDescription(""Get the weather in location"") // (2) function description public record Request(String location, Unit unit) {} It is a best practice to annotate the request object with information such that the generated JSON schema of that function is as descriptive as possible to help the AI model pick the correct function to invoke. The FunctionCallWithFunctionBeanIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/gemini/tool/FunctionCallWithFunctionBeanIT.java) demonstrates this approach. FunctionCallback Wrapper: Another way to register a function is to create a FunctionCallbackWrapper wrapper like this: @Configuration static class Config { @Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") // (1) function name .withDescription(""Get the current weather in a given location"") // (2) function description .withSchemaType(SchemaType.OPEN_API) // (3) schema type. Compulsory for Gemini function calling. .build(); } ... } It wraps the 3rd party MockWeatherService function and registers it as a CurrentWeather function with the VertexAiGeminiChatModel . It also provides a description (2) and sets the Schema type to Open API type (3). The default response converter does a JSON serialization of the Response object. The FunctionCallbackWrapper internally resolves the function call signature based on the MockWeatherService.Request class and internally generates an Open API schema for the function call. Specifying functions in Chat Options: To let the model know and call your CurrentWeather function you need to enable it in your prompt requests: VertexAiGeminiChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris?""); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), VertexAiGeminiChatOptions.builder().withFunction(""CurrentWeather"").build())); // (1) Enable the function logger.info(""Response: {}"", response); Above user question will trigger 3 calls to CurrentWeather function (one for each city) and the final response will be something like this: Here is the current weather for the requested cities: - San Francisco, CA: 30.0°C - Tokyo, Japan: 10.0°C - Paris, France: 15.0°C The FunctionCallWithFunctionWrapperIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/gemini/tool/FunctionCallWithFunctionWrapperIT.java) test demo this approach. Register/Call Functions with Prompt Options: In addition to the auto-configuration you can register callback functions, dynamically, with your Prompt requests: VertexAiGeminiChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris? Use Multi-turn function calling.""); var promptOptions = VertexAiGeminiChatOptions.builder() .withFunctionCallbacks(List.of(FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") .withSchemaType(SchemaType.OPEN_API) // IMPORTANT!! .withDescription(""Get the weather in location"") .build())) .build(); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), promptOptions)); The in-prompt registered functions are enabled by default for the duration of this request. This approach allows to dynamically chose different functions to be called based on the user input. The FunctionCallWithPromptFunctionIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/gemini/tool/FunctionCallWithPromptFunctionIT.java) integration test provides a complete example of how to register a function with the VertexAiGeminiChatModel and use it in a prompt request. VertexAI Gemini(../vertexai-gemini-chat.html) Groq(../groq-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/groq-chat.html","Groq Chat: Groq(https://groq.com/) is an extremely fast, LPU™ based, AI Inference Engine that support various AI Models(https://console.groq.com/docs/models) , supports Tool/Function Calling and exposes a OpenAI API compatible endpoint. Spring AI integrates with the Groq(https://groq.com/) by reusing the existing OpenAI(openai-chat.html) client. For this you need to obtain a Groq Api Key(https://console.groq.com/keys) , set the base-url to api.groq.com/openai(https://api.groq.com/openai) and select one of the provided Groq models(https://console.groq.com/docs/models) . The Groq API is not fully compatible with the OpenAI API. Be aware for the following compatability constrains(https://console.groq.com/docs/openai) . Additionally, currently Groq doesn’t support multimodal messages. Check the GroqWithOpenAiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/GroqWithOpenAiChatModelIT.java) tests for examples of using Groq with Spring AI. Prerequisites: Create an API Key. Please visit here(https://console.groq.com/keys) to create an API Key. The Spring AI project defines a configuration property named spring.ai.openai.api-key that you should set to the value of the API Key obtained from groq.com. Set the Groq URL. You have to set the spring.ai.openai.base-url property to api.groq.com/openai(https://api.groq.com/openai) . Select a Groq Model(https://console.groq.com/docs/models) . Use the spring.ai.openai.chat.model=<model name> property to set the Model. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_OPENAI_API_KEY=<INSERT GROQ API KEY HERE> export SPRING_AI_OPENAI_BASE_URL=https://api.groq.com/openai export SPRING_AI_OPENAI_CHAT_MODEL=llama3-70b-8192 Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OpenAI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the OpenAI chat model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.openai.base-url The URL to connect to. Must be set to api.groq.com/openai(https://api.groq.com/openai) - spring.ai.openai.api-key The Groq API Key - Configuration Properties: The prefix spring.ai.openai.chat is the property prefix that lets you configure the chat model implementation for OpenAI. Property Description Default spring.ai.openai.chat.enabled Enable OpenAI chat model. true spring.ai.openai.chat.base-url Optional overrides the spring.ai.openai.base-url to provide chat specific url. Must be set to api.groq.com/openai(https://api.groq.com/openai) - spring.ai.openai.chat.api-key Optional overrides the spring.ai.openai.api-key to provide chat specific api-key - spring.ai.openai.chat.options.model The avalable model(https://console.groq.com/docs/models) names are llama3-8b-8192 , llama3-70b-8192 , mixtral-8x7b-32768 , gemma-7b-it . - spring.ai.openai.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.8 spring.ai.openai.chat.options.frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim. 0.0f spring.ai.openai.chat.options.maxTokens The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. - spring.ai.openai.chat.options.n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs. 1 spring.ai.openai.chat.options.presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics. - spring.ai.openai.chat.options.responseFormat An object specifying the format that the model must output. Setting to { ""type"": ""json_object"" } enables JSON mode, which guarantees the message the model generates is valid JSON. - spring.ai.openai.chat.options.seed This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. - spring.ai.openai.chat.options.stop Up to 4 sequences where the API will stop generating further tokens. - spring.ai.openai.chat.options.topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. - spring.ai.openai.chat.options.tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. - spring.ai.openai.chat.options.toolChoice Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {""type: ""function"", ""function"": {""name"": ""my_function""}} forces the model to call that function. none is the default when no functions are present. auto is the default if functions are present. - spring.ai.openai.chat.options.user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. - spring.ai.openai.chat.options.functions List of functions, identified by their names, to enable for function calling in a single prompt requests. Functions with those names must exist in the functionCallbacks registry. - spring.ai.openai.chat.options.stream-usage (For streaming only) Set to add an additional chunk with token usage statistics for the entire request. The choices field for this chunk is an empty array and all other chunks will also include a usage field, but with a null value. false spring.ai.openai.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false All properties prefixed with spring.ai.openai.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The OpenAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the OpenAiChatModel(api, options) constructor or the spring.ai.openai.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", OpenAiChatOptions.builder() .withModel(""mixtral-8x7b-32768"") .withTemperature(0.4) .build() )); In addition to the model specific OpenAiChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Function Calling: Groq API endpoints support tool/function calling(https://console.groq.com/docs/tool-use) when selecting one of the Tool/Function supporting models. Check the Tool Supported Models(https://console.groq.com/docs/tool-use) . You can register custom Java functions with your ChatModel and have the provided Groq model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This is a powerful technique to connect the LLM capabilities with external tools and APIs. Tool Example: Here’s a simple example of how to use Groq function calling with Spring AI: @SpringBootApplication public class GroqApplication { public static void main(String[] args) { SpringApplication.run(GroqApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -> { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(""What is the weather in Amsterdam and Paris?"") .functions(""weatherFunction"") // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(""Get the weather in location"") public Function<WeatherRequest, WeatherResponse> weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function<WeatherRequest, WeatherResponse> { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(""Amsterdam"") ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } In this example, when the model needs weather information, it will automatically call the weatherFunction bean, which can then fetch real-time weather data. The expected response looks like this: ""The weather in Amsterdam is currently 20 degrees Celsius, and the weather in Paris is currently 25 degrees Celsius."" Read more about OpenAI Function Calling(https://docs.spring.io/spring-ai/reference/api/chat/functions/openai-chat-functions.html) . Multimodal: Currently the Groq API doesn’t support media content. Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-openai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the OpenAi chat model: spring.ai.openai.api-key=<GROQ_API_KEY> spring.ai.openai.base-url=https://api.groq.com/openai spring.ai.openai.chat.options.model=llama3-70b-8192 spring.ai.openai.chat.options.temperature=0.7 replace the api-key with your OpenAI credentials. This will create a OpenAiChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The OpenAiChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatModel.java) implements the ChatModel and StreamingChatModel and uses the [low-level-api](#low-level-api) to connect to the OpenAI service. Add the spring-ai-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a OpenAiChatModel and use it for text generations: var openAiApi = new OpenAiApi(""https://api.groq.com/openai"", System.getenv(""GROQ_API_KEY"")); var openAiChatOptions = OpenAiChatOptions.builder() .withModel(""llama3-70b-8192"") .withTemperature(0.4) .withMaxTokens(200) .build(); var chatModel = new OpenAiChatModel(openAiApi, openAiChatOptions); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); The OpenAiChatOptions provides the configuration information for the chat requests. The OpenAiChatOptions.Builder is fluent options builder. Function Calling(functions/vertexai-gemini-chat-functions.html) Hugging Face(huggingface.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/huggingface.html","Hugging Face Chat: Hugging Face Inference Endpoints allow you to deploy and serve machine learning models in the cloud, making them accessible via an API. Prerequisites: You will need to create an Inference Endpoint on Hugging Face and create an API token to access the endpoint. Further details can be found here(https://huggingface.co/docs/inference-endpoints/index) . The Spring AI project defines a configuration property named spring.ai.huggingface.chat.api-key that you should set to the value of the API token obtained from Hugging Face. There is also a configuration property named spring.ai.huggingface.chat.url that you should set to the inference endpoint URL obtained when provisioning your model in Hugging Face. You can find this on the Inference Endpoint’s UI here(https://ui.endpoints.huggingface.co/) . Exporting environment variables is one way to set these configuration properties: export SPRING_AI_HUGGINGFACE_CHAT_API_KEY=<INSERT KEY HERE> export SPRING_AI_HUGGINGFACE_CHAT_URL=<INSERT INFERENCE ENDPOINT URL HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Hugging Face Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-huggingface-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-huggingface-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: The prefix spring.ai.huggingface is the property prefix that lets you configure the chat model implementation for Hugging Face. Property Description Default spring.ai.huggingface.chat.api-key API Key to authenticate with the Inference Endpoint. - spring.ai.huggingface.chat.url URL of the Inference Endpoint to connect to - spring.ai.huggingface.chat.enabled Enable Hugging Face chat model. true Sample Controller (Auto-configuration): Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-huggingface-spring-boot-starter to your pom (or gradle) dependencies. Add an application.properties file, under the src/main/resources directory, to enable and configure the Hugging Face chat model: spring.ai.huggingface.chat.api-key=YOUR_API_KEY spring.ai.huggingface.chat.url=YOUR_INFERENCE_ENDPOINT_URL replace the api-key and url with your Hugging Face values. This will create a HuggingfaceChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final HuggingfaceChatModel chatModel; @Autowired public ChatController(HuggingfaceChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } } Manual Configuration: The HuggingfaceChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-huggingface/src/main/java/org/springframework/ai/huggingface/HuggingfaceChatModel.java) implements the ChatModel interface and uses the [low-level-api](#low-level-api) to connect to the Hugging Face inference endpoints. Add the spring-ai-huggingface dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-huggingface</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-huggingface' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a HuggingfaceChatModel and use it for text generations: HuggingfaceChatModel chatModel = new HuggingfaceChatModel(apiKey, url); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); System.out.println(response.getGeneration().getResult().getOutput().getContent()); Groq(groq-chat.html) Mistral AI(mistralai-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/mistralai-chat.html","Mistral AI Chat: Spring AI supports the various AI language models from Mistral AI. You can interact with Mistral AI language models and create a multilingual conversational assistant based on Mistral models. Mistral AI offers an OpenAI API-compatible endpoint as well. Check the OpenAI API compatibility(#_openai_api_compatibility) section to learn how to use the Spring AI OpenAI(openai-chat.html) integration to talk to a Mistral endpoint. Prerequisites: You will need to create an API with Mistral AI to access Mistral AI language models. Create an account at Mistral AI registration page(https://auth.mistral.ai/ui/registration) and generate the token on the API Keys page(https://console.mistral.ai/api-keys/) . The Spring AI project defines a configuration property named spring.ai.mistralai.api-key that you should set to the value of the API Key obtained from console.mistral.ai. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_MISTRALAI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Mistral AI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-mistral-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-mistral-ai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the Mistral AI chat model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.mistralai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.mistralai.base-url The URL to connect to api.mistral.ai(https://api.mistral.ai) spring.ai.mistralai.api-key The API Key - Configuration Properties: The prefix spring.ai.mistralai.chat is the property prefix that lets you configure the chat model implementation for Mistral AI. Property Description Default spring.ai.mistralai.chat.enabled Enable Mistral AI chat model. true spring.ai.mistralai.chat.base-url Optional override for the spring.ai.mistralai.base-url property to provide chat-specific URL. - spring.ai.mistralai.chat.api-key Optional override for the spring.ai.mistralai.api-key to provide chat-specific API Key. - spring.ai.mistralai.chat.options.model This is the Mistral AI Chat model to use open-mistral-7b , open-mixtral-8x7b , open-mixtral-8x22b , mistral-small-latest , mistral-large-latest spring.ai.mistralai.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.8 spring.ai.mistralai.chat.options.maxTokens The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. - spring.ai.mistralai.chat.options.safePrompt Indicates whether to inject a security prompt before all conversations. false spring.ai.mistralai.chat.options.randomSeed This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. - spring.ai.mistralai.chat.options.stop Stop generation if this token is detected. Or if one of these tokens is detected when providing an array. - spring.ai.mistralai.chat.options.topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. - spring.ai.mistralai.chat.options.responseFormat An object specifying the format that the model must output. Setting to { ""type"": ""json_object"" } enables JSON mode, which guarantees the message the model generates is valid JSON. - spring.ai.mistralai.chat.options.tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. - spring.ai.mistralai.chat.options.toolChoice Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {""type: ""function"", ""function"": {""name"": ""my_function""}} forces the model to call that function. none is the default when no functions are present. auto is the default if functions are present. - spring.ai.mistralai.chat.options.functions List of functions, identified by their names, to enable for function calling in a single prompt requests. Functions with those names must exist in the functionCallbacks registry. - spring.ai.mistralai.chat.options.functionCallbacks Mistral AI Tool Function Callbacks to register with the ChatModel. - spring.ai.mistralai.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false You can override the common spring.ai.mistralai.base-url and spring.ai.mistralai.api-key for the ChatModel and EmbeddingModel implementations. The spring.ai.mistralai.chat.base-url and spring.ai.mistralai.chat.api-key properties, if set, take precedence over the common properties. This is useful if you want to use different Mistral AI accounts for different models and different model endpoints. All properties prefixed with spring.ai.mistralai.chat.options can be overridden at runtime by adding request-specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The MistralAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/MistralAiChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the MistralAiChatModel(api, options) constructor or the spring.ai.mistralai.chat.options.* properties. At run-time, you can override the default options by adding new, request-specific options to the Prompt call. For example, to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", MistralAiChatOptions.builder() .withModel(MistralAiApi.ChatModel.LARGE.getValue()) .withTemperature(0.5) .build() )); In addition to the model specific MistralAiChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/MistralAiChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Function Calling: You can register custom Java functions with the MistralAiChatModel and have the Mistral AI model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This is a powerful technique to connect the LLM capabilities with external tools and APIs. Read more about Mistral AI Function Calling(functions/mistralai-chat-functions.html) . OpenAI API Compatibility: Mistral is OpenAI API-compatible and you can use the Spring AI OpenAI(openai-chat.html) client to talk to Mistrial. For this, you need to configure the OpenAI base URL to the Mistral AI platform: spring.ai.openai.chat.base-url=https://api.mistral.ai , and select a Mistral model: spring.ai.openai.chat.options.model=mistral-small-latest and set the Mistral AI API key: spring.ai.openai.chat.api-key=<YOUR MISTRAL API KEY . Check the MistralWithOpenAiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/MistralWithOpenAiChatModelIT.java) tests for examples of using Mistral over Spring AI OpenAI. Sample Controller (Auto-configuration): Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-mistral-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file under the src/main/resources directory to enable and configure the Mistral AI chat model: spring.ai.mistralai.api-key=YOUR_API_KEY spring.ai.mistralai.chat.options.model=mistral-small spring.ai.mistralai.chat.options.temperature=0.7 Replace the api-key with your Mistral AI credentials. This will create a MistralAiChatModel implementation that you can inject into your classes. Here is an example of a simple @RestController class that uses the chat model for text generations. @RestController public class ChatController { private final MistralAiChatModel chatModel; @Autowired public ChatController(MistralAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map<String,String> generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The MistralAiChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/MistralAiChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level MistralAiApi Client(#low-level-api) to connect to the Mistral AI service. Add the spring-ai-mistral-ai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-mistral-ai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-mistral-ai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a MistralAiChatModel and use it for text generations: var mistralAiApi = new MistralAiApi(System.getenv(""MISTRAL_AI_API_KEY"")); var chatModel = new MistralAiChatModel(mistralAiApi, MistralAiChatOptions.builder() .withModel(MistralAiApi.ChatModel.LARGE.getValue()) .withTemperature(0.4) .withMaxTokens(200) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); The MistralAiChatOptions provides the configuration information for the chat requests. The MistralAiChatOptions.Builder is a fluent options-builder. Low-level MistralAiApi Client: The MistralAiApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/MistralAiApi.java) provides is lightweight Java client for Mistral AI API(https://docs.mistral.ai/api/) . Here is a simple snippet showing how to use the API programmatically: MistralAiApi mistralAiApi = new MistralAiApi(System.getenv(""MISTRAL_AI_API_KEY"")); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(""Hello world"", Role.USER); // Sync request ResponseEntity<ChatCompletion> response = mistralAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), MistralAiApi.ChatModel.LARGE.getValue(), 0.8, false)); // Streaming request Flux<ChatCompletionChunk> streamResponse = mistralAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), MistralAiApi.ChatModel.LARGE.getValue(), 0.8, true)); Follow the MistralAiApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/api/MistralAiApi.java) 's JavaDoc for further information. MistralAiApi Samples: The MistralAiApiIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/test/java/org/springframework/ai/mistralai/api/MistralAiApiIT.java) tests provide some general examples of how to use the lightweight library. The PaymentStatusFunctionCallingIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/test/java/org/springframework/ai/mistralai/api/tool/PaymentStatusFunctionCallingIT.java) tests show how to use the low-level API to call tool functions. Based on the Mistral AI Function Calling(https://docs.mistral.ai/guides/function-calling/) tutorial. Hugging Face(huggingface.html) Function Calling(functions/mistralai-chat-functions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/functions/mistralai-chat-functions.html","Mistral AI Function Calling: You can register custom Java functions with the MistralAiChatModel and have the Mistral AI models intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This allows you to connect the LLM capabilities with external tools and APIs. The open-mixtral-8x22b , mistral-small-latest , and mistral-large-latest models are trained to detect when a function should be called and to respond with JSON that adheres to the function signature. The Mistral AI API does not call the function directly; instead, the model generates JSON that you can use to call the function in your code and return the result back to the model to complete the conversation. Spring AI provides flexible and user-friendly ways to register and call custom functions. In general, the custom functions need to provide a function name , description , and the function call signature (as JSON schema) to let the model know what arguments the function expects. The description helps the model to understand when to call the function. As a developer, you need to implement a function that takes the function call arguments sent from the AI model, and responds with the result back to the model. Your function can in turn invoke other 3rd party services to provide the results. Spring AI makes this as easy as defining a @Bean definition that returns a java.util.Function and supplying the bean name as an option when invoking the ChatModel . Under the hood, Spring wraps your POJO (the function) with the appropriate adapter code that enables interaction with the AI Model, saving you from writing tedious boilerplate code. The basis of the underlying infrastructure is the FunctionCallback.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallback.java) interface and the companion FunctionCallbackWrapper.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallbackWrapper.java) utility class to simplify the implementation and registration of Java callback functions. How it works: Suppose we want the AI model to respond with information that it does not have, for example, the current temperature at a given location. We can provide the AI model with metadata about our own functions that it can use to retrieve that information as it processes your prompt. For example, if during the processing of a prompt, the AI Model determines that it needs additional information about the temperature in a given location, it will start a server-side generated request/response interaction. The AI Model invokes a client side function. The AI Model provides method invocation details as JSON, and it is the responsibility of the client to execute that function and return the response. Spring AI greatly simplifies the code you need to write to support function invocation. It brokers the function invocation conversation for you. You can simply provide your function definition as a @Bean and then provide the bean name of the function in your prompt options. You can also reference multiple function bean names in your prompt. Quick Start: Let’s create a chatbot that answer questions by calling our own function. To support the response of the chatbot, we will register our own function that takes a location and returns the current weather in that location. When the model needs to answer a question such as ""What’s the weather like in Boston?"" the AI model will invoke the client providing the location value as an argument to be passed to the function. This RPC-like data is passed as JSON. Our function calls some SaaS-based weather service API and returns the weather response back to the model to complete the conversation. In this example, we will use a simple implementation named MockWeatherService that hard-codes the temperature for various locations. The following MockWeatherService.java represents the weather service API: public class MockWeatherService implements Function<Request, Response> { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } Registering Functions as Beans: With the MistralAiChatModel Auto-Configuration(../mistralai-chat.html#_auto_configuration) you have multiple ways to register custom functions as beans in the Spring context. We start by describing the most POJO-friendly options. Plain Java Functions: In this approach, you define a @Bean in your application context as you would any other Spring managed object. Internally, Spring AI ChatModel will create an instance of a FunctionCallbackWrapper that adds the logic for it being invoked via the AI model. The name of the @Bean is passed as a ChatOption . @Configuration static class Config { @Bean @Description(""Get the weather in location"") // function description public Function<MockWeatherService.Request, MockWeatherService.Response> currentWeather() { return new MockWeatherService(); } } The @Description annotation is optional and provides a function description that helps the model understand when to call the function. It is an important property to set to help the AI model determine what client side function to invoke. Another option for providing the description of the function is to use the @JsonClassDescription annotation on the MockWeatherService.Request : @Configuration static class Config { @Bean public Function<Request, Response> currentWeather() { // bean name as function name return new MockWeatherService(); } } @JsonClassDescription(""Get the weather in location"") // // function description public record Request(String location, Unit unit) {} It is a best practice to annotate the request object with information such that the generated JSON schema of that function is as descriptive as possible to help the AI model pick the correct function to invoke. The PaymentStatusBeanIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/mistralai/tool/PaymentStatusBeanIT.java) demonstrates this approach. The Mistral AI PaymentStatusBeanOpenAiIT(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/mistralai/tool/PaymentStatusBeanOpenAiIT) implements the same function using the OpenAI API. Mistral AI is almost identical to OpenAI in this regard. FunctionCallback Wrapper: Another way to register a function is to create a FunctionCallbackWrapper like this: @Configuration static class Config { @Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") // (1) function name .withDescription(""Get the weather in location"") // (2) function description .build(); } } It wraps the 3rd party MockWeatherService function and registers it as a CurrentWeather function with the MistralAiChatModel . It also provides a description (2) and an optional response converter to convert the response into a text as expected by the model. By default, the response converter performs a JSON serialization of the Response object. The FunctionCallbackWrapper internally resolves the function call signature based on the MockWeatherService.Request class. Specifying functions in Chat Options: To let the model know and call your CurrentWeather function you need to enable it in your prompt requests: MistralAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in Paris?""); ChatResponse response = chatModel.call(new Prompt(userMessage, MistralAiChatOptions.builder().withFunction(""CurrentWeather"").build())); // Enable the function logger.info(""Response: {}"", response); The above user question will trigger 3 calls to the CurrentWeather function (one for each city) and the final response will be something like this: Register/Call Functions with Prompt Options: In addition to the auto-configuration, you can register callback functions, dynamically, with your Prompt requests: MistralAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in Paris?""); var promptOptions = MistralAiChatOptions.builder() .withFunctionCallbacks(List.of(new FunctionCallbackWrapper<>( ""CurrentWeather"", // name ""Get the weather in location"", // function description new MockWeatherService()))) // function code .build(); ChatResponse response = chatModel.call(new Prompt(userMessage, promptOptions)); The in-prompt registered functions are enabled by default for the duration of this request. This approach allows to choose dynamically different functions to be called based on the user input. The PaymentStatusPromptIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/mistralai/tool/PaymentStatusPromptIT.java) integration test provides a complete example of how to register a function with the MistralAiChatModel and use it in a prompt request. Appendices: (Blog) Function Calling in Java and Spring AI using the latest Mistral AI API: Mistral AI API Function Calling Flow: The following diagram illustrates the flow of the Mistral AI low-level API for Function Calling(https://docs.mistral.ai/guides/function-calling) : (https://docs.mistral.ai/guides/function-calling) The PaymentStatusFunctionCallingIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/test/java/org/springframework/ai/mistralai/api/tool/PaymentStatusFunctionCallingIT.java) provides a complete example on how to use the Mistral AI API function calling. It is based on the Mistral AI Function Calling tutorial(https://docs.mistral.ai/guides/function-calling) . Mistral AI(../mistralai-chat.html) MiniMax(../minimax-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/minimax-chat.html","⇐ MiniMax Chat Spring AI supports the various AI language models from MiniMax. You can interact with MiniMax language models and create a multilingual conversational assistant based on MiniMax models. Prerequisites: You will need to create an API with MiniMax to access MiniMax language models. Create an account at MiniMax registration page(https://www.minimaxi.com/login) and generate the token on the API Keys page(https://www.minimaxi.com/user-center/basic-information/interface-key) . The Spring AI project defines a configuration property named spring.ai.minimax.api-key that you should set to the value of the API Key obtained from API Keys page(https://www.minimaxi.com/user-center/basic-information/interface-key) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_MINIMAX_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the MiniMax Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-minimax-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-minimax-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the MiniMax chat model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.minimax is used as the property prefix that lets you connect to MiniMax. Property Description Default spring.ai.minimax.base-url The URL to connect to api.minimax.chat(https://api.minimax.chat) spring.ai.minimax.api-key The API Key - Configuration Properties: The prefix spring.ai.minimax.chat is the property prefix that lets you configure the chat model implementation for MiniMax. Property Description Default spring.ai.minimax.chat.enabled Enable MiniMax chat model. true spring.ai.minimax.chat.base-url Optional overrides the spring.ai.minimax.base-url to provide chat specific url api.minimax.chat(https://api.minimax.chat) spring.ai.minimax.chat.api-key Optional overrides the spring.ai.minimax.api-key to provide chat specific api-key - spring.ai.minimax.chat.options.model This is the MiniMax Chat model to use abab6.5g-chat (the abab5.5-chat , abab5.5s-chat , abab6.5-chat , abab6.5g-chat , abab6.5t-chat and abab6.5s-chat point to the latest model versions) spring.ai.minimax.chat.options.maxTokens The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. - spring.ai.minimax.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.7 spring.ai.minimax.chat.options.topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. 1.0 spring.ai.minimax.chat.options.n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Default value is 1 and cannot be greater than 5. Specifically, when the temperature is very small and close to 0, we can only return 1 result. If n is already set and>1 at this time, service will return an illegal input parameter (invalid_request_error) 1 spring.ai.minimax.chat.options.presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics. 0.0f spring.ai.minimax.chat.options.frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim. 0.0f spring.ai.minimax.chat.options.stop The model will stop generating characters specified by stop, and currently only supports a single stop word in the format of [""stop_word1""] - You can override the common spring.ai.minimax.base-url and spring.ai.minimax.api-key for the ChatModel implementations. The spring.ai.minimax.chat.base-url and spring.ai.minimax.chat.api-key properties if set take precedence over the common properties. This is useful if you want to use different MiniMax accounts for different models and different model endpoints. All properties prefixed with spring.ai.minimax.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The MiniMaxChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/MiniMaxChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the MiniMaxChatModel(api, options) constructor or the spring.ai.minimax.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", MiniMaxChatOptions.builder() .withModel(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue()) .withTemperature(0.5) .build() )); In addition to the model specific MiniMaxChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/MiniMaxChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-minimax-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the MiniMax chat model: spring.ai.minimax.api-key=YOUR_API_KEY spring.ai.minimax.chat.options.model=abab6.5g-chat spring.ai.minimax.chat.options.temperature=0.7 replace the api-key with your MiniMax credentials. This will create a MiniMaxChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final MiniMaxChatModel chatModel; @Autowired public ChatController(MiniMaxChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The MiniMaxChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/MiniMaxChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level MiniMaxApi Client(#low-level-api) to connect to the MiniMax service. Add the spring-ai-minimax dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-minimax</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-minimax' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a MiniMaxChatModel and use it for text generations: var miniMaxApi = new MiniMaxApi(System.getenv(""MINIMAX_API_KEY"")); var chatModel = new MiniMaxChatModel(miniMaxApi, MiniMaxChatOptions.builder() .withModel(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue()) .withTemperature(0.4) .withMaxTokens(200) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> streamResponse = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); The MiniMaxChatOptions provides the configuration information for the chat requests. The MiniMaxChatOptions.Builder is fluent options builder. Low-level MiniMaxApi Client: The MiniMaxApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/MiniMaxApi.java) provides is lightweight Java client for MiniMax API(https://www.minimaxi.com/document/guides/chat-model/V2) . Here is a simple snippet how to use the api programmatically: MiniMaxApi miniMaxApi = new MiniMaxApi(System.getenv(""MINIMAX_API_KEY"")); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(""Hello world"", Role.USER); // Sync request ResponseEntity<ChatCompletion> response = miniMaxApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue(), 0.7f, false)); // Streaming request Flux<ChatCompletionChunk> streamResponse = miniMaxApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.getValue(), 0.7f, true)); Follow the MiniMaxApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/api/MiniMaxApi.java) 's JavaDoc for further information. WebSearch chat: The MiniMax model supported the web search feature. The web search feature allows you to search the web for information and return the results in the chat response. About web search follow the MiniMax ChatCompletion(https://platform.minimaxi.com/document/ChatCompletion%20v2) for further information. Here is a simple snippet how to use the web search: UserMessage userMessage = new UserMessage( ""How many gold medals has the United States won in total at the 2024 Olympics?""); List<Message> messages = new ArrayList<>(List.of(userMessage)); List<MiniMaxApi.FunctionTool> functionTool = List.of(MiniMaxApi.FunctionTool.webSearchFunctionTool()); MiniMaxChatOptions options = MiniMaxChatOptions.builder() .withModel(MiniMaxApi.ChatModel.ABAB_6_5_S_Chat.value) .withTools(functionTool) .build(); // Sync request ChatResponse response = chatModel.call(new Prompt(messages, options)); // Streaming request Flux<ChatResponse> streamResponse = chatModel.stream(new Prompt(messages, options)); MiniMaxApi Samples: The MiniMaxApiIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/test/java/org/springframework/ai/minimax/api/MiniMaxApiIT.java) test provides some general examples how to use the lightweight library. The MiniMaxApiToolFunctionCallIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/test/java/org/springframework/ai/minimax/api/MiniMaxApiToolFunctionCallIT.java) test shows how to use the low-level API to call tool functions.> Function Calling(functions/mistralai-chat-functions.html) Function Calling(functions/minimax-chat-functions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/functions/minimax-chat-functions.html","Function Calling: You can register custom Java functions with the MiniMaxChatModel and have the MiniMax model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This allows you to connect the LLM capabilities with external tools and APIs. The MiniMax models are trained to detect when a function should be called and to respond with JSON that adheres to the function signature. The MiniMax API does not call the function directly; instead, the model generates JSON that you can use to call the function in your code and return the result back to the model to complete the conversation. Spring AI provides flexible and user-friendly ways to register and call custom functions. In general, the custom functions need to provide a function name , description , and the function call signature (as JSON schema) to let the model know what arguments the function expects. The description helps the model to understand when to call the function. As a developer, you need to implement a function that takes the function call arguments sent from the AI model, and responds with the result back to the model. Your function can in turn invoke other 3rd party services to provide the results. Spring AI makes this as easy as defining a @Bean definition that returns a java.util.Function and supplying the bean name as an option when invoking the ChatModel . Under the hood, Spring wraps your POJO (the function) with the appropriate adapter code that enables interaction with the AI Model, saving you from writing tedious boilerplate code. The basis of the underlying infrastructure is the FunctionCallback.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallback.java) interface and the companion FunctionCallbackWrapper.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallbackWrapper.java) utility class to simplify the implementation and registration of Java callback functions. How it works: Suppose we want the AI model to respond with information that it does not have, for example the current temperature at a given location. We can provide the AI model with metadata about our own functions that it can use to retrieve that information as it processes your prompt. For example, if during the processing of a prompt, the AI Model determines that it needs additional information about the temperature in a given location, it will start a server side generated request/response interaction. The AI Model invokes a client side function. The AI Model provides method invocation details as JSON and it is the responsibility of the client to execute that function and return the response. The model-client interaction is illustrated in the [spring-ai-function-calling-flow](#spring-ai-function-calling-flow) diagram. Spring AI greatly simplifies code you need to write to support function invocation. It brokers the function invocation conversation for you. You can simply provide your function definition as a @Bean and then provide the bean name of the function in your prompt options. You can also reference multiple function bean names in your prompt. Quick Start: Let’s create a chatbot that answer questions by calling our own function. To support the response of the chatbot, we will register our own function that takes a location and returns the current weather in that location. When the response to the prompt to the model needs to answer a question such as ""What’s the weather like in Boston?"" the AI model will invoke the client providing the location value as an argument to be passed to the function. This RPC-like data is passed as JSON. Our function calls some SaaS based weather service API and returns the weather response back to the model to complete the conversation. In this example we will use a simple implementation named MockWeatherService that hard codes the temperature for various locations. The following MockWeatherService.java represents the weather service API: public class MockWeatherService implements Function<Request, Response> { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } Registering Functions as Beans: With the MiniMaxChatModel Auto-Configuration(../minimax-chat.html#_auto_configuration) you have multiple ways to register custom functions as beans in the Spring context. We start with describing the most POJO friendly options. Plain Java Functions: In this approach you define @Beans in your application context as you would any other Spring managed object. Internally, Spring AI ChatModel will create an instance of a FunctionCallbackWrapper wrapper that adds the logic for it being invoked via the AI model. The name of the @Bean is passed as a ChatOption . @Configuration static class Config { @Bean @Description(""Get the weather in location"") // function description public Function<MockWeatherService.Request, MockWeatherService.Response> weatherFunction1() { return new MockWeatherService(); } ... } The @Description annotation is optional and provides a function description (2) that helps the model to understand when to call the function. It is an important property to set to help the AI model determine what client side function to invoke. Another option to provide the description of the function is to the @JacksonDescription annotation on the MockWeatherService.Request to provide the function description: @Configuration static class Config { @Bean public Function<Request, Response> currentWeather3() { // (1) bean name as function name. return new MockWeatherService(); } ... } @JsonClassDescription(""Get the weather in location"") // (2) function description public record Request(String location, Unit unit) {} It is a best practice to annotate the request object with information such that the generates JSON schema of that function is as descriptive as possible to help the AI model pick the correct function to invoke. The FunctionCallbackWithPlainFunctionBeanIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/minimax/FunctionCallbackWithPlainFunctionBeanIT.java) demonstrates this approach. FunctionCallback Wrapper: Another way register a function is to create FunctionCallbackWrapper wrapper like this: @Configuration static class Config { @Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") // (1) function name .withDescription(""Get the weather in location"") // (2) function description .build(); } ... } It wraps the 3rd party, MockWeatherService function and registers it as a CurrentWeather function with the MiniMaxChatModel . It also provides a description (2) and an optional response converter (3) to convert the response into a text as expected by the model. By default, the response converter does a JSON serialization of the Response object. The FunctionCallbackWrapper internally resolves the function call signature based on the MockWeatherService.Request class. Specifying functions in Chat Options: To let the model know and call your CurrentWeather function you need to enable it in your prompt requests: MiniMaxChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris?""); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), MiniMaxChatOptions.builder().withFunction(""CurrentWeather"").build())); // (1) Enable the function logger.info(""Response: {}"", response); Above user question will trigger 3 calls to CurrentWeather function (one for each city) and the final response will be something like this: Here is the current weather for the requested cities: - San Francisco, CA: 30.0°C - Tokyo, Japan: 10.0°C - Paris, France: 15.0°C The FunctionCallbackWrapperIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/minimax/tool/FunctionCallbackWrapperIT.java) test demo this approach. Register/Call Functions with Prompt Options: In addition to the auto-configuration you can register callback functions, dynamically, with your Prompt requests: MiniMaxChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris?""); var promptOptions = MiniMaxChatOptions.builder() .withFunctionCallbacks(List.of(new FunctionCallbackWrapper<>( ""CurrentWeather"", // name ""Get the weather in location"", // function description new MockWeatherService()))) // function code .build(); ChatResponse response = chatModel.call(new Prompt(List.of(userMessage), promptOptions)); The in-prompt registered functions are enabled by default for the duration of this request. This approach allows to dynamically chose different functions to be called based on the user input. The FunctionCallbackInPromptIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/minimax/FunctionCallbackInPromptIT.java) integration test provides a complete example of how to register a function with the MiniMaxChatModel and use it in a prompt request. MiniMax(../minimax-chat.html) Moonshot AI(../moonshot-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/moonshot-chat.html","Moonshot AI Chat: Spring AI supports the various AI language models from Moonshot AI. You can interact with Moonshot AI language models and create a multilingual conversational assistant based on Moonshot models. Prerequisites: You will need to create an API with Moonshot to access Moonshot AI language models. Create an account at Moonshot AI registration page(https://platform.moonshot.cn/console) and generate the token on the API Keys page(https://platform.moonshot.cn/console/api-keys/) . The Spring AI project defines a configuration property named spring.ai.moonshot.api-key that you should set to the value of the API Key obtained from API Keys page(https://platform.moonshot.cn/console/api-keys/) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_MOONSHOT_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Moonshot Chat Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-moonshot-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-moonshot-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the Moonshot AI Chat model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.moonshot is used as the property prefix that lets you connect to Moonshot. Property Description Default spring.ai.moonshot.base-url The URL to connect to api.moonshot.cn(https://api.moonshot.cn) spring.ai.moonshot.api-key The API Key - Configuration Properties: The prefix spring.ai.moonshot.chat is the property prefix that lets you configure the chat model implementation for Moonshot. Property Description Default spring.ai.moonshot.chat.enabled Enable Moonshot chat model. true spring.ai.moonshot.chat.base-url Optional overrides the spring.ai.moonshot.base-url to provide chat specific url - spring.ai.moonshot.chat.api-key Optional overrides the spring.ai.moonshot.api-key to provide chat specific api-key - spring.ai.moonshot.chat.options.model This is the Moonshot Chat model to use moonshot-v1-8k (the moonshot-v1-8k , moonshot-v1-32k , and moonshot-v1-128k point to the latest model versions) spring.ai.moonshot.chat.options.maxTokens The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. - spring.ai.moonshot.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.7 spring.ai.moonshot.chat.options.topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. 1.0 spring.ai.moonshot.chat.options.n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Default value is 1 and cannot be greater than 5. Specifically, when the temperature is very small and close to 0, we can only return 1 result. If n is already set and>1 at this time, service will return an illegal input parameter (invalid_request_error) 1 spring.ai.moonshot.chat.options.presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics. 0.0f spring.ai.moonshot.chat.options.frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim. 0.0f spring.ai.moonshot.chat.options.stop Up to 5 sequences where the API will stop generating further tokens. Each string must not exceed 32 bytes - You can override the common spring.ai.moonshot.base-url and spring.ai.moonshot.api-key for the ChatModel implementations. The spring.ai.moonshot.chat.base-url and spring.ai.moonshot.chat.api-key properties if set take precedence over the common properties. This is useful if you want to use different Moonshot accounts for different models and different model endpoints. All properties prefixed with spring.ai.moonshot.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The MoonshotChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-moonshot/src/main/java/org/springframework/ai/moonshot/MoonshotChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the MoonshotChatModel(api, options) constructor or the spring.ai.moonshot.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", MoonshotChatOptions.builder() .withModel(MoonshotApi.ChatModel.MOONSHOT_V1_8K.getValue()) .withTemperature(0.5) .build() )); In addition to the model specific MoonshotChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-moonshot/src/main/java/org/springframework/ai/moonshot/MoonshotChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/ChatOptionsBuilder.java) . Sample Controller (Auto-configuration): Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-moonshot-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Moonshot Chat model: spring.ai.moonshot.api-key=YOUR_API_KEY spring.ai.moonshot.chat.options.model=moonshot-v1-8k spring.ai.moonshot.chat.options.temperature=0.7 replace the api-key with your Moonshot credentials. This will create a MoonshotChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final MoonshotChatModel chatModel; @Autowired public ChatController(MoonshotChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The MoonshotChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-moonshot/src/main/java/org/springframework/ai/moonshot/MoonshotChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level Moonshot Api Client(#low-level-api) to connect to the Moonshot service. Add the spring-ai-moonshot dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-moonshot</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-moonshot' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a MoonshotChatModel and use it for text generations: var moonshotApi = new MoonshotApi(System.getenv(""MOONSHOT_API_KEY"")); var chatModel = new MoonshotChatModel(moonshotApi, MoonshotChatOptions.builder() .withModel(MoonshotApi.ChatModel.MOONSHOT_V1_8K.getValue()) .withTemperature(0.4) .withMaxTokens(200) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> streamResponse = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); The MoonshotChatOptions provides the configuration information for the chat requests. The MoonshotChatOptions.Builder is fluent options builder. Low-level Moonshot Api Client: The MoonshotApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-moonshot/src/main/java/org/springframework/ai/moonshot/api/MoonshotApi.java) provides is lightweight Java client for Moonshot AI API(https://platform.moonshot.cn/docs/api-reference) . Here is a simple snippet how to use the api programmatically: MoonshotApi moonshotApi = new MoonshotApi(System.getenv(""MOONSHOT_API_KEY"")); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(""Hello world"", Role.USER); // Sync request ResponseEntity<ChatCompletion> response = moonshotApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), MoonshotApi.ChatModel.MOONSHOT_V1_8K.getValue(), 0.7, false)); // Streaming request Flux<ChatCompletionChunk> streamResponse = moonshotApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), MoonshotApi.ChatModel.MOONSHOT_V1_8K.getValue(), 0.7, true)); Follow the MoonshotApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-moonshot/src/main/java/org/springframework/ai/moonshot/api/MoonshotApi.java) 's JavaDoc for further information. MoonshotApi Samples: The MoonshotApiIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-moonshot/src/test/java/org/springframework/ai/moonshot/api/MoonshotApiIT.java) test provides some general examples how to use the lightweight library. The MoonshotApiToolFunctionCallIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-moonshot/src/test/java/org/springframework/ai/moonshot/api/MoonshotApiToolFunctionCallIT.java) test shows how to use the low-level API to call tool functions. Function Calling(functions/minimax-chat-functions.html) NVIDIA(nvidia-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/nvidia-chat.html","NVIDIA Chat: NVIDIA LLM API(https://docs.api.nvidia.com/nim/reference/llm-apis) is a proxy AI Inference Engine offering a wide range of models from various providers(https://docs.api.nvidia.com/nim/reference/llm-apis#models) . Spring AI integrates with the NVIDIA LLM API by reusing the existing OpenAI(openai-chat.html) client. For this you need to set the base-url to integrate.api.nvidia.com(https://integrate.api.nvidia.com) , select one of the provided LLM models(https://docs.api.nvidia.com/nim/reference/llm-apis#model) and get an api-key for it. NVIDIA LLM API requires the max-tokens parameter to be explicitly set or server error will be thrown. Check the NvidiaWithOpenAiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/NvidiaWithOpenAiChatModelIT.java) tests for examples of using NVIDIA LLM API with Spring AI. Prerequisite: Create NVIDIA(https://build.nvidia.com/explore/discover) account with sufficient credits. Select a LLM Model to use. For example the meta/llama-3.1-70b-instruct in the screenshot below. From the selected model’s page, you can get the api-key for accessing this model. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OpenAI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the OpenAI chat model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.openai.base-url The URL to connect to. Must be set to integrate.api.nvidia.com(https://integrate.api.nvidia.com) - spring.ai.openai.api-key The NVIDIA API Key - Configuration Properties: The prefix spring.ai.openai.chat is the property prefix that lets you configure the chat model implementation for OpenAI. Property Description Default spring.ai.openai.chat.enabled Enable OpenAI chat model. true spring.ai.openai.chat.base-url Optional overrides the spring.ai.openai.base-url to provide chat specific url. Must be set to integrate.api.nvidia.com(https://integrate.api.nvidia.com) - spring.ai.openai.chat.api-key Optional overrides the spring.ai.openai.api-key to provide chat specific api-key - spring.ai.openai.chat.options.model The NVIDIA LLM model(https://docs.api.nvidia.com/nim/reference/llm-apis#models) to use - spring.ai.openai.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.8 spring.ai.openai.chat.options.frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim. 0.0f spring.ai.openai.chat.options.maxTokens The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. NOTE: NVIDIA LLM API requires the max-tokens parameter to be explicitly set or server error will be thrown. spring.ai.openai.chat.options.n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs. 1 spring.ai.openai.chat.options.presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics. - spring.ai.openai.chat.options.responseFormat An object specifying the format that the model must output. Setting to { ""type"": ""json_object"" } enables JSON mode, which guarantees the message the model generates is valid JSON. - spring.ai.openai.chat.options.seed This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. - spring.ai.openai.chat.options.stop Up to 4 sequences where the API will stop generating further tokens. - spring.ai.openai.chat.options.topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. - spring.ai.openai.chat.options.tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. - spring.ai.openai.chat.options.toolChoice Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {""type: ""function"", ""function"": {""name"": ""my_function""}} forces the model to call that function. none is the default when no functions are present. auto is the default if functions are present. - spring.ai.openai.chat.options.user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. - spring.ai.openai.chat.options.functions List of functions, identified by their names, to enable for function calling in a single prompt requests. Functions with those names must exist in the functionCallbacks registry. - spring.ai.openai.chat.options.stream-usage (For streaming only) Set to add an additional chunk with token usage statistics for the entire request. The choices field for this chunk is an empty array and all other chunks will also include a usage field, but with a null value. false spring.ai.openai.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false All properties prefixed with spring.ai.openai.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The OpenAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the OpenAiChatModel(api, options) constructor or the spring.ai.openai.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", OpenAiChatOptions.builder() .withModel(""mixtral-8x7b-32768"") .withTemperature(0.4) .build() )); In addition to the model specific OpenAiChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Function Calling: NVIDIA LLM API supports Tool/Function calling when selecting a model that supports it. You can register custom Java functions with your ChatModel and have the provided model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This is a powerful technique to connect the LLM capabilities with external tools and APIs. Tool Example: Here’s a simple example of how to use NVIDIA LLM API function calling with Spring AI: spring.ai.openai.api-key=${NVIDIA_API_KEY} spring.ai.openai.base-url=https://integrate.api.nvidia.com spring.ai.openai.chat.options.model=meta/llama-3.1-70b-instruct spring.ai.openai.chat.options.max-tokens=2048 @SpringBootApplication public class NvidiaLlmApplication { public static void main(String[] args) { SpringApplication.run(NvidiaLlmApplication.class, args); } @Bean CommandLineRunner runner(ChatClient.Builder chatClientBuilder) { return args -> { var chatClient = chatClientBuilder.build(); var response = chatClient.prompt() .user(""What is the weather in Amsterdam and Paris?"") .functions(""weatherFunction"") // reference by bean name. .call() .content(); System.out.println(response); }; } @Bean @Description(""Get the weather in location"") public Function<WeatherRequest, WeatherResponse> weatherFunction() { return new MockWeatherService(); } public static class MockWeatherService implements Function<WeatherRequest, WeatherResponse> { public record WeatherRequest(String location, String unit) {} public record WeatherResponse(double temp, String unit) {} @Override public WeatherResponse apply(WeatherRequest request) { double temperature = request.location().contains(""Amsterdam"") ? 20 : 25; return new WeatherResponse(temperature, request.unit); } } } In this example, when the model needs weather information, it will automatically call the weatherFunction bean, which can then fetch real-time weather data. The expected response looks like this: ""The weather in Amsterdam is currently 20 degrees Celsius, and the weather in Paris is currently 25 degrees Celsius."" Read more about OpenAI Function Calling(https://docs.spring.io/spring-ai/reference/api/chat/functions/openai-chat-functions.html) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-openai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the OpenAi chat model: spring.ai.openai.api-key=${NVIDIA_API_KEY} spring.ai.openai.base-url=https://integrate.api.nvidia.com spring.ai.openai.chat.options.model=meta/llama-3.1-70b-instruct # The NVIDIA LLM API doesn't support embeddings, so we need to disable it. spring.ai.openai.embedding.enabled=false # The NVIDIA LLM API requires this parameter to be set explicitly or server internal error will be thrown. spring.ai.openai.chat.options.max-tokens=2048 replace the api-key with your NVIDIA credentials. NVIDIA LLM API requires the max-token parameter to be explicitly set or server error will be thrown. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Moonshot AI(moonshot-chat.html) Ollama(ollama-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/ollama-chat.html","Ollama Chat: With Ollama(https://ollama.ai/) you can run various Large Language Models (LLMs) locally and generate text from them. Spring AI supports the Ollama text generation capabilities with the OllamaChatModel API. Ollama offers an OpenAI API compatible endpoint as well. Check the OpenAI API compatibility(#_openai_api_compatibility) section to learn how to use the Spring AI OpenAI(openai-chat.html) project to talk to an Ollama server. Prerequisites: You first need to run Ollama on your local machine. Refer to the official Ollama project README(https://github.com/ollama/ollama) to get started running models on your local machine. Running ollama pull mistral will download a 4.1GB model artifact. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Ollama chat integration. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-ollama-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-ollama-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: The prefix spring.ai.ollama is the property prefix to configure the connection to Ollama. Property Description Default spring.ai.ollama.base-url Base URL where Ollama API server is running. localhost:11434(http://localhost:11434) The prefix spring.ai.ollama.chat.options is the property prefix that configures the Ollama chat model . It includes the Ollama request (advanced) parameters such as the model , keep-alive , and format as well as the Ollama model options properties. Here are the advanced request parameter for the Ollama chat model: Property Description Default spring.ai.ollama.chat.enabled Enable Ollama chat model. true spring.ai.ollama.chat.options.model The name of the supported model(https://github.com/ollama/ollama?tab=readme-ov-file#model-library) to use. mistral spring.ai.ollama.chat.options.format The format to return a response in. Currently, the only accepted value is json - spring.ai.ollama.chat.options.keep_alive Controls how long the model will stay loaded into memory following the request 5m The remaining options properties are based on the Ollama Valid Parameters and Values(https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values) and Ollama Types(https://github.com/ollama/ollama/blob/main/api/types.go) . The default values are based on the Ollama Types Defaults(https://github.com/ollama/ollama/blob/b538dc3858014f94b099730a592751a5454cab0a/api/types.go#L364) . Property Description Default spring.ai.ollama.chat.options.numa Whether to use NUMA. false spring.ai.ollama.chat.options.num-ctx Sets the size of the context window used to generate the next token. 2048 spring.ai.ollama.chat.options.num-batch Prompt processing maximum batch size. 512 spring.ai.ollama.chat.options.num-gpu The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. 1 here indicates that NumGPU should be set dynamically -1 spring.ai.ollama.chat.options.main-gpu When using multiple GPUs this option controls which GPU is used for small tensors for which the overhead of splitting the computation across all GPUs is not worthwhile. The GPU in question will use slightly more VRAM to store a scratch buffer for temporary results. 0 spring.ai.ollama.chat.options.low-vram - false spring.ai.ollama.chat.options.f16-kv - true spring.ai.ollama.chat.options.logits-all Return logits for all the tokens, not just the last one. To enable completions to return logprobs, this must be true. - spring.ai.ollama.chat.options.vocab-only Load only the vocabulary, not the weights. - spring.ai.ollama.chat.options.use-mmap By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you’re not using mlock. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all. null spring.ai.ollama.chat.options.use-mlock Lock the model in memory, preventing it from being swapped out when memory-mapped. This can improve performance but trades away some of the advantages of memory-mapping by requiring more RAM to run and potentially slowing down load times as the model loads into RAM. false spring.ai.ollama.chat.options.num-thread Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). 0 = let the runtime decide 0 spring.ai.ollama.chat.options.num-keep - 4 spring.ai.ollama.chat.options.seed Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. -1 spring.ai.ollama.chat.options.num-predict Maximum number of tokens to predict when generating text. (-1 = infinite generation, -2 = fill context) -1 spring.ai.ollama.chat.options.top-k Reduces the probability of generating nonsense. A higher value (e.g., 100) will give more diverse answers, while a lower value (e.g., 10) will be more conservative. 40 spring.ai.ollama.chat.options.top-p Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. 0.9 spring.ai.ollama.chat.options.tfs-z Tail-free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. 1.0 spring.ai.ollama.chat.options.typical-p - 1.0 spring.ai.ollama.chat.options.repeat-last-n Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx) 64 spring.ai.ollama.chat.options.temperature The temperature of the model. Increasing the temperature will make the model answer more creatively. 0.8 spring.ai.ollama.chat.options.repeat-penalty Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. 1.1 spring.ai.ollama.chat.options.presence-penalty - 0.0 spring.ai.ollama.chat.options.frequency-penalty - 0.0 spring.ai.ollama.chat.options.mirostat Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) 0 spring.ai.ollama.chat.options.mirostat-tau Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. 5.0 spring.ai.ollama.chat.options.mirostat-eta Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. 0.1 spring.ai.ollama.chat.options.penalize-newline - true spring.ai.ollama.chat.options.stop Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile. - spring.ai.ollama.chat.options.functions List of functions, identified by their names, to enable for function calling in a single prompt requests. Functions with those names must exist in the functionCallbacks registry. - spring.ai.ollama.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false All properties prefixed with spring.ai.ollama.chat.options can be overridden at runtime by adding request-specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The OllamaOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaOptions.java) class provides model configurations, such as the model to use, the temperature, etc. On start-up, the default options can be configured with the OllamaChatModel(api, options) constructor or the spring.ai.ollama.chat.options.* properties. At run-time, you can override the default options by adding new, request-specific options to the Prompt call. For example, to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", OllamaOptions.builder() .withModel(OllamaModel.LLAMA3_1) .withTemperature(0.4) .build() )); In addition to the model specific OllamaOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Function Calling: You can register custom Java functions with the OllamaChatModel and have the Ollama model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This is a powerful technique to connect the LLM capabilities with external tools and APIs. Read more about Ollama Function Calling(functions/ollama-chat-functions.html) . You need Ollama 0.2.8 or newer to use the functional calling capabilities. Currently, the Ollama API (0.3.8) does not support function calling in streaming mode. Multimodal: Multimodality refers to a model’s ability to simultaneously understand and process information from various sources, including text, images, audio, and other data formats. Some of the models available in Ollama with multimodality support are LLaVa(https://ollama.com/library/llava) and bakllava(https://ollama.com/library/bakllava) (see the full list(https://ollama.com/search?c=vision) ). For further details, refer to the LLaVA: Large Language and Vision Assistant(https://llava-vl.github.io/) . The Ollama Message API(https://github.com/ollama/ollama/blob/main/docs/api.md#parameters-1) provides an ""images"" parameter to incorporate a list of base64-encoded images with the message. Spring AI’s Message(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/messages/Message.java) interface facilitates multimodal AI models by introducing the Media(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/messages/Media.java) type. This type encompasses data and details regarding media attachments in messages, utilizing Spring’s org.springframework.util.MimeType and a org.springframework.core.io.Resource for the raw media data. Below is a straightforward code example excerpted from OllamaChatModelMultimodalIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/test/java/org/springframework/ai/ollama/OllamaChatModelMultimodalIT.java) , illustrating the fusion of user text with an image. var imageResource = new ClassPathResource(""/multimodal.test.png""); var userMessage = new UserMessage(""Explain what do you see on this picture?"", new Media(MimeTypeUtils.IMAGE_PNG, imageResource)); ChatResponse response = chatModel.call(new Prompt(userMessage, OllamaOptions.builder().withModel(OllamaModel.LLAVA)).build()); The example shows a model taking as an input the multimodal.test.png image: along with the text message ""Explain what do you see on this picture?"", and generating a response like this: The image shows a small metal basket filled with ripe bananas and red apples. The basket is placed on a surface, which appears to be a table or countertop, as there's a hint of what seems like a kitchen cabinet or drawer in the background. There's also a gold-colored ring visible behind the basket, which could indicate that this photo was taken in an area with metallic decorations or fixtures. The overall setting suggests a home environment where fruits are being displayed, possibly for convenience or aesthetic purposes. OpenAI API Compatibility: Ollama is OpenAI API-compatible and you can use the Spring AI OpenAI(openai-chat.html) client to talk to Ollama and use tools. For this, you need to configure the OpenAI base URL to your Ollama instance: spring.ai.openai.chat.base-url=http://localhost:11434 and select one of the provided Ollama models: spring.ai.openai.chat.options.model=mistral . Check the OllamaWithOpenAiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/proxy/OllamaWithOpenAiChatModelIT.java) tests for examples of using Ollama over Spring AI OpenAI. Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-ollama-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Ollama chat model: spring.ai.ollama.base-url=http://localhost:11434 spring.ai.ollama.chat.options.model=mistral spring.ai.ollama.chat.options.temperature=0.7 Replace the base-url with your Ollama server URL. This will create an OllamaChatModel implementation that you can inject into your classes. Here is an example of a simple @RestController class that uses the chat model for text generations. @RestController public class ChatController { private final OllamaChatModel chatModel; @Autowired public ChatController(OllamaChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map<String,String> generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: If you don’t want to use the Spring Boot auto-configuration, you can manually configure the OllamaChatModel in your application. The OllamaChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/OllamaChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level OllamaApi Client(#low-level-api) to connect to the Ollama service. To use it, add the spring-ai-ollama dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-ollama</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-ollama' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-ollama dependency provides access also to the OllamaEmbeddingModel . For more information about the OllamaEmbeddingModel refer to the Ollama Embedding Model(../embeddings/ollama-embeddings.html) section. Next, create an OllamaChatModel instance and use it to send requests for text generation: var ollamaApi = new OllamaApi(); var chatModel = new OllamaChatModel(ollamaApi, OllamaOptions.create() .withModel(OllamaOptions.DEFAULT_MODEL) .withTemperature(0.9)); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); The OllamaOptions provides the configuration information for all chat requests. Low-level OllamaApi Client: The OllamaApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaApi.java) provides a lightweight Java client for the Ollama Chat Completion API Ollama Chat Completion API(https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion) . The following class diagram illustrates the OllamaApi chat interfaces and building blocks: The OllamaApi is a low-level API and is not recommended for direct use. Use the OllamaChatModel instead. Here is a simple snippet showing how to use the API programmatically: OllamaApi ollamaApi = new OllamaApi(""YOUR_HOST:YOUR_PORT""); // Sync request var request = ChatRequest.builder(""orca-mini"") .withStream(false) // not streaming .withMessages(List.of( Message.builder(Role.SYSTEM) .withContent(""You are a geography teacher. You are talking to a student."") .build(), Message.builder(Role.USER) .withContent(""What is the capital of Bulgaria and what is the size? "" + ""What is the national anthem?"") .build())) .withOptions(OllamaOptions.create().withTemperature(0.9)) .build(); ChatResponse response = ollamaApi.chat(request); // Streaming request var request2 = ChatRequest.builder(""orca-mini"") .withStream(true) // streaming .withMessages(List.of(Message.builder(Role.USER) .withContent(""What is the capital of Bulgaria and what is the size? "" + ""What is the national anthem?"") .build())) .withOptions(OllamaOptions.create().withTemperature(0.9).toMap()) .build(); Flux<ChatResponse> streamingResponse = ollamaApi.streamingChat(request2); NVIDIA(nvidia-chat.html) Function Calling(functions/ollama-chat-functions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/functions/ollama-chat-functions.html","Function Calling: You need Ollama 0.2.8 or newer. You need Models(https://ollama.com/search?c=tools) pre-trained for Tools support. Usually, such models are tagged with a Tools tag. For example mistral , firefunction-v2 or llama3.1:70b . Currently, the Ollama API (0.3.8) does not support function calling in streaming mode. You can register custom Java functions with the OllamaChatModel and have the Ollama deployed model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This allows you to connect the LLM capabilities with external tools and APIs. The Ollama models tagged with the Tools label (see full list(https://ollama.com/search?c=tools) ) are trained to detect when a function should be called and to respond with JSON that adheres to the function signature. The Ollama API does not call the function directly; instead, the model generates JSON that you can use to call the function in your code and return the result back to the model to complete the conversation. Spring AI provides flexible and user-friendly ways to register and call custom functions. In general, the custom functions need to provide a function name , description , and the function call signature (as JSON schema) to let the model know what arguments the function expects. The description helps the model to understand when to call the function. As a developer, you need to implement a function that takes the function call arguments sent from the AI model, and responds with the result back to the model. Your function can in turn invoke other 3rd party services to provide the results. Spring AI makes this as easy as defining a @Bean definition that returns a java.util.Function and supplying the bean name as an option when invoking the ChatModel . Under the hood, Spring wraps your POJO (the function) with the appropriate adapter code that enables interaction with the AI Model, saving you from writing tedious boilerplate code. The basis of the underlying infrastructure is the FunctionCallback.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallback.java) interface and the companion FunctionCallbackWrapper.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallbackWrapper.java) utility class to simplify the implementation and registration of Java callback functions. How it works: Suppose we want the AI model to respond with information that it does not have, for example, the current temperature at a given location. We can provide the AI model with metadata about our own functions that it can use to retrieve that information as it processes your prompt. For example, if during the processing of a prompt, the AI Model determines that it needs additional information about the temperature in a given location, it will start a server-side generated request/response interaction. The AI Model invokes a client side function. The AI Model provides method invocation details as JSON, and it is the responsibility of the client to execute that function and return the response. The model-client interaction is illustrated in the Spring AI Function Calling Flow(#spring-ai-function-calling-flow) diagram. Spring AI greatly simplifies the code you need to write to support function invocation. It brokers the function invocation conversation for you. You can simply provide your function definition as a @Bean and then provide the bean name of the function in your prompt options. You can also reference multiple function bean names in your prompt. Quick Start: Let’s create a chatbot that answer questions by calling our own function. To support the response of the chatbot, we will register our own function that takes a location and returns the current weather in that location. When the model needs to answer a question such as ""What’s the weather like in Boston?"" the AI model will invoke the client providing the location value as an argument to be passed to the function. This RPC-like data is passed as JSON. Our function calls some SaaS based weather service API and returns the weather response back to the model to complete the conversation. In this example, we will use a simple implementation named MockWeatherService that hard-codes the temperature for various locations. The following MockWeatherService.java represents the weather service API: public class MockWeatherService implements Function<Request, Response> { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } Registering Functions as Beans: With the OllamaChatModel Auto-Configuration(../ollama-chat.html#_auto_configuration) you have multiple ways to register custom functions as beans in the Spring context. We start by describing the most POJO-friendly options. Plain Java Functions: In this approach, you define a @Bean in your application context as you would any other Spring managed object. Internally, Spring AI ChatModel will create an instance of a FunctionCallbackWrapper that adds the logic for it being invoked via the AI model. The name of the @Bean is passed as a ChatOption . @Configuration static class Config { @Bean @Description(""Get the weather in location"") // function description public Function<MockWeatherService.Request, MockWeatherService.Response> currentWeather() { return new MockWeatherService(); } } The @Description annotation is optional and provides a function description that helps the model understand when to call the function. It is an important property to set to help the AI model determine what client side function to invoke. Another option for providing the description of the function is to use the @JsonClassDescription annotation on the MockWeatherService.Request : @Configuration static class Config { @Bean public Function<Request, Response> currentWeather() { // bean name as function name return new MockWeatherService(); } } @JsonClassDescription(""Get the weather in location"") // // function description public record Request(String location, Unit unit) {} It is a best practice to annotate the request object with information such that the generated JSON schema of that function is as descriptive as possible to help the AI model pick the correct function to invoke. FunctionCallback Wrapper: Another way to register a function is to create a FunctionCallbackWrapper like this: @Configuration static class Config { @Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") // (1) function name .withDescription(""Get the weather in location"") // (2) function description .build(); } } It wraps the 3rd party MockWeatherService function and registers it as a CurrentWeather function with the OllamaChatModel . It also provides a description (2) and an optional response converter to convert the response into a text as expected by the model. By default, the response converter performs a JSON serialization of the Response object. The FunctionCallbackWrapper internally resolves the function call signature based on the MockWeatherService.Request class. Specifying functions in Chat Options: To let the model know and call your CurrentWeather function you need to enable it in your prompt requests: OllamaChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris?""); ChatResponse response = chatModel.call(new Prompt(userMessage, OllamaOptions.builder().withFunction(""CurrentWeather"").build())); // Enable the function logger.info(""Response: {}"", response); The above user question will trigger 3 calls to the CurrentWeather function (one for each city) and the final response will be something like this: Here is the current weather for the requested cities: - San Francisco, CA: 30.0°C - Tokyo, Japan: 10.0°C - Paris, France: 15.0°C The FunctionCallbackWrapperIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/ollama/tool/FunctionCallbackWrapperIT.java) test demo this approach. Register/Call Functions with Prompt Options: In addition to the auto-configuration, you can register callback functions, dynamically, with your Prompt requests: OllamaChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris?""); var promptOptions = OllamaOptions.builder() .withFunctionCallbacks(List.of(new FunctionCallbackWrapper<>( ""CurrentWeather"", // name ""Get the weather in location"", // function description new MockWeatherService()))) // function code .build(); ChatResponse response = chatModel.call(new Prompt(userMessage, promptOptions)); The in-prompt registered functions are enabled by default for the duration of this request. This approach allows to choose dynamically different functions to be called based on the user input. The FunctionCallbackInPromptIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/ollama/tool/FunctionCallbackInPromptIT.java) integration test provides a complete example of how to register a function with the OllamaChatModel and use it in a prompt request. Appendices:: Spring AI Function Calling Flow: The following diagram illustrates the flow of the OllamaChatModel Function Calling: OllamaAPI Function Calling Flow: The following diagram illustrates the flow of the Ollama API: The OllamaApiToolFunctionCallIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/test/java/org/springframework/ai/ollama/api/tool/OllamaApiToolFunctionCallIT.java) provides a complete example on how to use the Ollama API function calling. Ollama(../ollama-chat.html) OpenAI(../openai-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/openai-chat.html","OpenAI Chat: Spring AI supports the various AI language models from OpenAI, the company behind ChatGPT, which has been instrumental in sparking interest in AI-driven text generation thanks to its creation of industry-leading text generation models and embeddings. Prerequisites: You will need to create an API with OpenAI to access ChatGPT models. Create an account at OpenAI signup page(https://platform.openai.com/signup) and generate the token on the API Keys page(https://platform.openai.com/account/api-keys) . The Spring AI project defines a configuration property named spring.ai.openai.api-key that you should set to the value of the API Key obtained from openai.com. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_OPENAI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OpenAI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the OpenAI chat model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.openai.base-url The URL to connect to api.openai.com(https://api.openai.com) spring.ai.openai.api-key The API Key - spring.ai.openai.organization-id Optionally, you can specify which organization to use for an API request. - spring.ai.openai.project-id Optionally, you can specify which project to use for an API request. - For users that belong to multiple organizations (or are accessing their projects through their legacy user API key), you can optionally specify which organization and project is used for an API request. Usage from these API requests will count as usage for the specified organization and project. Configuration Properties: The prefix spring.ai.openai.chat is the property prefix that lets you configure the chat model implementation for OpenAI. Property Description Default spring.ai.openai.chat.enabled Enable OpenAI chat model. true spring.ai.openai.chat.base-url Optional override for the spring.ai.openai.base-url property to provide a chat-specific URL. - spring.ai.openai.chat.completions-path The path to append to the base URL. /v1/chat/completions spring.ai.openai.chat.api-key Optional override for the spring.ai.openai.api-key to provide a chat-specific API Key. - spring.ai.openai.chat.organization-id Optionally, you can specify which organization to use for an API request. - spring.ai.openai.chat.project-id Optionally, you can specify which project to use for an API request. - spring.ai.openai.chat.options.model Name of the OpenAI chat model to use. You can select between models such as: gpt-4o , gpt-4o-mini , gpt-4-turbo , gpt-3.5-turbo , and more. See the models(https://platform.openai.com/docs/models) page for more information. gpt-4o spring.ai.openai.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.8 spring.ai.openai.chat.options.frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim. 0.0f spring.ai.openai.chat.options.logitBias Modify the likelihood of specified tokens appearing in the completion. - spring.ai.openai.chat.options.maxTokens (Deprecated in favour of maxCompletionTokens ) The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. - spring.ai.openai.chat.options.maxCompletionTokens An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. - spring.ai.openai.chat.options.n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs. 1 spring.ai.openai.chat.options.presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics. - spring.ai.openai.chat.options.responseFormat.type Compatible with GPT-4o , GPT-4o mini , GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106 . The JSON_OBJECT type enables JSON mode, which guarantees the message the model generates is valid JSON. The JSON_SCHEMA type enables Structured Outputs(https://platform.openai.com/docs/guides/structured-outputs) which guarantees the model will match your supplied JSON schema. The JSON_SCHEMA type requires setting the responseFormat.schema property as well. - spring.ai.openai.chat.options.responseFormat.name Response format schema name. Applicable only for responseFormat.type=JSON_SCHEMA custom_schema spring.ai.openai.chat.options.responseFormat.schema Response format JSON schema. Applicable only for responseFormat.type=JSON_SCHEMA - spring.ai.openai.chat.options.responseFormat.strict Response format JSON schema adherence strictness. Applicable only for responseFormat.type=JSON_SCHEMA - spring.ai.openai.chat.options.seed This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. - spring.ai.openai.chat.options.stop Up to 4 sequences where the API will stop generating further tokens. - spring.ai.openai.chat.options.topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. - spring.ai.openai.chat.options.tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. - spring.ai.openai.chat.options.toolChoice Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. auto means the model can pick between generating a message or calling a function. Specifying a particular function via {""type: ""function"", ""function"": {""name"": ""my_function""}} forces the model to call that function. none is the default when no functions are present. auto is the default if functions are present. - spring.ai.openai.chat.options.user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. - spring.ai.openai.chat.options.functions List of functions, identified by their names, to enable for function calling in a single prompt requests. Functions with those names must exist in the functionCallbacks registry. - spring.ai.openai.chat.options.stream-usage (For streaming only) Set to add an additional chunk with token usage statistics for the entire request. The choices field for this chunk is an empty array and all other chunks will also include a usage field, but with a null value. false spring.ai.openai.chat.options.parallel-tool-calls Whether to enable parallel function calling(https://platform.openai.com/docs/guides/function-calling/parallel-function-calling) during tool use. true spring.ai.openai.chat.options.http-headers Optional HTTP headers to be added to the chat completion request. To override the api-key you need to use an Authorization header key, and you have to prefix the key value with the `Bearer ` prefix. - spring.ai.openai.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false You can override the common spring.ai.openai.base-url and spring.ai.openai.api-key for the ChatModel and EmbeddingModel implementations. The spring.ai.openai.chat.base-url and spring.ai.openai.chat.api-key properties, if set, take precedence over the common properties. This is useful if you want to use different OpenAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.openai.chat.options can be overridden at runtime by adding request-specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The OpenAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java) class provides model configurations such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the OpenAiChatModel(api, options) constructor or the spring.ai.openai.chat.options.* properties. At run-time, you can override the default options by adding new, request-specific options to the Prompt call. For example, to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", OpenAiChatOptions.builder() .withModel(""gpt-4-o"") .withTemperature(0.4) .build() )); In addition to the model specific OpenAiChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . Function Calling: You can register custom Java functions with the OpenAiChatModel and have the OpenAI model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This is a powerful technique to connect the LLM capabilities with external tools and APIs. Read more about OpenAI Function Calling(functions/openai-chat-functions.html) . Multimodal: Multimodality refers to a model’s ability to simultaneously understand and process information from various sources, including text, images, audio, and other data formats. OpenAI models that offer multimodal support include gpt-4 , gpt-4o , and gpt-4o-mini . Refer to the Vision(https://platform.openai.com/docs/guides/vision) guide for more information. The OpenAI User Message API(https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages) can incorporate a list of base64-encoded images or image urls with the message. Spring AI’s Message(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/messages/Message.java) interface facilitates multimodal AI models by introducing the Media(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/messages/Media.java) type. This type encompasses data and details regarding media attachments in messages, utilizing Spring’s org.springframework.util.MimeType and a org.springframework.core.io.Resource for the raw media data. Below is a code example excerpted from OpenAiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/c9a3e66f90187ce7eae7eb78c462ec622685de6c/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java#L293) , illustrating the fusion of user text with an image using the gpt-4o model. var imageResource = new ClassPathResource(""/multimodal.test.png""); var userMessage = new UserMessage(""Explain what do you see on this picture?"", new Media(MimeTypeUtils.IMAGE_PNG, imageResource)); ChatResponse response = chatModel.call(new Prompt(userMessage, OpenAiChatOptions.builder().withModel(OpenAiApi.ChatModel.GPT_4_O.getValue()).build())); GPT_4_VISION_PREVIEW will continue to be available only to existing users of this model starting June 17, 2024. If you are not an existing user, please use the GPT_4_O or GPT_4_TURBO models. More details here(https://platform.openai.com/docs/deprecations/2024-06-06-gpt-4-32k-and-vision-preview-models) or the image URL equivalent using the gpt-4o model: var userMessage = new UserMessage(""Explain what do you see on this picture?"", new Media(MimeTypeUtils.IMAGE_PNG, ""https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/_images/multimodal.test.png"")); ChatResponse response = chatModel.call(new Prompt(userMessage, OpenAiChatOptions.builder().withModel(OpenAiApi.ChatModel.GPT_4_O.getValue()).build())); You can pass multiple images as well. The example shows a model taking as an input the multimodal.test.png image: along with the text message ""Explain what do you see on this picture?"", and generating a response like this: This is an image of a fruit bowl with a simple design. The bowl is made of metal with curved wire edges that create an open structure, allowing the fruit to be visible from all angles. Inside the bowl, there are two yellow bananas resting on top of what appears to be a red apple. The bananas are slightly overripe, as indicated by the brown spots on their peels. The bowl has a metal ring at the top, likely to serve as a handle for carrying. The bowl is placed on a flat surface with a neutral-colored background that provides a clear view of the fruit inside. Structured Outputs: OpenAI provides custom Structured Outputs(https://platform.openai.com/docs/guides/structured-outputs) APIs that ensure your model generates responses conforming strictly to your provided JSON Schema . In addition to the existing Spring AI model-agnostic Structured Output Converter(../structured-output-converter.html) , these APIs offer enhanced control and precision. Currently, OpenAI supports a subset of the JSON Schema language(https://platform.openai.com/docs/guides/structured-outputs/supported-schemas) format. Configuration: Spring AI allows you to configure your response format either programmatically using the OpenAiChatOptions builder or through application properties. Using the Chat Options Builder: You can set the response format programmatically with the OpenAiChatOptions builder as shown below: String jsonSchema = """""" { ""type"": ""object"", ""properties"": { ""steps"": { ""type"": ""array"", ""items"": { ""type"": ""object"", ""properties"": { ""explanation"": { ""type"": ""string"" }, ""output"": { ""type"": ""string"" } }, ""required"": [""explanation"", ""output""], ""additionalProperties"": false } }, ""final_answer"": { ""type"": ""string"" } }, ""required"": [""steps"", ""final_answer""], ""additionalProperties"": false } """"""; Prompt prompt = new Prompt(""how can I solve 8x + 7 = -23"", OpenAiChatOptions.builder() .withModel(ChatModel.GPT_4_O_MINI) .withResponseFormat(new ResponseFormat(ResponseFormat.Type.JSON_SCHEMA, jsonSchema)) .build()); ChatResponse response = this.openAiChatModel.call(prompt); Adhere to the OpenAI subset of the JSON Schema language(https://platform.openai.com/docs/guides/structured-outputs/supported-schemas) format. Integrating with BeanOutputConverter Utilities: You can leverage existing BeanOutputConverter(../structured-output-converter.html#_bean_output_converter) utilities to automatically generate the JSON Schema from your domain objects and later convert the structured response into domain-specific instances: record MathReasoning( @JsonProperty(required = true, value = ""steps"") Steps steps, @JsonProperty(required = true, value = ""final_answer"") String finalAnswer) { record Steps( @JsonProperty(required = true, value = ""items"") Items[] items) { record Items( @JsonProperty(required = true, value = ""explanation"") String explanation, @JsonProperty(required = true, value = ""output"") String output) { } } } var outputConverter = new BeanOutputConverter<>(MathReasoning.class); var jsonSchema = outputConverter.getJsonSchema(); Prompt prompt = new Prompt(""how can I solve 8x + 7 = -23"", OpenAiChatOptions.builder() .withModel(ChatModel.GPT_4_O_MINI) .withResponseFormat(new ResponseFormat(ResponseFormat.Type.JSON_SCHEMA, jsonSchema)) .build()); ChatResponse response = this.openAiChatModel.call(prompt); String content = response.getResult().getOutput().getContent(); MathReasoning mathReasoning = outputConverter.convert(content); Ensure you use the @JsonProperty(required = true,…​) annotation. This is crucial for generating a schema that accurately marks fields as required . Although this is optional for JSON Schema, OpenAI mandates(https://platform.openai.com/docs/guides/structured-outputs/all-fields-must-be-required) it for the structured response to function correctly. Configuring via Application Properties: Alternatively, when using the OpenAI auto-configuration, you can configure the desired response format through the following application properties: spring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.chat.options.model=gpt-4o-mini spring.ai.openai.chat.options.response-format.type=JSON_SCHEMA spring.ai.openai.chat.options.response-format.name=MySchemaName spring.ai.openai.chat.options.response-format.schema={""type"":""object"",""properties"":{""steps"":{""type"":""array"",""items"":{""type"":""object"",""properties"":{""explanation"":{""type"":""string""},""output"":{""type"":""string""}},""required"":[""explanation"",""output""],""additionalProperties"":false}},""final_answer"":{""type"":""string""}},""required"":[""steps"",""final_answer""],""additionalProperties"":false} spring.ai.openai.chat.options.response-format.strict=true Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-openai-spring-boot-starter to your pom (or gradle) dependencies. Add an application.properties file under the src/main/resources directory to enable and configure the OpenAi chat model: spring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.chat.options.model=gpt-4o spring.ai.openai.chat.options.temperature=0.7 Replace the api-key with your OpenAI credentials. This will create an OpenAiChatModel implementation that you can inject into your classes. Here is an example of a simple @RestController class that uses the chat model for text generations. @RestController public class ChatController { private final OpenAiChatModel chatModel; @Autowired public ChatController(OpenAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map<String,String> generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { Prompt prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The OpenAiChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level OpenAiApi Client(#low-level-api) to connect to the OpenAI service. Add the spring-ai-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an OpenAiChatModel and use it for text generations: var openAiApi = new OpenAiApi(System.getenv(""OPENAI_API_KEY"")); var openAiChatOptions = OpenAiChatOptions.builder() .withModel(""gpt-3.5-turbo"") .withTemperature(0.4) .withMaxTokens(200) .build(); var chatModel = new OpenAiChatModel(openAiApi, openAiChatOptions); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> response = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); The OpenAiChatOptions provides the configuration information for the chat requests. The OpenAiChatOptions.Builder is a fluent options-builder. Low-level OpenAiApi Client: The OpenAiApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/api/OpenAiApi.java) provides is lightweight Java client for OpenAI Chat API OpenAI Chat API(https://platform.openai.com/docs/api-reference/chat) . Following class diagram illustrates the OpenAiApi chat interfaces and building blocks: Here is a simple snippet showing how to use the API programmatically: OpenAiApi openAiApi = new OpenAiApi(System.getenv(""OPENAI_API_KEY"")); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(""Hello world"", Role.USER); // Sync request ResponseEntity<ChatCompletion> response = openAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), ""gpt-3.5-turbo"", 0.8, false)); // Streaming request Flux<ChatCompletionChunk> streamResponse = openAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), ""gpt-3.5-turbo"", 0.8, true)); Follow the OpenAiApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/api/OpenAiApi.java) 's JavaDoc for further information. Low-level API Examples: The OpenAiApiIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/api/OpenAiApiIT.java) tests provide some general examples of how to use the lightweight library. The OpenAiApiToolFunctionCallIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/api/tool/OpenAiApiToolFunctionCallIT.java) tests show how to use the low-level API to call tool functions. Based on the OpenAI Function Calling(https://platform.openai.com/docs/guides/function-calling/parallel-function-calling) tutorial. Function Calling(functions/ollama-chat-functions.html) Function Calling(functions/openai-chat-functions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/functions/openai-chat-functions.html","Function Calling: You can register custom Java functions with the OpenAiChatModel and have the OpenAI model intelligently choose to output a JSON object containing arguments to call one or many of the registered functions. This allows you to connect the LLM capabilities with external tools and APIs. The OpenAI models are trained to detect when a function should be called and to respond with JSON that adheres to the function signature. The OpenAI API does not call the function directly; instead, the model generates JSON that you can use to call the function in your code and return the result back to the model to complete the conversation. Spring AI provides flexible and user-friendly ways to register and call custom functions. In general, the custom functions need to provide a function name , description , and the function call signature (as JSON schema) to let the model know what arguments the function expects. The description helps the model to understand when to call the function. As a developer, you need to implement a function that takes the function call arguments sent from the AI model, and responds with the result back to the model. Your function can in turn invoke other 3rd party services to provide the results. Spring AI makes this as easy as defining a @Bean definition that returns a java.util.Function and supplying the bean name as an option when invoking the ChatModel . Under the hood, Spring wraps your POJO (the function) with the appropriate adapter code that enables interaction with the AI Model, saving you from writing tedious boilerplate code. The basis of the underlying infrastructure is the FunctionCallback.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallback.java) interface and the companion FunctionCallbackWrapper.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/model/function/FunctionCallbackWrapper.java) utility class to simplify the implementation and registration of Java callback functions. How it works: Suppose we want the AI model to respond with information that it does not have, for example, the current temperature at a given location. We can provide the AI model with metadata about our own functions that it can use to retrieve that information as it processes your prompt. For example, if during the processing of a prompt, the AI Model determines that it needs additional information about the temperature in a given location, it will start a server-side generated request/response interaction. The AI Model invokes a client side function. The AI Model provides method invocation details as JSON, and it is the responsibility of the client to execute that function and return the response. The model-client interaction is illustrated in the Spring AI Function Calling Flow(#spring-ai-function-calling-flow) diagram. Spring AI greatly simplifies the code you need to write to support function invocation. It brokers the function invocation conversation for you. You can simply provide your function definition as a @Bean and then provide the bean name of the function in your prompt options. You can also reference multiple function bean names in your prompt. Quick Start: Let’s create a chatbot that answer questions by calling our own function. To support the response of the chatbot, we will register our own function that takes a location and returns the current weather in that location. When the model needs to answer a question such as ""What’s the weather like in Boston?"" the AI model will invoke the client providing the location value as an argument to be passed to the function. This RPC-like data is passed as JSON. Our function calls some SaaS-based weather service API and returns the weather response back to the model to complete the conversation. In this example, we will use a simple implementation named MockWeatherService that hard-codes the temperature for various locations. The following MockWeatherService.java represents the weather service API: public class MockWeatherService implements Function<Request, Response> { public enum Unit { C, F } public record Request(String location, Unit unit) {} public record Response(double temp, Unit unit) {} public Response apply(Request request) { return new Response(30.0, Unit.C); } } Registering Functions as Beans: With the OpenAiChatModel Auto-Configuration(../openai-chat.html#_auto_configuration) you have multiple ways to register custom functions as beans in the Spring context. We start by describing the most POJO-friendly options. Plain Java Functions: In this approach, you define a @Bean in your application context as you would any other Spring managed object. Internally, Spring AI ChatModel will create an instance of a FunctionCallbackWrapper that adds the logic for it being invoked via the AI model. The name of the @Bean is passed as a ChatOption . @Configuration static class Config { @Bean @Description(""Get the weather in location"") // function description public Function<MockWeatherService.Request, MockWeatherService.Response> currentWeather() { return new MockWeatherService(); } } The @Description annotation is optional and provides a function description that helps the model understand when to call the function. It is an important property to set to help the AI model determine what client side function to invoke. Another option for providing the description of the function is to use the @JsonClassDescription annotation on the MockWeatherService.Request : @Configuration static class Config { @Bean public Function<Request, Response> currentWeather() { // bean name as function name return new MockWeatherService(); } } @JsonClassDescription(""Get the weather in location"") // // function description public record Request(String location, Unit unit) {} It is a best practice to annotate the request object with information such that the generated JSON schema of that function is as descriptive as possible to help the AI model pick the correct function to invoke. The FunctionCallbackWithPlainFunctionBeanIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/openai/tool/FunctionCallbackWithPlainFunctionBeanIT.java) demonstrates this approach. FunctionCallback Wrapper: Another way to register a function is to create a FunctionCallbackWrapper like this: @Configuration static class Config { @Bean public FunctionCallback weatherFunctionInfo() { return FunctionCallbackWrapper.builder(new MockWeatherService()) .withName(""CurrentWeather"") // (1) function name .withDescription(""Get the weather in location"") // (2) function description .build(); } } It wraps the 3rd party MockWeatherService function and registers it as a CurrentWeather function with the OpenAiChatModel . It also provides a description (2) and an optional response converter to convert the response into a text as expected by the model. By default, the response converter performs a JSON serialization of the Response object. The FunctionCallbackWrapper internally resolves the function call signature based on the MockWeatherService.Request class. Specifying functions in Chat Options: To let the model know and call your CurrentWeather function you need to enable it in your prompt requests: OpenAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris?""); ChatResponse response = chatModel.call(new Prompt(userMessage, OpenAiChatOptions.builder().withFunction(""CurrentWeather"").build())); // Enable the function logger.info(""Response: {}"", response); The above user question will trigger 3 calls to the CurrentWeather function (one for each city) and the final response will be something like this: Here is the current weather for the requested cities: - San Francisco, CA: 30.0°C - Tokyo, Japan: 10.0°C - Paris, France: 15.0°C The FunctionCallbackWrapperIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/openai/tool/FunctionCallbackWrapperIT.java) test demo this approach. Register/Call Functions with Prompt Options: In addition to the auto-configuration, you can register callback functions, dynamically, with your Prompt requests: OpenAiChatModel chatModel = ... UserMessage userMessage = new UserMessage(""What's the weather like in San Francisco, Tokyo, and Paris?""); var promptOptions = OpenAiChatOptions.builder() .withFunctionCallbacks(List.of(new FunctionCallbackWrapper<>( ""CurrentWeather"", // name ""Get the weather in location"", // function description new MockWeatherService()))) // function code .build(); ChatResponse response = chatModel.call(new Prompt(userMessage, promptOptions)); The in-prompt registered functions are enabled by default for the duration of this request. This approach allows to choose dynamically different functions to be called based on the user input. The FunctionCallbackInPromptIT.java(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-spring-boot-autoconfigure/src/test/java/org/springframework/ai/autoconfigure/openai/tool/FunctionCallbackInPromptIT.java) integration test provides a complete example of how to register a function with the OpenAiChatModel and use it in a prompt request. Appendices:: Spring AI Function Calling Flow: The following diagram illustrates the flow of the OpenAiChatModel Function Calling: OpenAI API Function Calling Flow: The following diagram illustrates the flow of the OpenAI API Function Calling(https://platform.openai.com/docs/guides/function-calling) : (https://platform.openai.com/docs/guides/function-calling) The OpenAiApiToolFunctionCallIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/api/tool/OpenAiApiToolFunctionCallIT.java) provides a complete example on how to use the OpenAI API function calling. It is based on the OpenAI Function Calling tutorial(https://platform.openai.com/docs/guides/function-calling/parallel-function-calling) . OpenAI(../openai-chat.html) QianFan(../qianfan-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/qianfan-chat.html","QianFan Chat: Spring AI supports the various AI language models from QianFan. You can interact with QianFan language models and create a multilingual conversational assistant based on QianFan models. Prerequisites: You will need to create an API with QianFan to access QianFan language models. Create an account at QianFan registration page(https://login.bce.baidu.com/new-reg) and generate the token on the API Keys page(https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application) . The Spring AI project defines a configuration property named spring.ai.qianfan.api-key and spring.ai.qianfan.secret-key . you should set to the value of the API Key and Secret Key obtained from API Keys page(https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_QIANFAN_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the QianFan Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-qianfan-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-qianfan-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the QianFan Chat client. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.qianfan is used as the property prefix that lets you connect to QianFan. Property Description Default spring.ai.qianfan.base-url The URL to connect to api.qianfan.chat(https://api.qianfan.chat) spring.ai.qianfan.api-key The API Key - spring.ai.qianfan.secret-key The Secret Key - Configuration Properties: The prefix spring.ai.qianfan.chat is the property prefix that lets you configure the chat client implementation for QianFan. Property Description Default spring.ai.qianfan.chat.enabled Enable QianFan chat client. true spring.ai.qianfan.chat.base-url Optional overrides the spring.ai.qianfan.base-url to provide chat specific url api.qianfan.chat(https://api.qianfan.chat) spring.ai.qianfan.chat.api-key Optional overrides the spring.ai.qianfan.api-key to provide chat specific api-key - spring.ai.qianfan.chat.secret-key Optional overrides the spring.ai.qianfan.secret-key to provide chat specific secret-key - spring.ai.qianfan.chat.options.model This is the QianFan Chat model to use abab5.5-chat (the abab5.5s-chat , abab5.5-chat , and abab6-chat point to the latest model versions) spring.ai.qianfan.chat.options.maxTokens The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. - spring.ai.qianfan.chat.options.temperature The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completions request as the interaction of these two settings is difficult to predict. 0.7 spring.ai.qianfan.chat.options.topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. 1.0 spring.ai.qianfan.chat.options.presencePenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics. 0.0f spring.ai.qianfan.chat.options.frequencyPenalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim. 0.0f spring.ai.qianfan.chat.options.stop The model will stop generating characters specified by stop, and currently only supports a single stop word in the format of [""stop_word1""] - You can override the common spring.ai.qianfan.base-url , spring.ai.qianfan.chat.api-key and spring.ai.qianfan.chat.secret-key for the ChatClient implementations. The spring.ai.qianfan.chat.base-url , spring.ai.qianfan.chat.api-key and spring.ai.qianfan.chat.secret-key properties if set take precedence over the common properties. This is useful if you want to use different QianFan accounts for different models and different model endpoints. All properties prefixed with spring.ai.qianfan.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The QianFanChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/main/java/org/springframework/ai/qianfan/QianFanChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the QianFanChatModel(api, options) constructor or the spring.ai.qianfan.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatClient.call( new Prompt( ""Generate the names of 5 famous pirates."", QianFanChatOptions.builder() .withModel(QianFanApi.ChatModel.ERNIE_Speed_8K.getValue()) .withTemperature(0.5) .build() )); In addition to the model specific QianFanChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/main/java/org/springframework/ai/qianfan/QianFanChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-qianfan-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the QianFan Chat client: spring.ai.qianfan.api-key=YOUR_API_KEY spring.ai.qianfan.secret-key=YOUR_SECRET_KEY spring.ai.qianfan.chat.options.model=ernie_speed spring.ai.qianfan.chat.options.temperature=0.7 replace the api-key and secret-key with your QianFan credentials. This will create a QianFanChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat client for text generations. @RestController public class ChatController { private final QianFanChatModel chatClient; @Autowired public ChatController(QianFanChatModel chatClient) { this.chatClient = chatClient; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatClient.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { var prompt = new Prompt(new UserMessage(message)); return chatClient.stream(prompt); } } Manual Configuration: The QianFanChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/main/java/org/springframework/ai/qianfan/QianFanChatModel.java) implements the ChatClient and StreamingChatClient and uses the Low-level QianFanApi Client(#low-level-api) to connect to the QianFan service. Add the spring-ai-qianfan dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-qianfan</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-qianfan' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a QianFanChatModel and use it for text generations: var qianFanApi = new QianFanApi(System.getenv(""QIANFAN_API_KEY""), System.getenv(""QIANFAN_SECRET_KEY"")); var chatClient = new QianFanChatModel(qianFanApi, QianFanChatOptions.builder() .withModel(QianFanApi.ChatModel.ERNIE_Speed_8K.getValue()) .withTemperature(0.4) .withMaxTokens(200) .build()); ChatResponse response = chatClient.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> streamResponse = chatClient.stream( new Prompt(""Generate the names of 5 famous pirates."")); The QianFanChatOptions provides the configuration information for the chat requests. The QianFanChatOptions.Builder is fluent options builder. Low-level QianFanApi Client: The QianFanApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/main/java/org/springframework/ai/qianfan/api/QianFanApi.java) provides is lightweight Java client for QianFan API(https://cloud.baidu.com/doc/WENXINWORKSHOP/s/flfmc9do2) . Here is a simple snippet how to use the api programmatically: String systemMessage = ""Your name is QianWen""; QianFanApi qianFanApi = new QianFanApi(System.getenv(""QIANFAN_API_KEY""), System.getenv(""QIANFAN_SECRET_KEY"")); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(""Hello world"", Role.USER); // Sync request ResponseEntity<ChatCompletion> response = qianFanApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), systemMessage, QianFanApi.ChatModel.ERNIE_Speed_8K.getValue(), 0.7, false)); // Streaming request Flux<ChatCompletionChunk> streamResponse = qianFanApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), systemMessage, QianFanApi.ChatModel.ERNIE_Speed_8K.getValue(), 0.7, true)); Follow the QianFanApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/main/java/org/springframework/ai/qianfan/api/QianFanApi.java) 's JavaDoc for further information. QianFanApi Samples: The QianFanApiIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/test/java/org/springframework/ai/qianfan/api/QianFanApiIT.java) test provides some general examples how to use the lightweight library. Function Calling(functions/openai-chat-functions.html) ZhiPu AI(zhipuai-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/zhipuai-chat.html","ZhiPu AI Chat: Spring AI supports the various AI language models from ZhiPu AI. You can interact with ZhiPu AI language models and create a multilingual conversational assistant based on ZhiPuAI models. Prerequisites: You will need to create an API with ZhiPuAI to access ZhiPu AI language models. Create an account at ZhiPu AI registration page(https://open.bigmodel.cn/login) and generate the token on the API Keys page(https://open.bigmodel.cn/usercenter/apikeys) . The Spring AI project defines a configuration property named spring.ai.zhipuai.api-key that you should set to the value of the API Key obtained from API Keys page(https://open.bigmodel.cn/usercenter/apikeys) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_ZHIPU_AI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the ZhiPuAI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-zhipuai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-zhipuai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Chat Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the ZhiPu AI chat model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.zhiPu is used as the property prefix that lets you connect to ZhiPuAI. Property Description Default spring.ai.zhipuai.base-url The URL to connect to open.bigmodel.cn/api/paas(https://open.bigmodel.cn/api/paas) spring.ai.zhipuai.api-key The API Key - Configuration Properties: The prefix spring.ai.zhipuai.chat is the property prefix that lets you configure the chat model implementation for ZhiPuAI. Property Description Default spring.ai.zhipuai.chat.enabled Enable ZhiPuAI chat model. true spring.ai.zhipuai.chat.base-url Optional overrides the spring.ai.zhipuai.base-url to provide chat specific url open.bigmodel.cn/api/paas(https://open.bigmodel.cn/api/paas) spring.ai.zhipuai.chat.api-key Optional overrides the spring.ai.zhipuai.api-key to provide chat specific api-key - spring.ai.zhipuai.chat.options.model This is the ZhiPuAI Chat model to use GLM-3-Turbo (the GLM-3-Turbo , GLM-4 , GLM-4-Air , GLM-4-AirX , GLM-4-Flash , and GLM-4V point to the latest model versions) spring.ai.zhipuai.chat.options.maxTokens The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model’s context length. - spring.ai.zhipuai.chat.options.temperature What sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. 0.7 spring.ai.zhipuai.chat.options.topP An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.. 1.0 spring.ai.zhipuai.chat.options.stop The model will stop generating characters specified by stop, and currently only supports a single stop word in the format of [""stop_word1""] - spring.ai.zhipuai.chat.options.user A unique identifier representing your end-user, which can help ZhiPuAI to monitor and detect abuse. - spring.ai.zhipuai.chat.options.requestId The parameter is passed by the client and must ensure uniqueness. It is used to distinguish the unique identifier for each request. If the client does not provide it, the platform will generate it by default. - spring.ai.zhipuai.chat.options.doSample When do_sample is set to true, the sampling strategy is enabled. If do_sample is false, the sampling strategy parameters temperature and top_p will not take effect. true spring.ai.zhipuai.chat.options.proxy-tool-calls If true, the Spring AI will not handle the function calls internally, but will proxy them to the client. Then is the client’s responsibility to handle the function calls, dispatch them to the appropriate function, and return the results. If false (the default), the Spring AI will handle the function calls internally. Applicable only for chat models with function calling support false You can override the common spring.ai.zhipuai.base-url and spring.ai.zhipuai.api-key for the ChatModel implementations. The spring.ai.zhipuai.chat.base-url and spring.ai.zhipuai.chat.api-key properties if set take precedence over the common properties. This is useful if you want to use different ZhiPuAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.zhipuai.chat.options can be overridden at runtime by adding a request specific Runtime Options(#chat-options) to the Prompt call. Runtime Options: The ZhiPuAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the ZhiPuAiChatModel(api, options) constructor or the spring.ai.zhipuai.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", ZhiPuAiChatOptions.builder() .withModel(ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue()) .withTemperature(0.5) .build() )); In addition to the model specific ZhiPuAiChatOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/ChatOptionsBuilder.java) . Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-zhipuai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the ZhiPuAi chat model: spring.ai.zhipuai.api-key=YOUR_API_KEY spring.ai.zhipuai.chat.options.model=glm-4-air spring.ai.zhipuai.chat.options.temperature=0.7 replace the api-key with your ZhiPuAI credentials. This will create a ZhiPuAiChatModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class ChatController { private final ZhiPuAiChatModel chatModel; @Autowired public ChatController(ZhiPuAiChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/generate"") public Map generate(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } @GetMapping(""/ai/generateStream"") public Flux<ChatResponse> generateStream(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { var prompt = new Prompt(new UserMessage(message)); return chatModel.stream(prompt); } } Manual Configuration: The ZhiPuAiChatModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiChatModel.java) implements the ChatModel and StreamingChatModel and uses the Low-level ZhiPuAiApi Client(#low-level-api) to connect to the ZhiPuAI service. Add the spring-ai-zhipuai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-zhipuai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-zhipuai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a ZhiPuAiChatModel and use it for text generations: var zhiPuAiApi = new ZhiPuAiApi(System.getenv(""ZHIPU_AI_API_KEY"")); var chatModel = new ZhiPuAiChatModel(zhiPuAiApi, ZhiPuAiChatOptions.builder() .withModel(ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue()) .withTemperature(0.4) .withMaxTokens(200) .build()); ChatResponse response = chatModel.call( new Prompt(""Generate the names of 5 famous pirates."")); // Or with streaming responses Flux<ChatResponse> streamResponse = chatModel.stream( new Prompt(""Generate the names of 5 famous pirates."")); The ZhiPuAiChatOptions provides the configuration information for the chat requests. The ZhiPuAiChatOptions.Builder is fluent options builder. Low-level ZhiPuAiApi Client: The ZhiPuAiApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/ZhiPuAiApi.java) provides is lightweight Java client for ZhiPu AI API(https://open.bigmodel.cn/dev/api) . Here is a simple snippet how to use the api programmatically: ZhiPuAiApi zhiPuAiApi = new ZhiPuAiApi(System.getenv(""ZHIPU_AI_API_KEY"")); ChatCompletionMessage chatCompletionMessage = new ChatCompletionMessage(""Hello world"", Role.USER); // Sync request ResponseEntity<ChatCompletion> response = zhiPuAiApi.chatCompletionEntity( new ChatCompletionRequest(List.of(chatCompletionMessage), ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue(), 0.7, false)); // Streaming request Flux<ChatCompletionChunk> streamResponse = zhiPuAiApi.chatCompletionStream( new ChatCompletionRequest(List.of(chatCompletionMessage), ZhiPuAiApi.ChatModel.GLM_3_Turbo.getValue(), 0.7, true)); Follow the ZhiPuAiApi.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/api/ZhiPuAiApi.java) 's JavaDoc for further information. ZhiPuAiApi Samples: The ZhiPuAiApiIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/test/java/org/springframework/ai/zhipuai/api/ZhiPuAiApiIT.java) test provides some general examples how to use the lightweight library. QianFan(qianfan-chat.html) watsonx.AI(watsonx-ai-chat.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/watsonx-ai-chat.html","watsonx.ai Chat: With watsonx.ai(https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/overview-wx.html?context=wx&audience=wdp) you can run various Large Language Models (LLMs) locally and generate text from them. Spring AI supports the watsonx.ai text generation with WatsonxAiChatModel . Prerequisites: You first need to have a SaaS instance of watsonx.ai (as well as an IBM Cloud account). Refer to free-trial(https://eu-de.dataplatform.cloud.ibm.com/registration/stepone?context=wx&preselect_region=true) to try watsonx.ai for free More info can be found here(https://www.ibm.com/products/watsonx-ai/info/trial) Auto-configuration: Spring AI provides Spring Boot auto-configuration for the watsonx.ai Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-watsonx-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-watsonx-ai-spring-boot-starter' } Chat Properties: Connection Properties: The prefix spring.ai.watsonx.ai is used as the property prefix that lets you connect to watsonx.ai. Property Description Default spring.ai.watsonx.ai.base-url The URL to connect to us-south.ml.cloud.ibm.com(https://us-south.ml.cloud.ibm.com) spring.ai.watsonx.ai.stream-endpoint The streaming endpoint ml/v1/text/generation_stream?version=2023-05-29 spring.ai.watsonx.ai.text-endpoint The text endpoint ml/v1/text/generation?version=2023-05-29 spring.ai.watsonx.ai.project-id The project ID - spring.ai.watsonx.ai.iam-token The IBM Cloud account IAM token - Configuration Properties: The prefix spring.ai.watsonx.ai.chat is the property prefix that lets you configure the chat model implementation for Watsonx.AI. Property Description Default spring.ai.watsonx.ai.chat.enabled Enable Watsonx.AI chat model. true spring.ai.watsonx.ai.chat.options.temperature The temperature of the model. Increasing the temperature will make the model answer more creatively. 0.7 spring.ai.watsonx.ai.chat.options.top-p Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.2) will generate more focused and conservative text. 1.0 spring.ai.watsonx.ai.chat.options.top-k Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. 50 spring.ai.watsonx.ai.chat.options.decoding-method Decoding is the process that a model uses to choose the tokens in the generated output. greedy spring.ai.watsonx.ai.chat.options.max-new-tokens Sets the limit of tokens that the LLM follow. 20 spring.ai.watsonx.ai.chat.options.min-new-tokens Sets how many tokens must the LLM generate. 0 spring.ai.watsonx.ai.chat.options.stop-sequences Sets when the LLM should stop. (e.g., [""\n\n\n""]) then when the LLM generates three consecutive line breaks it will terminate. Stop sequences are ignored until after the number of tokens that are specified in the Min tokens parameter are generated. - spring.ai.watsonx.ai.chat.options.repetition-penalty Sets how strongly to penalize repetitions. A higher value (e.g., 1.8) will penalize repetitions more strongly, while a lower value (e.g., 1.1) will be more lenient. 1.0 spring.ai.watsonx.ai.chat.options.random-seed Produce repeatable results, set the same random seed value every time. randomly generated spring.ai.watsonx.ai.chat.options.model Model is the identifier of the LLM Model to be used. google/flan-ul2 Runtime Options: The WatsonxAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-watsonx-ai/src/main/java/org/springframework/ai/watsonx/WatsonxAiChatOptions.java) provides model configurations, such as the model to use, the temperature, the frequency penalty, etc. On start-up, the default options can be configured with the WatsonxAiChatModel(api, options) constructor or the spring.ai.watsonxai.chat.options.* properties. At run-time you can override the default options by adding new, request specific, options to the Prompt call. For example to override the default model and temperature for a specific request: ChatResponse response = chatModel.call( new Prompt( ""Generate the names of 5 famous pirates."", WatsonxAiChatOptions.builder() .withTemperature(0.4) .build() )); In addition to the model specific WatsonxAiChatOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-watsonx-ai/src/main/java/org/springframework/ai/watsonx/WatsonxAiChatOptions.java) you can use a portable ChatOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptions.java) instance, created with the ChatOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/chat/prompt/ChatOptionsBuilder.java) . For more information go to watsonx-parameters-info(https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-parameters.html?context=wx) Usage example: public class MyClass { private final static String MODEL = ""google/flan-ul2""; private final WatsonxAiChatModel chatModel; @Autowired MyClass(WatsonxAiChatModel chatModel) { this.chatModel = chatModel; } public String generate(String userInput) { WatsonxAiOptions options = WatsonxAiOptions.create() .withModel(MODEL) .withDecodingMethod(""sample"") .withRandomSeed(1); Prompt prompt = new Prompt(new SystemMessage(userInput), options); var results = chatModel.call(prompt); var generatedText = results.getResult().getOutput().getContent(); return generatedText; } public String generateStream(String userInput) { WatsonxAiOptions options = WatsonxAiOptions.create() .withModel(MODEL) .withDecodingMethod(""greedy"") .withRandomSeed(2); Prompt prompt = new Prompt(new SystemMessage(userInput), options); var results = chatModel.stream(prompt).collectList().block(); // wait till the stream is resolved (completed) var generatedText = results.stream() .map(generation -> generation.getResult().getOutput().getContent()) .collect(Collectors.joining()); return generatedText; } } ZhiPu AI(zhipuai-chat.html) Embeddings Model API(../embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings.html","Embeddings Model API: Embeddings are numerical representations of text, images, or videos that capture relationships between inputs. Embeddings work by converting text, image, and video into arrays of floating point numbers, called vectors. These vectors are designed to capture the meaning of the text, images, and videos. The length of the embedding array is called the vector’s dimensionality. By calculating the numerical distance between the vector representations of two pieces of text, an application can determine the similarity between the objects used to generate the embedding vectors. The EmbeddingModel interface is designed for straightforward integration with embedding models in AI and machine learning. Its primary function is to convert text into numerical vectors, commonly referred to as embeddings. These embeddings are crucial for various tasks such as semantic analysis and text classification. The design of the EmbeddingModel interface centers around two primary goals: Portability : This interface ensures easy adaptability across various embedding models. It allows developers to switch between different embedding techniques or models with minimal code changes. This design aligns with Spring’s philosophy of modularity and interchangeability. Simplicity : EmbeddingModel simplifies the process of converting text to embeddings. By providing straightforward methods like embed(String text) and embed(Document document) , it takes the complexity out of dealing with raw text data and embedding algorithms. This design choice makes it easier for developers, especially those new to AI, to utilize embeddings in their applications without delving deep into the underlying mechanics. API Overview: The Embedding Model API is built on top of the generic Spring AI Model API(https://github.com/spring-projects/spring-ai/tree/main/spring-ai-core/src/main/java/org/springframework/ai/model) , which is a part of the Spring AI library. As such, the EmbeddingModel interface extends the Model interface, which provides a standard set of methods for interacting with AI models. The EmbeddingRequest and EmbeddingResponse classes extend from the ModelRequest and ModelResponse are used to encapsulate the input and output of the embedding models, respectively. The Embedding API in turn is used by higher-level components to implement Embedding Models for specific embedding models, such as OpenAI, Titan, Azure OpenAI, Ollie, and others. Following diagram illustrates the Embedding API and its relationship with the Spring AI Model API and the Embedding Models: EmbeddingModel: This section provides a guide to the EmbeddingModel interface and associated classes. public interface EmbeddingModel extends Model<EmbeddingRequest, EmbeddingResponse> { @Override EmbeddingResponse call(EmbeddingRequest request); /** * Embeds the given document's content into a vector. * @param document the document to embed. * @return the embedded vector. */ float[] embed(Document document); /** * Embeds the given text into a vector. * @param text the text to embed. * @return the embedded vector. */ default float[] embed(String text) { Assert.notNull(text, ""Text must not be null""); return this.embed(List.of(text)).iterator().next(); } /** * Embeds a batch of texts into vectors. * @param texts list of texts to embed. * @return list of list of embedded vectors. */ default List<float[]> embed(List<String> texts) { Assert.notNull(texts, ""Texts must not be null""); return this.call(new EmbeddingRequest(texts, EmbeddingOptions.EMPTY)) .getResults() .stream() .map(Embedding::getOutput) .toList(); } /** * Embeds a batch of texts into vectors and returns the {@link EmbeddingResponse}. * @param texts list of texts to embed. * @return the embedding response. */ default EmbeddingResponse embedForResponse(List<String> texts) { Assert.notNull(texts, ""Texts must not be null""); return this.call(new EmbeddingRequest(texts, EmbeddingOptions.EMPTY)); } /** * @return the number of dimensions of the embedded vectors. It is generative * specific. */ default int dimensions() { return embed(""Test String"").size(); } } The embed methods offer various options for converting text into embeddings, accommodating single strings, structured Document objects, or batches of text. Multiple shortcut methods are provided for embedding text, including the embed(String text) method, which takes a single string and returns the corresponding embedding vector. All shortcuts are implemented around the call method, which is the primary method for invoking the embedding model. Typically the embedding returns a lists of floats, representing the embeddings in a numerical vector format. The embedForResponse method provides a more comprehensive output, potentially including additional information about the embeddings. The dimensions method is a handy tool for developers to quickly ascertain the size of the embedding vectors, which is important for understanding the embedding space and for subsequent processing steps. EmbeddingRequest: The EmbeddingRequest is a ModelRequest that takes a list of text objects and optional embedding request options. The following listing shows a truncated version of the EmbeddingRequest class, excluding constructors and other utility methods: public class EmbeddingRequest implements ModelRequest<List<String>> { private final List<String> inputs; private final EmbeddingOptions options; // other methods omitted } EmbeddingResponse: The structure of the EmbeddingResponse class is as follows: public class EmbeddingResponse implements ModelResponse<Embedding> { private List<Embedding> embeddings; private EmbeddingResponseMetadata metadata = new EmbeddingResponseMetadata(); // other methods omitted } The EmbeddingResponse class holds the AI Model’s output, with each Embedding instance containing the result vector data from a single text input. The EmbeddingResponse class also carries a EmbeddingResponseMetadata metadata about the AI Model’s response. Embedding: The Embedding represents a single embedding vector. public class Embedding implements ModelResult<float[]> { private float[] embedding; private Integer index; private EmbeddingResultMetadata metadata; // other methods omitted } Available Implementations: Internally the various EmbeddingModel implementations use different low-level libraries and APIs to perform the embedding tasks. The following are some of the available implementations of the EmbeddingModel implementations: Spring AI OpenAI Embeddings(embeddings/openai-embeddings.html) Spring AI Azure OpenAI Embeddings(embeddings/azure-openai-embeddings.html) Spring AI Ollama Embeddings(embeddings/ollama-embeddings.html) Spring AI Transformers (ONNX) Embeddings(embeddings/onnx.html) Spring AI PostgresML Embeddings(embeddings/postgresml-embeddings.html) Spring AI Bedrock Cohere Embeddings(embeddings/bedrock-cohere-embedding.html) Spring AI Bedrock Titan Embeddings(embeddings/bedrock-titan-embedding.html) Spring AI VertexAI Embeddings(embeddings/vertexai-embeddings-text.html) Spring AI VertexAI PaLM2 Embeddings(embeddings/vertexai-embeddings-palm2.html) Spring AI Mistral AI Embeddings(embeddings/mistralai-embeddings.html) Spring AI Oracle Cloud Infrastructure GenAI Embeddings(embeddings/oci-genai-embeddings.html) watsonx.AI(chat/watsonx-ai-chat.html) Amazon Bedrock(bedrock.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/bedrock.html","Amazon Bedrock: Amazon Bedrock(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) is a managed service that provides foundation models from various AI providers, available through a unified API. Spring AI supports all the Chat and Embedding AI models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) available through Amazon Bedrock by implementing the Spring interfaces ChatModel , StreamingChatModel , and EmbeddingModel . Additionally, Spring AI provides Spring Auto-Configurations and Boot Starters for all clients, making it easy to bootstrap and configure for the Bedrock models. Getting Started: There are a few steps to get started Add the Spring Boot starter for Bedrock to your project. Obtain AWS credentials: If you don’t have an AWS account and AWS CLI configured yet, this video guide can help you configure it: AWS CLI & SDK Setup in Less Than 4 Minutes!(https://youtu.be/gswVHTrRX8I?si=buaY7aeI0l3-bBVb) . You should be able to obtain your access and security keys. Enable the Models to use: Go to Amazon Bedrock(https://us-east-1.console.aws.amazon.com/bedrock/home) and from the Model Access(https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/modelaccess) menu on the left, configure access to the models you are going to use. Project Dependencies: Then add the Spring Boot Starter dependency to your project’s Maven pom.xml build file: <dependency> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> <groupId>org.springframework.ai</groupId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Connect to AWS Bedrock: Use the BedrockAwsConnectionProperties to configure AWS credentials and region: spring.ai.bedrock.aws.region=us-east-1 spring.ai.bedrock.aws.access-key=YOUR_ACCESS_KEY spring.ai.bedrock.aws.secret-key=YOUR_SECRET_KEY spring.ai.bedrock.aws.timeout=10m The region property is compulsory. AWS credentials are resolved in the following order: Spring-AI Bedrock spring.ai.bedrock.aws.access-key and spring.ai.bedrock.aws.secret-key properties. Java System Properties - aws.accessKeyId and aws.secretAccessKey . Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY . Web Identity Token credentials from system properties or environment variables. Credential profiles file at the default location ( ~/.aws/credentials ) shared by all AWS SDKs and the AWS CLI. Credentials delivered through the Amazon EC2 container service if the AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variable is set and the security manager has permission to access the variable. Instance profile credentials delivered through the Amazon EC2 metadata service or set the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. AWS region is resolved in the following order: Spring-AI Bedrock spring.ai.bedrock.aws.region property. Java System Properties - aws.region . Environment Variables - AWS_REGION . Credential profiles file at the default location ( ~/.aws/credentials ) shared by all AWS SDKs and the AWS CLI. Instance profile region delivered through the Amazon EC2 metadata service. In addition to the standard Spring-AI Bedrock credentials and region properties configuration, Spring-AI provides support for custom AwsCredentialsProvider and AwsRegionProvider beans. For example, using Spring-AI and Spring Cloud for Amazon Web Services(https://spring.io/projects/spring-cloud-aws) at the same time. Spring-AI is compatible with Spring Cloud for Amazon Web Services credential configuration. Enable selected Bedrock model: By default, all models are disabled. You have to enable the chosen Bedrock models explicitly using the spring.ai.bedrock.<model>.<chat|embedding>.enabled=true property. Here are the supported <model> and <chat|embedding> combinations: Model Chat Chat Streaming Embedding llama Yes Yes No jurassic2 Yes No No cohere Yes Yes Yes anthropic 2 Yes Yes No anthropic 3 Yes Yes No jurassic2 (WIP) Yes No No titan Yes Yes Yes (however, no batch support) For example, to enable the Bedrock Llama chat model, you need to set spring.ai.bedrock.llama.chat.enabled=true . Next, you can use the spring.ai.bedrock.<model>.<chat|embedding>.* properties to configure each model as provided. For more information, refer to the documentation below for each supported model. Spring AI Bedrock Anthropic 2 Chat(chat/bedrock/bedrock-anthropic.html) : spring.ai.bedrock.anthropic.chat.enabled=true Spring AI Bedrock Anthropic 3 Chat(chat/bedrock/bedrock-anthropic3.html) : spring.ai.bedrock.anthropic.chat.enabled=true Spring AI Bedrock Llama Chat(chat/bedrock/bedrock-llama.html) : spring.ai.bedrock.llama.chat.enabled=true Spring AI Bedrock Cohere Chat(chat/bedrock/bedrock-cohere.html) : spring.ai.bedrock.cohere.chat.enabled=true Spring AI Bedrock Cohere Embeddings(embeddings/bedrock-cohere-embedding.html) : spring.ai.bedrock.cohere.embedding.enabled=true Spring AI Bedrock Titan Chat(chat/bedrock/bedrock-titan.html) : spring.ai.bedrock.titan.chat.enabled=true Spring AI Bedrock Titan Embeddings(embeddings/bedrock-titan-embedding.html) : spring.ai.bedrock.titan.embedding.enabled=true Spring AI Bedrock Ai21 Jurassic2 Chat(chat/bedrock/bedrock-jurassic2.html) : spring.ai.bedrock.jurassic2.chat.enabled=true Embeddings Model API(embeddings.html) Cohere(embeddings/bedrock-cohere-embedding.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/bedrock-cohere-embedding.html","Cohere Embeddings: Provides Bedrock Cohere Embedding model. Integrate generative AI capabilities into essential apps and workflows that improve business outcomes. The AWS Bedrock Cohere Model Page(https://aws.amazon.com/bedrock/cohere-command-embed/) and Amazon Bedrock User Guide(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) contains detailed information on how to use the AWS hosted model. Prerequisites: Refer to the Spring AI documentation on Amazon Bedrock(../bedrock.html) for setting up API access. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Add the spring-ai-bedrock-ai-spring-boot-starter dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Enable Cohere Embedding Support: By default the Cohere model is disabled. To enable it set the spring.ai.bedrock.cohere.embedding.enabled property to true . Exporting environment variable is one way to set this configuration property: export SPRING_AI_BEDROCK_COHERE_EMBEDDING_ENABLED=true Embedding Properties: The prefix spring.ai.bedrock.aws is the property prefix to configure the connection to AWS Bedrock. Property Description Default spring.ai.bedrock.aws.region AWS region to use. us-east-1 spring.ai.bedrock.aws.access-key AWS access key. - spring.ai.bedrock.aws.secret-key AWS secret key. - The prefix spring.ai.bedrock.cohere.embedding (defined in BedrockCohereEmbeddingProperties ) is the property prefix that configures the embedding model implementation for Cohere. Property Description Default spring.ai.bedrock.cohere.embedding.enabled Enable or disable support for Cohere false spring.ai.bedrock.cohere.embedding.model The model id to use. See the CohereEmbeddingModel(https://github.com/spring-projects/spring-ai/blob/056b95a00efa5b014a1f488329fbd07a46c02378/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereEmbeddingBedrockApi.java#L150) for the supported models. cohere.embed-multilingual-v3 spring.ai.bedrock.cohere.embedding.options.input-type Prepends special tokens to differentiate each type from one another. You should not mix different types together, except when mixing types for search and retrieval. In this case, embed your corpus with the search_document type and embedded queries with type search_query type. SEARCH_DOCUMENT spring.ai.bedrock.cohere.embedding.options.truncate Specifies how the API handles inputs longer than the maximum token length. If you specify LEFT or RIGHT, the model discards the input until the remaining input is exactly the maximum input token length for the model. NONE Look at the CohereEmbeddingModel(https://github.com/spring-projects/spring-ai/blob/056b95a00efa5b014a1f488329fbd07a46c02378/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereEmbeddingBedrockApi.java#L150) for other model IDs. Supported values are: cohere.embed-multilingual-v3 and cohere.embed-english-v3 . Model ID values can also be found in the AWS Bedrock documentation for base model IDs(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) . All properties prefixed with spring.ai.bedrock.cohere.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The BedrockCohereEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereEmbeddingOptions.java) provides model configurations, such as input-type or truncate . On start-up, the default options can be configured with the BedrockCohereEmbeddingModel(api, options) constructor or the spring.ai.bedrock.cohere.embedding.options.* properties. At run-time you can override the default options by adding new, request specific, options to the EmbeddingRequest call. For example to override the default temperature for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), BedrockCohereEmbeddingOptions.builder() .withInputType(InputType.SEARCH_DOCUMENT) .build())); Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-bedrock-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Cohere Embedding model: spring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.bedrock.cohere.embedding.enabled=true spring.ai.bedrock.cohere.embedding.options.input-type=search-document replace the regions , access-key and secret-key with your AWS credentials. This will create a BedrockCohereEmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: The BedrockCohereEmbeddingModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereEmbeddingModel.java) implements the EmbeddingModel and uses the Low-level CohereEmbeddingBedrockApi Client(#low-level-api) to connect to the Bedrock Cohere service. Add the spring-ai-bedrock dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an BedrockCohereEmbeddingModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/BedrockCohereEmbeddingModel.java) and use it for text embeddings: var cohereEmbeddingApi =new CohereEmbeddingBedrockApi( CohereEmbeddingModel.COHERE_EMBED_MULTILINGUAL_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper()); var embeddingModel = new BedrockCohereEmbeddingModel(cohereEmbeddingApi); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); Low-level CohereEmbeddingBedrockApi Client: The CohereEmbeddingBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/cohere/api/CohereEmbeddingBedrockApi.java) provides is lightweight Java client on top of AWS Bedrock Cohere Command models(https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command.html) . Following class diagram illustrates the CohereEmbeddingBedrockApi interface and building blocks: The CohereEmbeddingBedrockApi supports the cohere.embed-english-v3 and cohere.embed-multilingual-v3 models for single and batch embedding computation. Here is a simple snippet how to use the api programmatically: CohereEmbeddingBedrockApi api = new CohereEmbeddingBedrockApi( CohereEmbeddingModel.COHERE_EMBED_MULTILINGUAL_V1.id(), EnvironmentVariableCredentialsProvider.create(), Region.US_EAST_1.id(), new ObjectMapper()); CohereEmbeddingRequest request = new CohereEmbeddingRequest( List.of(""I like to eat apples"", ""I like to eat oranges""), CohereEmbeddingRequest.InputType.search_document, CohereEmbeddingRequest.Truncate.NONE); CohereEmbeddingResponse response = api.embedding(request); Amazon Bedrock(../bedrock.html) Titan(bedrock-titan-embedding.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/bedrock-titan-embedding.html","Titan Embeddings: Provides Bedrock Titan Embedding model. Amazon Titan(https://aws.amazon.com/bedrock/titan/) foundation models (FMs) provide customers with a breadth of high-performing image, multimodal embeddings, and text model choices, via a fully managed API. Amazon Titan models are created by AWS and pretrained on large datasets, making them powerful, general-purpose models built to support a variety of use cases, while also supporting the responsible use of AI. Use them as is or privately customize them with your own data. Bedrock Titan Embedding supports Text and Image embedding. Bedrock Titan Embedding does NOT support batch embedding. The AWS Bedrock Titan Model Page(https://aws.amazon.com/bedrock/titan/) and Amazon Bedrock User Guide(https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) contains detailed information on how to use the AWS hosted model. Prerequisites: Refer to the Spring AI documentation on Amazon Bedrock(../bedrock.html) for setting up API access. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Add the spring-ai-bedrock-ai-spring-boot-starter dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock-ai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Enable Titan Embedding Support: By default the Titan embedding model is disabled. To enable it set the spring.ai.bedrock.titan.embedding.enabled property to true . Exporting environment variable is one way to set this configuration property: export SPRING_AI_BEDROCK_TITAN_EMBEDDING_ENABLED=true Embedding Properties: The prefix spring.ai.bedrock.aws is the property prefix to configure the connection to AWS Bedrock. Property Description Default spring.ai.bedrock.aws.region AWS region to use. us-east-1 spring.ai.bedrock.aws.access-key AWS access key. - spring.ai.bedrock.aws.secret-key AWS secret key. - The prefix spring.ai.bedrock.titan.embedding (defined in BedrockTitanEmbeddingProperties ) is the property prefix that configures the embedding model implementation for Titan. Property Description Default spring.ai.bedrock.titan.embedding.enabled Enable or disable support for Titan embedding false spring.ai.bedrock.titan.embedding.model The model id to use. See the TitanEmbeddingModel for the supported models. amazon.titan-embed-image-v1 Supported values are: amazon.titan-embed-image-v1 , amazon.titan-embed-text-v1 and amazon.titan-embed-text-v2:0 . Model ID values can also be found in the AWS Bedrock documentation for base model IDs(https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) . Runtime Options: The BedrockTitanEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanEmbeddingOptions.java) provides model configurations, such as input-type . On start-up, the default options can be configured with the BedrockTitanEmbeddingModel(api).withInputType(type) method or the spring.ai.bedrock.titan.embedding.input-type properties. At run-time you can override the default options by adding new, request specific, options to the EmbeddingRequest call. For example to override the default temperature for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), BedrockTitanEmbeddingOptions.builder() .withInputType(InputType.TEXT) .build())); Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-bedrock-ai-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the Titan Embedding model: spring.ai.bedrock.aws.region=eu-central-1 spring.ai.bedrock.aws.access-key=${AWS_ACCESS_KEY_ID} spring.ai.bedrock.aws.secret-key=${AWS_SECRET_ACCESS_KEY} spring.ai.bedrock.titan.embedding.enabled=true replace the regions , access-key and secret-key with your AWS credentials. This will create a EmbeddingController implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the chat model for text generations. @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: The BedrockTitanEmbeddingModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanEmbeddingModel.java) implements the EmbeddingModel and uses the Low-level TitanEmbeddingBedrockApi Client(#low-level-api) to connect to the Bedrock Titan service. Add the spring-ai-bedrock dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bedrock</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-bedrock' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an BedrockTitanEmbeddingModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/BedrockTitanEmbeddingModel.java) and use it for text embeddings: var titanEmbeddingApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_IMAGE_V1.id(), Region.US_EAST_1.id()); var embeddingModel = new BedrockTitanEmbeddingModel(titanEmbeddingApi); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"")); // NOTE titan does not support batch embedding. Low-level TitanEmbeddingBedrockApi Client: The TitanEmbeddingBedrockApi(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/main/java/org/springframework/ai/bedrock/titan/api/TitanEmbeddingBedrockApi.java) provides is lightweight Java client on top of AWS Bedrock Titan Embedding models(https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) . Following class diagram illustrates the TitanEmbeddingBedrockApi interface and building blocks: The TitanEmbeddingBedrockApi supports the amazon.titan-embed-image-v1 and amazon.titan-embed-image-v1 models for single and batch embedding computation. Here is a simple snippet how to use the api programmatically: TitanEmbeddingBedrockApi titanEmbedApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_TEXT_V1.id(), Region.US_EAST_1.id()); TitanEmbeddingRequest request = TitanEmbeddingRequest.builder() .withInputText(""I like to eat apples."") .build(); TitanEmbeddingResponse response = titanEmbedApi.embedding(request); To embed an image you need to convert it into base64 format: TitanEmbeddingBedrockApi titanEmbedApi = new TitanEmbeddingBedrockApi( TitanEmbeddingModel.TITAN_EMBED_IMAGE_V1.id(), Region.US_EAST_1.id()); byte[] image = new DefaultResourceLoader() .getResource(""classpath:/spring_framework.png"") .getContentAsByteArray(); TitanEmbeddingRequest request = TitanEmbeddingRequest.builder() .withInputImage(Base64.getEncoder().encodeToString(image)) .build(); TitanEmbeddingResponse response = titanEmbedApi.embedding(request); Cohere(bedrock-cohere-embedding.html) Azure OpenAI(azure-openai-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/azure-openai-embeddings.html","Azure OpenAI Embeddings: Azure’s OpenAI extends the OpenAI capabilities, offering safe text generation and Embeddings computation models for various task: Similarity embeddings are good at capturing semantic similarity between two or more pieces of text. Text search embeddings help measure whether long documents are relevant to a short query. Code search embeddings are useful for embedding code snippets and embedding natural language search queries. The Azure OpenAI embeddings rely on cosine similarity to compute similarity between documents and a query. Prerequisites: The Azure OpenAI client offers three options to connect: using an Azure API key or using an OpenAI API Key, or using Microsoft Entra ID. Azure API Key & Endpoint: Obtain your Azure OpenAI endpoint and api-key from the Azure OpenAI Service section on the Azure Portal(https://portal.azure.com) . Spring AI defines two configuration properties: spring.ai.azure.openai.api-key : Set this to the value of the API Key obtained from Azure. spring.ai.azure.openai.endpoint : Set this to the endpoint URL obtained when provisioning your model in Azure. You can set these configuration properties by exporting environment variables: export SPRING_AI_AZURE_OPENAI_API_KEY=<INSERT AZURE KEY HERE> export SPRING_AI_AZURE_OPENAI_ENDPOINT=<INSERT ENDPOINT URL HERE> OpenAI Key: To authenticate with the OpenAI service (not Azure), provide an OpenAI API key. This will automatically set the endpoint to api.openai.com/v1(https://api.openai.com/v1) . When using this approach, set the spring.ai.azure.openai.chat.options.deployment-name property to the name of the OpenAI model(https://platform.openai.com/docs/models) you wish to use. export SPRING_AI_AZURE_OPENAI_OPENAI_API_KEY=<INSERT OPENAI KEY HERE> Microsoft Entra ID: To authenticate using Microsoft Entra ID (formerly Azure Active Directory), create a TokenCredential bean in your configuration. If this bean is available, an OpenAIClient instance will be created using the token credentials. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure OpenAI Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-azure-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: The prefix spring.ai.azure.openai is the property prefix to configure the connection to Azure OpenAI. Property Description Default spring.ai.azure.openai.api-key The Key from Azure AI OpenAI Keys and Endpoint section under Resource Management - spring.ai.azure.openai.endpoint The endpoint from the Azure AI OpenAI Keys and Endpoint section under Resource Management - spring.ai.azure.openai.openai-api-key (non Azure) OpenAI API key. Used to authenticate with the OpenAI service, instead of Azure OpenAI. This automatically sets the endpoint to api.openai.com/v1(https://api.openai.com/v1) . Use either api-key or openai-api-key property. With this configuraiton the spring.ai.azure.openai.embedding.options.deployment-name is threated as an OpenAi Model(https://platform.openai.com/docs/models) name. - The prefix spring.ai.azure.openai.embedding is the property prefix that configures the EmbeddingModel implementation for Azure OpenAI Property Description Default spring.ai.azure.openai.embedding.enabled Enable Azure OpenAI embedding model. true spring.ai.azure.openai.embedding.metadata-mode Document content extraction mode EMBED spring.ai.azure.openai.embedding.options.deployment-name This is the value of the 'Deployment Name' as presented in the Azure AI Portal text-embedding-ada-002 spring.ai.azure.openai.embedding.options.user An identifier for the caller or end user of the operation. This may be used for tracking or rate-limiting purposes. - All properties prefixed with spring.ai.azure.openai.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The AzureOpenAiEmbeddingOptions provides the configuration information for the embedding requests. The AzureOpenAiEmbeddingOptions offers a builder to create the options. At start time use the AzureOpenAiEmbeddingModel constructor to set the default options used for all embedding requests. At run-time you can override the default options, by passing a AzureOpenAiEmbeddingOptions instance with your to the EmbeddingRequest request. For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), AzureOpenAiEmbeddingOptions.builder() .withModel(""Different-Embedding-Model-Deployment-Name"") .build())); Sample Code: This will create a EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingModel implementation. spring.ai.azure.openai.api-key=YOUR_API_KEY spring.ai.azure.openai.endpoint=YOUR_ENDPOINT spring.ai.azure.openai.embedding.options.model=text-embedding-ada-002 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: If you prefer not to use the Spring Boot auto-configuration, you can manually configure the AzureOpenAiEmbeddingModel in your application. For this add the spring-ai-azure-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-openai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-azure-openai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-azure-openai dependency also provide the access to the AzureOpenAiEmbeddingModel . For more information about the AzureOpenAiChatModel refer to the Azure OpenAI Embeddings(../embeddings/azure-openai-embeddings.html) section. Next, create an AzureOpenAiEmbeddingModel instance and use it to compute the similarity between two input texts: var openAIClient = OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(""AZURE_OPENAI_API_KEY""))) .endpoint(System.getenv(""AZURE_OPENAI_ENDPOINT"")) .buildClient(); var embeddingModel = new AzureOpenAiEmbeddingModel(openAIClient) .withDefaultOptions(AzureOpenAiEmbeddingOptions.builder() .withModel(""text-embedding-ada-002"") .withUser(""user-6"") .build()); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); the text-embedding-ada-002 is actually the Deployment Name as presented in the Azure AI Portal. Titan(bedrock-titan-embedding.html) Mistral AI(mistralai-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/mistralai-embeddings.html","Mistral AI Embeddings: Spring AI supports the Mistral AI’s text embeddings models. Embeddings are vectorial representations of text that capture the semantic meaning of paragraphs through their position in a high dimensional vector space. Mistral AI Embeddings API offers cutting-edge, state-of-the-art embeddings for text, which can be used for many NLP tasks. Prerequisites: You will need to create an API with MistralAI to access MistralAI embeddings models. Create an account at MistralAI registration page(https://auth.mistral.ai/ui/registration) and generate the token on the API Keys page(https://console.mistral.ai/api-keys/) . The Spring AI project defines a configuration property named spring.ai.mistralai.api-key that you should set to the value of the API Key obtained from console.mistral.ai. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_MISTRALAI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the MistralAI Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-mistral-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-mistral-ai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the Mistral AI Embedding model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.mistralai is used as the property prefix that lets you connect to MistralAI. Property Description Default spring.ai.mistralai.base-url The URL to connect to api.mistral.ai(https://api.mistral.ai) spring.ai.mistralai.api-key The API Key - Configuration Properties: The prefix spring.ai.mistralai.embedding is property prefix that configures the EmbeddingModel implementation for MistralAI. Property Description Default spring.ai.mistralai.embedding.enabled Enable OpenAI embedding model. true spring.ai.mistralai.embedding.base-url Optional overrides the spring.ai.mistralai.base-url to provide embedding specific url - spring.ai.mistralai.embedding.api-key Optional overrides the spring.ai.mistralai.api-key to provide embedding specific api-key - spring.ai.mistralai.embedding.metadata-mode Document content extraction mode. EMBED spring.ai.mistralai.embedding.options.model The model to use mistral-embed spring.ai.mistralai.embedding.options.encodingFormat The format to return the embeddings in. Can be either float or base64. - You can override the common spring.ai.mistralai.base-url and spring.ai.mistralai.api-key for the ChatModel and EmbeddingModel implementations. The spring.ai.mistralai.embedding.base-url and spring.ai.mistralai.embedding.api-key properties if set take precedence over the common properties. Similarly, the spring.ai.mistralai.embedding.base-url and spring.ai.mistralai.embedding.api-key properties if set take precedence over the common properties. This is useful if you want to use different MistralAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.mistralai.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The MistralAiEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/main/java/org/springframework/ai/mistralai/MistralAiEmbeddingOptions.java) provides the MistralAI configurations, such as the model to use and etc. The default options can be configured using the spring.ai.mistralai.embedding.options properties as well. At start-time use the MistralAiEmbeddingModel constructor to set the default options used for all embedding requests. At run-time you can override the default options, using a MistralAiEmbeddingOptions instance as part of your EmbeddingRequest . For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), MistralAiEmbeddingOptions.builder() .withModel(""Different-Embedding-Model-Deployment-Name"") .build())); Sample Controller: This will create a EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingModel implementation. spring.ai.mistralai.api-key=YOUR_API_KEY spring.ai.mistralai.embedding.options.model=mistral-embed @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { var embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: If you are not using Spring Boot, you can manually configure the OpenAI Embedding Model. For this add the spring-ai-mistral-ai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-mistral-ai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-mistral-ai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-mistral-ai dependency provides access also to the MistralAiChatModel . For more information about the MistralAiChatModel refer to the MistralAI Chat Client(../chat/mistralai-chat.html) section. Next, create an MistralAiEmbeddingModel instance and use it to compute the similarity between two input texts: var mistralAiApi = new MistralAiApi(System.getenv(""MISTRAL_AI_API_KEY"")); var embeddingModel = new MistralAiEmbeddingModel(mistralAiApi, MistralAiEmbeddingOptions.builder() .withModel(""mistral-embed"") .withEncodingFormat(""float"") .build()); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); The MistralAiEmbeddingOptions provides the configuration information for the embedding requests. The options class offers a builder() for easy options creation. Azure OpenAI(azure-openai-embeddings.html) MiniMax(minimax-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/minimax-embeddings.html","MiniMax Chat: Spring AI supports the various AI language models from MiniMax. You can interact with MiniMax language models and create a multilingual conversational assistant based on MiniMax models. Prerequisites: You will need to create an API with MiniMax to access MiniMax language models. Create an account at MiniMax registration page(https://www.minimaxi.com/login) and generate the token on the API Keys page(https://www.minimaxi.com/user-center/basic-information/interface-key) . The Spring AI project defines a configuration property named spring.ai.minimax.api-key that you should set to the value of the API Key obtained from API Keys page(https://www.minimaxi.com/user-center/basic-information/interface-key) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_MINIMAX_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure MiniMax Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-minimax-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-minimax-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the MiniMax Embedding model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.minimax is used as the property prefix that lets you connect to MiniMax. Property Description Default spring.ai.minimax.base-url The URL to connect to api.minimax.chat(https://api.minimax.chat) spring.ai.minimax.api-key The API Key - Configuration Properties: The prefix spring.ai.minimax.embedding is property prefix that configures the EmbeddingModel implementation for MiniMax. Property Description Default spring.ai.minimax.embedding.enabled Enable MiniMax embedding model. true spring.ai.minimax.embedding.base-url Optional overrides the spring.ai.minimax.base-url to provide embedding specific url - spring.ai.minimax.embedding.api-key Optional overrides the spring.ai.minimax.api-key to provide embedding specific api-key - spring.ai.minimax.embedding.options.model The model to use embo-01 You can override the common spring.ai.minimax.base-url and spring.ai.minimax.api-key for the ChatModel and EmbeddingModel implementations. The spring.ai.minimax.embedding.base-url and spring.ai.minimax.embedding.api-key properties if set take precedence over the common properties. Similarly, the spring.ai.minimax.embedding.base-url and spring.ai.minimax.embedding.api-key properties if set take precedence over the common properties. This is useful if you want to use different MiniMax accounts for different models and different model endpoints. All properties prefixed with spring.ai.minimax.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The MiniMaxEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-minimax/src/main/java/org/springframework/ai/minimax/MiniMaxEmbeddingOptions.java) provides the MiniMax configurations, such as the model to use and etc. The default options can be configured using the spring.ai.minimax.embedding.options properties as well. At start-time use the MiniMaxEmbeddingModel constructor to set the default options used for all embedding requests. At run-time you can override the default options, using a MiniMaxEmbeddingOptions instance as part of your EmbeddingRequest . For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), MiniMaxEmbeddingOptions.builder() .withModel(""Different-Embedding-Model-Deployment-Name"") .build())); Sample Controller: This will create a EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingC implementation. spring.ai.minimax.api-key=YOUR_API_KEY spring.ai.minimax.embedding.options.model=embo-01 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: If you are not using Spring Boot, you can manually configure the MiniMax Embedding Model. For this add the spring-ai-minimax dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-minimax</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-minimax' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-minimax dependency provides access also to the MiniMaxChatModel . For more information about the `MiniMaxChatModel refer to the MiniMax Chat Client(../chat/minimax-chat.html) section. Next, create an MiniMaxEmbeddingModel instance and use it to compute the similarity between two input texts: var miniMaxApi = new MiniMaxApi(System.getenv(""MINIMAX_API_KEY"")); var embeddingModel = new MiniMaxEmbeddingModel(miniMaxApi) .withDefaultOptions(MiniMaxChatOptions.build() .withModel(""embo-01"") .build()); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); The MiniMaxEmbeddingOptions provides the configuration information for the embedding requests. The options class offers a builder() for easy options creation. Mistral AI(mistralai-embeddings.html) OCI GenAI(oci-genai-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/oci-genai-embeddings.html","Oracle Cloud Infrastructure (OCI) GenAI Embeddings: OCI GenAI Service(https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/) offers text embedding with on-demand models, or dedicated AI clusters. The OCI Embedding Models Page(https://docs.oracle.com/en-us/iaas/Content/generative-ai/embed-models.htm) and OCI Text Embeddings Page(https://docs.oracle.com/en-us/iaas/Content/generative-ai/use-playground-embed.htm) provide detailed information about using and hosting embedding models on OCI. Prerequisites: Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OCI GenAI Embedding Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-oci-genai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-oci-genai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: The prefix spring.ai.oci.genai is the property prefix to configure the connection to OCI GenAI. Property Description Default spring.ai.oci.genai.authenticationType The type of authentication to use when authenticating to OCI. May be file , instance-principal , workload-identity , or simple . file spring.ai.oci.genai.region OCI service region. us-chicago-1 spring.ai.oci.genai.tenantId OCI tenant OCID, used when authenticating with simple auth. - spring.ai.oci.genai.userId OCI user OCID, used when authenticating with simple auth. - spring.ai.oci.genai.fingerprint Private key fingerprint, used when authenticating with simple auth. - spring.ai.oci.genai.privateKey Private key content, used when authenticating with simple auth. - spring.ai.oci.genai.passPhrase Optional private key passphrase, used when authenticating with simple auth and a passphrase protected private key. - spring.ai.oci.genai.file Path to OCI config file. Used when authenticating with file auth. <user’s home directory>/.oci/config spring.ai.oci.genai.profile OCI profile name. Used when authenticating with file auth. DEFAULT spring.ai.oci.genai.endpoint Optional OCI GenAI endpoint. - The prefix spring.ai.oci.genai.embedding is the property prefix that configures the EmbeddingModel implementation for OCI GenAI Property Description Default spring.ai.oci.genai.embedding.enabled Enable OCI GenAI embedding model. true spring.ai.oci.genai.embedding.compartment Model compartment OCID. - spring.ai.oci.genai.embedding.servingMode The model serving mode to be used. May be on-demand , or dedicated . on-demand spring.ai.oci.genai.embedding.truncate How to truncate text if it overruns the embedding context. May be START , or END . END spring.ai.oci.genai.embedding.model The model or model endpoint used for embeddings. - All properties prefixed with spring.ai.oci.genai.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The OCIEmbeddingOptions provides the configuration information for the embedding requests. The OCIEmbeddingOptions offers a builder to create the options. At start time use the OCIEmbeddingOptions constructor to set the default options used for all embedding requests. At run-time you can override the default options, by passing a OCIEmbeddingOptions instance with your to the EmbeddingRequest request. For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), OCIEmbeddingOptions.builder() .withModel(""my-other-embedding-model"") .build() )); Sample Code: This will create a EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingModel implementation. spring.ai.oci.genai.embedding.model=<your model> spring.ai.oci.genai.embedding.compartment=<your model compartment> @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: If you prefer not to use the Spring Boot auto-configuration, you can manually configure the OCIEmbeddingModel in your application. For this add the spring-oci-genai-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-oci-genai-openai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-oci-genai-openai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an OCIEmbeddingModel instance and use it to compute the similarity between two input texts: final String EMBEDDING_MODEL = ""cohere.embed-english-light-v2.0""; final String CONFIG_FILE = Paths.get(System.getProperty(""user.home""), "".oci"", ""config"").toString(); final String PROFILE = ""DEFAULT""; final String REGION = ""us-chicago-1""; final String COMPARTMENT_ID = System.getenv(""OCI_COMPARTMENT_ID""); var authProvider = new ConfigFileAuthenticationDetailsProvider( CONFIG_FILE, PROFILE); var aiClient = GenerativeAiInferenceClient.builder() .region(Region.valueOf(REGION)) .build(authProvider); var options = OCIEmbeddingOptions.builder() .withModel(EMBEDDING_MODEL) .withCompartment(COMPARTMENT_ID) .withServingMode(""on-demand"") .build(); var embeddingModel = new OCIEmbeddingModel(aiClient, options); List<Double> embedding = embeddingModel.embed(new Document(""How many provinces are in Canada?"")); MiniMax(minimax-embeddings.html) Ollama(ollama-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/ollama-embeddings.html","Ollama Embeddings: With Ollama(https://ollama.ai/) you can run various AI Models(https://ollama.com/search?c=embedding) locally and generate embeddings from them. An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. The OllamaEmbeddingModel implementation lerverages the Ollama Embeddings API(https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings) endpoint. Prerequisites: You first need to run Ollama on your local machine. Refer to the official Ollama project README(https://github.com/ollama/ollama) to get started running models on your local machine. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure Ollama Embedding Model. To enable it add the following dependency to your Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-ollama-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-ollama-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories section to add these repositories to your build system. Embedding Properties: The prefix spring.ai.ollama is the property prefix to configure the connection to Ollama Property Description Default spring.ai.ollama.base-url Base URL where Ollama API server is running. localhost:11434(http://localhost:11434) The prefix spring.ai.ollama.embedding.options is the property prefix that configures the Ollama embedding model . It includes the Ollama request (advanced) parameters such as the model , keep-alive , and truncate as well as the Ollama model options properties. Here are the advanced request parameter for the Ollama embedding model: Property Description Default spring.ai.ollama.embedding.enabled Enables the Ollama embedding model auto-configuration. true spring.ai.ollama.embedding.options.model The name of the supported model(https://github.com/ollama/ollama?tab=readme-ov-file#model-library) to use. You can use dedicated Embedding Model(https://ollama.com/search?c=embedding) types mistral spring.ai.ollama.embedding.options.keep_alive Controls how long the model will stay loaded into memory following the request 5m spring.ai.ollama.embedding.options.truncate Truncates the end of each input to fit within context length. Returns error if false and context length is exceeded. true The remaining options properties are based on the Ollama Valid Parameters and Values(https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values) and Ollama Types(https://github.com/ollama/ollama/blob/main/api/types.go) . The default values are based on: Ollama type defaults(https://github.com/ollama/ollama/blob/b538dc3858014f94b099730a592751a5454cab0a/api/types.go#L364) . Property Description Default spring.ai.ollama.embedding.options.numa Whether to use NUMA. false spring.ai.ollama.embedding.options.num-ctx Sets the size of the context window used to generate the next token. 2048 spring.ai.ollama.embedding.options.num-batch Prompt processing maximum batch size. 512 spring.ai.ollama.embedding.options.num-gpu The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. 1 here indicates that NumGPU should be set dynamically -1 spring.ai.ollama.embedding.options.main-gpu When using multiple GPUs this option controls which GPU is used for small tensors for which the overhead of splitting the computation across all GPUs is not worthwhile. The GPU in question will use slightly more VRAM to store a scratch buffer for temporary results. 0 spring.ai.ollama.embedding.options.low-vram - false spring.ai.ollama.embedding.options.f16-kv - true spring.ai.ollama.embedding.options.logits-all Return logits for all the tokens, not just the last one. To enable completions to return logprobs, this must be true. - spring.ai.ollama.embedding.options.vocab-only Load only the vocabulary, not the weights. - spring.ai.ollama.embedding.options.use-mmap By default, models are mapped into memory, which allows the system to load only the necessary parts of the model as needed. However, if the model is larger than your total amount of RAM or if your system is low on available memory, using mmap might increase the risk of pageouts, negatively impacting performance. Disabling mmap results in slower load times but may reduce pageouts if you’re not using mlock. Note that if the model is larger than the total amount of RAM, turning off mmap would prevent the model from loading at all. null spring.ai.ollama.embedding.options.use-mlock Lock the model in memory, preventing it from being swapped out when memory-mapped. This can improve performance but trades away some of the advantages of memory-mapping by requiring more RAM to run and potentially slowing down load times as the model loads into RAM. false spring.ai.ollama.embedding.options.num-thread Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). 0 = let the runtime decide 0 spring.ai.ollama.embedding.options.num-keep - 4 spring.ai.ollama.embedding.options.seed Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. -1 spring.ai.ollama.embedding.options.num-predict Maximum number of tokens to predict when generating text. (-1 = infinite generation, -2 = fill context) -1 spring.ai.ollama.embedding.options.top-k Reduces the probability of generating nonsense. A higher value (e.g., 100) will give more diverse answers, while a lower value (e.g., 10) will be more conservative. 40 spring.ai.ollama.embedding.options.top-p Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. 0.9 spring.ai.ollama.embedding.options.tfs-z Tail-free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. 1.0 spring.ai.ollama.embedding.options.typical-p - 1.0 spring.ai.ollama.embedding.options.repeat-last-n Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx) 64 spring.ai.ollama.embedding.options.temperature The temperature of the model. Increasing the temperature will make the model answer more creatively. 0.8 spring.ai.ollama.embedding.options.repeat-penalty Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. 1.1 spring.ai.ollama.embedding.options.presence-penalty - 0.0 spring.ai.ollama.embedding.options.frequency-penalty - 0.0 spring.ai.ollama.embedding.options.mirostat Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) 0 spring.ai.ollama.embedding.options.mirostat-tau Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. 5.0 spring.ai.ollama.embedding.options.mirostat-eta Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. 0.1 spring.ai.ollama.embedding.options.penalize-newline - true spring.ai.ollama.embedding.options.stop Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile. - spring.ai.ollama.embedding.options.functions List of functions, identified by their names, to enable for function calling in a single prompt requests. Functions with those names must exist in the functionCallbacks registry. - All properties prefixed with spring.ai.ollama.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The OllamaOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/main/java/org/springframework/ai/ollama/api/OllamaOptions.java) provides the Ollama configurations, such as the model to use, the low level GPU and CPU tuning, etc. The default options can be configured using the spring.ai.ollama.embedding.options properties as well. At start-time use the OllamaEmbeddingModel(OllamaApi ollamaApi, OllamaOptions defaultOptions) to configure the default options used for all embedding requests. At run-time you can override the default options, using a OllamaOptions instance as part of your EmbeddingRequest . For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), OllamaOptions.builder() .withModel(""Different-Embedding-Model-Deployment-Name"")) .withtTuncates(false) .build()); Sample Controller: This will create a EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingModel implementation. @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: If you are not using Spring Boot, you can manually configure the OllamaEmbeddingModel . For this add the spring-ai-ollama dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-ollama</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-ollama' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-ollama dependency provides access also to the OllamaChatModel . For more information about the OllamaChatModel refer to the Ollama Chat Client(../chat/ollama-chat.html) section. Next, create an OllamaEmbeddingModel instance and use it to compute the embeddings for two input texts using a dedicated chroma/all-minilm-l6-v2-f32 embedding models: var ollamaApi = new OllamaApi(); var embeddingModel = new OllamaEmbeddingModel(ollamaApi, OllamaOptions.builder() .withModel(OllamaModel.MISTRAL.id()) .build()); EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), OllamaOptions.builder() .withModel(""chroma/all-minilm-l6-v2-f32"")) .withTruncate(false) .build()); The OllamaOptions provides the configuration information for all embedding requests. OCI GenAI(oci-genai-embeddings.html) (ONNX) Transformers(onnx.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/onnx.html","Transformers (ONNX) Embeddings: The TransformersEmbeddingModel is an EmbeddingModel implementation that locally computes sentence embeddings(https://www.sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers) using a selected sentence transformer(https://www.sbert.net/) . You can use any HuggingFace Embedding model(https://huggingface.co/spaces/mteb/leaderboard) . It uses pre-trained(https://www.sbert.net/docs/pretrained_models.html) transformer models, serialized into the Open Neural Network Exchange (ONNX)(https://onnx.ai/) format. The Deep Java Library(https://djl.ai/) and the Microsoft ONNX Java Runtime(https://onnxruntime.ai/docs/get-started/with-java.html) libraries are applied to run the ONNX models and compute the embeddings in Java. Prerequisites: To run things in Java, we need to serialize the Tokenizer and the Transformer Model into ONNX format. Serialize with optimum-cli - One, quick, way to achieve this, is to use the optimum-cli(https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) command line tool. The following snippet prepares a python virtual environment, installs the required packages and serializes (e.g. exports) the specified model using optimum-cli : python3 -m venv venv source ./venv/bin/activate (venv) pip install --upgrade pip (venv) pip install optimum onnx onnxruntime sentence-transformers (venv) optimum-cli export onnx --model sentence-transformers/all-MiniLM-L6-v2 onnx-output-folder The snippet exports the sentence-transformers/all-MiniLM-L6-v2(https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) transformer into the onnx-output-folder folder. The latter includes the tokenizer.json and model.onnx files used by the embedding model. In place of the all-MiniLM-L6-v2 you can pick any huggingface transformer identifier or provide direct file path. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the ONNX Transformer Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-transformers-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-transformers-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To configure it, use the spring.ai.embedding.transformer.* properties. For example, add this to your application.properties file to configure the client with the intfloat/e5-small-v2(https://huggingface.co/intfloat/e5-small-v2) text embedding model: spring.ai.embedding.transformer.onnx.modelUri=https://huggingface.co/intfloat/e5-small-v2/resolve/main/model.onnx spring.ai.embedding.transformer.tokenizer.uri=https://huggingface.co/intfloat/e5-small-v2/raw/main/tokenizer.json The complete list of supported properties are: Embedding Properties: Property Description Default spring.ai.embedding.transformer.enabled Enable the Transformer Embedding model. true spring.ai.embedding.transformer.tokenizer.uri URI of a pre-trained HuggingFaceTokenizer created by the ONNX engine (e.g. tokenizer.json). onnx/all-MiniLM-L6-v2/tokenizer.json spring.ai.embedding.transformer.tokenizer.options HuggingFaceTokenizer options such as ‘addSpecialTokens’, ‘modelMaxLength’, ‘truncation’, ‘padding’, ‘maxLength’, ‘stride’, ‘padToMultipleOf’. Leave empty to fallback to the defaults. empty spring.ai.embedding.transformer.cache.enabled Enable remote Resource caching. true spring.ai.embedding.transformer.cache.directory Directory path to cache remote resources, such as the ONNX models ${java.io.tmpdir}/spring-ai-onnx-model spring.ai.embedding.transformer.onnx.modelUri Existing, pre-trained ONNX model. onnx/all-MiniLM-L6-v2/model.onnx spring.ai.embedding.transformer.onnx.modelOutputName The ONNX model’s output node name, which we’ll use for embedding calculation. last_hidden_state spring.ai.embedding.transformer.onnx.gpuDeviceId The GPU device ID to execute on. Only applicable if >= 0. Ignored otherwise.(Requires additional onnxruntime_gpu dependency) -1 spring.ai.embedding.transformer.metadataMode Specifies what parts of the Documents content and metadata will be used for computing the embeddings. NONE Errors and special cases: If you see an error like Caused by: ai.onnxruntime.OrtException: Supplied array is ragged,.. , you need to also enable the tokenizer padding in application.properties as follows: spring.ai.embedding.transformer.tokenizer.options.padding=true If you get an error like The generative output names don’t contain expected: last_hidden_state. Consider one of the available model outputs: token_embeddings, …​. , you need to set the model output name to a correct value per your models. Consider the names listed in the error message. For example: spring.ai.embedding.transformer.onnx.modelOutputName=token_embeddings If you get an error like ai.onnxruntime.OrtException: Error code - ORT_FAIL - message: Deserialize tensor onnx::MatMul_10319 failed.GetFileLength for ./model.onnx_data failed:Invalid fd was supplied: -1 , that means that you model is larger than 2GB and is serialized in two files: model.onnx and model.onnx_data . The model.onnx_data is called External Data(https://onnx.ai/onnx/repo-docs/ExternalData.html#external-data) and is expected to be under the same directory of the model.onnx . Currently the only workaround is to copy the large model.onnx_data in the folder you run your Boot applicaiton. If you get an error like ai.onnxruntime.OrtException: Error code - ORT_EP_FAIL - message: Failed to find CUDA shared provider , that means that you are using the GPU parameters spring.ai.embedding.transformer.onnx.gpuDeviceId , but the onnxruntime_gpu dependency are missing. <dependency> <groupId>com.microsoft.onnxruntime</groupId> <artifactId>onnxruntime_gpu</artifactId> </dependency> Please select the appropriate onnxruntime_gpu version based on the CUDA version( ONNX Java Runtime(https://onnxruntime.ai/docs/get-started/with-java.html) ). Manual Configuration: If you are not using Spring Boot, you can manually configure the Onnx Transformers Embedding Model. For this add the spring-ai-transformers dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-transformers</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. then create a new TransformersEmbeddingModel instance and use the setTokenizerResource(tokenizerJsonUri) and setModelResource(modelOnnxUri) methods to set the URIs of the exported tokenizer.json and model.onnx files. ( classpath: , file: or https: URI schemas are supported). If the model is not explicitly set, TransformersEmbeddingModel defaults to sentence-transformers/all-MiniLM-L6-v2(https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) : Dimensions 384 Avg. performance 58.80 Speed 14200 sentences/sec Size 80MB The following snippet illustrates how to use the TransformersEmbeddingModel manually: TransformersEmbeddingModel embeddingModel = new TransformersEmbeddingModel(); // (optional) defaults to classpath:/onnx/all-MiniLM-L6-v2/tokenizer.json embeddingModel.setTokenizerResource(""classpath:/onnx/all-MiniLM-L6-v2/tokenizer.json""); // (optional) defaults to classpath:/onnx/all-MiniLM-L6-v2/model.onnx embeddingModel.setModelResource(""classpath:/onnx/all-MiniLM-L6-v2/model.onnx""); // (optional) defaults to ${java.io.tmpdir}/spring-ai-onnx-model // Only the http/https resources are cached by default. embeddingModel.setResourceCacheDirectory(""/tmp/onnx-zoo""); // (optional) Set the tokenizer padding if you see an errors like: // ""ai.onnxruntime.OrtException: Supplied array is ragged, ..."" embeddingModel.setTokenizerOptions(Map.of(""padding"", ""true"")); embeddingModel.afterPropertiesSet(); List<List<Double>> embeddings = embeddingModel.embed(List.of(""Hello world"", ""World is big"")); If you create an instance of TransformersEmbeddingModel manually, you must call the afterPropertiesSet() method after setting the properties and before using the client. The first embed() call downloads the large ONNX model and caches it on the local file system. Therefore, the first call might take longer than usual. Use the #setResourceCacheDirectory(<path>) method to set the local folder where the ONNX models as stored. The default cache folder is ${java.io.tmpdir}/spring-ai-onnx-model . It is more convenient (and preferred) to create the TransformersEmbeddingModel as a Bean . Then you don’t have to call the afterPropertiesSet() manually. @Bean public EmbeddingModel embeddingModel() { return new TransformersEmbeddingModel(); } Ollama(ollama-embeddings.html) OpenAI(openai-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/openai-embeddings.html","OpenAI Embeddings: Spring AI supports the OpenAI’s text embeddings models. OpenAI’s text embeddings measure the relatedness of text strings. An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. Prerequisites: You will need to create an API with OpenAI to access OpenAI embeddings models. Create an account at OpenAI signup page(https://platform.openai.com/signup) and generate the token on the API Keys page(https://platform.openai.com/account/api-keys) . The Spring AI project defines a configuration property named spring.ai.openai.api-key that you should set to the value of the API Key obtained from openai.com. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_OPENAI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure OpenAI Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the OpenAI Embedding model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.openai.base-url The URL to connect to https://api.openai.com spring.ai.openai.api-key The API Key - spring.ai.openai.organization-id Optionally you can specify which organization used for an API request. - spring.ai.openai.project-id Optionally, you can specify which project is used for an API request. - For users that belong to multiple organizations (or are accessing their projects through their legacy user API key), optionally, you can specify which organization and project is used for an API request. Usage from these API requests will count as usage for the specified organization and project. Configuration Properties: The prefix spring.ai.openai.embedding is property prefix that configures the EmbeddingModel implementation for OpenAI. Property Description Default spring.ai.openai.embedding.enabled Enable OpenAI embedding model. true spring.ai.openai.embedding.base-url Optional overrides the spring.ai.openai.base-url to provide embedding specific url - spring.ai.openai.chat.embeddings-path The path to append to the base-url /v1/embeddings spring.ai.openai.embedding.api-key Optional overrides the spring.ai.openai.api-key to provide embedding specific api-key - spring.ai.openai.embedding.organization-id Optionally you can specify which organization used for an API request. - spring.ai.openai.embedding.project-id Optionally, you can specify which project is used for an API request. - spring.ai.openai.embedding.metadata-mode Document content extraction mode. EMBED spring.ai.openai.embedding.options.model The model to use text-embedding-ada-002 (other options: text-embedding-3-large, text-embedding-3-small) spring.ai.openai.embedding.options.encodingFormat The format to return the embeddings in. Can be either float or base64. - spring.ai.openai.embedding.options.user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. - spring.ai.openai.embedding.options.dimensions The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models. - You can override the common spring.ai.openai.base-url and spring.ai.openai.api-key for the ChatModel and EmbeddingModel implementations. The spring.ai.openai.embedding.base-url and spring.ai.openai.embedding.api-key properties if set take precedence over the common properties. Similarly, the spring.ai.openai.embedding.base-url and spring.ai.openai.embedding.api-key properties if set take precedence over the common properties. This is useful if you want to use different OpenAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.openai.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The OpenAiEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiEmbeddingOptions.java) provides the OpenAI configurations, such as the model to use and etc. The default options can be configured using the spring.ai.openai.embedding.options properties as well. At start-time use the OpenAiEmbeddingModel constructor to set the default options used for all embedding requests. At run-time you can override the default options, using a OpenAiEmbeddingOptions instance as part of your EmbeddingRequest . For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), OpenAiEmbeddingOptions.builder() .withModel(""Different-Embedding-Model-Deployment-Name"") .build())); Sample Controller: This will create a EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingModel implementation. spring.ai.openai.api-key=YOUR_API_KEY spring.ai.openai.embedding.options.model=text-embedding-ada-002 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: If you are not using Spring Boot, you can manually configure the OpenAI Embedding Model. For this add the spring-ai-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-openai dependency provides access also to the OpenAiChatModel . For more information about the OpenAiChatModel refer to the OpenAI Chat Client(../chat/openai-chat.html) section. Next, create an OpenAiEmbeddingModel instance and use it to compute the similarity between two input texts: var openAiApi = new OpenAiApi(System.getenv(""OPENAI_API_KEY"")); var embeddingModel = new OpenAiEmbeddingModel( openAiApi, MetadataMode.EMBED, OpenAiEmbeddingOptions.builder() .withModel(""text-embedding-ada-002"") .withUser(""user-6"") .build(), RetryUtils.DEFAULT_RETRY_TEMPLATE); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); The OpenAiEmbeddingOptions provides the configuration information for the embedding requests. The options class offers a builder() for easy options creation. (ONNX) Transformers(onnx.html) PostgresML(postgresml-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/postgresml-embeddings.html","PostgresML Embeddings: Spring AI supports the PostgresML text embeddings models. Embeddings are a numeric representation of text. They are used to represent words and sentences as vectors, an array of numbers. Embeddings can be used to find similar pieces of text, by comparing the similarity of the numeric vectors using a distance measure, or they can be used as input features for other machine learning models, since most algorithms can’t use text directly. Many pre-trained LLMs can be used to generate embeddings from text within PostgresML. You can browse all the models(https://huggingface.co/models?library=sentence-transformers) available to find the best solution on Hugging Face. Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure PostgresML Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-postgresml-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-postgresml-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Use the spring.ai.postgresml.embedding.options.* properties to configure your PostgresMlEmbeddingModel . links Embedding Properties: The prefix spring.ai.postgresml.embedding is property prefix that configures the EmbeddingModel implementation for PostgresML embeddings. Property Description Default spring.ai.postgresml.embedding.enabled Enable PostgresML embedding model. true spring.ai.postgresml.embedding.options.transformer The Hugging Face transformer model to use for the embedding. distilbert-base-uncased spring.ai.postgresml.embedding.options.kwargs Additional transformer specific options. empty map spring.ai.postgresml.embedding.options.vectorType PostgresML vector type to use for the embedding. Two options are supported: PG_ARRAY and PG_VECTOR . PG_ARRAY spring.ai.postgresml.embedding.options.metadataMode Document metadata aggregation mode EMBED All properties prefixed with spring.ai.postgresml.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: Use the PostgresMlEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/postgresml/PostgresMlEmbeddingOptions.java) to configure the PostgresMlEmbeddingModel with options, such as the model to use and etc. On start you can pass a PostgresMlEmbeddingOptions to the PostgresMlEmbeddingModel constructor to configure the default options used for all embedding requests. At run-time you can override the default options, using a PostgresMlEmbeddingOptions in your EmbeddingRequest . For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), PostgresMlEmbeddingOptions.builder() .withTransformer(""intfloat/e5-small"") .withVectorType(VectorType.PG_ARRAY) .withKwargs(Map.of(""device"", ""gpu"")) .build())); Sample Controller: This will create a EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingModel implementation. spring.ai.postgresml.embedding.options.transformer=distilbert-base-uncased spring.ai.postgresml.embedding.options.vectorType=PG_ARRAY spring.ai.postgresml.embedding.options.metadataMode=EMBED spring.ai.postgresml.embedding.options.kwargs.device=cpu @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual configuration: Instead of using the Spring Boot auto-configuration, you can create the PostgresMlEmbeddingModel manually. For this add the spring-ai-postgresml dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-postgresml</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-postgresml' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an PostgresMlEmbeddingModel instance and use it to compute the similarity between two input texts: var jdbcTemplate = new JdbcTemplate(dataSource); // your posgresml data source PostgresMlEmbeddingModel embeddingModel = new PostgresMlEmbeddingModel(this.jdbcTemplate, PostgresMlEmbeddingOptions.builder() .withTransformer(""distilbert-base-uncased"") // huggingface transformer model name. .withVectorType(VectorType.PG_VECTOR) //vector type in PostgreSQL. .withKwargs(Map.of(""device"", ""cpu"")) // optional arguments. .withMetadataMode(MetadataMode.EMBED) // Document metadata mode. .build()); embeddingModel.afterPropertiesSet(); // initialize the jdbc template and database. EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); When created manually, you must call the afterPropertiesSet() after setting the properties and before using the client. It is more convenient (and preferred) to create the PostgresMlEmbeddingModel as a @Bean . Then you don’t have to call the afterPropertiesSet() manually: @Bean public EmbeddingModel embeddingModel(JdbcTemplate jdbcTemplate) { return new PostgresMlEmbeddingModel(jdbcTemplate, PostgresMlEmbeddingOptions.builder() .... .build()); } OpenAI(openai-embeddings.html) QianFan(qianfan-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/qianfan-embeddings.html","QianFan Chat: Spring AI supports the various AI language models from QianFan. You can interact with QianFan language models and create a multilingual conversational assistant based on QianFan models. Prerequisites: You will need to create an API with QianFan to access QianFan language models. Create an account at QianFan registration page(https://login.bce.baidu.com/new-reg) and generate the token on the API Keys page(https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application) . The Spring AI project defines a configuration property named spring.ai.qianfan.api-key and spring.ai.qianfan.secret-key . you should set to the value of the API Key and Secret Key obtained from API Keys page(https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_QIANFAN_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure QianFan Embedding Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-qianfan-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-qianfan-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the QianFan Embedding client. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.qianfan is used as the property prefix that lets you connect to QianFan. Property Description Default spring.ai.qianfan.base-url The URL to connect to aip.baidubce.com/rpc/2.0/ai_custom(https://aip.baidubce.com/rpc/2.0/ai_custom) spring.ai.qianfan.api-key The API Key - spring.ai.qianfan.secret-key The Secret Key - Configuration Properties: The prefix spring.ai.qianfan.embedding is property prefix that configures the EmbeddingClient implementation for QianFan. Property Description Default spring.ai.qianfan.embedding.enabled Enable QianFan embedding client. true spring.ai.qianfan.embedding.base-url Optional overrides the spring.ai.qianfan.base-url to provide embedding specific url - spring.ai.qianfan.embedding.api-key Optional overrides the spring.ai.qianfan.api-key to provide embedding specific api-key - spring.ai.qianfan.embedding.secret-key Optional overrides the spring.ai.qianfan.secret-key to provide embedding specific secret-key - spring.ai.qianfan.embedding.options.model The model to use bge_large_zh You can override the common spring.ai.qianfan.base-url , spring.ai.qianfan.embedding.api-key and spring.ai.qianfan.embedding.secret-key for the ChatClient and EmbeddingClient implementations. The spring.ai.qianfan.embedding.base-url , spring.ai.qianfan.embedding.api-key and spring.ai.qianfan.embedding.secret-key properties if set take precedence over the common properties. Similarly, the spring.ai.qianfan.embedding.base-url , spring.ai.qianfan.embedding.api-key and spring.ai.qianfan.embedding.secret-key properties if set take precedence over the common properties. This is useful if you want to use different QianFan accounts for different models and different model endpoints. All properties prefixed with spring.ai.qianfan.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The QianFanEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/main/java/org/springframework/ai/qianfan/QianFanEmbeddingOptions.java) provides the QianFan configurations, such as the model to use and etc. The default options can be configured using the spring.ai.qianfan.embedding.options properties as well. At start-time use the QianFanEmbeddingModel constructor to set the default options used for all embedding requests. At run-time you can override the default options, using a QianFanEmbeddingOptions instance as part of your EmbeddingRequest . For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingClient.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), QianFanEmbeddingOptions.builder() .withModel(""Different-Embedding-Model-Deployment-Name"") .build())); Sample Controller: This will create a EmbeddingClient implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingClient implementation. spring.ai.qianfan.api-key=YOUR_API_KEY spring.ai.qianfan.secret-key=YOUR_SECRET_KEY spring.ai.qianfan.embedding.options.model=tao_8k @RestController public class EmbeddingController { private final EmbeddingClient embeddingClient; @Autowired public EmbeddingController(EmbeddingClient embeddingClient) { this.embeddingClient = embeddingClient; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingClient.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: If you are not using Spring Boot, you can manually configure the QianFan Embedding Client. For this add the spring-ai-qianfan dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-qianfan</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-qianfan' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-qianfan dependency provides access also to the QianFanChatModel . For more information about the QianFanChatModel refer to the QianFan Chat Client(../chat/qianfan-chat.html) section. Next, create an QianFanEmbeddingModel instance and use it to compute the similarity between two input texts: var qianFanApi = new QianFanApi(System.getenv(""MINIMAX_API_KEY""), System.getenv(""QIANFAN_SECRET_KEY"")); var embeddingClient = new QianFanEmbeddingModel(qianFanApi) .withDefaultOptions(QianFanChatOptions.build() .withModel(""bge_large_en"") .build()); EmbeddingResponse embeddingResponse = embeddingClient .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); The QianFanEmbeddingOptions provides the configuration information for the embedding requests. The options class offers a builder() for easy options creation. PostgresML(postgresml-embeddings.html) Text Embedding(vertexai-embeddings-text.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/vertexai-embeddings-text.html","Google VertexAI Text Embeddings: Vertex AI supports two types of embeddings models, text and multimodal. This document describes how to create a text embedding using the Vertex AI Text embeddings API(https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api) . Vertex AI text embeddings API uses dense vector representations. Unlike sparse vectors, which tend to directly map words to numbers, dense vectors are designed to better represent the meaning of a piece of text. The benefit of using dense vector embeddings in generative AI is that instead of searching for direct word or syntax matches, you can better search for passages that align to the meaning of the query, even if the passages don’t use the same language. Prerequisites: Install the gcloud(https://cloud.google.com/sdk/docs/install) CLI, appropriate for you OS. Authenticate by running the following command. Replace PROJECT_ID with your Google Cloud project ID and ACCOUNT with your Google Cloud username. gcloud config set project <PROJECT_ID> && gcloud auth application-default login <ACCOUNT> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the VertexAI Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-embedding-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-embedding-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: The prefix spring.ai.vertex.ai.embedding is used as the property prefix that lets you connect to VertexAI Embedding API. Property Description Default spring.ai.vertex.ai.embedding.project-id Google Cloud Platform project ID - spring.ai.vertex.ai.embedding.location Region - spring.ai.vertex.ai.embedding.apiEndpoint Vertex AI Embedding API endpoint. - The prefix spring.ai.vertex.ai.embedding.text is the property prefix that lets you configure the embedding model implementation for VertexAI Text Embedding. Property Description Default spring.ai.vertex.ai.embedding.text.enabled Enable Vertex AI Embedding API model. true spring.ai.vertex.ai.embedding.text.options.model This is the Vertex Text Embedding model(https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models) to use text-embedding-004 spring.ai.vertex.ai.embedding.text.options.task-type The intended downstream application to help the model produce better quality embeddings. Available task-types(https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api#request_body) RETRIEVAL_DOCUMENT spring.ai.vertex.ai.embedding.text.options.title Optional title, only valid with task_type=RETRIEVAL_DOCUMENT. - spring.ai.vertex.ai.embedding.text.options.dimensions The number of dimensions the resulting output embeddings should have. Supported for model version 004 and later. You can use this parameter to reduce the embedding size, for example, for storage optimization. - spring.ai.vertex.ai.embedding.text.options.auto-truncate When set to true, input text will be truncated. When set to false, an error is returned if the input text is longer than the maximum length supported by the model. true Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-vertex-ai-embedding-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the VertexAi chat model: spring.ai.vertex.ai.embedding.project-id=<YOUR_PROJECT_ID> spring.ai.vertex.ai.embedding.location=<YOUR_PROJECT_LOCATION> spring.ai.vertex.ai.embedding.text.options.model=text-embedding-004 This will create a VertexAiTextEmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the embedding model for embeddings generations. @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: The VertexAiTextEmbeddingModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-embedding/src/main/java/org/springframework/ai/vertexai/embedding/VertexAiTextEmbeddingModel.java) implements the EmbeddingModel . Add the spring-ai-vertex-ai-embedding dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-embedding</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-embedding' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a VertexAiTextEmbeddingModel and use it for text generations: VertexAiEmbeddigConnectionDetails connectionDetails = VertexAiEmbeddigConnectionDetails.builder() .withProjectId(System.getenv(<VERTEX_AI_GEMINI_PROJECT_ID>)) .withLocation(System.getenv(<VERTEX_AI_GEMINI_LOCATION>)) .build(); VertexAiTextEmbeddingOptions options = VertexAiTextEmbeddingOptions.builder() .withModel(VertexAiTextEmbeddingOptions.DEFAULT_MODEL_NAME) .build(); var embeddingModel = new VertexAiTextEmbeddingModel(connectionDetails, options); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); QianFan(qianfan-embeddings.html) Multimodal Embedding(vertexai-embeddings-multimodal.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/vertexai-embeddings-multimodal.html","Google VertexAI Multimodal Embeddings: EXPERIMENTAL. Used for experimental purposes only. Not compatible yet with the VectorStores . Vertex AI supports two types of embeddings models, text and multimodal. This document describes how to create a multimodal embedding using the Vertex AI Multimodal embeddings API(https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) . The multimodal embeddings model generates 1408-dimension vectors based on the input you provide, which can include a combination of image, text, and video data. The embedding vectors can then be used for subsequent tasks like image classification or video content moderation. The image embedding vector and text embedding vector are in the same semantic space with the same dimensionality. Consequently, these vectors can be used interchangeably for use cases like searching image by text, or searching video by image. The VertexAI Multimodal API imposes the following limits(https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#api-limits) . For text-only embedding use cases, we recommend using the Vertex AI text-embeddings model(vertexai-embeddings-text.html) instead. Prerequisites: Install the gcloud(https://cloud.google.com/sdk/docs/install) CLI, appropriate for you OS. Authenticate by running the following command. Replace PROJECT_ID with your Google Cloud project ID and ACCOUNT with your Google Cloud username. gcloud config set project <PROJECT_ID> && gcloud auth application-default login <ACCOUNT> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the VertexAI Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-embedding-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-embedding-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: The prefix spring.ai.vertex.ai.embedding is used as the property prefix that lets you connect to VertexAI Embedding API. Property Description Default spring.ai.vertex.ai.embedding.project-id Google Cloud Platform project ID - spring.ai.vertex.ai.embedding.location Region - spring.ai.vertex.ai.embedding.apiEndpoint Vertex AI Embedding API endpoint. - The prefix spring.ai.vertex.ai.embedding.multimodal is the property prefix that lets you configure the embedding model implementation for VertexAI Multimodal Embedding. Property Description Default spring.ai.vertex.ai.embedding.multimodal.enabled Enable Vertex AI Embedding API model. true spring.ai.vertex.ai.embedding.multimodal.options.model You can get multimodal embeddings by using the following model: multimodalembedding@001 spring.ai.vertex.ai.embedding.multimodal.options.dimensions Specify lower-dimension embeddings. By default, an embedding request returns a 1408 float vector for a data type. You can also specify lower-dimension embeddings (128, 256, or 512 float vectors) for text and image data. 1408 spring.ai.vertex.ai.embedding.multimodal.options.video-start-offset-sec The start offset of the video segment in seconds. If not specified, it’s calculated with max(0, endOffsetSec - 120). - spring.ai.vertex.ai.embedding.multimodal.options.video-end-offset-sec The end offset of the video segment in seconds. If not specified, it’s calculated with min(video length, startOffSec + 120). If both startOffSec and endOffSec are specified, endOffsetSec is adjusted to min(startOffsetSec+120, endOffsetSec). - spring.ai.vertex.ai.embedding.multimodal.options.video-interval-sec The interval of the video the embedding will be generated. The minimum value for interval_sec is 4. If the interval is less than 4, an InvalidArgumentError is returned. There are no limitations on the maximum value of the interval. However, if the interval is larger than min(video length, 120s), it impacts the quality of the generated embeddings. Default value: 16. - Manual Configuration: The VertexAiMultimodalEmbeddingModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-embedding/src/main/java/org/springframework/ai/vertexai/embedding/VertexAiMultimodalEmbeddingModel.java) implements the DocumentEmbeddingModel . Add the spring-ai-vertex-ai-embedding dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-embedding</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-embedding' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a VertexAiMultimodalEmbeddingModel and use it for embeddings generations: VertexAiEmbeddigConnectionDetails connectionDetails = VertexAiEmbeddigConnectionDetails.builder() .withProjectId(System.getenv(<VERTEX_AI_GEMINI_PROJECT_ID>)) .withLocation(System.getenv(<VERTEX_AI_GEMINI_LOCATION>)) .build(); VertexAiMultimodalEmbeddingOptions options = VertexAiMultimodalEmbeddingOptions.builder() .withModel(VertexAiMultimodalEmbeddingOptions.DEFAULT_MODEL_NAME) .build(); var embeddingModel = new VertexAiMultimodalEmbeddingModel(connectionDetails, options); Media imageMedial = new Media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource(""/test.image.png"")); Media videoMedial = new Media(new MimeType(""video"", ""mp4""), new ClassPathResource(""/test.video.mp4"")); var document = new Document(""Explain what do you see on this video?"", List.of(imageMedial, videoMedial), Map.of()); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); DocumentEmbeddingRequest embeddingRequest = new DocumentEmbeddingRequest(List.of(document), EmbeddingOptions.EMPTY); EmbeddingResponse embeddingResponse = multiModelEmbeddingModel.call(embeddingRequest); assertThat(embeddingResponse.getResults()).hasSize(3); Text Embedding(vertexai-embeddings-text.html) PaLM2 Embedding(vertexai-embeddings-palm2.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/vertexai-embeddings-palm2.html","Google VertexAI PaLM2 Embeddings: For text-only embedding use cases, we recommend using the Vertex AI text-embeddings model(vertexai-embeddings-text.html) instead. The Generative Language(https://developers.generativeai.google/api/rest/generativelanguage) PaLM API allows developers to build generative AI applications using the PaLM model. Large Language Models (LLMs) are a powerful, versatile type of machine learning model that enables computers to comprehend and generate natural language through a series of prompts. The PaLM API is based on Google’s next generation LLM, PaLM. It excels at a variety of different tasks like code generation, reasoning, and writing. You can use the PaLM API to build generative AI applications for use cases like content generation, dialogue agents, summarization and classification systems, and more. Based on the Models REST API(https://developers.generativeai.google/api/rest/generativelanguage/models) . Prerequisites: To access the PaLM2 REST API you need to obtain an access API KEY form makersuite(https://makersuite.google.com/app/apikey) . Currently the PaLM API it is not available outside US, but you can use VPN for testing. The Spring AI project defines a configuration property named spring.ai.vertex.ai.api-key that you should set to the value of the API Key obtained. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_VERTEX_AI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the VertexAI Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-palm2-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-palm2-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: The prefix spring.ai.vertex.ai is used as the property prefix that lets you connect to VertexAI. Property Description Default spring.ai.vertex.ai.ai.base-url The URL to connect to generativelanguage.googleapis.com/v1beta3(https://generativelanguage.googleapis.com/v1beta3) spring.ai.vertex.ai.api-key The API Key - The prefix spring.ai.vertex.ai.embedding is the property prefix that lets you configure the embedding model implementation for VertexAI Chat. Property Description Default spring.ai.vertex.ai.embedding.enabled Enable Vertex AI PaLM API Embedding model. true spring.ai.vertex.ai.embedding.model This is the Vertex Embedding model(https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings) to use embedding-gecko-001 Sample Controller: Create(https://start.spring.io/) a new Spring Boot project and add the spring-ai-vertex-ai-palm2-spring-boot-starter to your pom (or gradle) dependencies. Add a application.properties file, under the src/main/resources directory, to enable and configure the VertexAi chat model: spring.ai.vertex.ai.api-key=YOUR_API_KEY spring.ai.vertex.ai.embedding.model=embedding-gecko-001 replace the api-key with your VertexAI credentials. This will create a VertexAiPaLm2EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the embedding model for text generations. @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: The VertexAiPaLm2EmbeddingModel(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-palm2/src/main/java/org/springframework/ai/vertexai/palm2/VertexAiPaLm2EmbeddingModel.java) implements the EmbeddingModel and uses the Low-level VertexAiPaLm2Api Client(#low-level-api) to connect to the VertexAI service. Add the spring-ai-vertex-ai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-palm2</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-vertex-ai-palm2' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a VertexAiPaLm2EmbeddingModel and use it for text generations: VertexAiPaLm2Api vertexAiApi = new VertexAiPaLm2Api(< YOUR PALM_API_KEY>); var embeddingModel = new VertexAiPaLm2EmbeddingModel(vertexAiApi); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); Low-level VertexAiPaLm2Api Client: The VertexAiPaLm2Api(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-palm2/src/main/java/org/springframework/ai/vertexai/palm2/api/VertexAiPaLm2Api.java) provides is lightweight Java client for VertexAiPaLm2Api Chat API. Following class diagram illustrates the VertexAiPaLm2Api embedding interfaces and building blocks: Here is a simple snippet how to use the api programmatically: VertexAiPaLm2Api vertexAiApi = new VertexAiPaLm2Api(< YOUR PALM_API_KEY>); // Generate var prompt = new MessagePrompt(List.of(new Message(""0"", ""Hello, how are you?""))); GenerateMessageRequest request = new GenerateMessageRequest(prompt); GenerateMessageResponse response = vertexAiApi.generateMessage(request); // Embed text Embedding embedding = vertexAiApi.embedText(""Hello, how are you?""); // Batch embedding List<Embedding> embeddings = vertexAiApi.batchEmbedText(List.of(""Hello, how are you?"", ""I am fine, thank you!"")); Multimodal Embedding(vertexai-embeddings-multimodal.html) watsonx.AI(watsonx-ai-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/watsonx-ai-embeddings.html","watsonx.ai Embeddings: With Watsonx.ai(https://www.ibm.com/products/watsonx-ai) you can run various Large Language Models (LLMs) and generate embeddings from them. Spring AI supports the Watsonx.ai text embeddings with WatsonxAiEmbeddingModel . An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. Prerequisites: You first need to have a SaaS instance of watsonx.ai (as well as an IBM Cloud account). Refer to free-trial(https://eu-de.dataplatform.cloud.ibm.com/registration/stepone?context=wx&preselect_region=true) to try watsonx.ai for free More info can be found here(https://www.ibm.com/products/watsonx-ai/info/trial) Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Watsonx.ai Embedding Model. To enable it add the following dependency to your Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-watsonx-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-watsonx-ai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring.ai.watsonx.embedding.options.* properties are used to configure the default options used for all embedding requests. Embedding Properties: The prefix spring.ai.watsonx.ai is used as the property prefix that lets you connect to watsonx.ai. Property Description Default spring.ai.watsonx.ai.base-url The URL to connect to us-south.ml.cloud.ibm.com(https://us-south.ml.cloud.ibm.com) spring.ai.watsonx.ai.embedding-endpoint The text endpoint ml/v1/text/embeddings?version=2023-05-29 spring.ai.watsonx.ai.project-id The project ID - spring.ai.watsonx.ai.iam-token The IBM Cloud account IAM token - The prefix spring.ai.watsonx.embedding.options is the property prefix that configures the EmbeddingModel implementation for Watsonx.ai. Property Description Default spring.ai.watsonx.ai.embedding.enabled Enable Watsonx.ai embedding model true spring.ai.watsonx.ai.embedding.options.model The embedding model to be used ibm/slate-30m-english-rtrvr Runtime Options: The WatsonxAiEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-watsonx/src/main/java/org/springframework/ai/watsonx/api/WatsonxAiEmbeddingOptions.java) provides the Watsonx.ai configurations, such as the model to use. The default options can be configured using the spring.ai.watsonx.embedding.options properties as well. EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), WatsonxAiEmbeddingOptions.create() .withModel(""Different-Embedding-Model-Deployment-Name"")) ); Sample Controller: This will create an EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingModel implementation. @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public ResponseEntity<Embedding> embedding(@RequestParam String text) { EmbeddingResponse response = this.embeddingModel.embedForResponse(List.of(text)); return ResponseEntity.ok(response.getResult()); } } PaLM2 Embedding(vertexai-embeddings-palm2.html) ZhiPu AI(zhipuai-embeddings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/embeddings/zhipuai-embeddings.html","ZhiPuAI Embeddings: Spring AI supports the ZhiPuAI’s text embeddings models. ZhiPuAI’s text embeddings measure the relatedness of text strings. An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. Prerequisites: You will need to create an API with ZhiPuAI to access ZhiPu AI language models. Create an account at ZhiPu AI registration page(https://open.bigmodel.cn/login) and generate the token on the API Keys page(https://open.bigmodel.cn/usercenter/apikeys) . The Spring AI project defines a configuration property named spring.ai.zhipu.api-key that you should set to the value of the API Key obtained from API Keys page(https://open.bigmodel.cn/usercenter/apikeys) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_ZHIPU_AI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure ZhiPuAI Embedding Model. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-zhipuai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-zhipuai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Embedding Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the ZhiPuAI Embedding model. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Connection Properties: The prefix spring.ai.zhipuai is used as the property prefix that lets you connect to ZhiPuAI. Property Description Default spring.ai.zhipuai.base-url The URL to connect to open.bigmodel.cn/api/paas(https://open.bigmodel.cn/api/paas) spring.ai.zhipuai.api-key The API Key - Configuration Properties: The prefix spring.ai.zhipuai.embedding is property prefix that configures the EmbeddingModel implementation for ZhiPuAI. Property Description Default spring.ai.zhipuai.embedding.enabled Enable ZhiPuAI embedding model. true spring.ai.zhipuai.embedding.base-url Optional overrides the spring.ai.zhipuai.base-url to provide embedding specific url - spring.ai.zhipuai.embedding.api-key Optional overrides the spring.ai.zhipuai.api-key to provide embedding specific api-key - spring.ai.zhipuai.embedding.options.model The model to use embedding-2 You can override the common spring.ai.zhipuai.base-url and spring.ai.zhipuai.api-key for the ChatModel and EmbeddingModel implementations. The spring.ai.zhipuai.embedding.base-url and spring.ai.zhipuai.embedding.api-key properties if set take precedence over the common properties. Similarly, the spring.ai.zhipuai.embedding.base-url and spring.ai.zhipuai.embedding.api-key properties if set take precedence over the common properties. This is useful if you want to use different ZhiPuAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.zhipuai.embedding.options can be overridden at runtime by adding a request specific Runtime Options(#embedding-options) to the EmbeddingRequest call. Runtime Options: The ZhiPuAiEmbeddingOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiEmbeddingOptions.java) provides the ZhiPuAI configurations, such as the model to use and etc. The default options can be configured using the spring.ai.zhipuai.embedding.options properties as well. At start-time use the ZhiPuAiEmbeddingModel constructor to set the default options used for all embedding requests. At run-time you can override the default options, using a ZhiPuAiEmbeddingOptions instance as part of your EmbeddingRequest . For example to override the default model name for a specific request: EmbeddingResponse embeddingResponse = embeddingModel.call( new EmbeddingRequest(List.of(""Hello World"", ""World is big and salvation is near""), ZhiPuAiEmbeddingOptions.builder() .withModel(""Different-Embedding-Model-Deployment-Name"") .build())); Sample Controller: This will create a EmbeddingModel implementation that you can inject into your class. Here is an example of a simple @Controller class that uses the EmbeddingModel implementation. spring.ai.zhipuai.api-key=YOUR_API_KEY spring.ai.zhipuai.embedding.options.model=embedding-2 @RestController public class EmbeddingController { private final EmbeddingModel embeddingModel; @Autowired public EmbeddingController(EmbeddingModel embeddingModel) { this.embeddingModel = embeddingModel; } @GetMapping(""/ai/embedding"") public Map embed(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { EmbeddingResponse embeddingResponse = this.embeddingModel.embedForResponse(List.of(message)); return Map.of(""embedding"", embeddingResponse); } } Manual Configuration: If you are not using Spring Boot, you can manually configure the ZhiPuAI Embedding Model. For this add the spring-ai-zhipuai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-zhipuai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-zhipuai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The spring-ai-zhipuai dependency provides access also to the ZhiPuAiChatModel . For more information about the ZhiPuAiChatModel refer to the ZhiPuAI Chat Client(../chat/zhipuai-chat.html) section. Next, create an ZhiPuAiEmbeddingModel instance and use it to compute the similarity between two input texts: var zhiPuAiApi = new ZhiPuAiApi(System.getenv(""ZHIPU_AI_API_KEY"")); var embeddingModel = new ZhiPuAiEmbeddingModel(zhiPuAiApi) .withDefaultOptions(ZhiPuAiChatOptions.build() .withModel(""embedding-2"") .build()); EmbeddingResponse embeddingResponse = embeddingModel .embedForResponse(List.of(""Hello World"", ""World is big and salvation is near"")); The ZhiPuAiEmbeddingOptions provides the configuration information for the embedding requests. The options class offers a builder() for easy options creation. watsonx.AI(watsonx-ai-embeddings.html) Image Model API(../imageclient.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/imageclient.html","Image Model API: The Spring Image Model API is designed to be a simple and portable interface for interacting with various AI Models(../concepts.html#_models) specialized in image generation, allowing developers to switch between different image-related models with minimal code changes. This design aligns with Spring’s philosophy of modularity and interchangeability, ensuring developers can quickly adapt their applications to different AI capabilities related to image processing. Additionally, with the support of companion classes like ImagePrompt for input encapsulation and ImageResponse for output handling, the Image Model API unifies the communication with AI Models dedicated to image generation. It manages the complexity of request preparation and response parsing, offering a direct and simplified API interaction for image-generation functionalities. The Spring Image Model API is built on top of the Spring AI Generic Model API , providing image-specific abstractions and implementations. API Overview: This section provides a guide to the Spring Image Model API interface and associated classes. Image Model: Here is the ImageModel(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageModel.java) interface definition: @FunctionalInterface public interface ImageModel extends Model<ImagePrompt, ImageResponse> { ImageResponse call(ImagePrompt request); } ImagePrompt: The ImagePrompt(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImagePrompt.java) is a ModelRequest that encapsulates a list of ImageMessage(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageMessage.java) objects and optional model request options. The following listing shows a truncated version of the ImagePrompt class, excluding constructors and other utility methods: public class ImagePrompt implements ModelRequest<List<ImageMessage>> { private final List<ImageMessage> messages; private ImageOptions imageModelOptions; @Override public List<ImageMessage> getInstructions() {...} @Override public ImageOptions getOptions() {...} // constructors and utility methods omitted } ImageMessage: The ImageMessage class encapsulates the text to use and the weight that the text should have in influencing the generated image. For models that support weights, they can be positive or negative. public class ImageMessage { private String text; private Float weight; public String getText() {...} public Float getWeight() {...} // constructors and utility methods omitted } ImageOptions: Represents the options that can be passed to the Image generation model. The ImageOptions class extends the ModelOptions interface and is used to define few portable options that can be passed to the AI model. The ImageOptions class is defined as follows: public interface ImageOptions extends ModelOptions { Integer getN(); String getModel(); Integer getWidth(); Integer getHeight(); String getResponseFormat(); // openai - url or base64 : stability ai byte[] or base64 } Additionally, every model specific ImageModel implementation can have its own options that can be passed to the AI model. For example, the OpenAI Image Generation model has its own options like quality , style , etc. This is a powerful feature that allows developers to use model specific options when starting the application and then override them with at runtime using the ImagePrompt . ImageResponse: The structure of the ChatResponse class is as follows: public class ImageResponse implements ModelResponse<ImageGeneration> { private final ImageResponseMetadata imageResponseMetadata; private final List<ImageGeneration> imageGenerations; @Override public ImageGeneration getResult() { // get the first result } @Override public List<ImageGeneration> getResults() {...} @Override public ImageResponseMetadata getMetadata() {...} // other methods omitted } The ImageResponse(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageResponse.java) class holds the AI Model’s output, with each ImageGeneration instance containing one of potentially multiple outputs resulting from a single prompt. The ImageResponse class also carries a ImageResponseMetadata metadata about the AI Model’s response. ImageGeneration: Finally, the ImageGeneration(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageGeneration.java) class extends from the ModelResult to represent the output response and related metadata about this result: public class ImageGeneration implements ModelResult<Image> { private ImageGenerationMetadata imageGenerationMetadata; private Image image; @Override public Image getOutput() {...} @Override public ImageGenerationMetadata getMetadata() {...} // other methods omitted } Available Implementations: ImageModel implementations are provided for the following Model providers: OpenAI Image Generation(image/openai-image.html) StabilityAI Image Generation(image/stabilityai-image.html) API Docs: You can find the Javadoc here(https://docs.spring.io/spring-ai/docs/current-SNAPSHOT/) . Feedback and Contributions: The project’s GitHub discussions(https://github.com/spring-projects/spring-ai/discussions) is a great place to send feedback. ZhiPu AI(embeddings/zhipuai-embeddings.html) Azure OpenAI(image/azure-openai-image.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/image/azure-openai-image.html","Azure OpenAI Image Generation: Spring AI supports DALL-E, the Image generation model from Azure OpenAI. Prerequisites: Obtain your Azure OpenAI endpoint and api-key from the Azure OpenAI Service section on the Azure Portal(https://portal.azure.com) . Spring AI defines a configuration property named spring.ai.azure.openai.api-key that you should set to the value of the API Key obtained from Azure. There is also a configuration property named spring.ai.azure.openai.endpoint that you should set to the endpoint URL obtained when provisioning your model in Azure. Exporting environment variables is one way to set these configuration properties: export SPRING_AI_AZURE_OPENAI_API_KEY=<INSERT KEY HERE> export SPRING_AI_AZURE_OPENAI_ENDPOINT=<INSERT ENDPOINT URL HERE> Deployment Name: To use run Azure AI applications, create an Azure AI Deployment through the [Azure AI Portal]( oai.azure.com/portal(https://oai.azure.com/portal) ). In Azure, each client must specify a Deployment Name to connect to the Azure OpenAI service. It’s essential to understand that the Deployment Name is different from the model you choose to deploy For instance, a deployment named 'MyImgAiDeployment' could be configured to use either the Dalle3 model or the Dalle2 model. For now, to keep things simple, you can create a deployment using the following settings: Deployment Name: MyImgAiDeployment Model Name: Dalle3 This Azure configuration will align with the default configurations of the Spring Boot Azure AI Starter and its Autoconfiguration feature. If you use a different Deployment Name, update the configuration property accordingly: spring.ai.azure.openai.image.options.deployment-name=<my deployment name> The different deployment structures of Azure OpenAI and OpenAI leads to a property in the Azure OpenAI client library named deploymentOrModelName . This is because in OpenAI there is no Deployment Name , only a Model Name . Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure OpenAI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-azure-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Image Generation Properties: The prefix spring.ai.openai.image is the property prefix that lets you configure the ImageModel implementation for OpenAI. Property Description Default spring.ai.azure.openai.image.enabled Enable OpenAI image model. true spring.ai.azure.openai.image.options.n The number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported. - spring.ai.azure.openai.image.options.model The model to use for image generation. AzureOpenAiImageOptions.DEFAULT_IMAGE_MODEL spring.ai.azure.openai.image.options.quality The quality of the image that will be generated. HD creates images with finer details and greater consistency across the image. This parameter is only supported for dall-e-3. - spring.ai.azure.openai.image.options.response_format The format in which the generated images are returned. Must be one of URL or b64_json. - spring.ai.openai.image.options.size The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 for dall-e-2. Must be one of 1024x1024, 1792x1024, or 1024x1792 for dall-e-3 models. - spring.ai.openai.image.options.size_width The width of the generated images. Must be one of 256, 512, or 1024 for dall-e-2. - spring.ai.openai.image.options.size_height The height of the generated images. Must be one of 256, 512, or 1024 for dall-e-2. - spring.ai.openai.image.options.style The style of the generated images. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This parameter is only supported for dall-e-3. - spring.ai.openai.image.options.user A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse. - Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to Azure OpenAI. Property Description Default spring.ai.azure.openai.endpoint The URL to connect to my-dalle3.openai.azure.com/(https://my-dalle3.openai.azure.com/) spring.ai.azure.openai.apiKey The API Key - Runtime Options: The OpenAiImageOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiImageOptions.java) provides model configurations, such as the model to use, the quality, the size, etc. On start-up, the default options can be configured with the AzureOpenAiImageModel(OpenAiImageApi openAiImageApi) constructor and the withDefaultOptions(OpenAiImageOptions defaultOptions) method. Alternatively, use the spring.ai.azure.openai.image.options.* properties described previously. At runtime you can override the default options by adding new, request specific, options to the ImagePrompt call. For example to override the OpenAI specific options such as quality and the number of images to create, use the following code example: ImageResponse response = azureOpenaiImageModel.call( new ImagePrompt(""A light cream colored mini golden doodle"", OpenAiImageOptions.builder() .withQuality(""hd"") .withN(4) .withHeight(1024) .withWidth(1024).build()) ); In addition to the model specific AzureOpenAiImageOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/main/java/org/springframework/ai/azure/openai/AzureOpenAiImageOptions.java) you can use a portable ImageOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptions.java) instance, created with the ImageOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptionsBuilder.java) . Image Model API(../imageclient.html) OpenAI(openai-image.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/image/openai-image.html","OpenAI Image Generation: Spring AI supports DALL-E, the Image generation model from OpenAI. Prerequisites: You will need to create an API key with OpenAI to access ChatGPT models. Create an account at OpenAI signup page(https://platform.openai.com/signup) and generate the token on the API Keys page(https://platform.openai.com/account/api-keys) . The Spring AI project defines a configuration property named spring.ai.openai.api-key that you should set to the value of the API Key obtained from openai.com. Exporting an environment variable is one way to set that configuration property: export SPRING_AI_OPENAI_API_KEY=<INSERT KEY HERE> Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OpenAI Image Generation Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Image Generation Properties: Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.openai.base-url The URL to connect to api.openai.com(https://api.openai.com) spring.ai.openai.api-key The API Key - spring.ai.openai.organization-id Optionally you can specify which organization used for an API request. - spring.ai.openai.project-id Optionally, you can specify which project is used for an API request. - For users that belong to multiple organizations (or are accessing their projects through their legacy user API key), optionally, you can specify which organization and project is used for an API request. Usage from these API requests will count as usage for the specified organization and project. Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the OpenAI Image client. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Configuration Properties: The prefix spring.ai.openai.image is the property prefix that lets you configure the ImageModel implementation for OpenAI. Property Description Default spring.ai.openai.image.enabled Enable OpenAI image model. true spring.ai.openai.image.base-url Optional overrides the spring.ai.openai.base-url to provide chat specific url - spring.ai.openai.image.api-key Optional overrides the spring.ai.openai.api-key to provide chat specific api-key - spring.ai.openai.image.organization-id Optionally you can specify which organization used for an API request. - spring.ai.openai.image.project-id Optionally, you can specify which project is used for an API request. - spring.ai.openai.image.options.n The number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported. - spring.ai.openai.image.options.model The model to use for image generation. OpenAiImageApi.DEFAULT_IMAGE_MODEL spring.ai.openai.image.options.quality The quality of the image that will be generated. HD creates images with finer details and greater consistency across the image. This parameter is only supported for dall-e-3. - spring.ai.openai.image.options.response_format The format in which the generated images are returned. Must be one of URL or b64_json. - spring.ai.openai.image.options.size The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 for dall-e-2. Must be one of 1024x1024, 1792x1024, or 1024x1792 for dall-e-3 models. - spring.ai.openai.image.options.size_width The width of the generated images. Must be one of 256, 512, or 1024 for dall-e-2. - spring.ai.openai.image.options.size_height The height of the generated images. Must be one of 256, 512, or 1024 for dall-e-2. - spring.ai.openai.image.options.style The style of the generated images. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This parameter is only supported for dall-e-3. - spring.ai.openai.image.options.user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. - You can override the common spring.ai.openai.base-url , spring.ai.openai.api-key , spring.ai.openai.organization-id and spring.ai.openai.project-id properties. The spring.ai.openai.image.base-url , spring.ai.openai.image.api-key , spring.ai.openai.image.organization-id and spring.ai.openai.image.project-id properties if set take precedence over the common properties. This is useful if you want to use different OpenAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.openai.image.options can be overridden at runtime. Runtime Options: The OpenAiImageOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiImageOptions.java) provides model configurations, such as the model to use, the quality, the size, etc. On start-up, the default options can be configured with the OpenAiImageModel(OpenAiImageApi openAiImageApi) constructor and the withDefaultOptions(OpenAiImageOptions defaultOptions) method. Alternatively, use the spring.ai.openai.image.options.* properties described previously. At runtime you can override the default options by adding new, request specific, options to the ImagePrompt call. For example to override the OpenAI specific options such as quality and the number of images to create, use the following code example: ImageResponse response = openaiImageModel.call( new ImagePrompt(""A light cream colored mini golden doodle"", OpenAiImageOptions.builder() .withQuality(""hd"") .withN(4) .withHeight(1024) .withWidth(1024).build()) ); In addition to the model specific OpenAiImageOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/main/java/org/springframework/ai/openai/OpenAiImageOptions.java) you can use a portable ImageOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptions.java) instance, created with the ImageOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptionsBuilder.java) . Azure OpenAI(azure-openai-image.html) Stability(stabilityai-image.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/image/stabilityai-image.html","Stability AI Image Generation: Spring AI supports Stability AI’s text to image generation model(https://platform.stability.ai/docs/api-reference#tag/v1generation) . Prerequisites: You will need to create an API key with Stability AI to access their AI models, follow their Getting Started documentation(https://platform.stability.ai/docs/getting-started/authentication) . The Spring AI project defines a configuration property named spring.ai.stabilityai.api-key that you should set to the value of the API Key obtained from Stability AI. Exporting an environment variable is one way to set that configuration property. export SPRING_AI_STABILITYAI_API_KEY=<INSERT KEY HERE> Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Stability AI Image Generation Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-stability-ai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-stability-ai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Image Generation Properties: The prefix spring.ai.stabilityai is used as the property prefix that lets you connect to Stability AI. Property Description Default spring.ai.stabilityai.base-url The URL to connect to api.stability.ai/v1(https://api.stability.ai/v1) spring.ai.stabilityai.api-key The API Key - The prefix spring.ai.stabilityai.image is the property prefix that lets you configure the ImageModel implementation for Stability AI. Property Description Default spring.ai.stabilityai.image.enabled Enable Stability AI image model. true spring.ai.stabilityai.image.base-url Optional overrides the spring.ai.openai.base-url to provide a specific url api.stability.ai/v1(https://api.stability.ai/v1) spring.ai.stabilityai.image.api-key Optional overrides the spring.ai.openai.api-key to provide a specific api-key - spring.ai.stabilityai.image.option.n The number of images to be generated. Must be between 1 and 10. 1 spring.ai.stabilityai.image.option.model The engine/model to use in Stability AI. The model is passed in the URL as a path parameter. stable-diffusion-v1-6 spring.ai.stabilityai.image.option.width Width of the image to generate, in pixels, in an increment divisible by 64. Engine-specific dimension validation applies. 512 spring.ai.stabilityai.image.option.height Height of the image to generate, in pixels, in an increment divisible by 64. Engine-specific dimension validation applies. 512 spring.ai.stabilityai.image.option.responseFormat The format in which the generated images are returned. Must be ""application/json"" or ""image/png"". - spring.ai.stabilityai.image.option.cfg_scale The strictness level of the diffusion process adherence to the prompt text. Range: 0 to 35. 7 spring.ai.stabilityai.image.option.clip_guidance_preset Pass in a style preset to guide the image model towards a particular style. This list of style presets is subject to change. NONE spring.ai.stabilityai.image.option.sampler Which sampler to use for the diffusion process. If this value is omitted, an appropriate sampler will be automatically selected. - spring.ai.stabilityai.image.option.seed Random noise seed (omit this option or use 0 for a random seed). Valid range: 0 to 4294967295. 0 spring.ai.stabilityai.image.option.steps Number of diffusion steps to run. Valid range: 10 to 50. 30 spring.ai.stabilityai.image.option.style_preset Pass in a style preset to guide the image model towards a particular style. This list of style presets is subject to change. - Runtime Options: The StabilityAiImageOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-stabilityai/src/main/java/org/springframework/ai/stabilityai/api/StabilityAiImageOptions.java) provides model configurations, such as the model to use, the style, the size, etc. On start-up, the default options can be configured with the StabilityAiImageModel(StabilityAiApi stabilityAiApi, StabilityAiImageOptions options) constructor. Alternatively, use the spring.ai.openai.image.options.* properties described previously. At runtime, you can override the default options by adding new, request specific, options to the ImagePrompt call. For example to override the Stability AI specific options such as quality and the number of images to create, use the following code example: ImageResponse response = openaiImageModel.call( new ImagePrompt(""A light cream colored mini golden doodle"", StabilityAiImageOptions.builder() .withStylePreset(""cinematic"") .withN(4) .withHeight(1024) .withWidth(1024).build()) ); In addition to the model specific StabilityAiImageOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-stabilityai/src/main/java/org/springframework/ai/stabilityai/api/StabilityAiImageOptions.java) you can use a portable ImageOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptions.java) instance, created with the ImageOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptionsBuilder.java) . OpenAI(openai-image.html) ZhiPuAI(zhipuai-image.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/image/zhipuai-image.html","ZhiPuAI Image Generation: Spring AI supports CogView, the Image generation model from ZhiPuAI. Prerequisites: You will need to create an API with ZhiPuAI to access ZhiPu AI language models. Create an account at ZhiPu AI registration page(https://open.bigmodel.cn/login) and generate the token on the API Keys page(https://open.bigmodel.cn/usercenter/apikeys) . The Spring AI project defines a configuration property named spring.ai.zhipuai.api-key that you should set to the value of the API Key obtained from API Keys page(https://open.bigmodel.cn/usercenter/apikeys) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_ZHIPU_AI_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the ZhiPuAI Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-zhipuai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-zhipuai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Image Generation Properties: The prefix spring.ai.zhipuai.image is the property prefix that lets you configure the ImageModel implementation for ZhiPuAI. Property Description Default spring.ai.zhipuai.image.enabled Enable ZhiPuAI image model. true spring.ai.zhipuai.image.base-url Optional overrides the spring.ai.zhipuai.base-url to provide chat specific url - spring.ai.zhipuai.image.api-key Optional overrides the spring.ai.zhipuai.api-key to provide chat specific api-key - spring.ai.zhipuai.image.options.model The model to use for image generation. cogview-3 spring.ai.zhipuai.image.options.user A unique identifier representing your end-user, which can help ZhiPuAI to monitor and detect abuse. - Connection Properties: The prefix spring.ai.zhipuai is used as the property prefix that lets you connect to ZhiPuAI. Property Description Default spring.ai.zhipuai.base-url The URL to connect to open.bigmodel.cn/api/paas(https://open.bigmodel.cn/api/paas) spring.ai.zhipuai.api-key The API Key - Configuration Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the ZhiPuAI Image client. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Runtime Options: The ZhiPuAiImageOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiImageOptions.java) provides model configurations, such as the model to use, the quality, the size, etc. On start-up, the default options can be configured with the ZhiPuAiImageModel(ZhiPuAiImageApi zhiPuAiImageApi) constructor and the withDefaultOptions(ZhiPuAiImageOptions defaultOptions) method. Alternatively, use the spring.ai.zhipuai.image.options.* properties described previously. At runtime you can override the default options by adding new, request specific, options to the ImagePrompt call. For example to override the ZhiPuAI specific options such as quality and the number of images to create, use the following code example: ImageResponse response = zhiPuAiImageModel.call( new ImagePrompt(""A light cream colored mini golden doodle"", ZhiPuAiImageOptions.builder() .withQuality(""hd"") .withN(4) .withHeight(1024) .withWidth(1024).build()) ); In addition to the model specific ZhiPuAiImageOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-zhipuai/src/main/java/org/springframework/ai/zhipuai/ZhiPuAiImageOptions.java) you can use a portable ImageOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptions.java) instance, created with the ImageOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptionsBuilder.java) . Stability(stabilityai-image.html) QianFan(qianfan-image.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/image/qianfan-image.html","QianFan Image Generation: Spring AI supports CogView, the Image generation model from QianFan. Prerequisites: You will need to create an API with QianFan to access QianFan language models. Create an account at QianFan registration page(https://login.bce.baidu.com/new-reg) and generate the token on the API Keys page(https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application) . The Spring AI project defines a configuration property named spring.ai.qianfan.api-key and spring.ai.qianfan.secret-key . you should set to the value of the API Key and Secret Key obtained from API Keys page(https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application) . Exporting an environment variable is one way to set that configuration property: export SPRING_AI_QIANFAN_API_KEY=<INSERT KEY HERE> Add Repositories and BOM: Spring AI artifacts are published in Spring Milestone and Snapshot repositories. Refer to the Repositories(../../getting-started.html#repositories) section to add these repositories to your build system. To help with dependency management, Spring AI provides a BOM (bill of materials) to ensure that a consistent version of Spring AI is used throughout the entire project. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build system. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the QianFan Chat Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-qianfan-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-qianfan-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Image Generation Properties: The prefix spring.ai.qianfan.image is the property prefix that lets you configure the ImageModel implementation for QianFan. Property Description Default spring.ai.qianfan.image.enabled Enable QianFan image model. true spring.ai.qianfan.image.base-url Optional overrides the spring.ai.qianfan.base-url to provide chat specific url - spring.ai.qianfan.image.api-key Optional overrides the spring.ai.qianfan.api-key to provide chat specific api-key - spring.ai.qianfan.image.secret-key Optional overrides the spring.ai.qianfan.secret-key to provide chat specific secret-key - spring.ai.qianfan.image.options.model The model to use for image generation. sd_xl spring.ai.qianfan.image.options.user A unique identifier representing your end-user, which can help QianFan to monitor and detect abuse. - Connection Properties: The prefix spring.ai.qianfan is used as the property prefix that lets you connect to QianFan. Property Description Default spring.ai.qianfan.base-url The URL to connect to aip.baidubce.com/rpc/2.0/ai_custom(https://aip.baidubce.com/rpc/2.0/ai_custom) spring.ai.qianfan.api-key The API Key - spring.ai.qianfan.secret-key The Secret Key - Configuration Properties: Retry Properties: The prefix spring.ai.retry is used as the property prefix that lets you configure the retry mechanism for the QianFan Image client. Property Description Default spring.ai.retry.max-attempts Maximum number of retry attempts. 10 spring.ai.retry.backoff.initial-interval Initial sleep duration for the exponential backoff policy. 2 sec. spring.ai.retry.backoff.multiplier Backoff interval multiplier. 5 spring.ai.retry.backoff.max-interval Maximum backoff duration. 3 min. spring.ai.retry.on-client-errors If false, throw a NonTransientAiException, and do not attempt retry for 4xx client error codes false spring.ai.retry.exclude-on-http-codes List of HTTP status codes that should not trigger a retry (e.g. to throw NonTransientAiException). empty spring.ai.retry.on-http-codes List of HTTP status codes that should trigger a retry (e.g. to throw TransientAiException). empty Runtime Options: The QianFanImageOptions.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/main/java/org/springframework/ai/qianfan/QianFanImageOptions.java) provides model configurations, such as the model to use, the quality, the size, etc. On start-up, the default options can be configured with the QianFanImageModel(QianFanImageApi qianFanImageApi) constructor and the withDefaultOptions(QianFanImageOptions defaultOptions) method. Alternatively, use the spring.ai.qianfan.image.options.* properties described previously. At runtime you can override the default options by adding new, request specific, options to the ImagePrompt call. For example to override the QianFan specific options such as quality and the number of images to create, use the following code example: ImageResponse response = qianFanImageModel.call( new ImagePrompt(""A light cream colored mini golden doodle"", QianFanImageOptions.builder() .withN(4) .withHeight(1024) .withWidth(1024).build()) ); In addition to the model specific QianFanImageOptions(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-qianfan/src/main/java/org/springframework/ai/qianfan/QianFanImageOptions.java) you can use a portable ImageOptions(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptions.java) instance, created with the ImageOptionsBuilder#builder()(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/image/ImageOptionsBuilder.java) . ZhiPuAI(zhipuai-image.html) Transcription API(../audio/transcriptions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/index.html#api/audio","Spring AI: The Spring AI project aims to streamline the development of applications that incorporate artificial intelligence functionality without unnecessary complexity. The project draws inspiration from notable Python projects, such as LangChain and LlamaIndex, but Spring AI is not a direct port of those projects. The project was founded with the belief that the next wave of Generative AI applications will not be only for Python developers but will be ubiquitous across many programming languages. At its core, Spring AI addresses the fundamental challenge of AI integration: Connecting your enterprise Data and APIs with the AI Models . Spring AI provides abstractions that serve as the foundation for developing AI applications. These abstractions have multiple implementations, enabling easy component swapping with minimal code changes. Spring AI provides the following features: Support for all major Model providers such as OpenAI, Microsoft, Amazon, Google, and Hugging Face. Supported Model types are Chat, Text to Image, Audio Transcription, Text to Speech, Moderation, and more on the way. Portable API across AI providers for all models. Both synchronous and stream API options are supported. Dropping down to access model specific features is also supported. Mapping of AI Model output to POJOs. Support for all major Vector Database providers such as Apache Cassandra, Azure Vector Search, Chroma, Milvus, MongoDB Atlas, Neo4j, Oracle, PostgreSQL/PGVector, PineCone, Qdrant, Redis, and Weaviate. Portable API across Vector Store providers, including a novel SQL-like metadata filter API that is also portable. Function calling. Spring Boot Auto Configuration and Starters for AI Models and Vector Stores. ETL framework for Data Engineering. This feature set lets you implement common use cases such as “Q&A over your documentation” or “Chat with your documentation.” The concepts section(concepts.html) provides a high-level overview of AI concepts and their representation in Spring AI. The Getting Started(getting-started.html) section shows you how to create your first AI application. Subsequent sections delve into each component and common use cases with a code-focused approach. AI Concepts(concepts.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/audio/transcriptions.html","Transcription API: Spring AI provides support for OpenAI’s Transcription API. When additional providers for Transcription are implemented, a common AudioTranscriptionModel interface will be extracted. QianFan(../image/qianfan-image.html) Azure OpenAI(transcriptions/azure-openai-transcriptions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/audio/transcriptions/azure-openai-transcriptions.html","Azure OpenAI Transcriptions: Spring AI supports Azure Whisper model(https://learn.microsoft.com/en-us/azure/ai-services/openai/whisper-quickstart?tabs=command-line%2Cpython-new&pivots=rest-api) . Prerequisites: Obtain your Azure OpenAI endpoint and api-key from the Azure OpenAI Service section on the Azure Portal(https://portal.azure.com) . Spring AI defines a configuration property named spring.ai.azure.openai.api-key that you should set to the value of the API Key obtained from Azure. There is also a configuration property named spring.ai.azure.openai.endpoint that you should set to the endpoint URL obtained when provisioning your model in Azure. Exporting an environment variable is one way to set that configuration property: Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Azure OpenAI Transcription Generation Client. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-azure-openai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Transcription Properties: The prefix spring.ai.openai.audio.transcription is used as the property prefix that lets you configure the retry mechanism for the OpenAI image model. Property Description Default spring.ai.azure.openai.audio.transcription.enabled Enable Azure OpenAI transcription model. true spring.ai.azure.openai.audio.transcription.options.model ID of the model to use. Only whisper is currently available. whisper spring.ai.azure.openai.audio.transcription.options.deployment-name The deployment name under which the model is deployed. spring.ai.azure.openai.audio.transcription.options.response-format The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. json spring.ai.azure.openai.audio.transcription.options.prompt An optional text to guide the model’s style or continue a previous audio segment. The prompt should match the audio language. spring.ai.azure.openai.audio.transcription.options.language The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. spring.ai.azure.openai.audio.transcription.options.temperature The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. 0 spring.ai.azure.openai.audio.transcription.options.timestamp-granularities The timestamp granularities to populate for this transcription. response_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. segment Runtime Options: The AzureOpenAiAudioTranscriptionOptions class provides the options to use when making a transcription. On start-up, the options specified by spring.ai.azure.openai.audio.transcription are used, but you can override these at runtime. For example: AzureOpenAiAudioTranscriptionOptions.TranscriptResponseFormat responseFormat = AzureOpenAiAudioTranscriptionOptions.TranscriptResponseFormat.VTT; AzureOpenAiAudioTranscriptionOptions transcriptionOptions = AzureOpenAiAudioTranscriptionOptions.builder() .withLanguage(""en"") .withPrompt(""Ask not this, but ask that"") .withTemperature(0f) .withResponseFormat(responseFormat) .build(); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(audioFile, transcriptionOptions); AudioTranscriptionResponse response = azureOpenAiTranscriptionModel.call(transcriptionRequest); Manual Configuration: Add the spring-ai-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-openai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-azure-openai' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a AzureOpenAiAudioTranscriptionModel var openAIClient = new OpenAIClientBuilder() .credential(new AzureKeyCredential(System.getenv(""AZURE_OPENAI_API_KEY""))) .endpoint(System.getenv(""AZURE_OPENAI_ENDPOINT"")) .buildClient(); var azureOpenAiAudioTranscriptionModel = new AzureOpenAiAudioTranscriptionModel(openAIClient, null); var transcriptionOptions = AzureOpenAiAudioTranscriptionOptions.builder() .withResponseFormat(TranscriptResponseFormat.TEXT) .withTemperature(0f) .build(); var audioFile = new FileSystemResource(""/path/to/your/resource/speech/jfk.flac""); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(audioFile, transcriptionOptions); AudioTranscriptionResponse response = azureOpenAiAudioTranscriptionModel.call(transcriptionRequest); Transcription API(../transcriptions.html) OpenAI(openai-transcriptions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/audio/transcriptions/openai-transcriptions.html","OpenAI Transcriptions: Spring AI supports OpenAI’s Transcription model(https://platform.openai.com/docs/api-reference/audio/createTranscription) . Prerequisites: You will need to create an API key with OpenAI to access ChatGPT models. Create an account at OpenAI signup page(https://platform.openai.com/signup) and generate the token on the API Keys page(https://platform.openai.com/account/api-keys) . The Spring AI project defines a configuration property named spring.ai.openai.api-key that you should set to the value of the API Key obtained from openai.com. Exporting an environment variable is one way to set that configuration property: Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OpenAI Transcription Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Transcription Properties: Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.openai.base-url The URL to connect to api.openai.com(https://api.openai.com) spring.ai.openai.api-key The API Key - spring.ai.openai.organization-id Optionally you can specify which organization used for an API request. - spring.ai.openai.project-id Optionally, you can specify which project is used for an API request. - For users that belong to multiple organizations (or are accessing their projects through their legacy user API key), optionally, you can specify which organization and project is used for an API request. Usage from these API requests will count as usage for the specified organization and project. Configuration Properties: The prefix spring.ai.openai.audio.transcription is used as the property prefix that lets you configure the retry mechanism for the OpenAI transcription model. Property Description Default spring.ai.openai.audio.transcription.base-url The URL to connect to api.openai.com(https://api.openai.com) spring.ai.openai.audio.transcription.api-key The API Key - spring.ai.openai.audio.transcription.organization-id Optionally you can specify which organization used for an API request. - spring.ai.openai.audio.transcription.project-id Optionally, you can specify which project is used for an API request. - spring.ai.openai.audio.transcription.options.model ID of the model to use. Only whisper-1 (which is powered by our open source Whisper V2 model) is currently available. whisper-1 spring.ai.openai.audio.transcription.options.response-format The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. json spring.ai.openai.audio.transcription.options.prompt An optional text to guide the model’s style or continue a previous audio segment. The prompt should match the audio language. spring.ai.openai.audio.transcription.options.language The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. spring.ai.openai.audio.transcription.options.temperature The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. 0 spring.ai.openai.audio.transcription.options.timestamp_granularities The timestamp granularities to populate for this transcription. response_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. segment You can override the common spring.ai.openai.base-url , spring.ai.openai.api-key , spring.ai.openai.organization-id and spring.ai.openai.project-id properties. The spring.ai.openai.audio.transcription.base-url , spring.ai.openai.audio.transcription.api-key , spring.ai.openai.audio.transcription.organization-id and spring.ai.openai.audio.transcription.project-id properties if set take precedence over the common properties. This is useful if you want to use different OpenAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.openai.transcription.options can be overridden at runtime. Runtime Options: The OpenAiAudioTranscriptionOptions class provides the options to use when making a transcription. On start-up, the options specified by spring.ai.openai.audio.transcription are used but you can override these at runtime. For example: OpenAiAudioApi.TranscriptResponseFormat responseFormat = OpenAiAudioApi.TranscriptResponseFormat.VTT; OpenAiAudioTranscriptionOptions transcriptionOptions = OpenAiAudioTranscriptionOptions.builder() .withLanguage(""en"") .withPrompt(""Ask not this, but ask that"") .withTemperature(0f) .withResponseFormat(responseFormat) .build(); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(audioFile, transcriptionOptions); AudioTranscriptionResponse response = openAiTranscriptionModel.call(transcriptionRequest); Manual Configuration: Add the spring-ai-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create a OpenAiAudioTranscriptionModel var openAiAudioApi = new OpenAiAudioApi(System.getenv(""OPENAI_API_KEY"")); var openAiAudioTranscriptionModel = new OpenAiAudioTranscriptionModel(openAiAudioApi); var transcriptionOptions = OpenAiAudioTranscriptionOptions.builder() .withResponseFormat(TranscriptResponseFormat.TEXT) .withTemperature(0f) .build(); var audioFile = new FileSystemResource(""/path/to/your/resource/speech/jfk.flac""); AudioTranscriptionPrompt transcriptionRequest = new AudioTranscriptionPrompt(audioFile, transcriptionOptions); AudioTranscriptionResponse response = openAiTranscriptionModel.call(transcriptionRequest); Example Code: The OpenAiTranscriptionModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/audio/transcription/OpenAiTranscriptionModelIT.java) test provides some general examples how to use the library. Azure OpenAI(azure-openai-transcriptions.html) Text-To-Speech (TTS) API(../speech.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/audio/speech.html","Text-To-Speech (TTS) API: Spring AI provides support for OpenAI’s Speech API. When additional providers for Speech are implemented, a common SpeechModel and StreamingSpeechModel interface will be extracted. OpenAI(transcriptions/openai-transcriptions.html) OpenAI(speech/openai-speech.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/audio/speech/openai-speech.html","OpenAI Text-to-Speech (TTS): Introduction: The Audio API provides a speech endpoint based on OpenAI’s TTS (text-to-speech) model, enabling users to: Narrate a written blog post. Produce spoken audio in multiple languages. Give real-time audio output using streaming. Prerequisites: Create an OpenAI account and obtain an API key. You can sign up at the OpenAI signup page(https://platform.openai.com/signup) and generate an API key on the API Keys page(https://platform.openai.com/account/api-keys) . Add the spring-ai-openai dependency to your project’s build file. For more information, refer to the Dependency Management(../../../getting-started.html#dependency-management) section. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OpenAI Text-to-Speech Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file: dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Speech Properties: Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.openai.base-url The URL to connect to api.openai.com(https://api.openai.com) spring.ai.openai.api-key The API Key - spring.ai.openai.organization-id Optionally you can specify which organization used for an API request. - spring.ai.openai.project-id Optionally, you can specify which project is used for an API request. - For users that belong to multiple organizations (or are accessing their projects through their legacy user API key), optionally, you can specify which organization and project is used for an API request. Usage from these API requests will count as usage for the specified organization and project. Configuration Properties: The prefix spring.ai.openai.audio.speech is used as the property prefix that lets you configure the OpenAI Text-to-Speech client. Property Description Default spring.ai.openai.audio.speech.base-url The URL to connect to api.openai.com(https://api.openai.com) spring.ai.openai.audio.speech.api-key The API Key - spring.ai.openai.audio.speech.organization-id Optionally you can specify which organization used for an API request. - spring.ai.openai.audio.speech.project-id Optionally, you can specify which project is used for an API request. - spring.ai.openai.audio.speech.options.model ID of the model to use. Only tts-1 is currently available. tts-1 spring.ai.openai.audio.speech.options.voice The voice to use for the TTS output. Available options are: alloy, echo, fable, onyx, nova, and shimmer. alloy spring.ai.openai.audio.speech.options.response-format The format of the audio output. Supported formats are mp3, opus, aac, flac, wav, and pcm. mp3 spring.ai.openai.audio.speech.options.speed The speed of the voice synthesis. The acceptable range is from 0.25 (slowest) to 4.0 (fastest). 1.0 You can override the common spring.ai.openai.base-url , spring.ai.openai.api-key , spring.ai.openai.organization-id and spring.ai.openai.project-id properties. The spring.ai.openai.audio.speech.base-url , spring.ai.openai.audio.speech.api-key , spring.ai.openai.audio.speech.organization-id and spring.ai.openai.audio.speech.project-id properties if set take precedence over the common properties. This is useful if you want to use different OpenAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.openai.image.options can be overridden at runtime. Runtime Options: The OpenAiAudioSpeechOptions class provides the options to use when making a text-to-speech request. On start-up, the options specified by spring.ai.openai.audio.speech are used but you can override these at runtime. For example: OpenAiAudioSpeechOptions speechOptions = OpenAiAudioSpeechOptions.builder() .withModel(""tts-1"") .withVoice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY) .withResponseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .withSpeed(1.0f) .build(); SpeechPrompt speechPrompt = new SpeechPrompt(""Hello, this is a text-to-speech example."", speechOptions); SpeechResponse response = openAiAudioSpeechModel.call(speechPrompt); Manual Configuration: Add the spring-ai-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai</artifactId> </dependency> or to your Gradle build.gradle build file: dependencies { implementation 'org.springframework.ai:spring-ai-openai' } Refer to the Dependency Management(../../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an OpenAiAudioSpeechModel : var openAiAudioApi = new OpenAiAudioApi(System.getenv(""OPENAI_API_KEY"")); var openAiAudioSpeechModel = new OpenAiAudioSpeechModel(openAiAudioApi); var speechOptions = OpenAiAudioSpeechOptions.builder() .withResponseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .withSpeed(1.0f) .withModel(OpenAiAudioApi.TtsModel.TTS_1.value) .build(); var speechPrompt = new SpeechPrompt(""Hello, this is a text-to-speech example."", speechOptions); SpeechResponse response = openAiAudioSpeechModel.call(speechPrompt); // Accessing metadata (rate limit info) OpenAiAudioSpeechResponseMetadata metadata = response.getMetadata(); byte[] responseAsBytes = response.getResult().getOutput(); Streaming Real-time Audio: The Speech API provides support for real-time audio streaming using chunk transfer encoding. This means that the audio is able to be played before the full file has been generated and made accessible. var openAiAudioApi = new OpenAiAudioApi(System.getenv(""OPENAI_API_KEY"")); var openAiAudioSpeechModel = new OpenAiAudioSpeechModel(openAiAudioApi); OpenAiAudioSpeechOptions speechOptions = OpenAiAudioSpeechOptions.builder() .withVoice(OpenAiAudioApi.SpeechRequest.Voice.ALLOY) .withSpeed(1.0f) .withResponseFormat(OpenAiAudioApi.SpeechRequest.AudioResponseFormat.MP3) .withModel(OpenAiAudioApi.TtsModel.TTS_1.value) .build(); SpeechPrompt speechPrompt = new SpeechPrompt(""Today is a wonderful day to build something people love!"", speechOptions); Flux<SpeechResponse> responseStream = openAiAudioSpeechModel.stream(speechPrompt); Example Code: The OpenAiSpeechModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/audio/speech/OpenAiSpeechModelIT.java) test provides some general examples of how to use the library. Text-To-Speech (TTS) API(../speech.html) OpenAI(../../moderation/openai-moderation.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/index.html#api/moderation","Spring AI: The Spring AI project aims to streamline the development of applications that incorporate artificial intelligence functionality without unnecessary complexity. The project draws inspiration from notable Python projects, such as LangChain and LlamaIndex, but Spring AI is not a direct port of those projects. The project was founded with the belief that the next wave of Generative AI applications will not be only for Python developers but will be ubiquitous across many programming languages. At its core, Spring AI addresses the fundamental challenge of AI integration: Connecting your enterprise Data and APIs with the AI Models . Spring AI provides abstractions that serve as the foundation for developing AI applications. These abstractions have multiple implementations, enabling easy component swapping with minimal code changes. Spring AI provides the following features: Support for all major Model providers such as OpenAI, Microsoft, Amazon, Google, and Hugging Face. Supported Model types are Chat, Text to Image, Audio Transcription, Text to Speech, Moderation, and more on the way. Portable API across AI providers for all models. Both synchronous and stream API options are supported. Dropping down to access model specific features is also supported. Mapping of AI Model output to POJOs. Support for all major Vector Database providers such as Apache Cassandra, Azure Vector Search, Chroma, Milvus, MongoDB Atlas, Neo4j, Oracle, PostgreSQL/PGVector, PineCone, Qdrant, Redis, and Weaviate. Portable API across Vector Store providers, including a novel SQL-like metadata filter API that is also portable. Function calling. Spring Boot Auto Configuration and Starters for AI Models and Vector Stores. ETL framework for Data Engineering. This feature set lets you implement common use cases such as “Q&A over your documentation” or “Chat with your documentation.” The concepts section(concepts.html) provides a high-level overview of AI concepts and their representation in Spring AI. The Getting Started(getting-started.html) section shows you how to create your first AI application. Subsequent sections delve into each component and common use cases with a code-focused approach. AI Concepts(concepts.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/moderation/openai-moderation.html","Moderation: Introduction: Spring AI supports OpenAI’s Moderation model, which allows you to detect potentially harmful or sensitive content in text. Follow this guide(https://platform.openai.com/docs/guides/moderation) to for more information on OpenAI’s moderation model. Prerequisites: Create an OpenAI account and obtain an API key. You can sign up at the OpenAI signup page(https://platform.openai.com/signup) and generate an API key on the API Keys page(https://platform.openai.com/account/api-keys) . Add the spring-ai-openai dependency to your project’s build file. For more information, refer to the Dependency Management(../../getting-started.html#dependency-management) section. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OpenAI Text-to-Speech Client. To enable it add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file: dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Moderation Properties: Connection Properties: The prefix spring.ai.openai is used as the property prefix that lets you connect to OpenAI. Property Description Default spring.ai.openai.base-url The URL to connect to api.openai.com(https://api.openai.com) spring.ai.openai.api-key The API Key - spring.ai.openai.organization-id Optionally you can specify which organization is used for an API request. - spring.ai.openai.project-id Optionally, you can specify which project is used for an API request. - For users that belong to multiple organizations (or are accessing their projects through their legacy user API key), optionally, you can specify which organization and project is used for an API request. Usage from these API requests will count as usage for the specified organization and project. Configuration Properties: The prefix spring.ai.openai.moderation is used as the property prefix for configuring the OpenAI moderation model. Property Description Default spring.ai.openai.moderation.base-url The URL to connect to api.openai.com(https://api.openai.com) spring.ai.openai.moderation.api-key The API Key - spring.ai.openai.moderation.organization-id Optionally you can specify which organization is used for an API request. - spring.ai.openai.moderation.project-id Optionally, you can specify which project is used for an API request. - spring.ai.openai.moderation.options.model ID of the model to use for moderation. text-moderation-latest You can override the common spring.ai.openai.base-url , spring.ai.openai.api-key , spring.ai.openai.organization-id and spring.ai.openai.project-id properties. The spring.ai.openai.moderation.base-url , spring.ai.openai.moderation.api-key , spring.ai.openai.moderation.organization-id and spring.ai.openai.moderation.project-id properties, if set, take precedence over the common properties. This is useful if you want to use different OpenAI accounts for different models and different model endpoints. All properties prefixed with spring.ai.openai.moderation.options can be overridden at runtime. Runtime Options: The OpenAiModerationOptions class provides the options to use when making a moderation request. On start-up, the options specified by spring.ai.openai.moderation are used, but you can override these at runtime. For example: OpenAiModerationOptions moderationOptions = OpenAiModerationOptions.builder() .withModel(""text-moderation-latest"") .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(""Text to be moderated"", moderationOptions); ModerationResponse response = openAiModerationModel.call(moderationPrompt); // Access the moderation results Moderation moderation = moderationResponse.getResult().getOutput(); // Print general information System.out.println(""Moderation ID: "" + moderation.getId()); System.out.println(""Model used: "" + moderation.getModel()); // Access the moderation results (there's usually only one, but it's a list) for (ModerationResult result : moderation.getResults()) { System.out.println(""\nModeration Result:""); System.out.println(""Flagged: "" + result.isFlagged()); // Access categories Categories categories = result.getCategories(); System.out.println(""\nCategories:""); System.out.println(""Sexual: "" + categories.isSexual()); System.out.println(""Hate: "" + categories.isHate()); System.out.println(""Harassment: "" + categories.isHarassment()); System.out.println(""Self-Harm: "" + categories.isSelfHarm()); System.out.println(""Sexual/Minors: "" + categories.isSexualMinors()); System.out.println(""Hate/Threatening: "" + categories.isHateThreatening()); System.out.println(""Violence/Graphic: "" + categories.isViolenceGraphic()); System.out.println(""Self-Harm/Intent: "" + categories.isSelfHarmIntent()); System.out.println(""Self-Harm/Instructions: "" + categories.isSelfHarmInstructions()); System.out.println(""Harassment/Threatening: "" + categories.isHarassmentThreatening()); System.out.println(""Violence: "" + categories.isViolence()); // Access category scores CategoryScores scores = result.getCategoryScores(); System.out.println(""\nCategory Scores:""); System.out.println(""Sexual: "" + scores.getSexual()); System.out.println(""Hate: "" + scores.getHate()); System.out.println(""Harassment: "" + scores.getHarassment()); System.out.println(""Self-Harm: "" + scores.getSelfHarm()); System.out.println(""Sexual/Minors: "" + scores.getSexualMinors()); System.out.println(""Hate/Threatening: "" + scores.getHateThreatening()); System.out.println(""Violence/Graphic: "" + scores.getViolenceGraphic()); System.out.println(""Self-Harm/Intent: "" + scores.getSelfHarmIntent()); System.out.println(""Self-Harm/Instructions: "" + scores.getSelfHarmInstructions()); System.out.println(""Harassment/Threatening: "" + scores.getHarassmentThreatening()); System.out.println(""Violence: "" + scores.getViolence()); } Manual Configuration: Add the spring-ai-openai dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai</artifactId> </dependency> or to your Gradle build.gradle build file: dependencies { implementation 'org.springframework.ai:spring-ai-openai' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Next, create an OpenAiModerationModel: OpenAiModerationApi openAiModerationApi = new OpenAiModerationApi(System.getenv(""OPENAI_API_KEY"")); OpenAiModerationModel openAiModerationModel = new OpenAiModerationModel(openAiModerationApi); OpenAiModerationOptions moderationOptions = OpenAiModerationOptions.builder() .withModel(""text-moderation-latest"") .build(); ModerationPrompt moderationPrompt = new ModerationPrompt(""Text to be moderated"", moderationOptions); ModerationResponse response = openAiModerationModel.call(moderationPrompt); Example Code: The OpenAiModerationModelIT test provides some general examples of how to use the library. You can refer to this test for more detailed usage examples. OpenAI(../audio/speech/openai-speech.html) Vector Databases(../vectordbs.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs.html","Vector Databases: A vector databases is a specialized type of database that plays an essential role in AI applications. In vector databases, queries differ from traditional relational databases. Instead of exact matches, they perform similarity searches. When given a vector as a query, a vector database returns vectors that are “similar” to the query vector. Further details on how this similarity is calculated at a high-level is provided in a Vector Similarity(vectordbs/understand-vectordbs.html#vectordbs-similarity) . Vector databases are used to integrate your data with AI models. The first step in their usage is to load your data into a vector database. Then, when a user query is to be sent to the AI model, a set of similar documents is first retrieved. These documents then serve as the context for the user’s question and are sent to the AI model, along with the user’s query. This technique is known as Retrieval Augmented Generation (RAG)(../concepts.html#concept-rag) . The following sections describe the Spring AI interface for using multiple vector database implementations and some high-level sample usage. The last section is intended to demystify the underlying approach of similarity searching in vector databases. API Overview: This section serves as a guide to the VectorStore interface and its associated classes within the Spring AI framework. Spring AI offers an abstracted API for interacting with vector databases through the VectorStore interface. Here is the VectorStore interface definition: public interface VectorStore { void add(List<Document> documents); Optional<Boolean> delete(List<String> idList); List<Document> similaritySearch(String query); List<Document> similaritySearch(SearchRequest request); } and the related SearchRequest builder: public class SearchRequest { public final String query; private int topK = 4; private double similarityThreshold = SIMILARITY_THRESHOLD_ALL; private Filter.Expression filterExpression; public static SearchRequest query(String query) { return new SearchRequest(query); } private SearchRequest(String query) { this.query = query; } public SearchRequest withTopK(int topK) {...} public SearchRequest withSimilarityThreshold(double threshold) {...} public SearchRequest withSimilarityThresholdAll() {...} public SearchRequest withFilterExpression(Filter.Expression expression) {...} public SearchRequest withFilterExpression(String textExpression) {...} public String getQuery() {...} public int getTopK() {...} public double getSimilarityThreshold() {...} public Filter.Expression getFilterExpression() {...} } To insert data into the vector database, encapsulate it within a Document object. The Document class encapsulates content from a data source, such as a PDF or Word document, and includes text represented as a string. It also contains metadata in the form of key-value pairs, including details such as the filename. Upon insertion into the vector database, the text content is transformed into a numerical array, or a float[] , known as vector embeddings, using an embedding model. Embedding models, such as Word2Vec(https://en.wikipedia.org/wiki/Word2vec) , GLoVE(https://en.wikipedia.org/wiki/GloVe_(machine_learning)) , and BERT(https://en.wikipedia.org/wiki/BERT_(language_model)) , or OpenAI’s text-embedding-ada-002 , are used to convert words, sentences, or paragraphs into these vector embeddings. The vector database’s role is to store and facilitate similarity searches for these embeddings. It does not generate the embeddings itself. For creating vector embeddings, the EmbeddingModel should be utilized. The similaritySearch methods in the interface allow for retrieving documents similar to a given query string. These methods can be fine-tuned by using the following parameters: k : An integer that specifies the maximum number of similar documents to return. This is often referred to as a 'top K' search, or 'K nearest neighbors' (KNN). threshold : A double value ranging from 0 to 1, where values closer to 1 indicate higher similarity. By default, if you set a threshold of 0.75, for instance, only documents with a similarity above this value are returned. Filter.Expression : A class used for passing a fluent DSL (Domain-Specific Language) expression that functions similarly to a 'where' clause in SQL, but it applies exclusively to the metadata key-value pairs of a Document . filterExpression : An external DSL based on ANTLR4 that accepts filter expressions as strings. For example, with metadata keys like country, year, and isActive , you could use an expression such as: country == 'UK' && year >= 2020 && isActive == true. Find more information on the Filter.Expression in the Metadata Filters(#metadata-filters) section. Schema Initialization: Some vector stores require their backend schema to be initialized before usage. It will not be initialized for you by default. You must opt-in, by passing a boolean for the appropriate constructor argument or, if using Spring Boot, setting the appropriate initialize-schema property to true in application.properties or application.yml . Check the documentation for the vector store you are using for the specific property name. Batching Strategy: When working with vector stores, it’s often necessary to embed large numbers of documents. While it might seem straightforward to make a single call to embed all documents at once, this approach can lead to issues. Embedding models process text as tokens and have a maximum token limit, often referred to as the context window size. This limit restricts the amount of text that can be processed in a single embedding request. Attempting to embed too many tokens in one call can result in errors or truncated embeddings. To address this token limit, Spring AI implements a batching strategy. This approach breaks down large sets of documents into smaller batches that fit within the embedding model’s maximum context window. Batching not only solves the token limit issue but can also lead to improved performance and more efficient use of API rate limits. Spring AI provides this functionality through the BatchingStrategy interface, which allows for processing documents in sub-batches based on their token counts. The core BatchingStrategy interface is defined as follows: public interface BatchingStrategy { List<List<Document>> batch(List<Document> documents); } This interface defines a single method, batch , which takes a list of documents and returns a list of document batches. Default Implementation: Spring AI provides a default implementation called TokenCountBatchingStrategy . This strategy batches documents based on their token counts, ensuring that each batch does not exceed a calculated maximum input token count. Key features of TokenCountBatchingStrategy : Uses OpenAI’s max input token count(https://platform.openai.com/docs/guides/embeddings/embedding-models) (8191) as the default upper limit. Incorporates a reserve percentage (default 10%) to provide a buffer for potential overhead. Calculates the actual max input token count as: actualMaxInputTokenCount = originalMaxInputTokenCount * (1 - RESERVE_PERCENTAGE) The strategy estimates the token count for each document, groups them into batches without exceeding the max input token count, and throws an exception if a single document exceeds this limit. You can also customize the TokenCountBatchingStrategy to better suit your specific requirements. This can be done by creating a new instance with custom parameters in a Spring Boot @Configuration class. Here’s an example of how to create a custom TokenCountBatchingStrategy bean: @Configuration public class EmbeddingConfig { @Bean public BatchingStrategy customTokenCountBatchingStrategy() { return new TokenCountBatchingStrategy( EncodingType.CL100K_BASE, // Specify the encoding type 8000, // Set the maximum input token count 0.9 // Set the threshold factor ); } } In this configuration: EncodingType.CL100K_BASE : Specifies the encoding type used for tokenization. This encoding type is used by the JTokkitTokenCountEstimator to accurately estimate token counts. 8000 : Sets the maximum input token count. This value should be less than or equal to the maximum context window size of your embedding model. 0.9 : Sets the threshold factor. This factor determines how full a batch can be before starting a new one. A value of 0.9 means each batch will be filled up to 90% of the maximum input token count. By default, this constructor uses Document.DEFAULT_CONTENT_FORMATTER for content formatting and MetadataMode.NONE for metadata handling. If you need to customize these parameters, you can use the full constructor with additional parameters. Once defined, this custom TokenCountBatchingStrategy bean will be automatically used by the EmbeddingModel implementations in your application, replacing the default strategy. The TokenCountBatchingStrategy internally uses a TokenCountEstimator (specifically, JTokkitTokenCountEstimator ) to calculate token counts for efficient batching. This ensures accurate token estimation based on the specified encoding type. Additionally, TokenCountBatchingStrategy provides flexibility by allowing you to pass in your own implementation of the TokenCountEstimator interface. This feature enables you to use custom token counting strategies tailored to your specific needs. For example: TokenCountEstimator customEstimator = new YourCustomTokenCountEstimator(); TokenCountBatchingStrategy strategy = new TokenCountBatchingStrategy( customEstimator, 8000, // maxInputTokenCount 0.1, // reservePercentage Document.DEFAULT_CONTENT_FORMATTER, MetadataMode.NONE ); Custom Implementation: While TokenCountBatchingStrategy provides a robust default implementation, you can customize the batching strategy to fit your specific needs. This can be done through Spring Boot’s auto-configuration. To customize the batching strategy, define a BatchingStrategy bean in your Spring Boot application: @Configuration public class EmbeddingConfig { @Bean public BatchingStrategy customBatchingStrategy() { return new CustomBatchingStrategy(); } } This custom BatchingStrategy will then be automatically used by the EmbeddingModel implementations in your application. Vector stores supported by Spring AI are configured to use the default TokenCountBatchingStrategy . SAP Hana vector store is not currently configured for batching. VectorStore Implementations: These are the available implementations of the VectorStore interface: Azure Vector Search(vectordbs/azure.html) - The Azure(https://learn.microsoft.com/en-us/azure/search/vector-search-overview) vector store. Apache Cassandra(vectordbs/apache-cassandra.html) - The Apache Cassandra(https://cassandra.apache.org/doc/latest/cassandra/vector-search/overview.html) vector store. Chroma Vector Store(vectordbs/chroma.html) - The Chroma(https://www.trychroma.com/) vector store. Elasticsearch Vector Store(vectordbs/elasticsearch.html) - The Elasticsearch(https://www.elastic.co/) vector store. GemFire Vector Store(vectordbs/gemfire.html) - The GemFire(https://tanzu.vmware.com/content/blog/vmware-gemfire-vector-database-extension) vector store. Milvus Vector Store(vectordbs/milvus.html) - The Milvus(https://milvus.io/) vector store. MongoDB Atlas Vector Store(vectordbs/mongodb.html) - The MongoDB Atlas(https://www.mongodb.com/atlas/database) vector store. Neo4j Vector Store(vectordbs/neo4j.html) - The Neo4j(https://neo4j.com/) vector store. OpenSearch Vector Store(vectordbs/opensearch.html) - The OpenSearch(https://opensearch.org/platform/search/vector-database.html) vector store. Oracle Vector Store(vectordbs/oracle.html) - The Oracle Database(https://docs.oracle.com/en/database/oracle/oracle-database/23/vecse/overview-ai-vector-search.html) vector store. PgVector Store(vectordbs/pgvector.html) - The PostgreSQL/PGVector(https://github.com/pgvector/pgvector) vector store. Pinecone Vector Store(vectordbs/pinecone.html) - PineCone(https://www.pinecone.io/) vector store. Qdrant Vector Store(vectordbs/qdrant.html) - Qdrant(https://www.qdrant.tech/) vector store. Redis Vector Store(vectordbs/redis.html) - The Redis(https://redis.io/) vector store. SAP Hana Vector Store(vectordbs/hana.html) - The SAP HANA(https://news.sap.com/2024/04/sap-hana-cloud-vector-engine-ai-with-business-context/) vector store. Typesense Vector Store(vectordbs/typesense.html) - The Typesense(https://typesense.org/docs/0.24.0/api/vector-search.html) vector store. Weaviate Vector Store(vectordbs/weaviate.html) - The Weaviate(https://weaviate.io/) vector store. SimpleVectorStore(https://github.com/spring-projects/spring-ai/blob/main/spring-ai-core/src/main/java/org/springframework/ai/vectorstore/SimpleVectorStore.java) - A simple implementation of persistent vector storage, good for educational purposes. More implementations may be supported in future releases. If you have a vector database that needs to be supported by Spring AI, open an issue on GitHub or, even better, submit a pull request with an implementation. Information on each of the VectorStore implementations can be found in the subsections of this chapter. Example Usage: To compute the embeddings for a vector database, you need to pick an embedding model that matches the higher-level AI model being used. For example, with OpenAI’s ChatGPT, we use the OpenAiEmbeddingModel and a model named text-embedding-ada-002 . The Spring Boot starter’s auto-configuration for OpenAI makes an implementation of EmbeddingModel available in the Spring application context for dependency injection. The general usage of loading data into a vector store is something you would do in a batch-like job, by first loading data into Spring AI’s Document class and then calling the save method. Given a String reference to a source file that represents a JSON file with data we want to load into the vector database, we use Spring AI’s JsonReader to load specific fields in the JSON, which splits them up into small pieces and then passes those small pieces to the vector store implementation. The VectorStore implementation computes the embeddings and stores the JSON and the embedding in the vector database: @Autowired VectorStore vectorStore; void load(String sourceFile) { JsonReader jsonReader = new JsonReader(new FileSystemResource(sourceFile), ""price"", ""name"", ""shortDescription"", ""description"", ""tags""); List<Document> documents = jsonReader.get(); this.vectorStore.add(documents); } Later, when a user question is passed into the AI model, a similarity search is done to retrieve similar documents, which are then ""'stuffed'"" into the prompt as context for the user’s question. String question = <question from user> List<Document> similarDocuments = store.similaritySearch(question); Additional options can be passed into the similaritySearch method to define how many documents to retrieve and a threshold of the similarity search. Metadata Filters: This section describes various filters that you can use against the results of a query. Filter String: You can pass in an SQL-like filter expressions as a String to one of the similaritySearch overloads. Consider the following examples: ""country == 'BG'"" ""genre == 'drama' && year >= 2020"" ""genre in ['comedy', 'documentary', 'drama']"" Filter.Expression: You can create an instance of Filter.Expression with a FilterExpressionBuilder that exposes a fluent API. A simple example is as follows: FilterExpressionBuilder b = new FilterExpressionBuilder(); Expression expression = b.eq(""country"", ""BG"").build(); You can build up sophisticated expressions by using the following operators: EQUALS: '==' MINUS : '-' PLUS: '+' GT: '>' GE: '>=' LT: '<' LE: '<=' NE: '!=' You can combine expressions by using the following operators: AND: 'AND' | 'and' | '&&'; OR: 'OR' | 'or' | '||'; Considering the following example: Expression exp = b.and(b.eq(""genre"", ""drama""), b.gte(""year"", 2020)).build(); You can also use the following operators: IN: 'IN' | 'in'; NIN: 'NIN' | 'nin'; NOT: 'NOT' | 'not'; Consider the following example: Expression exp = b.and(b.eq(""genre"", ""drama""), b.gte(""year"", 2020)).build(); Understanding Vectors: Understanding Vectors(vectordbs/understand-vectordbs.html) OpenAI(moderation/openai-moderation.html) Azure AI Service(vectordbs/azure.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/azure.html","Azure AI Service: This section will walk you through setting up the AzureVectorStore to store document embeddings and perform similarity searches using the Azure AI Search Service. Azure AI Search(https://azure.microsoft.com/en-us/products/ai-services/ai-search/) is a versatile cloud-hosted cloud information retrieval system that is part of Microsoft’s larger AI platform. Among other features, it allows users to query information using vector-based storage and retrieval. Prerequisites: Azure Subscription: You will need an Azure subscription(https://azure.microsoft.com/en-us/free/) to use any Azure service. Azure AI Search Service: Create an AI Search service(https://portal.azure.com/#create/Microsoft.Search) . Once the service is created, obtain the admin apiKey from the Keys section under Settings and retrieve the endpoint from the Url field under the Overview section. (Optional) Azure OpenAI Service: Create an Azure OpenAI service(https://portal.azure.com/#create/Microsoft.AIServicesOpenAI) . NOTE: You may have to fill out a separate form to gain access to Azure Open AI services. Once the service is created, obtain the endpoint and apiKey from the Keys and Endpoint section under Resource Management . Configuration: On startup, the AzureVectorStore can attempt to create a new index within your AI Search service instance if you’ve opted in by setting the relevant initialize-schema boolean property to true in the constructor or, if using Spring Boot, setting …​initialize-schema=true in your application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. Alternatively, you can create the index manually. To set up an AzureVectorStore, you will need the settings retrieved from the prerequisites above along with your index name: Azure AI Search Endpoint Azure AI Search Key (optional) Azure OpenAI API Endpoint (optional) Azure OpenAI API Key You can provide these values as OS environment variables. export AZURE_AI_SEARCH_API_KEY=<My AI Search API Key> export AZURE_AI_SEARCH_ENDPOINT=<My AI Search Index> export OPENAI_API_KEY=<My Azure AI API Key> (Optional) You can replace Azure Open AI implementation with any valid OpenAI implementation that supports the Embeddings interface. For example, you could use Spring AI’s Open AI or TransformersEmbedding implementations for embeddings instead of the Azure implementation. Dependencies: Add these dependencies to your project: 1. Select an Embeddings interface implementation. You can choose between:: OpenAI Embedding: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> Or Azure AI Embedding: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId> </dependency> Or Local Sentence Transformers Embedding: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-transformers-spring-boot-starter</artifactId> </dependency> 2. Azure (AI Search) Vector Store: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-azure-store</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Configuration Properties: You can use the following properties in your Spring Boot configuration to customize the Azure vector store. Property Default value spring.ai.vectorstore.azure.url spring.ai.vectorstore.azure.api-key spring.ai.vectorstore.azure.initialze-schema false spring.ai.vectorstore.azure.index-name spring_ai_azure_vector_store spring.ai.vectorstore.azure.default-top-k 4 spring.ai.vectorstore.azure.default-similarity-threshold 0.0 spring.ai.vectorstore.azure.embedding-property embedding spring.ai.vectorstore.azure.index-name spring-ai-document-index Sample Code: To configure an Azure SearchIndexClient in your application, you can use the following code: @Bean public SearchIndexClient searchIndexClient() { return new SearchIndexClientBuilder().endpoint(System.getenv(""AZURE_AI_SEARCH_ENDPOINT"")) .credential(new AzureKeyCredential(System.getenv(""AZURE_AI_SEARCH_API_KEY""))) .buildClient(); } To create a vector store, you can use the following code by injecting the SearchIndexClient bean created in the above sample along with an EmbeddingModel provided by the Spring AI library that implements the desired Embeddings interface. @Bean public VectorStore vectorStore(SearchIndexClient searchIndexClient, EmbeddingModel embeddingModel) { return new AzureVectorStore(searchIndexClient, embeddingModel, // Define the metadata fields to be used // in the similarity search filters. List.of(MetadataField.text(""country""), MetadataField.int64(""year""), MetadataField.bool(""active""))); } You must list explicitly all metadata field names and types for any metadata key used in the filter expression. The list above registers filterable metadata fields: country of type TEXT , year of type INT64 , and active of type BOOLEAN . If the filterable metadata fields are expanded with new entries, you have to (re)upload/update the documents with this metadata. In your main code, create some documents: List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""country"", ""BG"", ""year"", 2020)), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""country"", ""NL"", ""year"", 2023))); Add the documents to your vector store: vectorStore.add(documents); And finally, retrieve documents similar to a query: List<Document> results = vectorStore.similaritySearch( SearchRequest .query(""Spring"") .withTopK(5)); If all goes well, you should retrieve the document containing the text ""Spring AI rocks!!"". Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with AzureVectorStore as well. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest .query(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""country in ['UK', 'NL'] && year >= 2020"")); or programmatically using the expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch( SearchRequest .query(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""country"", ""UK"", ""NL""), b.gte(""year"", 2020)).build())); The portable filter expressions get automatically converted into the proprietary Azure Search OData filters(https://learn.microsoft.com/en-us/azure/search/search-query-odata-filter) . For example, the following portable filter expression: country in ['UK', 'NL'] && year >= 2020 is converted into the following Azure OData filter expression(https://learn.microsoft.com/en-us/azure/search/search-query-odata-filter) : $filter search.in(meta_country, 'UK,NL', ',') and meta_year ge 2020 Vector Databases(../vectordbs.html) Apache Cassandra(apache-cassandra.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/apache-cassandra.html","Apache Cassandra: This section walks you through setting up CassandraVectorStore to store document embeddings and perform similarity searches. What is Apache Cassandra ?: Apache Cassandra®(https://cassandra.apache.org) is a true open source distributed database reknown for linear scalability, proven fault-tolerance and low latency, making it the perfect platform for mission-critical transactional data. Its Vector Similarity Search (VSS) is based on the JVector library that ensures best-in-class performance and relevancy. A vector search in Apache Cassandra is done as simply as: SELECT content FROM table ORDER BY content_vector ANN OF query_embedding ; More docs on this can be read here(https://cassandra.apache.org/doc/latest/cassandra/getting-started/vector-search-quickstart.html) . This Spring AI Vector Store is designed to work for both brand-new RAG applications and be able to be retrofitted on top of existing data and tables. The store can also be used for non-RAG use-cases in an existing database, e.g. semantic searches, geo-proximity searches, etc. The store will automatically create, or enhance, the schema as needed according to its configuration. If you don’t want the schema modifications, configure the store with disallowSchemaChanges . When using spring-boot-autoconfigure disallowSchemaChanges defaults to true, per Spring Boot standards, and you must opt-in to schema creation/modifications by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. What is JVector ?: JVector(https://github.com/jbellis/jvector) is a pure Java embedded vector search engine. It stands out from other HNSW Vector Similarity Search implementations by being Algorithmic-fast. JVector uses state of the art graph algorithms inspired by DiskANN and related research that offer high recall and low latency. Implementation-fast. JVector uses the Panama SIMD API to accelerate index build and queries. Memory efficient. JVector compresses vectors using product quantization so they can stay in memory during searches. (As part of our PQ implementation, our SIMD-accelerated kmeans class is 5x faster than the one in Apache Commons Math.) Disk-aware. JVector’s disk layout is designed to do the minimum necessary iops at query time. Concurrent. Index builds scale linearly to at least 32 threads. Double the threads, half the build time. Incremental. Query your index as you build it. No delay between adding a vector and being able to find it in search results. Easy to embed. API designed for easy embedding, by people using it in production. Prerequisites: A Embedding instance to compute the document embeddings. This is usually configured as a Spring Bean. Several options are available: Transformers Embedding - computes the embedding in your local environment. The default is via ONNX and the all-MiniLM-L6-v2 Sentence Transformers. This just works. If you want to use OpenAI’s Embeddings` - uses the OpenAI embedding endpoint. You need to create an account at OpenAI Signup(https://platform.openai.com/signup) and generate the api-key token at API Keys(https://platform.openai.com/account/api-keys) . There are many more choices, see Embeddings API docs. An Apache Cassandra instance, from version 5.0-beta1 DIY Quick Start(https://cassandra.apache.org/_/quickstart.html) For a managed offering Astra DB(https://astra.datastax.com/) offers a healthy free tier offering. Dependencies: Add these dependencies to your project: For just the Cassandra Vector Store <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-cassandra-store</artifactId> </dependency> Or, for everything you need in a RAG application (using the default ONNX Embedding Model) <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-cassandra-store-spring-boot-starter</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Configuration Properties: You can use the following properties in your Spring Boot configuration to customize the Apache Cassandra vector store. Property Default value spring.ai.vectorstore.cassandra.keyspace springframework spring.ai.vectorstore.cassandra.table ai_vector_store spring.ai.vectorstore.cassandra.initialize-schema false spring.ai.vectorstore.cassandra.index-name spring.ai.vectorstore.cassandra.content-column-name content spring.ai.vectorstore.cassandra.embedding-column-name embedding spring.ai.vectorstore.cassandra.return-embeddings false spring.ai.vectorstore.cassandra.fixed-thread-pool-executor-size 16 Usage: Create a CassandraVectorStore instance connected to your Apache Cassandra database: @Bean public VectorStore vectorStore(EmbeddingModel embeddingModel) { CassandraVectorStoreConfig config = CassandraVectorStoreConfig.builder().build(); return new CassandraVectorStore(config, embeddingModel); } The default configuration connects to Cassandra at localhost:9042 and will automatically create a default schema in keyspace springframework , table ai_vector_store . The Cassandra Java Driver is easiest configured via an application.conf file on the classpath. More info here(https://github.com/apache/cassandra-java-driver/tree/4.x/manual/core/configuration) . Then in your main code, create some documents: List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""country"", ""UK"", ""year"", 2020)), new Document(""The World is Big and Salvation Lurks Around the Corner"", Map.of()), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""country"", ""NL"", ""year"", 2023))); Now add the documents to your vector store: vectorStore.add(documents); And finally, retrieve documents similar to a query: List<Document> results = vectorStore.similaritySearch( SearchRequest.query(""Spring"").withTopK(5)); If all goes well, you should retrieve the document containing the text ""Spring AI rocks!!"". You can also limit results based on a similarity threshold: List<Document> results = vectorStore.similaritySearch( SearchRequest.query(""Spring"").withTopK(5) .withSimilarityThreshold(0.5d)); Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with the CassandraVectorStore as well. Metadata columns must be configured in CassandraVectorStoreConfig . For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest.query(""The World"").withTopK(TOP_K) .withFilterExpression(""country in ['UK', 'NL'] && year >= 2020"")); or programmatically using the expression DSL: Filter.Expression f = new FilterExpressionBuilder() .and(f.in(""country"", ""UK"", ""NL""), f.gte(""year"", 2020)).build(); vectorStore.similaritySearch( SearchRequest.query(""The World"").withTopK(TOP_K) .withFilterExpression(f)); The portable filter expressions get automatically converted into CQL queries(https://cassandra.apache.org/doc/latest/cassandra/developing/cql/index.html) . For metadata columns to be searchable they must be either primary keys or SAI indexed. To make non-primary-key columns indexed configure the metadata column with the SchemaColumnTags.INDEXED . Advanced Example: Vector Store ontop full Wikipedia dataset: The following example demonstrates how to use the store on an existing schema. Here we use the schema from the github.com/datastax-labs/colbert-wikipedia-data(https://github.com/datastax-labs/colbert-wikipedia-data) project which comes with the full wikipedia dataset ready vectorised for you. Usage: Create the schema in the Cassandra database first: wget https://s.apache.org/colbert-wikipedia-schema-cql -O colbert-wikipedia-schema.cql cqlsh -f colbert-wikipedia-schema.cql Then configure the store like: @Bean public CassandraVectorStore store(EmbeddingModel embeddingModel) { List<SchemaColumn> partitionColumns = List.of(new SchemaColumn(""wiki"", DataTypes.TEXT), new SchemaColumn(""language"", DataTypes.TEXT), new SchemaColumn(""title"", DataTypes.TEXT)); List<SchemaColumn> clusteringColumns = List.of(new SchemaColumn(""chunk_no"", DataTypes.INT), new SchemaColumn(""bert_embedding_no"", DataTypes.INT)); List<SchemaColumn> extraColumns = List.of(new SchemaColumn(""revision"", DataTypes.INT), new SchemaColumn(""id"", DataTypes.INT)); CassandraVectorStoreConfig conf = CassandraVectorStoreConfig.builder() .withKeyspaceName(""wikidata"") .withTableName(""articles"") .withPartitionKeys(partitionColumns) .withClusteringKeys(clusteringColumns) .withContentColumnName(""body"") .withEmbeddingColumndName(""all_minilm_l6_v2_embedding"") .withIndexName(""all_minilm_l6_v2_ann"") .disallowSchemaChanges() .addMetadataColumns(extraColumns) .withPrimaryKeyTranslator((List<Object> primaryKeys) -> { // the deliminator used to join fields together into the document's id is arbitary // here ""§¶"" is used if (primaryKeys.isEmpty()) { return ""test§¶0""; } return format(""%s§¶%s"", primaryKeys.get(2), primaryKeys.get(3)); }) .withDocumentIdTranslator((id) -> { String[] parts = id.split(""§¶""); String title = parts[0]; int chunk_no = 0 < parts.length ? Integer.parseInt(parts[1]) : 0; return List.of(""simplewiki"", ""en"", title, chunk_no, 0); }) .build(); return new CassandraVectorStore(conf, embeddingModel()); } @Bean public EmbeddingModel embeddingModel() { // default is ONNX all-MiniLM-L6-v2 which is what we want return new TransformersEmbeddingModel(); } Complete wikipedia dataset: And, if you would like to load the full wikipedia dataset. First download the simplewiki-sstable.tar from this link s.apache.org/simplewiki-sstable-tar(https://s.apache.org/simplewiki-sstable-tar) . This will take a while, the file is tens of GBs. tar -xf simplewiki-sstable.tar -C ${CASSANDRA_DATA}/data/wikidata/articles-*/ nodetool import wikidata articles ${CASSANDRA_DATA}/data/wikidata/articles-*/ If you have existing data in this table you’ll want to check the tarball’s files don’t clobber existing sstables when doing the tar . An alternative to the nodetool import is to just restart Cassandra. If there are any failures in the indexes they will be rebuilt automatically. Azure AI Service(azure.html) Chroma(chroma.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/chroma.html","Chroma: This section will walk you through setting up the Chroma VectorStore to store document embeddings and perform similarity searches. Chroma(https://docs.trychroma.com/) is the open-source embedding database. It gives you the tools to store document embeddings, content, and metadata and to search through those embeddings, including metadata filtering. Prerequisites: Access to ChromeDB. The setup local ChromaDB(#_run_chroma_locally) appendix shows how to set up a DB locally with a Docker container. EmbeddingModel instance to compute the document embeddings. Several options are available: If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the ChromaVectorStore . On startup, the ChromaVectorStore creates the required collection if one is not provisioned already. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Chroma Vector Store. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-chroma-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-chroma-store-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. Additionally, you will need a configured EmbeddingModel bean. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. Here is an example of the needed bean: @Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(""SPRING_AI_OPENAI_API_KEY""))); } To connect to Chroma you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.properties , # Chroma Vector Store connection properties spring.ai.vectorstore.chroma.client.host=<your Chroma instance host> spring.ai.vectorstore.chroma.client.port=<your Chroma instance port> spring.ai.vectorstore.chroma.client.key-token=<your access token (if configure)> spring.ai.vectorstore.chroma.client.username=<your username (if configure)> spring.ai.vectorstore.chroma.client.password=<your password (if configure)> # Chroma Vector Store collection properties spring.ai.vectorstore.chroma.initialize-schema=<true or false> spring.ai.vectorstore.chroma.collection-name=<your collection name> # Chroma Vector Store configuration properties # OpenAI API key if the OpenAI auto-configuration is used. spring.ai.openai.api.key=<OpenAI Api-key> Please have a look at the list of configuration parameters(#_configuration_properties) for the vector store to learn about the default values and configuration options. Now you can auto-wire the Chroma Vector Store in your application and use it @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the vector store. Property Description Default value spring.ai.vectorstore.chroma.client.host Server connection host localhost(http://localhost) spring.ai.vectorstore.chroma.client.port Server connection port 8000 spring.ai.vectorstore.chroma.client.key-token Access token (if configured) - spring.ai.vectorstore.chroma.client.username Access username (if configured) - spring.ai.vectorstore.chroma.client.password Access password (if configured) - spring.ai.vectorstore.chroma.collection-name Collection name SpringAiCollection spring.ai.vectorstore.chroma.initialize-schema Whether to initialize the required schema false For ChromaDB secured with Static API Token Authentication(https://docs.trychroma.com/usage-guide#static-api-token-authentication) use the ChromaApi#withKeyToken(<Your Token Credentials>) method to set your credentials. Check the ChromaWhereIT for an example. For ChromaDB secured with Basic Authentication(https://docs.trychroma.com/usage-guide#basic-authentication) use the ChromaApi#withBasicAuth(<your user>, <your password>) method to set your credentials. Check the BasicAuthChromaWhereIT for an example. Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with ChromaVector store as well. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && article_type == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); Those (portable) filter expressions get automatically converted into the proprietary Chroma where filter expressions(https://docs.trychroma.com/usage-guide#using-where-filters) . For example, this portable filter expression: author in ['john', 'jill'] && article_type == 'blog' is converted into the proprietary Chroma format {""$and"":[ {""author"": {""$in"": [""john"", ""jill""]}}, {""article_type"":{""$eq"":""blog""}}] } Manual Configuration: If you prefer to configure the Chroma Vector Store manually, you can do so by creating a ChromaVectorStore bean in your Spring Boot application. Add these dependencies to your project: * Chroma VectorStore. <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-chroma-store</artifactId> </dependency> OpenAI: Required for calculating embeddings. You can use any other embedding model implementation. <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Sample Code: Create a RestClient.Builder instance with proper ChromaDB authorization configurations and Use it to create a ChromaApi instance: @Bean public RestClient.Builder builder() { return RestClient.builder().requestFactory(new SimpleClientHttpRequestFactory()); } @Bean public ChromaApi chromaApi(RestClient.Builder restClientBuilder) { String chromaUrl = ""http://localhost:8000""; ChromaApi chromaApi = new ChromaApi(chromaUrl, restClientBuilder); return chromaApi; } Integrate with OpenAI’s embeddings by adding the Spring Boot OpenAI starter to your project. This provides you with an implementation of the Embeddings client: @Bean public VectorStore chromaVectorStore(EmbeddingModel embeddingModel, ChromaApi chromaApi) { return new ChromaVectorStore(embeddingModel, chromaApi, ""TestCollection"", false); } In your main code, create some documents: List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); Add the documents to your vector store: vectorStore.add(documents); And finally, retrieve documents similar to a query: List<Document> results = vectorStore.similaritySearch(""Spring""); If all goes well, you should retrieve the document containing the text ""Spring AI rocks!!"". Run Chroma Locally: docker run -it --rm --name chroma -p 8000:8000 ghcr.io/chroma-core/chroma:0.4.15 Starts a chroma store at localhost:8000/api/v1(http://localhost:8000/api/v1) Apache Cassandra(apache-cassandra.html) Elasticsearch(elasticsearch.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/elasticsearch.html","Elasticsearch: This section walks you through setting up the Elasticsearch VectorStore to store document embeddings and perform similarity searches. Elasticsearch(https://www.elastic.co/elasticsearch) is an open source search and analytics engine based on the Apache Lucene library. Prerequisites: A running Elasticsearch instance. The following options are available: Docker(https://hub.docker.com/_/elasticsearch/) Self-Managed Elasticsearch(https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html#elasticsearch-install-packages) Elastic Cloud(https://www.elastic.co/cloud/elasticsearch-service/signup?page=docs&placement=docs-body) Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Elasticsearch Vector Store. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-elasticsearch-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-elasticsearch-store-spring-boot-starter' } For spring-boot versions pre 3.3.0 it’s necessary to explicitly add the elasticsearch-java dependency with version > 8.13.3, otherwise the older version used will be incompatible with the queries performed: <dependency> <groupId>co.elastic.clients</groupId> <artifactId>elasticsearch-java</artifactId> <version>8.13.3</version> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. Alternatively you can opt-out the initialization and create the index manually using the Elasticsearch client, which can be useful if the index needs advanced mapping or additional configuration. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. Please have a look at the list of configuration parameters(#elasticsearchvector-properties) for the vector store to learn about the default values and configuration options. These properties can be also set by configuring the ElasticsearchVectorStoreOptions bean. Additionally, you will need a configured EmbeddingModel bean. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. Now you can auto-wire the ElasticsearchVectorStore as a vector store in your application. @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents to Qdrant vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration Properties: To connect to Elasticsearch and use the ElasticsearchVectorStore , you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.yml , spring: elasticsearch: uris: <elasticsearch instance URIs> username: <elasticsearch username> password: <elasticsearch password> # API key if needed, e.g. OpenAI ai: openai: api: key: <api-key> environment variables, export SPRING_ELASTICSEARCH_URIS=<elasticsearch instance URIs> export SPRING_ELASTICSEARCH_USERNAME=<elasticsearch username> export SPRING_ELASTICSEARCH_PASSWORD=<elasticsearch password> # API key if needed, e.g. OpenAI export SPRING_AI_OPENAI_API_KEY=<api-key> or can be a mix of those. For example, if you want to store your password as an environment variable but keep the rest in the plain application.yml file. If you choose to create a shell script for ease in future work, be sure to run it prior to starting your application by ""sourcing"" the file, i.e. source <your_script_name>.sh . Spring Boot’s auto-configuration feature for the Elasticsearch RestClient will create a bean instance that will be used by the ElasticsearchVectorStore . The Spring Boot properties starting with spring.elasticsearch.* are used to configure the Elasticsearch client: Property Description Default Value spring.elasticsearch.connection-timeout Connection timeout used when communicating with Elasticsearch. 1s spring.elasticsearch.password Password for authentication with Elasticsearch. - spring.elasticsearch.username Username for authentication with Elasticsearch. - spring.elasticsearch.uris Comma-separated list of the Elasticsearch instances to use. localhost:9200(http://localhost:9200) spring.elasticsearch.path-prefix Prefix added to the path of every request sent to Elasticsearch. - spring.elasticsearch.restclient.sniffer.delay-after-failure Delay of a sniff execution scheduled after a failure. 1m spring.elasticsearch.restclient.sniffer.interval Interval between consecutive ordinary sniff executions. 5m spring.elasticsearch.restclient.ssl.bundle SSL bundle name. - spring.elasticsearch.socket-keep-alive Whether to enable socket keep alive between client and Elasticsearch. false spring.elasticsearch.socket-timeout Socket timeout used when communicating with Elasticsearch. 30s Properties starting with the spring.ai.vectorstore.elasticsearch.* prefix are used to configure ElasticsearchVectorStore . Property Description Default Value spring.ai.vectorstore.elasticsearch.initialize-schema Whether to initialize the required schema false spring.ai.vectorstore.elasticsearch.index-name The name of the index to store the vectors. spring-ai-document-index spring.ai.vectorstore.elasticsearch.dimensions The number of dimensions in the vector. 1536 spring.ai.vectorstore.elasticsearch.similarity The similarity function to use. cosine spring.ai.vectorstore.elasticsearch.initialize-schema whether to initialize the required schema false The following similarity functions are available: cosine l2_norm dot_product More details about each in the Elasticsearch Documentation(https://www.elastic.co/guide/en/elasticsearch/reference/master/dense-vector.html#dense-vector-params) on dense vectors. Metadata Filtering: You can leverage the generic, portable metadata filters(../vectordbs.html#metadata-filters) with Elasticsearch as well. For example, you can use either the text expression language: vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && 'article_type' == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); Those (portable) filter expressions get automatically converted into the proprietary Elasticsearch Query string query(https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html) . For example, this portable filter expression: author in ['john', 'jill'] && 'article_type' == 'blog' is converted into the proprietary Elasticsearch filter format: (metadata.author:john OR jill) AND metadata.article_type:blog Manual Configuration: Instead of using the Spring Boot auto-configuration, you can manually configure the Elasticsearch vector store. For this you need to add the spring-ai-elasticsearch-store to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-elasticsearch-store</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-elasticsearch-store' } Create an Elasticsearch RestClient bean. Read the Elasticsearch Documentation(https://www.elastic.co/guide/en/elasticsearch/client/java-api-client/current/java-rest-low-usage-initialization.html) for more in-depth information about the configuration of a custom RestClient. @Bean public RestClient restClient() { RestClient.builder(new HttpHost(""<host>"", 9200, ""http"")) .setDefaultHeaders(new Header[]{ new BasicHeader(""Authorization"", ""Basic <encoded username and password>"") }) .build(); } and then create the ElasticsearchVectorStore bean: @Bean public ElasticsearchVectorStore vectorStore(EmbeddingModel embeddingModel, RestClient restClient) { return new ElasticsearchVectorStore( restClient, embeddingModel); } // This can be any EmbeddingModel implementation. @Bean public EmbeddingModel embeddingModel() { return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(""OPENAI_API_KEY""))); } Chroma(chroma.html) GemFire(gemfire.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/gemfire.html","GemFire Vector Store: This section walks you through setting up the GemFireVectorStore to store document embeddings and perform similarity searches. GemFire(https://tanzu.vmware.com/gemfire) is a distributed, in-memory, key-value store performing read and write operations at blazingly fast speeds. It offers highly available parallel message queues, continuous availability, and an event-driven architecture you can scale dynamically without downtime. As your data size requirements increase to support high-performance, real-time apps, GemFire can easily scale linearly. GemFire VectorDB(https://docs.vmware.com/en/VMware-GemFire-VectorDB/1.0/gemfire-vectordb/overview.html) extends GemFire’s capabilities, serving as a versatile vector database that efficiently stores, retrieves, and performs vector similarity searches. Prerequisites: A GemFire cluster with the GemFire VectorDB extension enabled Install GemFire VectorDB extension(https://docs.vmware.com/en/VMware-GemFire-VectorDB/1.0/gemfire-vectordb/install.html) An EmbeddingModel bean to compute the document embeddings. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. An option that runs locally on your machine is ONNX(../embeddings/onnx.html) and the all-MiniLM-L6-v2 Sentence Transformers. Auto-configuration: Add the GemFire VectorStore Spring Boot starter to you project’s Maven build file pom.xml : <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-gemfire-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle file dependencies { implementation 'org.springframework.ai:spring-ai-gemfire-store-spring-boot-starter' } Configuration properties: You can use the following properties in your Spring Boot configuration to further configure the GemFireVectorStore . Property Default value spring.ai.vectorstore.gemfire.host localhost spring.ai.vectorstore.gemfire.port 8080 spring.ai.vectorstore.gemfire.initialize-schema false spring.ai.vectorstore.gemfire.index-name spring-ai-gemfire-store spring.ai.vectorstore.gemfire.beam-width 100 spring.ai.vectorstore.gemfire.max-connections 16 spring.ai.vectorstore.gemfire.vector-similarity-function COSINE spring.ai.vectorstore.gemfire.fields [] spring.ai.vectorstore.gemfire.buckets 0 Manual Configuration: To use just the GemFireVectorStore , without Spring Boot’s Auto-configuration add the following dependency to your project’s Maven pom.xml : <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-gemfire-store</artifactId> </dependency> For Gradle users, add the following to your build.gradle file under the dependencies block to use just the GemFireVectorStore : dependencies { implementation 'org.springframework.ai:spring-ai-gemfire-store' } Usage: Here is a sample that creates an instance of the GemfireVectorStore instead of using AutoConfiguration @Bean public VectorStore vectorStore(EmbeddingModel embeddingModel) { return new GemFireVectorStore(new GemFireVectorStoreConfig() .setIndexName(""my-vector-index"") .setPort(7071), embeddingClient); } The GemFire VectorStore does not yet support metadata filters(../vectordbs.html#metadata-filters) . The default configuration connects to a GemFire cluster at localhost:8080 In your application, create a few documents: List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""country"", ""UK"", ""year"", 2020)), new Document(""The World is Big and Salvation Lurks Around the Corner"", Map.of()), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""country"", ""NL"", ""year"", 2023))); Add the documents to the vector store: vectorStore.add(documents); And to retrieve documents using similarity search: List<Document> results = vectorStore.similaritySearch( SearchRequest.query(""Spring"").withTopK(5)); You should retrieve the document containing the text ""Spring AI rocks!!"". You can also limit the number of results using a similarity threshold: List<Document> results = vectorStore.similaritySearch( SearchRequest.query(""Spring"").withTopK(5) .withSimilarityThreshold(0.5d)); Elasticsearch(elasticsearch.html) Milvus(milvus.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/milvus.html","Milvus: Milvus(https://milvus.io/) is an open-source vector database that has garnered significant attention in the fields of data science and machine learning. One of its standout features lies in its robust support for vector indexing and querying. Milvus employs state-of-the-art, cutting-edge algorithms to accelerate the search process, making it exceptionally efficient at retrieving similar vectors, even when handling extensive datasets. Prerequisites: A running Milvus instance. The following options are available: Milvus Standalone(https://milvus.io/docs/install_standalone-docker.md) : Docker, Operator, Helm,DEB/RPM, Docker Compose. Milvus Cluster(https://milvus.io/docs/install_cluster-milvusoperator.md) : Operator, Helm. If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the MilvusVectorStore . Dependencies: Then add the Milvus VectorStore boot starter dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-milvus-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-milvus-store-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. The Vector Store, also requires an EmbeddingModel instance to calculate embeddings for the documents. You can pick one of the available EmbeddingModel Implementations(../embeddings.html#available-implementations) . To connect to and configure the MilvusVectorStore , you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.yml spring: ai: vectorstore: milvus: client: host: ""localhost"" port: 19530 username: ""root"" password: ""milvus"" databaseName: ""default"" collectionName: ""vector_store"" embeddingDimension: 1536 indexType: IVF_FLAT metricType: COSINE Check the list of configuration parameters(#milvus-properties) to learn about the default values and configuration options. Now you can Auto-wire the Milvus Vector Store in your application and use it @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents to PGVector vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Manual Configuration: Instead of using the Spring Boot auto-configuration, you can manually configure the MilvusVectorStore . To add the following dependencies to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-milvus-store</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. To configure MilvusVectorStore in your application, you can use the following setup: @Bean public VectorStore vectorStore(MilvusServiceClient milvusClient, EmbeddingModel embeddingModel) { MilvusVectorStoreConfig config = MilvusVectorStoreConfig.builder() .withCollectionName(""test_vector_store"") .withDatabaseName(""default"") .withIndexType(IndexType.IVF_FLAT) .withMetricType(MetricType.COSINE) .build(); return new MilvusVectorStore(milvusClient, embeddingModel, config); } @Bean public MilvusServiceClient milvusClient() { return new MilvusServiceClient(ConnectParam.newBuilder() .withAuthorization(""minioadmin"", ""minioadmin"") .withUri(milvusContainer.getEndpoint()) .build()); } Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with the Milvus store. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && article_type == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""author"",""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); These filter expressions are converted into the equivalent Milvus filters. Milvus VectorStore properties: You can use the following properties in your Spring Boot configuration to customize the Milvus vector store. Property Description Default value spring.ai.vectorstore.milvus.database-name The name of the Milvus database to use. default spring.ai.vectorstore.milvus.collection-name Milvus collection name to store the vectors vector_store spring.ai.vectorstore.milvus.initialize-schema whether to initialize Milvus' backend false spring.ai.vectorstore.milvus.embedding-dimension The dimension of the vectors to be stored in the Milvus collection. 1536 spring.ai.vectorstore.milvus.index-type The type of the index to be created for the Milvus collection. IVF_FLAT spring.ai.vectorstore.milvus.metric-type The metric type to be used for the Milvus collection. COSINE spring.ai.vectorstore.milvus.index-parameters The index parameters to be used for the Milvus collection. {""nlist"":1024} spring.ai.vectorstore.milvus.client.host The name or address of the host. localhost spring.ai.vectorstore.milvus.client.port The connection port. 19530 spring.ai.vectorstore.milvus.client.uri The uri of Milvus instance - spring.ai.vectorstore.milvus.client.token Token serving as the key for identification and authentication purposes. - spring.ai.vectorstore.milvus.client.connect-timeout-ms Connection timeout value of client channel. The timeout value must be greater than zero . 10000 spring.ai.vectorstore.milvus.client.keep-alive-time-ms Keep-alive time value of client channel. The keep-alive value must be greater than zero. 55000 spring.ai.vectorstore.milvus.client.keep-alive-timeout-ms The keep-alive timeout value of client channel. The timeout value must be greater than zero. 20000 spring.ai.vectorstore.milvus.client.rpc-deadline-ms Deadline for how long you are willing to wait for a reply from the server. With a deadline setting, the client will wait when encounter fast RPC fail caused by network fluctuations. The deadline value must be larger than or equal to zero. 0 spring.ai.vectorstore.milvus.client.client-key-path The client.key path for tls two-way authentication, only takes effect when ""secure"" is true - spring.ai.vectorstore.milvus.client.client-pem-path The client.pem path for tls two-way authentication, only takes effect when ""secure"" is true - spring.ai.vectorstore.milvus.client.ca-pem-path The ca.pem path for tls two-way authentication, only takes effect when ""secure"" is true - spring.ai.vectorstore.milvus.client.server-pem-path server.pem path for tls one-way authentication, only takes effect when ""secure"" is true. - spring.ai.vectorstore.milvus.client.server-name Sets the target name override for SSL host name checking, only takes effect when ""secure"" is True. Note: this value is passed to grpc.ssl_target_name_override - spring.ai.vectorstore.milvus.client.secure Secure the authorization for this connection, set to True to enable TLS. false spring.ai.vectorstore.milvus.client.idle-timeout-ms Idle timeout value of client channel. The timeout value must be larger than zero. 24h spring.ai.vectorstore.milvus.client.username The username and password for this connection. root spring.ai.vectorstore.milvus.client.password The password for this connection. milvus Starting Milvus Store: From within the src/test/resources/ folder run: docker-compose up To clean the environment: docker-compose down; rm -Rf ./volumes Then connect to the vector store on http://localhost:19530(http://localhost:19530) or for management http://localhost:9001(http://localhost:9001) (user: minioadmin , pass: minioadmin ) Troubleshooting: If Docker complains about resources, then execute: docker system prune --all --force --volumes GemFire(gemfire.html) MongoDB Atlas(mongodb.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/mongodb.html","MongoDB Atlas: This section walks you through setting up MongoDB Atlas as a vector store to use with Spring AI. What is MongoDB Atlas?: MongoDB Atlas(https://www.mongodb.com/products/platform/atlas-database) is the fully-managed cloud database from MongoDB available in AWS, Azure, and GCP. Atlas supports native Vector Search and full text search on your MongoDB document data. MongoDB Atlas Vector Search(https://www.mongodb.com/products/platform/atlas-vector-search) allows you to store your embeddings in MongoDB documents, create vector search indexes, and perform KNN searches with an approximate nearest neighbor algorithm (Hierarchical Navigable Small Worlds). You can use the $vectorSearch aggregation operator in a MongoDB aggregation stage to perform a search on your vector embeddings. Prerequisites: An Atlas cluster running MongoDB version 6.0.11, 7.0.2, or later. To get started with MongoDB Atlas, you can follow the instructions here(https://www.mongodb.com/docs/atlas/getting-started/) . Ensure that your IP address is included in your Atlas project’s https://www.mongodb.com/docs/atlas/security/ip-access-list/#std-label-access-list[access list]. An EmbeddingModel instance to compute the document embeddings. Several options are available. Refer to the EmbeddingModel(https://docs.spring.io/spring-ai/reference/api/embeddings.html#available-implementations) section for more information. An environment to set up and run a Java application. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the MongoDB Atlas Vector Store. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-mongodb-atlas-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-mongodb-atlas-store-spring-boot-starter' } The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. Schema Initialization: The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting spring.ai.vectorstore.mongodb.initialize-schema=true in the application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. When initializeSchema is set to true , the following actions are performed automatically: Collection Creation : The specified collection for storing vectors will be created if it does not already exist. Search Index Creation : A search index will be created based on the configuration properties. If you’re running a free or shared tier cluster, you must separately create the index through the Atlas UI, Atlas Administration API, or Atlas CLI. If you have an existing Atlas Vector Search index called vector_index on the springai_test.vector_store collection , Spring AI won’t create an additional index. Because of this, you might experience errors later if the existing index was configured with incompatible settings, such as a different number of dimensions. Ensure that your index has the following configuration: { ""fields"": [ { ""numDimensions"": 1536, ""path"": ""embedding"", ""similarity"": ""cosine"", ""type"": ""vector"" } ] } Additionally, you will need a configured EmbeddingModel bean. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. Here is an example of the needed bean: @Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(""SPRING_AI_OPENAI_API_KEY""))); } Configuration properties: You can use the following properties in your Spring Boot configuration to customize the MongoDB Atlas vector store. ... spring.data.mongodb.uri=<connection string> spring.data.mongodb.database=<database name> spring.ai.vectorstore.mongodb.collection-name=vector_store spring.ai.vectorstore.mongodb.initialize-schema=true spring.ai.vectorstore.mongodb.path-name=embedding spring.ai.vectorstore.mongodb.indexName=vector_index spring.ai.vectorstore.mongodb.metadata-fields-to-filter=foo Property Description Default value spring.ai.vectorstore.mongodb.collection-name The name of the collection to store the vectors. vector_store spring.ai.vectorstore.mongodb.initialize-schema whether to initialize the backend schema for you false spring.ai.vectorstore.mongodb.path-name The name of the path to store the vectors. embedding spring.ai.vectorstore.mongodb.indexName The name of the index to store the vectors. vector_index spring.ai.vectorstore.mongodb.metadata-fields-to-filter comma separated values that specifies which metadata fields can be used for filtering when querying the vector store. Needed so that metadata indexes are created if they already don’t exist empty list Manual Configuration Properties: If you prefer to manually configure the MongoDB Atlas vector store without auto-configuration, you can do so by directly setting up the MongoDBAtlasVectorStore and its dependencies. Example Configuration: @Configuration public class VectorStoreConfig { @Bean public MongoDBAtlasVectorStore vectorStore(MongoTemplate mongoTemplate, EmbeddingModel embeddingModel) { MongoDBVectorStoreConfig config = MongoDBVectorStoreConfig.builder() .withCollectionName(""custom_vector_store"") .withVectorIndexName(""custom_vector_index"") .withPathName(""custom_embedding_path"") .withMetadataFieldsToFilter(List.of(""author"", ""year"")) .build(); return new MongoDBAtlasVectorStore(mongoTemplate, embeddingModel, config, true); } } Properties: collectionName : The name of the collection to store the vectors. vectorIndexName : The name of the vector index. pathName : The path where vectors are stored. metadataFieldsToFilter : A list of metadata fields to filter. You can enable schema initialization by passing true as the last parameter in the MongoDBAtlasVectorStore constructor Adding Documents: To add documents to the vector store, you need to convert your input documents into the Document type and call the addDocuments() method. This method will use the EmbeddingModel to compute the embeddings and save them to the MongoDB collection. List<Document> docs = List.of( new Document(""Proper tuber planting involves site selection, timing, and care. Choose well-drained soil and adequate sun exposure. Plant in spring, with eyes facing upward at a depth two to three times the tuber's height. Ensure 4-12 inch spacing based on tuber size. Adequate moisture is needed, but avoid overwatering. Mulching helps preserve moisture and prevent weeds."", Map.of(""author"", ""A"", ""type"", ""post"")), new Document(""Successful oil painting requires patience, proper equipment, and technique. Prepare a primed canvas, sketch lightly, and use high-quality brushes and oils. Paint 'fat over lean' to prevent cracking. Allow each layer to dry before applying the next. Clean brushes often and work in a well-ventilated space."", Map.of(""author"", ""A"")), new Document(""For a natural lawn, select the right grass type for your climate. Water 1 to 1.5 inches per week, avoid overwatering, and use organic fertilizers. Regular aeration helps root growth and prevents compaction. Practice natural pest control and overseeding to maintain a dense lawn."", Map.of(""author"", ""B"", ""type"", ""post"")) ); vectorStore.add(docs); Deleting Documents: To delete documents from the vector store, use the delete() method. This method takes a list of document IDs and removes the corresponding documents from the MongoDB collection. List<String> ids = List.of(""id1"", ""id2"", ""id3""); // Replace with actual document IDs vectorStore.delete(ids); Performing Similarity Search: To perform a similarity search, construct a SearchRequest object with the desired query parameters and call the similaritySearch() method. This method will return a list of documents that match the query based on vector similarity. List<Document> results = vectorStore.similaritySearch( SearchRequest .query(""learn how to grow things"") .withTopK(2) ); Metadata Filtering: Metadata filtering allows for more refined queries by filtering results based on specified metadata fields. This feature uses the MongoDB Query API to perform filtering operations in conjunction with vector searches. Filter Expressions: The MongoDBAtlasFilterExpressionConverter class converts filter expressions into MongoDB Atlas metadata filter expressions. The supported operations include: $and $or $eq $ne $lt $lte $gt $gte $in $nin These operations enable filtering logic to be applied to metadata fields associated with documents in the vector store. Example of a Filter Expression: Here’s an example of how to use a filter expression in a similarity search: FilterExpressionBuilder b = new FilterExpressionBuilder(); List<Document> results = vectorStore.similaritySearch( SearchRequest.defaults() .withQuery(""learn how to grow things"") .withTopK(2) .withSimilarityThreshold(0.5) .withFilterExpression(b.eq(""author"", ""A"").build()) ); Tutorials and Code Examples: To get started with Spring AI and MongoDB: See the Getting Started guide for Spring AI Integration(https://www.mongodb.com/docs/atlas/atlas-vector-search/ai-integrations/spring-ai/#std-label-spring-ai) . For a comprehensive code example demonstrating Retrieval Augmented Generation (RAG) with Spring AI and MongoDB, refer to this detailed tutorial(https://www.mongodb.com/developer/languages/java/retrieval-augmented-generation-spring-ai/) . Milvus(milvus.html) Neo4j(neo4j.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/neo4j.html","Neo4j: This section walks you through setting up Neo4jVectorStore to store document embeddings and perform similarity searches. Neo4j(https://neo4j.com) is an open-source NoSQL graph database. It is a fully transactional database (ACID) that stores data structured as graphs consisting of nodes, connected by relationships. Inspired by the structure of the real world, it allows for high query performance on complex data while remaining intuitive and simple for the developer. The Neo4j’s Vector Search(https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/) allows users to query vector embeddings from large datasets. An embedding is a numerical representation of a data object, such as text, image, audio, or document. Embeddings can be stored on Node properties and can be queried with the db.index.vector.queryNodes() function. Those indexes are powered by Lucene using a Hierarchical Navigable Small World Graph (HNSW) to perform a k approximate nearest neighbors (k-ANN) query over the vector fields. Prerequisites: A running Neo4j (5.15+) instance. The following options are available: Docker(https://hub.docker.com/_/neo4j) image Neo4j Desktop(https://neo4j.com/download/) Neo4j Aura(https://neo4j.com/cloud/aura-free/) Neo4j Server(https://neo4j.com/deployment-center/) instance If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the Neo4jVectorStore . Dependencies: Add the Neo4j Vector Store dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-neo4j-store</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-neo4j-store' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. Configuration: To connect to Neo4j and use the Neo4jVectorStore , you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.properties , spring.neo4j.uri=<uri_for_your_neo4j_instance> spring.neo4j.authentication.username=<your_username> spring.neo4j.authentication.password=<your_password> # API key if needed, e.g. OpenAI spring.ai.openai.api.key=<api-key> environment variables, export SPRING_NEO4J_URI=<uri_for_your_neo4j_instance> export SPRING_NEO4J_AUTHENTICATION_USERNAME=<your_username> export SPRING_NEO4J_AUTHENTICATION_PASSWORD=<your_password> # API key if needed, e.g. OpenAI export SPRING_AI_OPENAI_API_KEY=<api-key> or can be a mix of those. For example, if you want to store your API key as an environment variable but keep the rest in the plain application.properties file. If you choose to create a shell script for ease in future work, be sure to run it prior to starting your application by ""sourcing"" the file, i.e. source <your_script_name>.sh . Besides application.properties and environment variables, Spring Boot offers additional configuration options(https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.external-config) . Spring Boot’s auto-configuration feature for the Neo4j Driver will create a bean instance that will be used by the Neo4jVectorStore . Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Neo4j Vector Store. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-neo4j-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-neo4j-store-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Please have a look at the list of configuration parameters(#_neo4jvectorstore_properties) for the vector store to learn about the default values and configuration options. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. Additionally, you will need a configured EmbeddingModel bean. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. Here is an example of the needed bean: @Bean public EmbeddingModel embeddingModel() { // Can be any other Embeddingmodel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(""SPRING_AI_OPENAI_API_KEY""))); } In cases where the Spring Boot auto-configured Neo4j Driver bean is not what you want or need, you can still define your own bean. Please read the Neo4j Java Driver reference(https://neo4j.com/docs/java-manual/current/client-applications/) for more in-depth information about the configuration of a custom driver. @Bean public Driver driver() { return GraphDatabase.driver(""neo4j://<host>:<bolt-port>"", AuthTokens.basic(""<username>"", ""<password>"")); } Now you can auto-wire the Neo4jVectorStore as a vector store in your application. Metadata filtering: You can leverage the generic, portable metadata filters(../vectordbs.html#metadata-filters) with Neo4j store as well. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && 'article_type' == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""author"", ""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); Those (portable) filter expressions get automatically converted into the proprietary Neo4j WHERE filter expressions(https://neo4j.com/developer/cypher/filtering-query-results/) . For example, this portable filter expression: author in ['john', 'jill'] && 'article_type' == 'blog' is converted into the proprietary Neo4j filter format: node.`metadata.author` IN [""john"",""jill""] AND node.`metadata.'article_type'` = ""blog"" Neo4jVectorStore properties: You can use the following properties in your Spring Boot configuration to customize the Neo4j vector store. Property Default value spring.ai.vectorstore.neo4j.database-name neo4j spring.ai.vectorstore.neo4j.initialize-schema false spring.ai.vectorstore.neo4j.embedding-dimension 1536 spring.ai.vectorstore.neo4j.distance-type cosine spring.ai.vectorstore.neo4j.label Document spring.ai.vectorstore.neo4j.embedding-property embedding spring.ai.vectorstore.neo4j.index-name spring-ai-document-index MongoDB Atlas(mongodb.html) OpenSearch(opensearch.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/opensearch.html","OpenSearch: This section guides you through setting up the OpenSearch VectorStore to store document embeddings and perform similarity searches. OpenSearch(https://opensearch.org) is an open-source search and analytics engine originally forked from Elasticsearch, distributed under the Apache License 2.0. It enhances AI application development by simplifying the integration and management of AI-generated assets. OpenSearch supports vector, lexical, and hybrid search capabilities, leveraging advanced vector database functionalities to facilitate low-latency queries and similarity searches as detailed on the vector database page(https://opensearch.org/platform/search/vector-database.html) . This platform is ideal for building scalable AI-driven applications and offers robust tools for data management, fault tolerance, and resource access controls. Prerequisites: A running OpenSearch instance. The following options are available: Self-Managed OpenSearch(https://opensearch.org/docs/latest/opensearch/install/index/) Amazon OpenSearch Service(https://docs.aws.amazon.com/opensearch-service/) EmbeddingModel instance to compute the document embeddings. Several options are available: If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the OpenSearchVectorStore . Dependencies: Add the OpenSearch Vector Store dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-opensearch-store</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-opensearch-store' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Configuration: To connect to OpenSearch and use the OpenSearchVectorStore , you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.yml , spring: opensearch: uris: <opensearch instance URIs> username: <opensearch username> password: <opensearch password> indexName: <opensearch index name> mappingJson: <JSON mapping for opensearch index> # API key if needed, e.g. OpenAI ai: openai: api: key: <api-key> Check the list of configuration parameters(#_configuration_properties) to learn about the default values and configuration options. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the OpenSearch Vector Store. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-opensearch-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-opensearch-store-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Here is an example of the needed bean: @Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(""SPRING_AI_OPENAI_API_KEY""))); } Now you can auto-wire the OpenSearchVectorStore as a vector store in your application. @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents to OpenSearch vectorStore.add(List.of(document)); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the OpenSearch vector store. Property Description Default value spring.ai.vectorstore.opensearch.uris URIs of the OpenSearch cluster endpoints. - spring.ai.vectorstore.opensearch.username Username for accessing the OpenSearch cluster. - spring.ai.vectorstore.opensearch.password Password for the specified username. - spring.ai.vectorstore.opensearch.indexName Name of the default index to be used within the OpenSearch cluster. spring-ai-document-index spring.ai.vectorstore.opensearch.mappingJson JSON string defining the mapping for the index; specifies how documents and their fields are stored and indexed. { ""properties"":{ ""embedding"":{ ""type"":""knn_vector"", ""dimension"":1536 } } } spring.opensearch.aws.host Hostname of the OpenSearch instance. - spring.opensearch.aws.service-name AWS service name for the OpenSearch instance. - spring.opensearch.aws.access-key AWS access key for the OpenSearch instance. - spring.opensearch.aws.secret-key AWS secret key for the OpenSearch instance. - spring.opensearch.aws.region AWS region for the OpenSearch instance. - Customizing OpenSearch Client Configuration: In cases where the Spring Boot auto-configured OpenSearchClient with Apache HttpClient 5 Transport bean is not what you want or need, you can still define your own bean. Please read the OpenSearch Java Client Documentation(https://opensearch.org/docs/latest/clients/java/) for more in-depth information about the configuration of Amazon OpenSearch Service. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-aws-opensearch-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-aws-opensearch-store-spring-boot-starter' } Metadata Filtering: You can leverage the generic, portable metadata filters(../vectordbs.html#metadata-filters) with OpenSearch as well. For example, you can use either the text expression language: vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && 'article_type' == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); Those (portable) filter expressions get automatically converted into the proprietary OpenSearch Query string query(https://opensearch.org/docs/latest/query-dsl/full-text/query-string/) . For example, this portable filter expression: author in ['john', 'jill'] && 'article_type' == 'blog' is converted into the proprietary OpenSearch filter format: (metadata.author:john OR jill) AND metadata.article_type:blog Neo4j(neo4j.html) Oracle(oracle.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/oracle.html","Oracle Database 23ai - AI Vector Search: The AI Vector Search(https://docs.oracle.com/en/database/oracle/oracle-database/23/vecse/overview-ai-vector-search.html) capabilities of the Oracle Database 23ai (23.4+) are available as a Spring AI VectorStore to help you to store document embeddings and perform similarity searches. Of course, all other features are also available. The Run Oracle Database 23ai locally(#_run_oracle_database_23ai_locally) appendix shows how to start a database with a lightweight Docker container. Auto-Configuration: Start by adding the Oracle Vector Store boot starter dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-oracle-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-oracle-store-spring-boot-starter' } If you need this vector store to initialize the schema for you then you’ll need to pass true for the initializeSchema boolean parameter in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. The Vector Store, also requires an EmbeddingModel instance to calculate embeddings for the documents. You can pick one of the available EmbeddingModel Implementations(../embeddings.html#available-implementations) . For example to use the OpenAI EmbeddingModel(../embeddings/openai-embeddings.html) add the following dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. To connect to and configure the OracleVectorStore , you need to provide access details for your database. A simple configuration can either be provided via Spring Boot’s application.yml spring: datasource: url: jdbc:oracle:thin:@//localhost:1521/freepdb1 username: mlops password: mlops ai: vectorstore: oracle: index-type: IVF distance-type: COSINE dimensions: 1536 Check the list of configuration parameters(#oracle-properties) to learn about the default values and configuration options. Now you can Auto-wire the OracleVectorStore in your application and use it: @Autowired VectorStore vectorStore; // ... List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents to Oracle Vector Store vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the OracleVectorStore . Property Description Default value spring.ai.vectorstore.oracle.index-type Nearest neighbor search index type. Options are NONE - exact nearest neighbor search, IVF - Inverted Flat File index. It has faster build times and uses less memory than HNSW, but has lower query performance (in terms of speed-recall tradeoff). HNSW - creates a multilayer graph. It has slower build times and uses more memory than IVF, but has better query performance (in terms of speed-recall tradeoff). NONE spring.ai.vectorstore.oracle.distance-type Search distance type among COSINE (default), DOT , EUCLIDEAN , EUCLIDEAN_SQUARED , and MANHATTAN . NOTE: If vectors are normalized, you can use DOT or COSINE for best performance. COSINE spring.ai.vectorstore.oracle.forced-normalization Allows enabling vector normalization (if true) before insertion and for similarity search. CAUTION: Setting this to true is a requirement to allow for search request similarity threshold(../vectordbs.html#api-overview) . NOTE: If vectors are normalized, you can use DOT or COSINE for best performance. false spring.ai.vectorstore.oracle.dimensions Embeddings dimension. If not specified explicitly the OracleVectorStore will allow the maximum: 65535. Dimensions are set to the embedding column on table creation. If you change the dimensions your would have to re-create the table as well. 65535 spring.ai.vectorstore.oracle.remove-existing-vector-store-table Drops the existing table on start up. false spring.ai.vectorstore.oracle.initialize-schema Whether to initialize the required schema. false spring.ai.vectorstore.oracle.search-accuracy Denote the requested accuracy target in the presence of index. Disabled by default. You need to provide an integer in the range [1,100] to override the default index accuracy (95). Using lower accuracy provides approximate similarity search trading off speed versus accuracy. -1 ( DEFAULT_SEARCH_ACCURACY ) Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with the OracleVectorStore . For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && article_type == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""author"",""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); These filter expressions are converted into the equivalent OracleVectorStore filters. Manual Configuration: Instead of using the Spring Boot auto-configuration, you can manually configure the OracleVectorStore . For this you need to add the Oracle JDBC driver and JdbcTemplate auto-configuration dependencies to your project: <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-jdbc</artifactId> </dependency> <dependency> <groupId>com.oracle.database.jdbc</groupId> <artifactId>ojdbc11</artifactId> <scope>runtime</scope> </dependency> <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-oracle-store</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. To configure the OracleVectorStore in your application, you can use the following setup: @Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return new OracleVectorStore(jdbcTemplate, embeddingModel, true); } Run Oracle Database 23ai locally: docker run --rm --name oracle23ai -p 1521:1521 -e APP_USER=mlops -e APP_USER_PASSWORD=mlops -e ORACLE_PASSWORD=mlops gvenzl/oracle-free:23-slim You can then connect to the database using: sql mlops/mlops@localhost/freepdb1 OpenSearch(opensearch.html) PGvector(pgvector.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/pgvector.html","PGvector: This section walks you through setting up the PGvector VectorStore to store document embeddings and perform similarity searches. PGvector(https://github.com/pgvector/pgvector) is an open-source extension for PostgreSQL that enables storing and searching over machine learning-generated embeddings. It provides different capabilities that let users identify both exact and approximate nearest neighbors. It is designed to work seamlessly with other PostgreSQL features, including indexing and querying. Prerequisites: First you need access to PostgreSQL instance with enabled vector , hstore and uuid-ossp extensions. You can run a PGvector database as a Spring Boot dev service via Docker Compose(../docker-compose.html) or Testcontainers(../testcontainers.html) . In alternative, the setup local Postgres/PGVector(#_run_postgres_pgvector_db_locally) appendix shows how to set up a DB locally with a Docker container. On startup, the PgVectorStore will attempt to install the required database extensions and create the required vector_store table with an index if not existing. Optionally, you can do this manually like so: CREATE EXTENSION IF NOT EXISTS vector; CREATE EXTENSION IF NOT EXISTS hstore; CREATE EXTENSION IF NOT EXISTS ""uuid-ossp""; CREATE TABLE IF NOT EXISTS vector_store ( id uuid DEFAULT uuid_generate_v4() PRIMARY KEY, content text, metadata json, embedding vector(1536) // 1536 is the default embedding dimension ); CREATE INDEX ON vector_store USING HNSW (embedding vector_cosine_ops); replace the 1536 with the actual embedding dimension if you are using a different dimension. PGvector supports at most 2000 dimensions for HNSW indexes. Next, if required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the PgVectorStore . Auto-Configuration: Then add the PgVectorStore boot starter dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-pgvector-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-pgvector-store-spring-boot-starter' } The vector store implementation can initialize the required schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. This is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. The Vector Store also requires an EmbeddingModel instance to calculate embeddings for the documents. You can pick one of the available EmbeddingModel Implementations(../embeddings.html#available-implementations) . For example, to use the OpenAI EmbeddingModel(../embeddings/openai-embeddings.html) , add the following dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. To connect to and configure the PgVectorStore , you need to provide access details for your instance. A simple configuration can be provided via Spring Boot’s application.yml . spring: datasource: url: jdbc:postgresql://localhost:5432/postgres username: postgres password: postgres ai: vectorstore: pgvector: index-type: HNSW distance-type: COSINE_DISTANCE dimensions: 1536 If you run PGvector as a Spring Boot dev service via Docker Compose(https://docs.spring.io/spring-boot/reference/features/dev-services.html#features.dev-services.docker-compose) or Testcontainers(https://docs.spring.io/spring-boot/reference/features/dev-services.html#features.dev-services.testcontainers) , you don’t need to configure URL, username and password since they are autoconfigured by Spring Boot. Check the list of configuration parameters(#pgvector-properties) to learn about the default values and configuration options. Now you can auto-wire the PgVectorStore in your application and use it @Autowired VectorStore vectorStore; // ... List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents to PGVector vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the PGVector vector store. Property Description Default value spring.ai.vectorstore.pgvector.index-type Nearest neighbor search index type. Options are NONE - exact nearest neighbor search, IVFFlat - index divides vectors into lists, and then searches a subset of those lists that are closest to the query vector. It has faster build times and uses less memory than HNSW, but has lower query performance (in terms of speed-recall tradeoff). HNSW - creates a multilayer graph. It has slower build times and uses more memory than IVFFlat, but has better query performance (in terms of speed-recall tradeoff). There’s no training step like IVFFlat, so the index can be created without any data in the table. HNSW spring.ai.vectorstore.pgvector.distance-type Search distance type. Defaults to COSINE_DISTANCE . But if vectors are normalized to length 1, you can use EUCLIDEAN_DISTANCE or NEGATIVE_INNER_PRODUCT for best performance. COSINE_DISTANCE spring.ai.vectorstore.pgvector.dimensions Embeddings dimension. If not specified explicitly the PgVectorStore will retrieve the dimensions form the provided EmbeddingModel . Dimensions are set to the embedding column the on table creation. If you change the dimensions your would have to re-create the vector_store table as well. - spring.ai.vectorstore.pgvector.remove-existing-vector-store-table Deletes the existing vector_store table on start up. false spring.ai.vectorstore.pgvector.initialize-schema Whether to initialize the required schema false spring.ai.vectorstore.pgvector.schema-name Vector store schema name public spring.ai.vectorstore.pgvector.table-name Vector store table name vector_store spring.ai.vectorstore.pgvector.schema-validation Enables schema and table name validation to ensure they are valid and existing objects. false If you configure a custom schema and/or table name, consider enabling schema validation by setting spring.ai.vectorstore.pgvector.schema-validation=true . This ensures the correctness of the names and reduces the risk of SQL injection attacks. Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with the PgVector store. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && article_type == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""author"",""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); These filter expressions are converted into the equivalent PgVector filters. Manual Configuration: Instead of using the Spring Boot auto-configuration, you can manually configure the PgVectorStore . For this you need to add the PostgreSQL connection and JdbcTemplate auto-configuration dependencies to your project: <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-jdbc</artifactId> </dependency> <dependency> <groupId>org.postgresql</groupId> <artifactId>postgresql</artifactId> <scope>runtime</scope> </dependency> <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-pgvector-store</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. To configure PgVector in your application, you can use the following setup: @Bean public VectorStore vectorStore(JdbcTemplate jdbcTemplate, EmbeddingModel embeddingModel) { return new PgVectorStore(jdbcTemplate, embeddingModel); } Run Postgres & PGVector DB locally: docker run -it --rm --name postgres -p 5432:5432 -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=postgres pgvector/pgvector You can connect to this server like this: psql -U postgres -h localhost -p 5432 Oracle(oracle.html) Pinecone(pinecone.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/pinecone.html","Pinecone: This section walks you through setting up the Pinecone VectorStore to store document embeddings and perform similarity searches. Pinecone(https://www.pinecone.io/) is a popular cloud-based vector database, which allows you to store and search vectors efficiently. Prerequisites: Pinecone Account: Before you start, sign up for a Pinecone account(https://app.pinecone.io/) . Pinecone Project: Once registered, create a new project, an index, and generate an API key. You’ll need these details for configuration. EmbeddingModel instance to compute the document embeddings. Several options are available: If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the PineconeVectorStore . To set up PineconeVectorStore , gather the following details from your Pinecone account: Pinecone API Key Pinecone Environment Pinecone Project ID Pinecone Index Name Pinecone Namespace This information is available to you in the Pinecone UI portal. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Pinecone Vector Store. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-pinecone-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-pinecone-store-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. Additionally, you will need a configured EmbeddingModel bean. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. Here is an example of the needed bean: @Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(""SPRING_AI_OPENAI_API_KEY""))); } To connect to Pinecone you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.properties , spring.ai.vectorstore.pinecone.apiKey=<your api key> spring.ai.vectorstore.pinecone.environment=<your environment> spring.ai.vectorstore.pinecone.projectId=<your project id> spring.ai.vectorstore.pinecone.index-name=<your index name> # API key if needed, e.g. OpenAI spring.ai.openai.api.key=<api-key> Please have a look at the list of configuration parameters(#_configuration_properties) for the vector store to learn about the default values and configuration options. Now you can Auto-wire the Pinecone Vector Store in your application and use it @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the Pinecone vector store. Property Description Default value spring.ai.vectorstore.pinecone.api-key Pinecone API Key - spring.ai.vectorstore.pinecone.environment Pinecone environment gcp-starter spring.ai.vectorstore.pinecone.project-id Pinecone project ID - spring.ai.vectorstore.pinecone.index-name Pinecone index name - spring.ai.vectorstore.pinecone.namespace Pinecone namespace - spring.ai.vectorstore.pinecone.namespace Pinecone namespace - spring.ai.vectorstore.pinecone.content-field-name Pinecone metadata field name used to store the original text content. document_content spring.ai.vectorstore.pinecone.distance-metadata-field-name Pinecone metadata field name used to store the computed distance. distance spring.ai.vectorstore.pinecone.server-side-timeout 20 sec. Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with the Pinecone store. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && article_type == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""author"",""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); These filter expressions are converted into the equivalent Pinecone filters. Manual Configuration: If you prefer to configure the PineconeVectorStore manually, you can do so by creating a PineconeVectorStoreConfig bean and passing it to the PineconeVectorStore constructor. Add these dependencies to your project: OpenAI: Required for calculating embeddings. <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> Pinecone <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-pinecone-store</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Sample Code: To configure Pinecone in your application, you can use the following setup: @Bean public PineconeVectorStoreConfig pineconeVectorStoreConfig() { return PineconeVectorStoreConfig.builder() .withApiKey(<PINECONE_API_KEY>) .withEnvironment(""gcp-starter"") .withProjectId(""89309e6"") .withIndexName(""spring-ai-test-index"") .withNamespace("""") // the free tier doesn't support namespaces. .withContentFieldName(""my_content"") // optional field to store the original content. Defaults to `document_content` .build(); } Integrate with OpenAI’s embeddings by adding the Spring Boot OpenAI starter to your project. This provides you with an implementation of the Embeddings client: @Bean public VectorStore vectorStore(PineconeVectorStoreConfig config, EmbeddingModel embeddingModel) { return new PineconeVectorStore(config, embeddingModel); } In your main code, create some documents: List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); Add the documents to Pinecone: vectorStore.add(documents); And finally, retrieve documents similar to a query: List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); If all goes well, you should retrieve the document containing the text ""Spring AI rocks!!"". PGvector(pgvector.html) Qdrant(qdrant.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/qdrant.html","Qdrant: This section walks you through setting up the Qdrant VectorStore to store document embeddings and perform similarity searches. Qdrant(https://www.qdrant.tech/) is an open-source, high-performance vector search engine/database. Prerequisites: Qdrant Instance: Set up a Qdrant instance by following the installation instructions(https://qdrant.tech/documentation/guides/installation/) in the Qdrant documentation. If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the QdrantVectorStore . To set up QdrantVectorStore , you’ll need the following information from your Qdrant instance: Host , GRPC Port , Collection Name , and API Key (if required). It is recommended that the Qdrant collection is created(https://qdrant.tech/documentation/concepts/collections/#create-a-collection) in advance with the appropriate dimensions and configurations. If the collection is not created, the QdrantVectorStore will attempt to create one using the Cosine similarity and the dimension of the configured EmbeddingModel . Auto-configuration: Then add the Qdrant boot starter dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-qdrant-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-qdrant-store-spring-boot-starter' } The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. The Vector Store, also requires an EmbeddingModel instance to calculate embeddings for the documents. You can pick one of the available EmbeddingModel Implementations(../embeddings.html#available-implementations) . For example to use the OpenAI EmbeddingModel(../embeddings/openai-embeddings.html) add the following dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. To connect to Qdrant and use the QdrantVectorStore , you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.properties , spring.ai.vectorstore.qdrant.host=<host of your qdrant instance> spring.ai.vectorstore.qdrant.port=<the GRPC port of your qdrant instance> spring.ai.vectorstore.qdrant.api-key=<your api key> spring.ai.vectorstore.qdrant.collection-name=<The name of the collection to use in Qdrant> # API key if needed, e.g. OpenAI spring.ai.openai.api.key=<api-key> Check the list of configuration parameters(#qdrant-vectorstore-properties) to learn about the default values and configuration options. Now you can Auto-wire the Qdrant Vector Store in your application and use it @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents to Qdrant vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the Qdrant vector store. Property Description Default value spring.ai.vectorstore.qdrant.host The host of the Qdrant server. localhost spring.ai.vectorstore.qdrant.port The gRPC port of the Qdrant server. 6334 spring.ai.vectorstore.qdrant.api-key The API key to use for authentication with the Qdrant server. - spring.ai.vectorstore.qdrant.collection-name The name of the collection to use in Qdrant. - spring.ai.vectorstore.qdrant.use-tls Whether to use TLS(HTTPS). false spring.ai.vectorstore.qdrant.initialize-schema Whether to initialize the backend schema or not false Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with the Qdrant vector store. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""author in ['john', 'jill'] && article_type == 'blog'"")); or programmatically using the Filter.Expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch(SearchRequest.defaults() .withQuery(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""author"", ""john"", ""jill""), b.eq(""article_type"", ""blog"")).build())); These filter expressions are converted into the equivalent Qdrant filters(https://qdrant.tech/documentation/concepts/filtering/) . Manual Configuration: Instead of using the Spring Boot auto-configuration, you can manually configure the QdrantVectorStore . For this you need to add the spring-ai-qdrant-store dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-qdrant-store</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-qdrant' } To configure Qdrant in your application, you can create a QdrantClient: @Bean public QdrantClient qdrantClient() { QdrantGrpcClient.Builder grpcClientBuilder = QdrantGrpcClient.newBuilder( ""<QDRANT_HOSTNAME>"", <QDRANT_GRPC_PORT>, <IS_TSL>); grpcClientBuilder.withApiKey(""<QDRANT_API_KEY>""); return new QdrantClient(grpcClientBuilder.build()); } Integrate with OpenAI’s embeddings by adding the Spring Boot OpenAI starter to your project. This provides you with an implementation of the Embeddings client: @Bean public QdrantVectorStore vectorStore(EmbeddingModel embeddingModel, QdrantClient qdrantClient) { return new QdrantVectorStore(qdrantClient, ""<QDRANT_COLLECTION_NAME>"", embeddingModel); } Pinecone(pinecone.html) Redis(redis.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/redis.html","Redis: This section walks you through setting up RedisVectorStore to store document embeddings and perform similarity searches. Redis(https://redis.io) is an open source (BSD licensed), in-memory data structure store used as a database, cache, message broker, and streaming engine. Redis provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes, and streams. Redis Search and Query(https://redis.io/docs/interact/search-and-query/) extends the core features of Redis OSS and allows you to use Redis as a vector database: Store vectors and the associated metadata within hashes or JSON documents Retrieve vectors Perform vector searches Prerequisites: A Redis Stack instance Redis Cloud(https://app.redislabs.com/#/) (recommended) Docker(https://hub.docker.com/r/redis/redis-stack) image redis/redis-stack:latest EmbeddingModel instance to compute the document embeddings. Several options are available: If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the RedisVectorStore . Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Redis Vector Store. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-redis-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-redis-store-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. Additionally, you will need a configured EmbeddingModel bean. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. Here is an example of the needed bean: @Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(""SPRING_AI_OPENAI_API_KEY""))); } To connect to Redis you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.properties , spring.ai.vectorstore.redis.uri=<your redis instance uri> spring.ai.vectorstore.redis.index=<your index name> spring.ai.vectorstore.redis.prefix=<your prefix> # API key if needed, e.g. OpenAI spring.ai.openai.api.key=<api-key> Please have a look at the list of configuration parameters(#_configuration_properties) for the vector store to learn about the default values and configuration options. Now you can Auto-wire the Redis Vector Store in your application and use it @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents to Redis vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the Redis vector store. Property Description Default value spring.ai.vectorstore.redis.uri Server connection URI redis://localhost:6379 spring.ai.vectorstore.redis.index Index name default-index spring.ai.vectorstore.redis.initialize-schema Whether to initialize the required schema false spring.ai.vectorstore.redis.prefix Prefix default: Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with RedisVectorStore as well. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest .query(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""country in ['UK', 'NL'] && year >= 2020"")); or programmatically using the expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch( SearchRequest .query(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""country"", ""UK"", ""NL""), b.gte(""year"", 2020)).build())); The portable filter expressions get automatically converted into Redis search queries(https://redis.io/docs/interact/search-and-query/query/) . For example, the following portable filter expression: country in ['UK', 'NL'] && year >= 2020 is converted into Redis query: @country:{UK | NL} @year:[2020 inf] Manual configuration: If you prefer not to use the auto-configuration, you can manually configure the Redis Vector Store. Add the Redis Vector Store and Jedis dependencies <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-redis-store</artifactId> </dependency> <dependency> <groupId>redis.clients</groupId> <artifactId>jedis</artifactId> <version>5.1.0</version> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Then, create a RedisVectorStore bean in your Spring configuration: @Bean public VectorStore vectorStore(EmbeddingModel embeddingModel) { RedisVectorStoreConfig config = RedisVectorStoreConfig.builder() .withURI(""redis://localhost:6379"") // Define the metadata fields to be used // in the similarity search filters. .withMetadataFields( MetadataField.tag(""country""), MetadataField.numeric(""year"")) .build(); return new RedisVectorStore(config, embeddingModel); } It is more convenient and preferred to create the RedisVectorStore as a Bean. But if you decide to create it manually, then you must call the RedisVectorStore#afterPropertiesSet() after setting the properties and before using the client. You must list explicitly all metadata field names and types ( TAG , TEXT , or NUMERIC ) for any metadata field used in filter expression. The withMetadataFields above registers filterable metadata fields: country of type TAG , year of type NUMERIC . Then in your main code, create some documents: List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""country"", ""UK"", ""year"", 2020)), new Document(""The World is Big and Salvation Lurks Around the Corner"", Map.of()), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""country"", ""NL"", ""year"", 2023))); Now add the documents to your vector store: vectorStore.add(documents); And finally, retrieve documents similar to a query: List<Document> results = vectorStore.similaritySearch( SearchRequest .query(""Spring"") .withTopK(5)); If all goes well, you should retrieve the document containing the text ""Spring AI rocks!!"". Qdrant(qdrant.html) SAP Hana(hana.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/hana.html","SAP HANA Cloud: Prerequisites: You need a SAP HANA Cloud vector engine account - Refer SAP HANA Cloud vector engine - provision a trial account(hanadb-provision-a-trial-account.html) guide to create a trial account. If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the vector store. Auto-configuration: Spring AI provides Spring Boot auto-configuration for the SAP Hana Vector Store. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-hanadb-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-hanadb-store-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Please have a look at the list of configuration parameters(#_hanacloudvectorstore_properties) for the vector store to learn about the default values and configuration options. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. Additionally, you will need a configured EmbeddingModel bean. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. HanaCloudVectorStore properties: You can use the following properties in your Spring Boot configuration to customize the SAP Hana vector store. It uses spring.datasource. properties to configure the Hana datasource and the spring.ai.vectorstore.hanadb. properties to configure the Hana vector store. Property Description Default value spring.datasource.driver-class-name Driver class name com.sap.db.jdbc.Driver spring.datasource.url Hana Datasource URL - spring.datasource.username Hana datasource username - spring.datasource.password Hana datasource password - spring.ai.vectorstore.hanadb.top-k TODO - spring.ai.vectorstore.hanadb.table-name TODO - spring.ai.vectorstore.hanadb.initialize-schema whether to initialize the required schema false Build a Sample RAG application: Shows how to setup a project that uses SAP Hana Cloud as the vector DB and leverage OpenAI to implement RAG pattern Create a table CRICKET_WORLD_CUP in SAP Hana DB: CREATE TABLE CRICKET_WORLD_CUP ( _ID VARCHAR2(255) PRIMARY KEY, CONTENT CLOB, EMBEDDING REAL_VECTOR(1536) ) Add the following dependencies in your pom.xml You may set the property spring-ai-version as <spring-ai-version>1.0.0-SNAPSHOT</spring-ai-version> : <dependencyManagement> <dependencies> <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-bom</artifactId> <version>${spring-ai-version}</version> <type>pom</type> <scope>import</scope> </dependency> </dependencies> </dependencyManagement> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web</artifactId> </dependency> <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-pdf-document-reader</artifactId> </dependency> <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-hanadb-store-spring-boot-starter</artifactId> </dependency> <dependency> <groupId>org.projectlombok</groupId> <artifactId>lombok</artifactId> <version>1.18.30</version> <scope>provided</scope> </dependency> Add the following properties in application.properties file: spring.ai.openai.api-key=${OPENAI_API_KEY} spring.ai.openai.embedding.options.model=text-embedding-ada-002 spring.datasource.driver-class-name=com.sap.db.jdbc.Driver spring.datasource.url=${HANA_DATASOURCE_URL} spring.datasource.username=${HANA_DATASOURCE_USERNAME} spring.datasource.password=${HANA_DATASOURCE_PASSWORD} spring.ai.vectorstore.hanadb.tableName=CRICKET_WORLD_CUP spring.ai.vectorstore.hanadb.topK=3 Create an Entity class named CricketWorldCup that extends from HanaVectorEntity:: package com.interviewpedia.spring.ai.hana; import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.Table; import lombok.Data; import lombok.NoArgsConstructor; import lombok.extern.jackson.Jacksonized; import org.springframework.ai.vectorstore.HanaVectorEntity; @Entity @Table(name = ""CRICKET_WORLD_CUP"") @Data @Jacksonized @NoArgsConstructor public class CricketWorldCup extends HanaVectorEntity { @Column(name = ""content"") private String content; } Create a Repository named CricketWorldCupRepository that implements HanaVectorRepository interface: package com.interviewpedia.spring.ai.hana; import jakarta.persistence.EntityManager; import jakarta.persistence.PersistenceContext; import jakarta.transaction.Transactional; import org.springframework.ai.vectorstore.HanaVectorRepository; import org.springframework.stereotype.Repository; import java.util.List; @Repository public class CricketWorldCupRepository implements HanaVectorRepository<CricketWorldCup> { @PersistenceContext private EntityManager entityManager; @Override @Transactional public void save(String tableName, String id, String embedding, String content) { String sql = String.format("""""" INSERT INTO %s (_ID, EMBEDDING, CONTENT) VALUES(:_id, TO_REAL_VECTOR(:embedding), :content) """""", tableName); entityManager.createNativeQuery(sql) .setParameter(""_id"", id) .setParameter(""embedding"", embedding) .setParameter(""content"", content) .executeUpdate(); } @Override @Transactional public int deleteEmbeddingsById(String tableName, List<String> idList) { String sql = String.format("""""" DELETE FROM %s WHERE _ID IN (:ids) """""", tableName); return entityManager.createNativeQuery(sql) .setParameter(""ids"", idList) .executeUpdate(); } @Override @Transactional public int deleteAllEmbeddings(String tableName) { String sql = String.format("""""" DELETE FROM %s """""", tableName); return entityManager.createNativeQuery(sql).executeUpdate(); } @Override public List<CricketWorldCup> cosineSimilaritySearch(String tableName, int topK, String queryEmbedding) { String sql = String.format("""""" SELECT TOP :topK * FROM %s ORDER BY COSINE_SIMILARITY(EMBEDDING, TO_REAL_VECTOR(:queryEmbedding)) DESC """""", tableName); return entityManager.createNativeQuery(sql, CricketWorldCup.class) .setParameter(""topK"", topK) .setParameter(""queryEmbedding"", queryEmbedding) .getResultList(); } } Now, create a REST Controller class CricketWorldCupHanaController , and autowire ChatModel and VectorStore as dependencies In this controller class, create the following REST endpoints: /ai/hana-vector-store/cricket-world-cup/purge-embeddings - to purge all the embeddings from the Vector Store /ai/hana-vector-store/cricket-world-cup/upload - to upload the Cricket_World_Cup.pdf so that its data gets stored in SAP Hana Cloud Vector DB as embeddings /ai/hana-vector-store/cricket-world-cup - to implement RAG using Cosine_Similarity in SAP Hana DB(https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-vector-engine-guide/vectors-vector-embeddings-and-metrics) package com.interviewpedia.spring.ai.hana; import lombok.extern.slf4j.Slf4j; import org.springframework.ai.chat.model.ChatModel; import org.springframework.ai.chat.messages.UserMessage; import org.springframework.ai.chat.prompt.Prompt; import org.springframework.ai.chat.prompt.SystemPromptTemplate; import org.springframework.ai.document.Document; import org.springframework.ai.reader.pdf.PagePdfDocumentReader; import org.springframework.ai.transformer.splitter.TokenTextSplitter; import org.springframework.ai.vectorstore.HanaCloudVectorStore; import org.springframework.ai.vectorstore.VectorStore; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.core.io.Resource; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.multipart.MultipartFile; import java.io.IOException; import java.util.List; import java.util.Map; import java.util.function.Function; import java.util.function.Supplier; import java.util.stream.Collectors; @RestController @Slf4j public class CricketWorldCupHanaController { private final VectorStore hanaCloudVectorStore; private final ChatModel chatModel; @Autowired public CricketWorldCupHanaController(ChatModel chatModel, VectorStore hanaCloudVectorStore) { this.chatModel = chatModel; this.hanaCloudVectorStore = hanaCloudVectorStore; } @PostMapping(""/ai/hana-vector-store/cricket-world-cup/purge-embeddings"") public ResponseEntity<String> purgeEmbeddings() { int deleteCount = ((HanaCloudVectorStore) this.hanaCloudVectorStore).purgeEmbeddings(); log.info(""{} embeddings purged from CRICKET_WORLD_CUP table in Hana DB"", deleteCount); return ResponseEntity.ok().body(String.format(""%d embeddings purged from CRICKET_WORLD_CUP table in Hana DB"", deleteCount)); } @PostMapping(""/ai/hana-vector-store/cricket-world-cup/upload"") public ResponseEntity<String> handleFileUpload(@RequestParam(""pdf"") MultipartFile file) throws IOException { Resource pdf = file.getResource(); Supplier<List<Document>> reader = new PagePdfDocumentReader(pdf); Function<List<Document>, List<Document>> splitter = new TokenTextSplitter(); List<Document> documents = splitter.apply(reader.get()); log.info(""{} documents created from pdf file: {}"", documents.size(), pdf.getFilename()); hanaCloudVectorStore.accept(documents); return ResponseEntity.ok().body(String.format(""%d documents created from pdf file: %s"", documents.size(), pdf.getFilename())); } @GetMapping(""/ai/hana-vector-store/cricket-world-cup"") public Map<String, String> hanaVectorStoreSearch(@RequestParam(value = ""message"") String message) { var documents = this.hanaCloudVectorStore.similaritySearch(message); var inlined = documents.stream().map(Document::getContent).collect(Collectors.joining(System.lineSeparator())); var similarDocsMessage = new SystemPromptTemplate(""Based on the following: {documents}"") .createMessage(Map.of(""documents"", inlined)); var userMessage = new UserMessage(message); Prompt prompt = new Prompt(List.of(similarDocsMessage, userMessage)); String generation = chatModel.call(prompt).getResult().getOutput().getContent(); log.info(""Generation: {}"", generation); return Map.of(""generation"", generation); } } Use a contextual pdf file from wikipedia Go to wikipedia(https://en.wikipedia.org/wiki/Cricket_World_Cup) and download(https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf&page=Cricket_World_Cup&action=show-download-screen) Cricket World Cup page as a PDF file. Upload this PDF file using the file-upload REST endpoint that we created in the previous step. Redis(redis.html) Typesense(typesense.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/typesense.html","Typesense: This section walks you through setting up TypesenseVectorStore to store document embeddings and perform similarity searches. Typesense(https://typesense.org) Typesense is an open source, typo tolerant search engine that is optimized for instant sub-50ms searches, while providing an intuitive developer experience. Prerequisites: A Typesense instance Typesense Cloud(https://typesense.org/docs/guide/install-typesense.html) (recommended) Docker(https://hub.docker.com/r/typesense/typesense/) image typesense/typesense:latest EmbeddingModel instance to compute the document embeddings. Several options are available: If required, an API key for the EmbeddingModel(../embeddings.html#available-implementations) to generate the embeddings stored by the TypesenseVectorStore . Auto-configuration: Spring AI provides Spring Boot auto-configuration for the Typesense Vector Sore. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-typesense-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-typesense-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. Additionally, you will need a configured EmbeddingModel bean. Refer to the EmbeddingModel(../embeddings.html#available-implementations) section for more information. Here is an example of the needed bean: @Bean public EmbeddingModel embeddingModel() { // Can be any other EmbeddingModel implementation. return new OpenAiEmbeddingModel(new OpenAiApi(System.getenv(""SPRING_AI_OPENAI_API_KEY""))); } To connect to Typesense you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.yml , spring: ai: vectorstore: typesense: collectionName: ""vector_store"" embeddingDimension: 1536 client: protocl: http host: localhost port: 8108 apiKey: xyz Please have a look at the list of configuration parameters(#_configuration_properties) for the vector store to learn about the default values and configuration options. Now you can Auto-wire the Typesense Vector Store in your application and use it @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents to Typesense vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the Typesense vector store. Property Description Default value spring.ai.vectorstore.typesense.client.protocol HTTP Protocol http spring.ai.vectorstore.typesense.client.host Hostname localhost spring.ai.vectorstore.typesense.client.port Port 8108 spring.ai.vectorstore.typesense.client.apiKey ApiKey xyz spring.ai.vectorstore.typesense.initialize-schema Whether to initialize the required schema false spring.ai.vectorstore.typesense.collection-name Collection Name vector_store spring.ai.vectorstore.typesense.embedding-dimension Embedding Dimension 1536 Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with TypesenseVectorStore as well. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest .query(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""country in ['UK', 'NL'] && year >= 2020"")); or programmatically using the expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch( SearchRequest .query(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""country"", ""UK"", ""NL""), b.gte(""year"", 2020)).build())); The portable filter expressions get automatically converted into Typesense Search Filters(https://typesense.org/docs/0.24.0/api/search.html#filter-parameters) . For example, the following portable filter expression: country in ['UK', 'NL'] && year >= 2020 is converted into Typesense filter: country: ['UK', 'NL'] && year: >=2020 Manual configuration: If you prefer not to use the auto-configuration, you can manually configure the Typesense Vector Store. Add the Typesense Vector Store and Jedis dependencies <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-typesense</artifactId> </dependency> Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Then, create a TypesenseVectorStore bean in your Spring configuration: @Bean public VectorStore vectorStore(Client client, EmbeddingModel embeddingModel) { TypesenseVectorStoreConfig config = TypesenseVectorStoreConfig.builder() .withCollectionName(""test_vector_store"") .withEmbeddingDimension(embeddingModel.dimensions()) .build(); return new TypesenseVectorStore(client, embeddingModel, config); } @Bean public Client typesenseClient() { List<Node> nodes = new ArrayList<>(); nodes .add(new Node(""http"", typesenseContainer.getHost(), typesenseContainer.getMappedPort(8108).toString())); Configuration configuration = new Configuration(nodes, Duration.ofSeconds(5), ""xyz""); return new Client(configuration); } It is more convenient and preferred to create the TypesenseVectorStore as a Bean. But if you decide to create it manually, then you must call the TypesenseVectorStore#afterPropertiesSet() after setting the properties and before using the client. Then in your main code, create some documents: List<Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""country"", ""UK"", ""year"", 2020)), new Document(""The World is Big and Salvation Lurks Around the Corner"", Map.of()), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""country"", ""NL"", ""year"", 2023))); Now add the documents to your vector store: vectorStore.add(documents); And finally, retrieve documents similar to a query: List<Document> results = vectorStore.similaritySearch( SearchRequest .query(""Spring"") .withTopK(5)); If all goes well, you should retrieve the document containing the text ""Spring AI rocks!!"". If you are not retrieving the documents in the expected order or the search results are not as expected, check the embedding model you are using. Embedding models can have a significant impact on the search results (i.e. make sure if your data is in Spanish to use a Spanish or multilingual embedding model). SAP Hana(hana.html) Weaviate(weaviate.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/vectordbs/weaviate.html","Weaviate: This section will walk you through setting up the Weaviate VectorStore to store document embeddings and perform similarity searches. What is Weaviate?: Weaviate(https://weaviate.io/) is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML-models and scale seamlessly into billions of data objects. It provides tools to store document embeddings, content, and metadata and to search through those embeddings, including metadata filtering. Prerequisites: EmbeddingModel instance to compute the document embeddings. Several options are available: Transformers Embedding - computes the embedding in your local environment. Follow the ONNX Transformers Embedding instructions. OpenAI Embedding - uses the OpenAI embedding endpoint. You need to create an account at OpenAI Signup(https://platform.openai.com/signup) and generate the api-key token at API Keys(https://platform.openai.com/account/api-keys) . You can also use the Azure OpenAI Embedding or the PostgresML Embedding Model . Weaviate cluster . You can set up a cluster locally in a Docker container or create a Weaviate Cloud Service(https://console.weaviate.cloud/) . For the latter, you need to create a Weaviate account, set up a cluster, and get your access API key from the dashboard details(https://console.weaviate.cloud/dashboard) . On startup, the WeaviateVectorStore creates the required SpringAiWeaviate object schema if it’s not already provisioned. Auto-configuration: Then add the WeaviateVectorStore boot starter dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-weaviate-store-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-weaviate-store-spring-boot-starter' } The vector store implementation can initialize the requisite schema for you, but you must opt-in by specifying the initializeSchema boolean in the appropriate constructor or by setting …​initialize-schema=true in the application.properties file. this is a breaking change! In earlier versions of Spring AI, this schema initialization happened by default. The Vector Store, also requires an EmbeddingModel instance to calculate embeddings for the documents. You can pick one of the available EmbeddingModel Implementations(../embeddings.html#available-implementations) . For example to use the OpenAI EmbeddingModel(../embeddings/openai-embeddings.html) add the following dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-openai-spring-boot-starter' } Refer to the Dependency Management(../../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Refer to the Repositories(../../getting-started.html#repositories) section to add Milestone and/or Snapshot Repositories to your build file. To connect to Weaviate and use the WeaviateVectorStore , you need to provide access details for your instance. A simple configuration can either be provided via Spring Boot’s application.properties , spring.ai.vectorstore.weaviate.host=<host of your Weaviate instance> spring.ai.vectorstore.weaviate.api-key=<your api key> spring.ai.vectorstore.weaviate.scheme=http # API key if needed, e.g. OpenAI spring.ai.openai.api.key=<api-key> Check the list of configuration parameters(#weaviate-vectorstore-properties) to learn about the default values and configuration options. Now you can Auto-wire the Weaviate Vector Store in your application and use it @Autowired VectorStore vectorStore; // ... List <Document> documents = List.of( new Document(""Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!! Spring AI rocks!!"", Map.of(""meta1"", ""meta1"")), new Document(""The World is Big and Salvation Lurks Around the Corner""), new Document(""You walk forward facing the past and you turn back toward the future."", Map.of(""meta2"", ""meta2""))); // Add the documents vectorStore.add(documents); // Retrieve documents similar to a query List<Document> results = vectorStore.similaritySearch(SearchRequest.query(""Spring"").withTopK(5)); Configuration properties: You can use the following properties in your Spring Boot configuration to customize the weaviate vector store. Property Description Default value spring.ai.vectorstore.weaviate.host The host of the Weaviate server. localhost:8080 spring.ai.vectorstore.weaviate.scheme Connection schema. http spring.ai.vectorstore.weaviate.api-key The API key to use for authentication with the Weaviate server. - spring.ai.vectorstore.weaviate.object-class ""SpringAiWeaviate"" spring.ai.vectorstore.weaviate.consistency-level Desired tradeoff between consistency and speed ConsistentLevel.ONE spring.ai.vectorstore.weaviate.filter-field spring.ai.vectorstore.weaviate.filter-field.<field-name>=<field-type> - spring.ai.vectorstore.weaviate.headers - spring.ai.vectorstore.weaviate.initialize-schema Whether to initialize the required schema false Metadata filtering: You can leverage the generic, portable metadata filters(https://docs.spring.io/spring-ai/reference/api/vectordbs.html#_metadata_filters) with WeaviateVectorStore as well. For example, you can use either the text expression language: vectorStore.similaritySearch( SearchRequest .query(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(""country in ['UK', 'NL'] && year >= 2020"")); or programmatically using the expression DSL: FilterExpressionBuilder b = new FilterExpressionBuilder(); vectorStore.similaritySearch( SearchRequest .query(""The World"") .withTopK(TOP_K) .withSimilarityThreshold(SIMILARITY_THRESHOLD) .withFilterExpression(b.and( b.in(""country"", ""UK"", ""NL""), b.gte(""year"", 2020)).build())); The portable filter expressions get automatically converted into the proprietary Weaviate where filters(https://weaviate.io/developers/weaviate/api/graphql/filters) . For example, the following portable filter expression: country in ['UK', 'NL'] && year >= 2020 is converted into Weaviate GraphQL where filter expression(https://weaviate.io/developers/weaviate/api/graphql/filters) : operator:And operands: [{ operator:Or operands: [{ path:[""meta_country""] operator:Equal valueText:""UK"" }, { path:[""meta_country""] operator:Equal valueText:""NL"" }] }, { path:[""meta_year""] operator:GreaterThanEqual valueNumber:2020 }] Manual Configuration: Instead of using the Spring Boot auto-configuration, you can manually configure the WeaviateVectorStore . For this you need to add the spring-ai-weaviate-store dependency to your project: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-weaviate-store</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-weaviate-store' } To configure Weaviate in your application, you can create a WeaviateClient: @Bean public WeaviateClient weaviateClient() { try { return WeaviateAuthClient.apiKey( new Config(<YOUR SCHEME>, <YOUR HOST>, <YOUR HEADERS>), <YOUR API KEY>); } catch (AuthException e) { throw new IllegalArgumentException(""WeaviateClient could not be created."", e); } } Integrate with OpenAI’s embeddings by adding the Spring Boot OpenAI starter to your project. This provides you with an implementation of the Embeddings client: @Bean public WeaviateVectorStore vectorStore(EmbeddingModel embeddingModel, WeaviateClient weaviateClient) { WeaviateVectorStoreConfig.Builder configBuilder = WeaviateVectorStore.WeaviateVectorStoreConfig.builder() .withObjectClass(<YOUR OBJECT CLASS>) .withConsistencyLevel(<YOUR CONSISTENCY LEVEL>); return new WeaviateVectorStore(configBuilder.build(), embeddingModel, weaviateClient); } Run Weaviate cluster in docker container: Start Weaviate in a docker container: docker run -it --rm --name weaviate -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true -e PERSISTENCE_DATA_PATH=/var/lib/weaviate -e QUERY_DEFAULTS_LIMIT=25 -e DEFAULT_VECTORIZER_MODULE=none -e CLUSTER_HOSTNAME=node1 -p 8080:8080 semitechnologies/weaviate:1.22.4 Starts a Weaviate cluster at localhost:8080/v1(http://localhost:8080/v1) with scheme=http, host=localhost:8080, and apiKey="""". Then follow the usage instructions. Typesense(typesense.html) Function Calling(../functions.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/functions.html","Function Calling API: The integration of function support in AI models, permits the model to request the execution of client-side functions, thereby accessing necessary information or performing tasks dynamically as required. Spring AI currently supports function invocation for the following AI Models: Anthropic Claude: Refer to the Anthropic Claude function invocation docs(chat/functions/anthropic-chat-functions.html) . Azure OpenAI: Refer to the Azure OpenAI function invocation docs(chat/functions/azure-open-ai-chat-functions.html) . Google VertexAI Gemini: Refer to the Vertex AI Gemini function invocation docs(chat/functions/vertexai-gemini-chat-functions.html) . Groq: Refer to the Groq function invocation docs(chat/groq-chat.html#_function_calling) . Mistral AI: Refer to the Mistral AI function invocation docs(chat/functions/mistralai-chat-functions.html) . Ollama: Refer to the Ollama function invocation docs(chat/functions/ollama-chat-functions.html) (streaming not supported yet). OpenAI: Refer to the OpenAI function invocation docs(chat/functions/openai-chat-functions.html) . Weaviate(vectordbs/weaviate.html) Multimodality(multimodality.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/multimodality.html","Multimodality API: Humans process knowledge, simultaneously across multiple modes of data inputs. The way we learn, our experiences are all multimodal. We don’t have just vision, just audio and just text. These foundational principles of learning were articulated by the father of modern education John Amos Comenius(https://en.wikipedia.org/wiki/John_Amos_Comenius) , in his work, ""Orbis Sensualium Pictus"", dating back to 1658. ""All things that are naturally connected ought to be taught in combination"" Contrary to those principles, in the past, our approach to Machine Learning was often focused on specialized models tailored to process a single modality. For instance, we developed audio models for tasks like text-to-speech or speech-to-text, and computer vision models for tasks such as object detection and classification. However, a new wave of multimodal large language models starts to emerge. Examples include OpenAI’s GPT-4 Vision, Google’s Vertex AI Gemini Pro Vision, Anthropic’s Claude3, and open source offerings LLaVA and balklava are able to accept multiple inputs, including text images, audio and video and generate text responses by integrating these inputs. The multimodal large language model (LLM) features enable the models to process and generate text in conjunction with other modalities such as images, audio, or video. Spring AI Multimodality: Multimodality refers to a model’s ability to simultaneously understand and process information from various sources, including text, images, audio, and other data formats. The Spring AI Message API provides all necessary abstractions to support multimodal LLMs. The UserMessage’s content field is used primarily for text inputs, while the optional media field allows adding one or more additional content of different modalities such as images, audio and video. The MimeType specifies the modality type. Depending on the used LLMs, the Media data field can be either the raw media content as a Resource object or a URI to the content. The media field is currently applicable only for user input messages (e.g., UserMessage ). It does not hold significance for system messages. The AssistantMessage , which includes the LLM response, provides text content only. To generate non-text media outputs, you should utilize one of the dedicated, single-modality models.* For example, we can take the following picture ( multimodal.test.png ) as an input and ask the LLM to explain what it sees. For most of the multimodal LLMs, the Spring AI code would look something like this: var imageResource = new ClassPathResource(""/multimodal.test.png""); var userMessage = new UserMessage( ""Explain what do you see in this picture?"", // content new Media(MimeTypeUtils.IMAGE_PNG, imageResource)); // media ChatResponse response = chatModel.call(new Prompt(userMessage)); or with the fluent ChatClient(chatclient.html) API: String response = ChatClient.create(chatModel).prompt() .user(u -> u.text(""Explain what do you see on this picture?"") .media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource(""/multimodal.test.png""))) .call() .content(); and produce a response like: This is an image of a fruit bowl with a simple design. The bowl is made of metal with curved wire edges that create an open structure, allowing the fruit to be visible from all angles. Inside the bowl, there are two yellow bananas resting on top of what appears to be a red apple. The bananas are slightly overripe, as indicated by the brown spots on their peels. The bowl has a metal ring at the top, likely to serve as a handle for carrying. The bowl is placed on a flat surface with a neutral-colored background that provides a clear view of the fruit inside. Spring AI provides multimodal support for the following chat models: OpenAI (e.g. GPT-4 and GPT-4o models)(chat/openai-chat.html#_multimodal) Ollama (e.g. LlaVa and Baklava models)(chat/ollama-chat.html#_multimodal) Vertex AI Gemini (e.g. gemini-1.5-pro-001, gemini-1.5-flash-001 models)(chat/vertexai-gemini-chat.html#_multimodal) Anthropic Claude 3(chat/anthropic-chat.html#_multimodal) AWS Bedrock Anthropic Claude 3(chat/bedrock/bedrock-anthropic3.html#_multimodal) Azure Open AI (e.g. GPT-4o models)(chat/azure-openai-chat.html#_multimodal) Function Calling(functions.html) Prompts(prompt.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/prompt.html","Prompts: Prompts are the inputs that guide an AI model to generate specific outputs. The design and phrasing of these prompts significantly influence the model’s responses. At the lowest level of interaction with AI models in Spring AI, handling prompts in Spring AI is somewhat similar to managing the ""View"" in Spring MVC. This involves creating extensive text with placeholders for dynamic content. These placeholders are then replaced based on user requests or other code in the application. Another analogy is a SQL statement that contain placeholders for certain expressions. As Spring AI evolves, it will introduce higher levels of abstraction for interacting with AI models. The foundational classes described in this section can be likened to JDBC in terms of their role and functionality. The ChatModel class, for instance, is analogous to the core JDBC library in the JDK. The ChatClient class can be likened to the JdbcClient , built on top of ChatModel and providing more advanced constructs via Advisor to consider past interactions with the model, augment the prompt with additional contextual documents, and introduce agentic behavior. The structure of prompts has evolved over time within the AI field. Initially, prompts were simple strings. Over time, they grew to include placeholders for specific inputs, like ""USER:"", which the AI model recognizes. OpenAI have introduced even more structure to prompts by categorizing multiple message strings into distinct roles before they are processed by the AI model. API Overview: Prompt: It is common to use the call() method of ChatModel that takes a Prompt instance and returns a ChatResponse . The Prompt class functions as a container for an organized series of Message objects and a request ChatOptions . Every Message embodies a unique role within the prompt, differing in its content and intent. These roles can encompass a variety of elements, from user inquiries to AI-generated responses to relevant background information. This arrangement enables intricate and detailed interactions with AI models, as the prompt is constructed from multiple messages, each assigned a specific role to play in the dialogue. Below is a truncated version of the Prompt class, with constructors and utility methods omitted for brevity: public class Prompt implements ModelRequest<List<Message>> { private final List<Message> messages; private ChatOptions chatOptions; } Message: The Message interface encapsulates a Prompt textual content, a collection of metadata attributes, and a categorization known as MessageType . The interface is defined as follows: public interface Content { String getContent(); Map<String, Object> getMetadata(); } public interface Message extends Content { MessageType getMessageType(); } The multimodal message types implement also the MediaContent interface providing a list of Media content objects. public interface MediaContent extends Content { Collection<Media> getMedia(); } Various implementations of the Message interface correspond to different categories of messages that an AI model can process. The Models distinguish between message categories based on conversational roles. These roles are effectively mapped by the MessageType , as discussed below. Roles: Each message is assigned a specific role. These roles categorize the messages, clarifying the context and purpose of each segment of the prompt for the AI model. This structured approach enhances the nuance and effectiveness of communication with the AI, as each part of the prompt plays a distinct and defined role in the interaction. The primary roles are: System Role: Guides the AI’s behavior and response style, setting parameters or rules for how the AI interprets and replies to the input. It’s akin to providing instructions to the AI before initiating a conversation. User Role: Represents the user’s input – their questions, commands, or statements to the AI. This role is fundamental as it forms the basis of the AI’s response. Assistant Role: The AI’s response to the user’s input. More than just an answer or reaction, it’s crucial for maintaining the flow of the conversation. By tracking the AI’s previous responses (its 'Assistant Role' messages), the system ensures coherent and contextually relevant interactions. The Assistant message may contain Function Tool Call request information as well. It’s like a special feature in the AI, used when needed to perform specific functions such as calculations, fetching data, or other tasks beyond just talking. Tool/Function Role: The Too/Function Role focuses on returning additional information in response to Tool Call Assistant Messages. Roles are represented as an enumeration in Spring AI as shown below public enum MessageType { USER(""user""), ASSISTANT(""assistant""), SYSTEM(""system""), TOOL(""tool""); ... } PromptTemplate: A key component for prompt templating in Spring AI is the PromptTemplate class. This class uses the OSS StringTemplate(https://www.stringtemplate.org/) engine, developed by Terence Parr, for constructing and managing prompts. The PromptTemplate class is designed to facilitate the creation of structured prompts that are then sent to the AI model for processing public class PromptTemplate implements PromptTemplateActions, PromptTemplateMessageActions { // Other methods to be discussed later } The interfaces implemented by this class support different aspects of prompt creation: PromptTemplateStringActions focuses on creating and rendering prompt strings, representing the most basic form of prompt generation. PromptTemplateMessageActions is tailored for prompt creation through the generation and manipulation of Message objects. PromptTemplateActions is designed to return the Prompt object, which can be passed to ChatModel for generating a response. While these interfaces might not be used extensively in many projects, they show the different approaches to prompt creation. The implemented interfaces are public interface PromptTemplateStringActions { String render(); String render(Map<String, Object> model); } The method String render() : Renders a prompt template into a final string format without external input, suitable for templates without placeholders or dynamic content. The method String render(Map<String, Object> model) : Enhances rendering functionality to include dynamic content. It uses a Map<String, Object> where map keys are placeholder names in the prompt template, and values are the dynamic content to be inserted. public interface PromptTemplateMessageActions { Message createMessage(); Message createMessage(List<Media> mediaList); Message createMessage(Map<String, Object> model); } The method Message createMessage() : Creates a Message object without additional data, used for static or predefined message content. The method Message createMessage(List<Media> mediaList) : Creates a Message object with static textual and media content. The method Message createMessage(Map<String, Object> model) : Extends message creation to integrate dynamic content, accepting a Map<String, Object> where each entry represents a placeholder in the message template and its corresponding dynamic value. public interface PromptTemplateActions extends PromptTemplateStringActions { Prompt create(); Prompt create(ChatOptions modelOptions); Prompt create(Map<String, Object> model); Prompt create(Map<String, Object> model, ChatOptions modelOptions); } The method Prompt create() : Generates a Prompt object without external data inputs, ideal for static or predefined prompts. The method Prompt create(ChatOptions modelOptions) : Generates a Prompt object without external data inputs and with specific options for the chat request. The method Prompt create(Map<String, Object> model) : Expands prompt creation capabilities to include dynamic content, taking a Map<String, Object> where each map entry is a placeholder in the prompt template and its associated dynamic value. The method Prompt create(Map<String, Object> model, ChatOptions modelOptions) : Expands prompt creation capabilities to include dynamic content, taking a Map<String, Object> where each map entry is a placeholder in the prompt template and its associated dynamic value, and specific options for the chat request. Example Usage: A simple example taken from the AI Workshop on PromptTemplates(https://github.com/Azure-Samples/spring-ai-azure-workshop/blob/main/2-README-prompt-templating.md) is shown below. PromptTemplate promptTemplate = new PromptTemplate(""Tell me a {adjective} joke about {topic}""); Prompt prompt = promptTemplate.create(Map.of(""adjective"", adjective, ""topic"", topic)); return chatModel.call(prompt).getResult(); Another example taken from the AI Workshop on Roles(https://github.com/Azure-Samples/spring-ai-azure-workshop/blob/main/3-README-prompt-roles.md) is shown below. String userText = """""" Tell me about three famous pirates from the Golden Age of Piracy and why they did. Write at least a sentence for each pirate. """"""; Message userMessage = new UserMessage(userText); String systemText = """""" You are a helpful AI assistant that helps people find information. Your name is {name} You should reply to the user's request with your name and also in the style of a {voice}. """"""; SystemPromptTemplate systemPromptTemplate = new SystemPromptTemplate(systemText); Message systemMessage = systemPromptTemplate.createMessage(Map.of(""name"", name, ""voice"", voice)); Prompt prompt = new Prompt(List.of(userMessage, systemMessage)); List<Generation> response = chatModel.call(prompt).getResults(); This shows how you can build up the Prompt instance by using the SystemPromptTemplate to create a Message with the system role passing in placeholder values. The message with the role user is then combined with the message of the role system to form the prompt. The prompt is then passed to the ChatModel to get a generative response. Using resources instead of raw Strings: Spring AI supports the org.springframework.core.io.Resource abstraction, so you can put prompt data in a file that can directly be used in a PromptTemplate . For example, you can define a field in your Spring managed component to retrieve the Resource . @Value(""classpath:/prompts/system-message.st"") private Resource systemResource; and then pass that resource to the SystemPromptTemplate directly. SystemPromptTemplate systemPromptTemplate = new SystemPromptTemplate(systemResource); Prompt Engineering: In generative AI, the creation of prompts is a crucial task for developers. The quality and structure of these prompts significantly influence the effectiveness of the AI’s output. Investing time and effort in designing thoughtful prompts can greatly improve the results from the AI. Sharing and discussing prompts is a common practice in the AI community. This collaborative approach not only creates a shared learning environment but also leads to the identification and use of highly effective prompts. Research in this area often involves analyzing and comparing different prompts to assess their effectiveness in various situations. For example, a significant study demonstrated that starting a prompt with ""Take a deep breath and work on this problem step by step"" significantly enhanced problem-solving efficiency. This highlights the impact that well-chosen language can have on generative AI systems' performance. Grasping the most effective use of prompts, particularly with the rapid advancement of AI technologies, is a continuous challenge. You should recognize the importance of prompt engineering and consider using insights from the community and research to improve prompt creation strategies. Creating effective prompts: When developing prompts, it’s important to integrate several key components to ensure clarity and effectiveness: Instructions : Offer clear and direct instructions to the AI, similar to how you would communicate with a person. This clarity is essential for helping the AI ""understand"" what is expected. External Context : Include relevant background information or specific guidance for the AI’s response when necessary. This ""external context"" frames the prompt and aids the AI in grasping the overall scenario. User Input : This is the straightforward part - the user’s direct request or question forming the core of the prompt. Output Indicator : This aspect can be tricky. It involves specifying the desired format for the AI’s response, such as JSON. However, be aware that the AI might not always adhere strictly to this format. For instance, it might prepend a phrase like ""here is your JSON"" before the actual JSON data, or sometimes generate a JSON-like structure that is not accurate. Providing the AI with examples of the anticipated question and answer format can be highly beneficial when crafting prompts. This practice helps the AI ""understand"" the structure and intent of your query, leading to more precise and relevant responses. While this documentation does not delve deeply into these techniques, they provide a starting point for further exploration in AI prompt engineering. Following is a list of resources for further investigation. Simple Techniques: Text Summarization(https://www.promptingguide.ai/introduction/examples.en#text-summarization) : Reduces extensive text into concise summaries, capturing key points and main ideas while omitting less critical details. Question Answering(https://www.promptingguide.ai/introduction/examples.en#question-answering) : Focuses on deriving specific answers from provided text, based on user-posed questions. It’s about pinpointing and extracting relevant information in response to queries. Text Classification(https://www.promptingguide.ai/introduction/examples.en#text-classification) : Systematically categorizes text into predefined categories or groups, analyzing the text and assigning it to the most fitting category based on its content. Conversation(https://www.promptingguide.ai/introduction/examples.en#conversation) : Creates interactive dialogues where the AI can engage in back-and-forth communication with users, simulating a natural conversation flow. Code Generation(https://www.promptingguide.ai/introduction/examples.en#code-generation) : Generates functional code snippets based on specific user requirements or descriptions, translating natural language instructions into executable code. Advanced Techniques: Zero-shot(https://www.promptingguide.ai/techniques/zeroshot) , Few-shot Learning(https://www.promptingguide.ai/techniques/fewshot) : Enables the model to make accurate predictions or responses with minimal to no prior examples of the specific problem type, understanding and acting on new tasks using learned generalizations. Chain-of-Thought(https://www.promptingguide.ai/techniques/cot) : Links multiple AI responses to create a coherent and contextually aware conversation. It helps the AI maintain the thread of the discussion, ensuring relevance and continuity. ReAct (Reason + Act)(https://www.promptingguide.ai/techniques/react) : In this method, the AI first analyzes (reasons about) the input, then determines the most appropriate course of action or response. It combines understanding with decision-making. Microsoft Guidance: Framework for Prompt Creation and Optimization(https://github.com/microsoft/guidance) : Microsoft offers a structured approach to developing and refining prompts. This framework guides users in creating effective prompts that elicit the desired responses from AI models, optimizing the interaction for clarity and efficiency. Tokens: Tokens are essential in how AI models process text, acting as a bridge that converts words (as we understand them) into a format that AI models can process. This conversion occurs in two stages: words are transformed into tokens upon input, and these tokens are then converted back into words in the output. Tokenization, the process of breaking down text into tokens, is fundamental to how AI models comprehend and process language. The AI model works with this tokenized format to understand and respond to prompts. To better understand tokens, think of them as portions of words. Typically, a token represents about three-quarters of a word. For instance, the complete works of Shakespeare, totaling roughly 900,000 words, would translate to around 1.2 million tokens. Experiment with the OpenAI Tokenizer UI(https://platform.openai.com/tokenizer) to see how words are converted into tokens. Tokens have practical implications beyond their technical role in AI processing, especially regarding billing and model capabilities: Billing: AI model services often bill based on token usage. Both the input (prompt) and the output (response) contribute to the total token count, making shorter prompts more cost-effective. Model Limits: Different AI models have varying token limits, defining their ""context window"" – the maximum amount of information they can process at a time. For example, GPT-3’s limit is 4K tokens, while other models like Claude 2 and Meta Llama 2 have limits of 100K tokens, and some research models can handle up to 1 million tokens. Context Window: A model’s token limit determines its context window. Inputs exceeding this limit are not processed by the model. It’s crucial to send only the minimal effective set of information for processing. For example, when inquiring about ""Hamlet,"" there’s no need to include tokens from all of Shakespeare’s other works. Response Metadata: The metadata of a response from an AI model includes the number of tokens used, a vital piece of information for managing usage and costs. Multimodality(multimodality.html) Structured Output(structured-output-converter.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/structured-output-converter.html","Structured Output Converter: As of 02.05.2024 the old OutputParser , BeanOutputParser , ListOutputParser and MapOutputParser classes are deprecated in favor of the new StructuredOutputConverter , BeanOutputConverter , ListOutputConverter and MapOutputConverter implementations. the latter are drop-in replacements for the former ones and provide the same functionality. The reason for the change was primarily naming, as there isn’t any parsing being done, but also have aligned with the Spring org.springframework.core.convert.converter package brining in some improved functionality. The ability of LLMs to produce structured outputs is important for downstream applications that rely on reliably parsing output values. Developers want to quickly turn results from an AI model into data types, such as JSON, XML or Java classes, that can be passed to other application functions and methods. The Spring AI Structured Output Converters help to convert the LLM output into a structured format. As shown in the following diagram, this approach operates around the LLM text completion endpoint: Generating structured outputs from Large Language Models (LLMs) using generic completion APIs requires careful handling of inputs and outputs. The structured output converter plays a crucial role before and after the LLM call, ensuring the desired output structure is achieved. Before the LLM call, the converter appends format instructions to the prompt, providing explicit guidance to the models on generating the desired output structure. These instructions act as a blueprint, shaping the model’s response to conform to the specified format. After the LLM call, the converter takes the model’s output text and transforms it into instances of the structured type. This conversion process involves parsing the raw text output and mapping it to the corresponding structured data representation, such as JSON, XML, or domain-specific data structures. The StructuredOutputConverter is a best effort to convert the model output into a structured output. The AI Model is not guaranteed to return the structured output as requested. The model may not understand the prompt or be unable to generate the structured output as requested. Consider implementing a validation mechanism to ensure the model output is as expected. The StructuredOutputConverter is not used for LLM Function Calling(functions.html) , as this feature inherently provides structured outputs by default. Structured Output API: The StructuredOutputConverter interface allows you to obtain structured output, such as mapping the output to a Java class or an array of values from the text-based AI Model output. The interface definition is: public interface StructuredOutputConverter<T> extends Converter<String, T>, FormatProvider { } It combines the Spring Converter<String, T>(https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/core/convert/converter/Converter.html) interface and the FormatProvider interface public interface FormatProvider { String getFormat(); } The following diagram shows the data flow when using the structured output API. The FormatProvider supplies specific formatting guidelines to the AI Model, enabling it to produce text outputs that can be converted into the designated target type T using the Converter . Here is an example of such formatting instructions: Your response should be in JSON format. The data structure for the JSON should match this Java class: java.util.HashMap Do not include any explanations, only provide a RFC8259 compliant JSON response following this format without deviation. The format instructions are most often appended to the end of the user input using the PromptTemplate(prompt.html#_prompttemplate) like this: StructuredOutputConverter outputConverter = ... String userInputTemplate = """""" ... user text input .... {format} """"""; // user input with a ""format"" placeholder. Prompt prompt = new Prompt( new PromptTemplate( userInputTemplate, Map.of(..., ""format"", outputConverter.getFormat()) // replace the ""format"" placeholder with the converter's format. ).createMessage()); The Converter<String, T> is responsible to transform output text from the model into instances of the specified type T . Available Converters: Currently, Spring AI provides AbstractConversionServiceOutputConverter , AbstractMessageOutputConverter , BeanOutputConverter , MapOutputConverter and ListOutputConverter implementations: AbstractConversionServiceOutputConverter<T> - Offers a pre-configured GenericConversionService(https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/core/convert/support/GenericConversionService.html) for transforming LLM output into the desired format. No default FormatProvider implementation is provided. AbstractMessageOutputConverter<T> - Supplies a pre-configured MessageConverter(https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/jms/support/converter/MessageConverter.html) for converting LLM output into the desired format. No default FormatProvider implementation is provided. BeanOutputConverter<T> - Configured with a designated Java class (e.g., Bean) or a ParameterizedTypeReference(https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/core/ParameterizedTypeReference.html) , this converter employs a FormatProvider implementation that directs the AI Model to produce a JSON response compliant with a DRAFT_2020_12 , JSON Schema derived from the specified Java class. Subsequently, it utilizes an ObjectMapper to deserialize the JSON output into a Java object instance of the target class. MapOutputConverter - Extends the functionality of AbstractMessageOutputConverter with a FormatProvider implementation that guides the AI Model to generate an RFC8259 compliant JSON response. Additionally, it incorporates a converter implementation that utilizes the provided MessageConverter to translate the JSON payload into a java.util.Map<String, Object> instance. ListOutputConverter - Extends the AbstractConversionServiceOutputConverter and includes a FormatProvider implementation tailored for comma-delimited list output. The converter implementation employs the provided ConversionService to transform the model text output into a java.util.List . Using Converters: The following sections provide guides how to use the available converters to generate structured outputs. Bean Output Converter: The following example shows how to use BeanOutputConverter to generate the filmography for an actor. The target record representing actor’s filmography: record ActorsFilms(String actor, List<String> movies) { } Here is how to apply the BeanOutputConverter using the high-level, fluent ChatClient API: ActorsFilms actorsFilms = ChatClient.create(chatModel).prompt() .user(u -> u.text(""Generate the filmography of 5 movies for {actor}."") .param(""actor"", ""Tom Hanks"")) .call() .entity(ActorsFilms.class); or using the low-level ChatModel API directly: BeanOutputConverter<ActorsFilms> beanOutputConverter = new BeanOutputConverter<>(ActorsFilms.class); String format = beanOutputConverter.getFormat(); String actor = ""Tom Hanks""; String template = """""" Generate the filmography of 5 movies for {actor}. {format} """"""; Generation generation = chatModel.call( new PromptTemplate(template, Map.of(""actor"", actor, ""format"", format)).create()).getResult(); ActorsFilms actorsFilms = beanOutputConverter.convert(generation.getOutput().getContent()); Generic Bean Types: Use the ParameterizedTypeReference constructor to specify a more complex target class structure. For example, to represent a list of actors and their filmographies: List<ActorsFilms> actorsFilms = ChatClient.create(chatModel).prompt() .user(""Generate the filmography of 5 movies for Tom Hanks and Bill Murray."") .call() .entity(new ParameterizedTypeReference<List<ActorsFilms>>() {}); or using the low-level ChatModel API directly: BeanOutputConverter<List<ActorsFilms>> outputConverter = new BeanOutputConverter<>( new ParameterizedTypeReference<List<ActorsFilms>>() { }); String format = outputConverter.getFormat(); String template = """""" Generate the filmography of 5 movies for Tom Hanks and Bill Murray. {format} """"""; Prompt prompt = new PromptTemplate(template, Map.of(""format"", format)).create(); Generation generation = chatModel.call(prompt).getResult(); List<ActorsFilms> actorsFilms = outputConverter.convert(generation.getOutput().getContent()); Map Output Converter: The following snippet shows how to use MapOutputConverter to convert the model output to a list of numbers in a map. Map<String, Object> result = ChatClient.create(chatModel).prompt() .user(u -> u.text(""Provide me a List of {subject}"") .param(""subject"", ""an array of numbers from 1 to 9 under they key name 'numbers'"")) .call() .entity(new ParameterizedTypeReference<Map<String, Object>>() {}); or using the low-level ChatModel API directly: MapOutputConverter mapOutputConverter = new MapOutputConverter(); String format = mapOutputConverter.getFormat(); String template = """""" Provide me a List of {subject} {format} """"""; Prompt prompt = new PromptTemplate(template, Map.of(""subject"", ""an array of numbers from 1 to 9 under they key name 'numbers'"", ""format"", format)).create(); Generation generation = chatModel.call(prompt).getResult(); Map<String, Object> result = mapOutputConverter.convert(generation.getOutput().getContent()); List Output Converter: The following snippet shows how to use ListOutputConverter to convert the model output into a list of ice cream flavors. List<String> flavors = ChatClient.create(chatModel).prompt() .user(u -> u.text(""List five {subject}"") .param(""subject"", ""ice cream flavors"")) .call() .entity(new ListOutputConverter(new DefaultConversionService())); or using the low-level ChatModel API directly: ListOutputConverter listOutputConverter = new ListOutputConverter(new DefaultConversionService()); String format = listOutputConverter.getFormat(); String template = """""" List five {subject} {format} """"""; Prompt prompt = new PromptTemplate(template, Map.of(""subject"", ""ice cream flavors"", ""format"", format)).create(); Generation generation = this.chatModel.call(prompt).getResult(); List<String> list = listOutputConverter.convert(generation.getOutput().getContent()); Supported AI Models: The following AI Models have been tested to support List, Map and Bean structured outputs. Model Integration Tests / Samples OpenAI(chat/openai-chat.html) OpenAiChatModelIT(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-openai/src/test/java/org/springframework/ai/openai/chat/OpenAiChatModelIT.java) Anthropic Claude 3(chat/anthropic-chat.html) AnthropicChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-anthropic/src/test/java/org/springframework/ai/anthropic/AnthropicChatModelIT.java) Azure OpenAI(chat/azure-openai-chat.html) AzureOpenAiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-azure-openai/src/test/java/org/springframework/ai/azure/openai/AzureOpenAiChatModelIT.java) Mistral AI(chat/mistralai-chat.html) MistralAiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-mistral-ai/src/test/java/org/springframework/ai/mistralai/MistralAiChatModelIT.java) Ollama(chat/ollama-chat.html) OllamaChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-ollama/src/test/java/org/springframework/ai/ollama/OllamaChatModelIT.java) Vertex AI Gemini(chat/vertexai-gemini-chat.html) VertexAiGeminiChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-vertex-ai-gemini/src/test/java/org/springframework/ai/vertexai/gemini/VertexAiGeminiChatModelIT.java) Bedrock Anthropic 2(chat/bedrock/bedrock-anthropic.html) BedrockAnthropicChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/test/java/org/springframework/ai/bedrock/anthropic/BedrockAnthropicChatModelIT.java) Bedrock Anthropic 3(chat/bedrock/bedrock-anthropic3.html) BedrockAnthropic3ChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/test/java/org/springframework/ai/bedrock/anthropic3/BedrockAnthropic3ChatModelIT.java) Bedrock Cohere(chat/bedrock/bedrock-cohere.html) BedrockCohereChatModelIT.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/test/java/org/springframework/ai/bedrock/cohere/BedrockCohereChatModelIT.java) Bedrock Llama(chat/bedrock/bedrock-llama.html) BedrockLlamaChatModelIT.java.java(https://github.com/spring-projects/spring-ai/blob/main/models/spring-ai-bedrock/src/test/java/org/springframework/ai/bedrock/llama/BedrockLlamaChatModelIT.java) Built-in JSON mode: Some AI Models provide dedicated configuration options to generate structured (usually JSON) output. OpenAI Structured Outputs(chat/openai-chat.html#_structured_outputs) can ensure your model generates responses conforming strictly to your provided JSON Schema. You can choose between the JSON_OBJECT that guarantees the message the model generates is valid JSON or JSON_SCHEMA with a supplied schema that guarantees the model will generate a response that matches your supplied schema ( spring.ai.openai.chat.options.responseFormat option). Azure OpenAI(chat/azure-openai-chat.html) - provides a spring.ai.azure.openai.chat.options.responseFormat options specifying the format that the model must output. Setting to { ""type"": ""json_object"" } enables JSON mode, which guarantees the message the model generates is valid JSON. Ollama(chat/ollama-chat.html) - provides a spring.ai.ollama.chat.options.format option to specify the format to return a response in. Currently, the only accepted value is json . Mistral AI(chat/mistralai-chat.html) - provides a spring.ai.mistralai.chat.options.responseFormat option to specify the format to return a response in. Setting it to { ""type"": ""json_object"" } enables JSON mode, which guarantees the message the model generates is valid JSON. Prompts(prompt.html) ETL Pipeline(etl-pipeline.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/etl-pipeline.html","ETL Pipeline: The Extract, Transform, and Load (ETL) framework serves as the backbone of data processing within the Retrieval Augmented Generation (RAG) use case. The ETL pipeline orchestrates the flow from raw data sources to a structured vector store, ensuring data is in the optimal format for retrieval by the AI model. The RAG use case is text to augment the capabilities of generative models by retrieving relevant information from a body of data to enhance the quality and relevance of the generated output. API Overview: The ETL pipelines creates, transforms and stores Document instances. The Document class contains text, metadata and optionally additionall media types like images, audio and video. There are three main components of the ETL pipeline, DocumentReader that implements Supplier<List<Document>> DocumentTransformer that implements Function<List<Document>, List<Document>> DocumentWriter that implements Consumer<List<Document>> The Document class content is created from PDFs, text files and other document types throught the help of DocumentReader . To construct a simple ETL pipeline, you can chain together an instance of each type. Let’s say we have the following instances of those three ETL types PagePdfDocumentReader an implementation of DocumentReader TokenTextSplitter an implementation of DocumentTransformer VectorStore an implementation of DocumentWriter To perform the basic loading of data into a Vector Database for use with the Retrieval Augmented Generation pattern, use the following code in Java function style syntax. vectorStore.accept(tokenTextSplitter.apply(pdfReader.get())); Alternatively, you can use method names that are more naturally expressive for the domain vectorStore.write(tokenTextSplitter.split(pdfReader.read())); ETL Interfaces: The ETL pipeline is composed of the following interfaces and implementations. Detailed ETL class diagram is shown in the ETL Class Diagram(#etl-class-diagram) section. DocumentReader: Provides a source of documents from diverse origins. public interface DocumentReader extends Supplier<List<Document>> { default List<Document> read() { return get(); } } DocumentTransformer: Transforms a batch of documents as part of the processing workflow. public interface DocumentTransformer extends Function<List<Document>, List<Document>> { default List<Document> transform(List<Document> transform) { return apply(transform); } } DocumentWriter: Manages the final stage of the ETL process, preparing documents for storage. public interface DocumentWriter extends Consumer<List<Document>> { default void write(List<Document> documents) { accept(documents); } } ETL Class Diagram: The following class diagram illustrates the ETL interfaces and implementations. DocumentReaders: JSON: The JsonReader processes JSON documents, converting them into a list of Document objects. Example: @Component class MyJsonReader { private final Resource resource; MyAiAppComponent(@Value(""classpath:bikes.json"") Resource resource) { this.resource = resource; } List<Document> loadJsonAsDocuments() { JsonReader jsonReader = new JsonReader(resource, ""description"", ""content""); return jsonReader.get(); } } Constructor Options: The JsonReader provides several constructor options: JsonReader(Resource resource) JsonReader(Resource resource, String…​ jsonKeysToUse) JsonReader(Resource resource, JsonMetadataGenerator jsonMetadataGenerator, String…​ jsonKeysToUse) Parameters: resource : A Spring Resource object pointing to the JSON file. jsonKeysToUse : An array of keys from the JSON that should be used as the text content in the resulting Document objects. jsonMetadataGenerator : An optional JsonMetadataGenerator to create metadata for each Document . Behavior: The JsonReader processes JSON content as follows: It can handle both JSON arrays and single JSON objects. For each JSON object (either in an array or a single object): It extracts the content based on the specified jsonKeysToUse . If no keys are specified, it uses the entire JSON object as content. It generates metadata using the provided JsonMetadataGenerator (or an empty one if not provided). It creates a Document object with the extracted content and metadata. Example JSON Structure: [ { ""id"": 1, ""brand"": ""Trek"", ""description"": ""A high-performance mountain bike for trail riding."" }, { ""id"": 2, ""brand"": ""Cannondale"", ""description"": ""An aerodynamic road bike for racing enthusiasts."" } ] In this example, if the JsonReader is configured with ""description"" as the jsonKeysToUse , it will create Document objects where the content is the value of the ""description"" field for each bike in the array. Notes: The JsonReader uses Jackson for JSON parsing. It can handle large JSON files efficiently by using streaming for arrays. If multiple keys are specified in jsonKeysToUse , the content will be a concatenation of the values for those keys. The reader is flexible and can be adapted to various JSON structures by customizing the jsonKeysToUse and JsonMetadataGenerator . Text: The TextReader processes plain text documents, converting them into a list of Document objects. Example: @Component class MyTextReader { private final Resource resource; MyTextReader(@Value(""classpath:text-source.txt"") Resource resource) { this.resource = resource; } List<Document> loadText() { TextReader textReader = new TextReader(resource); textReader.getCustomMetadata().put(""filename"", ""text-source.txt""); return textReader.read(); } } Constructor Options: The TextReader provides two constructor options: TextReader(String resourceUrl) TextReader(Resource resource) Parameters: resourceUrl : A string representing the URL of the resource to be read. resource : A Spring Resource object pointing to the text file. Configuration: setCharset(Charset charset) : Sets the character set used for reading the text file. Default is UTF-8. getCustomMetadata() : Returns a mutable map where you can add custom metadata for the documents. Behavior: The TextReader processes text content as follows: It reads the entire content of the text file into a single Document object. The content of the file becomes the content of the Document . Metadata is automatically added to the Document : charset : The character set used to read the file (default: ""UTF-8""). source : The filename of the source text file. Any custom metadata added via getCustomMetadata() is included in the Document . Notes: The TextReader reads the entire file content into memory, so it may not be suitable for very large files. If you need to split the text into smaller chunks, you can use a text splitter like TokenTextSplitter after reading the document: List<Document> documents = textReader.get(); List<Document> splitDocuments = new TokenTextSplitter().apply(documents); The reader uses Spring’s Resource abstraction, allowing it to read from various sources (classpath, file system, URL, etc.). Custom metadata can be added to all documents created by the reader using the getCustomMetadata() method. Markdown: The MarkdownDocumentReader processes Markdown documents, converting them into a list of Document objects. Example: @Component class MyMarkdownReader { private final Resource resource; MyMarkdownReader(@Value(""classpath:code.md"") Resource resource) { this.resource = resource; } List<Document> loadMarkdown() { MarkdownDocumentReaderConfig config = MarkdownDocumentReaderConfig.builder() .withHorizontalRuleCreateDocument(true) .withIncludeCodeBlock(false) .withIncludeBlockquote(false) .withAdditionalMetadata(""filename"", ""code.md"") .build(); MarkdownDocumentReader reader = new MarkdownDocumentReader(resource, config); return reader.get(); } } The MarkdownDocumentReaderConfig allows you to customize the behavior of the MarkdownDocumentReader: horizontalRuleCreateDocument : When set to true , horizontal rules in the Markdown will create new Document objects. includeCodeBlock : When set to true , code blocks will be included in the same Document as the surrounding text. When false , code blocks create separate Document objects. includeBlockquote : When set to true , blockquotes will be included in the same Document as the surrounding text. When false , blockquotes create separate Document objects. additionalMetadata : Allows you to add custom metadata to all created Document objects. Sample Document: code.md: This is a Java sample application: ```java package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } ``` Markdown also provides the possibility to `use inline code formatting throughout` the entire sentence. --- Another possibility is to set block code without specific highlighting: ``` ./mvnw spring-javaformat:apply ``` Behavior: The MarkdownDocumentReader processes the Markdown content and creates Document objects based on the configuration: Headers become metadata in the Document objects. Paragraphs become the content of Document objects. Code blocks can be separated into their own Document objects or included with surrounding text. Blockquotes can be separated into their own Document objects or included with surrounding text. Horizontal rules can be used to split the content into separate Document objects. The reader preserves formatting like inline code, lists, and text styling within the content of the Document objects. PDF Page: The PagePdfDocumentReader uses Apache PdfBox library to parse PDF documents Add the dependency to your project using Maven or Gradle. <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-pdf-document-reader</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-pdf-document-reader' } Example: @Component public class MyPagePdfDocumentReader { List<Document> getDocsFromPdf() { PagePdfDocumentReader pdfReader = new PagePdfDocumentReader(""classpath:/sample1.pdf"", PdfDocumentReaderConfig.builder() .withPageTopMargin(0) .withPageExtractedTextFormatter(ExtractedTextFormatter.builder() .withNumberOfTopTextLinesToDelete(0) .build()) .withPagesPerDocument(1) .build()); return pdfReader.read(); } } PDF Paragraph: The ParagraphPdfDocumentReader uses the PDF catalog (e.g. TOC) information to split the input PDF into text paragraphs and output a single Document per paragraph. NOTE: Not all PDF documents contain the PDF catalog. Dependencies: Add the dependency to your project using Maven or Gradle. <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-pdf-document-reader</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-pdf-document-reader' } Example: @Component public class MyPagePdfDocumentReader { List<Document> getDocsFromPdfwithCatalog() { ParagraphPdfDocumentReader pdfReader = new ParagraphPdfDocumentReader(""classpath:/sample1.pdf"", PdfDocumentReaderConfig.builder() .withPageTopMargin(0) .withPageExtractedTextFormatter(ExtractedTextFormatter.builder() .withNumberOfTopTextLinesToDelete(0) .build()) .withPagesPerDocument(1) .build()); return pdfReader.read(); } } Tika (DOCX, PPTX, HTML…​): The TikaDocumentReader uses Apache Tika to extract text from a variety of document formats, such as PDF, DOC/DOCX, PPT/PPTX, and HTML. For a comprehensive list of supported formats, refer to the Tika documentation(https://tika.apache.org/2.9.0/formats.html) . Dependencies: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-tika-document-reader</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-tika-document-reader' } Example: @Component class MyTikaDocumentReader { private final Resource resource; MyTikaDocumentReader(@Value(""classpath:/word-sample.docx"") Resource resource) { this.resource = resource; } List<Document> loadText() { TikaDocumentReader tikaDocumentReader = new TikaDocumentReader(resource); return tikaDocumentReader.read(); } } Transformers: TextSplitter: The TextSplitter an abstract base class that helps divides documents to fit the AI model’s context window. TokenTextSplitter: The TokenTextSplitter is an implementation of TextSplitter that splits text into chunks based on token count, using the `CL100K_BASE encoding. Usage: @Component class MyTokenTextSplitter { public List<Document> splitDocuments(List<Document> documents) { TokenTextSplitter splitter = new TokenTextSplitter(); return splitter.apply(documents); } public List<Document> splitCustomized(List<Document> documents) { TokenTextSplitter splitter = new TokenTextSplitter(1000, 400, 10, 5000, true); return splitter.apply(documents); } } Constructor Options: The TokenTextSplitter provides two constructor options: TokenTextSplitter() : Creates a splitter with default settings. TokenTextSplitter(int defaultChunkSize, int minChunkSizeChars, int minChunkLengthToEmbed, int maxNumChunks, boolean keepSeparator) Parameters: defaultChunkSize : The target size of each text chunk in tokens (default: 800). minChunkSizeChars : The minimum size of each text chunk in characters (default: 350). minChunkLengthToEmbed : The minimum length of a chunk to be included (default: 5). maxNumChunks : The maximum number of chunks to generate from a text (default: 10000). keepSeparator : Whether to keep separators (like newlines) in the chunks (default: true). Behavior: The TokenTextSplitter processes text content as follows: It encodes the input text into tokens using the CL100K_BASE encoding. It splits the encoded text into chunks based on the defaultChunkSize . For each chunk: It decodes the chunk back into text. It attempts to find a suitable break point (period, question mark, exclamation mark, or newline) after the minChunkSizeChars . If a break point is found, it truncates the chunk at that point. It trims the chunk and optionally removes newline characters based on the keepSeparator setting. If the resulting chunk is longer than minChunkLengthToEmbed , it’s added to the output. This process continues until all tokens are processed or maxNumChunks is reached. Any remaining text is added as a final chunk if it’s longer than minChunkLengthToEmbed . Example: Document doc1 = new Document(""This is a long piece of text that needs to be split into smaller chunks for processing."", Map.of(""source"", ""example.txt"")); Document doc2 = new Document(""Another document with content that will be split based on token count."", Map.of(""source"", ""example2.txt"")); TokenTextSplitter splitter = new TokenTextSplitter(); List<Document> splitDocuments = splitter.apply(List.of(doc1, doc2)); for (Document doc : splitDocuments) { System.out.println(""Chunk: "" + doc.getContent()); System.out.println(""Metadata: "" + doc.getMetadata()); } Notes: The TokenTextSplitter uses the CL100K_BASE encoding from the jtokkit library, which is compatible with newer OpenAI models. The splitter attempts to create semantically meaningful chunks by breaking at sentence boundaries where possible. Metadata from the original documents is preserved and copied to all chunks derived from that document. The content formatter (if set) from the original document is also copied to the derived chunks if copyContentFormatter is set to true (default behavior). This splitter is particularly useful for preparing text for large language models that have token limits, ensuring that each chunk is within the model’s processing capacity. === ContentFormatTransformer Ensures uniform content formats across all documents. KeywordMetadataEnricher: The KeywordMetadataEnricher is a DocumentTransformer that uses a generative AI model to extract keywords from document content and add them as metadata. Usage: @Component class MyKeywordEnricher { private final ChatModel chatModel; MyKeywordEnricher(ChatModel chatModel) { this.chatModel = chatModel; } List<Document> enrichDocuments(List<Document> documents) { KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, 5); return enricher.apply(documents); } } Constructor: The KeywordMetadataEnricher constructor takes two parameters: ChatModel chatModel : The AI model used for generating keywords. int keywordCount : The number of keywords to extract for each document. Behavior: The KeywordMetadataEnricher processes documents as follows: For each input document, it creates a prompt using the document’s content. It sends this prompt to the provided ChatModel to generate keywords. The generated keywords are added to the document’s metadata under the key ""excerpt_keywords"". The enriched documents are returned. Customization: The keyword extraction prompt can be customized by modifying the KEYWORDS_TEMPLATE constant in the class. The default template is: \{context_str}. Give %s unique keywords for this document. Format as comma separated. Keywords: Where {context_str} is replaced with the document content, and %s is replaced with the specified keyword count. Example: ChatModel chatModel = // initialize your chat model KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, 5); Document doc = new Document(""This is a document about artificial intelligence and its applications in modern technology.""); List<Document> enrichedDocs = enricher.apply(List.of(doc)); Document enrichedDoc = enrichedDocs.get(0); String keywords = (String) enrichedDoc.getMetadata().get(""excerpt_keywords""); System.out.println(""Extracted keywords: "" + keywords); Notes: The KeywordMetadataEnricher requires a functioning ChatModel to generate keywords. The keyword count must be 1 or greater. The enricher adds the ""excerpt_keywords"" metadata field to each processed document. The generated keywords are returned as a comma-separated string. This enricher is particularly useful for improving document searchability and for generating tags or categories for documents. SummaryMetadataEnricher: The SummaryMetadataEnricher is a DocumentTransformer that uses a generative AI model to create summaries for documents and add them as metadata. It can generate summaries for the current document, as well as adjacent documents (previous and next). Usage: @Configuration class EnricherConfig { @Bean public SummaryMetadataEnricher summaryMetadata(OpenAiChatModel aiClient) { return new SummaryMetadataEnricher(aiClient, List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT)); } } @Component class MySummaryEnricher { private final SummaryMetadataEnricher enricher; MySummaryEnricher(SummaryMetadataEnricher enricher) { this.enricher = enricher; } List<Document> enrichDocuments(List<Document> documents) { return enricher.apply(documents); } } Constructor: The SummaryMetadataEnricher provides two constructors: SummaryMetadataEnricher(ChatModel chatModel, List<SummaryType> summaryTypes) SummaryMetadataEnricher(ChatModel chatModel, List<SummaryType> summaryTypes, String summaryTemplate, MetadataMode metadataMode) Parameters: chatModel : The AI model used for generating summaries. summaryTypes : A list of SummaryType enum values indicating which summaries to generate (PREVIOUS, CURRENT, NEXT). summaryTemplate : A custom template for summary generation (optional). metadataMode : Specifies how to handle document metadata when generating summaries (optional). Behavior: The SummaryMetadataEnricher processes documents as follows: For each input document, it creates a prompt using the document’s content and the specified summary template. It sends this prompt to the provided ChatModel to generate a summary. Depending on the specified summaryTypes , it adds the following metadata to each document: section_summary : Summary of the current document. prev_section_summary : Summary of the previous document (if available and requested). next_section_summary : Summary of the next document (if available and requested). The enriched documents are returned. Customization: The summary generation prompt can be customized by providing a custom summaryTemplate . The default template is: """""" Here is the content of the section: {context_str} Summarize the key topics and entities of the section. Summary: """""" Example: ChatModel chatModel = // initialize your chat model SummaryMetadataEnricher enricher = new SummaryMetadataEnricher(chatModel, List.of(SummaryType.PREVIOUS, SummaryType.CURRENT, SummaryType.NEXT)); Document doc1 = new Document(""Content of document 1""); Document doc2 = new Document(""Content of document 2""); List<Document> enrichedDocs = enricher.apply(List.of(doc1, doc2)); // Check the metadata of the enriched documents for (Document doc : enrichedDocs) { System.out.println(""Current summary: "" + doc.getMetadata().get(""section_summary"")); System.out.println(""Previous summary: "" + doc.getMetadata().get(""prev_section_summary"")); System.out.println(""Next summary: "" + doc.getMetadata().get(""next_section_summary"")); } The provided example demonstrates the expected behavior: For a list of two documents, both documents receive a section_summary . The first document receives a next_section_summary but no prev_section_summary . The second document receives a prev_section_summary but no next_section_summary . The section_summary of the first document matches the prev_section_summary of the second document. The next_section_summary of the first document matches the section_summary of the second document. Notes: The SummaryMetadataEnricher requires a functioning ChatModel to generate summaries. The enricher can handle document lists of any size, properly handling edge cases for the first and last documents. This enricher is particularly useful for creating context-aware summaries, allowing for better understanding of document relationships in a sequence. The MetadataMode parameter allows control over how existing metadata is incorporated into the summary generation process. Writers: File: The FileDocumentWriter is a DocumentWriter implementation that writes the content of a list of Document objects into a file. Usage: @Component class MyDocumentWriter { public void writeDocuments(List<Document> documents) { FileDocumentWriter writer = new FileDocumentWriter(""output.txt"", true, MetadataMode.ALL, false); writer.accept(documents); } } Constructors: The FileDocumentWriter provides three constructors: FileDocumentWriter(String fileName) FileDocumentWriter(String fileName, boolean withDocumentMarkers) FileDocumentWriter(String fileName, boolean withDocumentMarkers, MetadataMode metadataMode, boolean append) Parameters: fileName : The name of the file to write the documents to. withDocumentMarkers : Whether to include document markers in the output (default: false). metadataMode : Specifies what document content to be written to the file (default: MetadataMode.NONE). append : If true, data will be written to the end of the file rather than the beginning (default: false). Behavior: The FileDocumentWriter processes documents as follows: It opens a FileWriter for the specified file name. For each document in the input list: If withDocumentMarkers is true, it writes a document marker including the document index and page numbers. It writes the formatted content of the document based on the specified metadataMode . The file is closed after all documents have been written. Document Markers: When withDocumentMarkers is set to true, the writer includes markers for each document in the following format: ### Doc: [index], pages:[start_page_number,end_page_number] Metadata Handling: The writer uses two specific metadata keys: page_number : Represents the starting page number of the document. end_page_number : Represents the ending page number of the document. These are used when writing document markers. Example: List<Document> documents = // initialize your documents FileDocumentWriter writer = new FileDocumentWriter(""output.txt"", true, MetadataMode.ALL, true); writer.accept(documents); This will write all documents to ""output.txt"", including document markers, using all available metadata, and appending to the file if it already exists. Notes: The writer uses FileWriter , so it writes text files with the default character encoding of the operating system. If an error occurs during writing, a RuntimeException is thrown with the original exception as its cause. The metadataMode parameter allows control over how existing metadata is incorporated into the written content. This writer is particularly useful for debugging or creating human-readable outputs of document collections. VectorStore: Provides integration with various vector stores. See Vector DB Documentation(vectordbs.html) for a full listing. Structured Output(structured-output-converter.html) Evaluation Testing(testing.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/testing.html","Evaluation Testing: Testing AI applications requires evaluating the generated content to ensure the AI model has not produced a hallucinated response. One method to evaluate the response is to use the AI model itself for evaluation. Select the best AI model for the evaluation, which may not be the same model used to generate the response. The Spring AI interface for evaluating responses is Evaluator , defined as: @FunctionalInterface public interface Evaluator { EvaluationResponse evaluate(EvaluationRequest evaluationRequest) } The input to the evaluation is the EvaluationRequest defined as public class EvaluationRequest { private final String userText; private final List<Content> dataList; private final String responseContent; public EvaluationRequest(String userText, List<Content> dataList, String responseContent) { this.userText = userText; this.dataList = dataList; this.responseContent = responseContent; } ... } userText : The raw input from the user as a String dataList : Contextual data, such as from Retrieval Augmented Generation, appended to the raw input. responseContent : The AI model’s response content as a String RelevancyEvaluator: One implementation is the RelevancyEvaluator , which uses the AI model for evaluation. More implementations will be available in future releases. The RelevancyEvaluator uses the input ( userText ) and the AI model’s output ( chatResponse ) to ask the question: Your task is to evaluate if the response for the query is in line with the context information provided.\n You have two options to answer. Either YES/ NO.\n Answer - YES, if the response for the query is in line with context information otherwise NO.\n Query: \n {query}\n Response: \n {response}\n Context: \n {context}\n Answer: "" Here is an example of a JUnit test that performs a RAG query over a PDF document loaded into a Vector Store and then evaluates if the response is relevant to the user text. @Test void testEvaluation() { dataController.delete(); dataController.load(); String userText = ""What is the purpose of Carina?""; String responseContent = ChatClient.builder(chatModel) .build().prompt() .advisors(new QuestionAnswerAdvisor(vectorStore, SearchRequest.defaults())) .user(userText) .call() .content(); var relevancyEvaluator = new RelevancyEvaluator(ChatClient.builder(chatModel)); EvaluationRequest evaluationRequest = new EvaluationRequest(userText, (List<Content>) response.getMetadata().get(QuestionAnswerAdvisor.RETRIEVED_DOCUMENTS), responseContent); EvaluationResponse evaluationResponse = relevancyEvaluator.evaluate(evaluationRequest); assertTrue(evaluationResponse.isPass(), ""Response is not relevant to the question""); } The code above is from the example application located here(https://github.com/rd-1-2022/ai-azure-rag.git) . ETL Pipeline(etl-pipeline.html) Generic Model API(generic-model.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/generic-model.html","Generic Model API: In order to provide a foundation for all AI Models, the Generic Model API was created. This makes it easy to contribute new AI Model support to Spring AI by following a common pattern. The following sections walk through this API. Class Diagram: Model: The Model interface provides a generic API for invoking AI models. It is designed to handle the interaction with various types of AI models by abstracting the process of sending requests and receiving responses. The interface uses Java generics to accommodate different types of requests and responses, enhancing flexibility and adaptability across different AI model implementations. The interface is defined below: public interface Model<TReq extends ModelRequest<?>, TRes extends ModelResponse<?>> { /** * Executes a method call to the AI model. * @param request the request object to be sent to the AI model * @return the response from the AI model */ TRes call(TReq request); } StreamingModel: The StreamingModel interface provides a generic API for invoking an AI model with streaming response. It abstracts the process of sending requests and receiving a streaming response. The interface uses Java generics to accommodate different types of requests and responses, enhancing flexibility and adaptability across different AI model implementations. public interface StreamingModel<TReq extends ModelRequest<?>, TResChunk extends ModelResponse<?>> { /** * Executes a method call to the AI model. * @param request the request object to be sent to the AI model * @return the streaming response from the AI model */ Flux<TResChunk> stream(TReq request); } ModelRequest: The ModelRequest interface represents a request to an AI model. It encapsulates the necessary information required to interact with an AI model, including instructions or inputs (of generic type T ) and additional model options. It provides a standardized way to send requests to AI models, ensuring that all necessary details are included and can be easily managed. public interface ModelRequest<T> { /** * Retrieves the instructions or input required by the AI model. * @return the instructions or input required by the AI model */ T getInstructions(); // required input /** * Retrieves the customizable options for AI model interactions. * @return the customizable options for AI model interactions */ ModelOptions getOptions(); } ModelOptions: The ModelOptions interface represents the customizable options for AI model interactions. This marker interface allows for the specification of various settings and parameters that can influence the behavior and output of AI models. It is designed to provide flexibility and adaptability in different AI scenarios, ensuring that the AI models can be fine-tuned according to specific requirements. public interface ModelOptions { } ModelResponse: The ModelResponse interface represents the response received from an AI model. This interface provides methods to access the main result or a list of results generated by the AI model, along with the response metadata. It serves as a standardized way to encapsulate and manage the output from AI models, ensuring easy retrieval and processing of the generated information. public interface ModelResponse<T extends ModelResult<?>> { /** * Retrieves the result of the AI model. * @return the result generated by the AI model */ T getResult(); /** * Retrieves the list of generated outputs by the AI model. * @return the list of generated outputs */ List<T> getResults(); /** * Retrieves the response metadata associated with the AI model's response. * @return the response metadata */ ResponseMetadata getMetadata(); } ModelResult: The ModelResult interface provides methods to access the main output of the AI model and the metadata associated with this result. It is designed to offer a standardized and comprehensive way to handle and interpret the outputs generated by AI models. public interface ModelResult<T> { /** * Retrieves the output generated by the AI model. * @return the output generated by the AI model */ T getOutput(); /** * Retrieves the metadata associated with the result of an AI model. * @return the metadata associated with the result */ ResultMetadata getMetadata(); } Evaluation Testing(testing.html) Observability(../observabilty/index.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/observabilty/index.html","Observability: Spring AI builds upon the observability features in the Spring ecosystem to provide insights into AI-related operations. Spring AI provides metrics and tracing capabilities for its core components: ChatClient (including Advisors), ChatModel, EmbeddingModel, ImageModel and VectorStore. Low cardinality keys will be added to metrics and traces, while high cardinality keys will only be added to traces. ChatClient: Table 1. Low cardinality Keys Name Description gen_ai.operation.name framework gen_ai.system spring_ai spring.ai.kind Spring AI kind - chat_client spring.ai.chat.client.stream Is the chat model response a stream - true or false Table 2. High cardinality Keys Name Description spring.ai.chat.client.advisor.params Map of advisor parameters. spring.ai.chat.client.advisors List of configured chat client advisors. spring.ai.chat.client.stream Is the chat model response a stream. spring.ai.chat.client.system.params Chat client system parameters. spring.ai.chat.client.system.text Chat client system text. spring.ai.chat.client.tool.function.names Enabled tool function names. spring.ai.chat.client.tool.functioncallbacks List of configured chat client function callbacks. spring.ai.chat.client.user.params Chat client user parameters. spring.ai.chat.client.user.text Chat client user text. ChatClient input data: The ChatClient input data is typically too big to be included in an observation as span attributes. The preferred way to store large data it is as span events, which are supported by OpenTelemetry but not yet surfaced through the Micrometer APIs. Spring AI supports storing these fields as events in OpenTelemetry and will provide a more general event based solution once the issue github.com/micrometer-metrics/micrometer/issues/5238(https://github.com/micrometer-metrics/micrometer/issues/5238) is resolved. Property Description Default spring.ai.chat.client.observations.include-input Enables the inclusion of the input content in the observations. This brings the risk of exposing sensitive or private information. Please, be careful! false ChatClient Advisors: Table 3. Low Cardinality Keys Name Description spring.ai.kind Spring AI kind - chat_client_advisor spring.ai.chat.client.advisor.type Where the advisor applies it’s logic in the request processing, one of BEFORE , AFTER , or AROUND . Table 4. High Cardinality Keys Name Description spring.ai.chat.client.advisor.name Name of the advisor ChatModel: Observability features are currently supported only for ChatModel and EmbeddingModel implementations from the following AI model providers: OpenAI, Ollama, Anthropic, and Mistral. Additional AI model providers will be supported in a future release. Table 5. Low Cardinality Keys Name Description gen_ai.operation.name The name of the operation being performed. gen_ai.system The model provider as identified by the client instrumentation. gen_ai.request.model The name of the model a request is being made to. gen_ai.response.model The name of the model that generated the response. Table 6. High Cardinality Keys Name Description gen_ai.request.frequency_penalty The frequency penalty setting for the model request. gen_ai.request.max_tokens The maximum number of tokens the model generates for a request. gen_ai.request.presence_penalty The presence penalty setting for the model request. gen_ai.request.stop_sequences List of sequences that the model will use to stop generating further tokens. gen_ai.request.temperature The temperature setting for the model request. gen_ai.request.top_k The top_k sampling setting for the model request. gen_ai.request.top_p The top_p sampling setting for the model request. gen_ai.response.finish_reasons Reasons the model stopped generating tokens, corresponding to each generation received. gen_ai.response.id The unique identifier for the AI response. gen_ai.usage.input_tokens The number of tokens used in the model input (prompt). gen_ai.usage.output_tokens The number of tokens used in the model output (completion). gen_ai.usage.total_tokens The total number of tokens used in the model exchange. gen_ai.prompt The full prompt sent to the model. gen_ai.completion The full response received from the model. Chat prompt and completion data: The chat prompt and completion data are typically too big to be included in an observation as span attributes. The preferred way to store large data it is as span events, which are supported by OpenTelemetry but not yet surfaced through the Micrometer APIs. Spring AI supports storing these fields as events in OpenTelemetry and will provide a more general event based solution once the issue github.com/micrometer-metrics/micrometer/issues/5238(https://github.com/micrometer-metrics/micrometer/issues/5238) is resolved. Property Description Default spring.ai.chat.observations.include-prompt true or false false spring.ai.chat.observations.include-completion true or false false EmbeddingModel: Observability features are currently supported only for ChatModel and EmbeddingModel implementations from the following AI model providers: OpenAI, Ollama, Anthropic, and Mistral. Additional AI model providers will be supported in a future release. Table 7. Low Cardinality Keys Name Description gen_ai.operation.name The name of the operation being performed. gen_ai.system The model provider as identified by the client instrumentation. gen_ai.request.model The name of the model a request is being made to. gen_ai.response.model The name of the model that generated the response. Table 8. High Cardinality Keys Name Description gen_ai.request.embedding.dimensions The number of dimensions the resulting output embeddings have. gen_ai.usage.input_tokens The number of tokens used in the model input. gen_ai.usage.total_tokens The total number of tokens used in the model exchange. ImageModel: Table 9. Low Cardinality Keys Name Description gen_ai.operation.name The name of the operation being performed. gen_ai.system The model provider as identified by the client instrumentation. gen_ai.request.model The name of the model a request is being made to. Table 10. High Cardinality Keys Name Description gen_ai.request.image.response_format The format in which the generated image is returned. gen_ai.request.image.size The size of the image to generate. gen_ai.request.image.style The style of the image to generate. gen_ai.response.id The unique identifier for the AI response. gen_ai.response.model The name of the model that generated the response. gen_ai.usage.input_tokens The number of tokens used in the model input (prompt). gen_ai.usage.output_tokens The number of tokens used in the model output (generation). gen_ai.usage.total_tokens The total number of tokens used in the model exchange. gen_ai.prompt The full prompt sent to the model. Image prompt data: The image prompt data are typically too big to be included in an observation as span attributes. The preferred way to store large data it is as span events, which are supported by OpenTelemetry but not yet surfaced through the Micrometer APIs. Spring AI supports storing these fields as events in OpenTelemetry and will provide a more general event based solution once the issue github.com/micrometer-metrics/micrometer/issues/5238(https://github.com/micrometer-metrics/micrometer/issues/5238) is resolved. Property Description Default spring.ai.image.observations.include-prompt true or false false Vector Stores: All vector store implementations in Spring AI are instrumented to provide metrics and distributed tracing data through Micrometer. Table 11. Low Cardinality Keys Name Description spring.ai.kind Spring AI kind - vector_store db.system The database management system (DBMS) product as identified by the client instrumentation. One of pg_vector , azure , cassandra , chroma , elasticsearch , milvus , neo4j , opensearch , qdrant , redis , typesense , weaviate , pinecone , oracle , mongodb , gemfire , hana , simple db.operation.name The name of the operation or command being executed. One of add , delete , or query . Table 12. High Cardinality Keys Name Description db.collection.name The name of a collection (table, container) within the database. db.vector.dimension_count The dimension of the vector. db.vector.field_name The name field as of the vector (e.g. a field name). db.vector.query.filter The metadata filters used in the search query. db.namespace The namespace of the database. db.vector.query.content The content of the search query being executed. db.vector.query.response.documents Returned documents from a similarity search query. Needs to be enabled with auto-configuration and use of OpenTelemetry events. db.vector.similarity_metric The metric used in similarity search. db.vector.query.similarity_threshold Similarity threshold that accepts all search scores. A threshold value of 0.0 means any similarity is accepted or disable the similarity threshold filtering. A threshold value of 1.0 means an exact match is required. db.vector.query.top_k The top-k most similar vectors returned by a query. Vector Store response data: The Vector Store response data are typically too big to be included in an observation as span attributes. The preferred way to store large data it is as span events, which are supported by OpenTelemetry but not yet surfaced through the Micrometer APIs. Spring AI supports storing these fields as events in OpenTelemetry and will provide a more general event based solution once the issue github.com/micrometer-metrics/micrometer/issues/5238(https://github.com/micrometer-metrics/micrometer/issues/5238) is resolved. Property Description Default spring.ai.vectorstore.observations.include-query-response true or false false Generic Model API(../api/generic-model.html) Docker Compose(../api/docker-compose.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/docker-compose.html","Docker Compose: Spring AI provides Spring Boot auto-configuration for establishing a connection to a model service or vector store running via Docker Compose. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-spring-boot-docker-compose</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-spring-boot-docker-compose' } Refer to the Dependency Management(../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Service Connections: The following service connection factories are provided in the spring-ai-spring-boot-docker-compose module: Connection Details Matched on ChromaConnectionDetails Containers named chromadb/chroma , ghcr.io/chroma-core/chroma OllamaConnectionDetails Containers named ollama/ollama OpenSearchConnectionDetails Containers named opensearchproject/opensearch QdrantConnectionDetails Containers named qdrant/qdrant TypesenseConnectionDetails Containers named typesense/typesense WeaviateConnectionDetails Containers named semitechnologies/weaviate , cr.weaviate.io/semitechnologies/weaviate Observability(../observabilty/index.html) Testcontainers(testcontainers.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/testcontainers.html","Testcontainers: Spring AI provides Spring Boot auto-configuration for establishing a connection to a model service or vector store running via Testcontainers. To enable it, add the following dependency to your project’s Maven pom.xml file: <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-spring-boot-testcontainers</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-spring-boot-testcontainers' } Refer to the Dependency Management(../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Service Connections: The following service connection factories are provided in the spring-ai-spring-boot-testcontainers module: Connection Details Matched on ChromaConnectionDetails Containers of type ChromaDBContainer MilvusServiceClientConnectionDetails Containers of type MilvusContainer OllamaConnectionDetails Containers of type OllamaContainer OpenSearchConnectionDetails Containers of type OpensearchContainer QdrantConnectionDetails Containers of type QdrantContainer TypesenseConnectionDetails Containers named ""typesense/typesense"" WeaviateConnectionDetails Containers of type WeaviateContainer Docker Compose(docker-compose.html) Cloud Bindings(cloud-bindings.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/cloud-bindings.html","Cloud Bindings: Spring AI provides support for cloud bindings based on the foundations in spring-cloud-bindings(https://github.com/spring-cloud/spring-cloud-bindings) . This allows applications to specify a binding type for a provider and then express properties using a generic format. The spring-ai cloud bindings will process these properties and bind them to spring-ai native properties. For example, when using OpenAi , the binding type is openai . Using the property spring.ai.cloud.bindings.openai.enabled , the binding processor can be enabled or disabled. By default, when specifying a binding type, this property will be enabled. Configuration for api-key , uri , username , password , etc. can be specified and spring-ai will map them to the corresponding properties in the supported system. To enable cloud binding support, include the following dependency in the application. <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-spring-cloud-bindings</artifactId> </dependency> or to your Gradle build.gradle build file. dependencies { implementation 'org.springframework.ai:spring-ai-spring-cloud-bindings' } Refer to the Dependency Management(../getting-started.html#dependency-management) section to add the Spring AI BOM to your build file. Available Cloud Bindings: The following are the components for which the cloud binding support is currently available in the spring-ai-spring-clou-bindings module: Service Type Binding Type Source Properties Target Properties Chroma Vecor Store chroma uri , username , passwor spring.ai.vectorstore.chroma.client.host , spring.ai.vectorstore.chroma.client.port , spring.ai.vectorstore.chroma.client.username , spring.ai.vectorstore.chroma.client.host.password Ollama ollama uri spring.ai.ollama.base-url OpenAi openai api-key , uri spring.ai.openai.api-key , spring.ai.openai.base-url Weaviate weaviate uri , api-key spring.ai.vectorstore.weaviate.scheme , spring.ai.vectorstore.weaviate.host , spring.ai.vectorstore.weaviate.api-key Tanzu GenAI genai uri , api-key , model-capabilities ( chat and embedding ), model-name spring.ai.openai.chat.base-url , , spring.ai.openai.chat.api-key`, spring.ai.openai.chat.options.model , spring.ai.openai.embedding.base-url , , spring.ai.openai.embedding.api-key`, spring.ai.openai.embedding.options.model Testcontainers(testcontainers.html) Contribution Guidelines(../contribution-guidelines.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/contribution-guidelines.html","Contribution Guidelines: Code Formatting and Javadoc: Before submitting a PR, please run the following commands to ensure proper formatting and Javadoc processing ./mvnw spring-javaformat:apply javadoc:javadoc -Pjavadoc The -Pjavadoc is a profile that enables Javadoc processing so as to avoid a long build time when developing. Contributing a New AI Model Implementation: This section outlines the steps for contributing a new AI model implementation. AI models vary significantly, with diverse inputs and outputs — from chat models that translate text input into text output, to text-to-image models that generate images from text descriptions. Complex models may even handle multiple types of input and output, such as combining text, images, and videos to produce mixed media output. To contribute a new model, adhere to the following steps: Create a Low-Level Client API Class : If no existing Java client suits the AI model, you’ll need to develop a low-level client API class. This often involves utilizing the RestClient class from the Spring Framework, similar to the OpenAiApi class. Create a Model implementation Ensure your client conforms to the Generic Model API(https://docs.spring.io/spring-ai/reference/api/generic-model.html) . Use existing request and response classes if your model’s inputs and outputs are supported. If not, create new classes for the Generic Model API and establish a new Java package. Implement Auto-Configuration and a Spring Boot Starter : This step involves creating the necessary auto-configuration and Spring Boot Starter to easily instantiate the new model with Spring Boot applications. Write Tests : All new classes should be accompanied by comprehensive tests. Existing tests can serve as a useful reference for structuring and implementing your tests. Document Your Contribution : Ensure your documentation follows the existing format, For an example of the suggested structure and formatting, refer to the Open AI Chat documentation(https://docs.spring.io/spring-ai/reference/api/chat/openai-chat.html) . By following these guidelines, we can greatly expand the framework’s range of supported models while following a common implementation and documentation pattern. Cloud Bindings(api/cloud-bindings.html) Upgrading Notes(upgrade-notes.html)"
"https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/upgrade-notes.html","Upgrading Notes: Upgrading to 1.0.0.RC1: The type of the portable chat options ( frequencyPenalty , presencePenalty , temperature , topP ) has been changed from Float to Double . Upgrading to 1.0.0.M2: The configuration prefix for the Chroma Vector Store has been changes from spring.ai.vectorstore.chroma.store to spring.ai.vectorstore.chroma in order to align with the naming conventions of other vector stores. The default value of the initialize-schema property on vector stores capable of initializing a schema is now set to false . This implies that the applications now need to explicitly opt-in for schema initialization on supported vector stores, if the schema is expected to be created at application startup. Not all vector stores support this property. See the corresponding vector store documentation for more details. The following are the vector stores that currently don’t support the initialize-schema property. Hana Pinecone Weaviate In Bedrock Jurassic 2, the chat options countPenalty , frequencyPenalty , and presencePenalty have been renamed to countPenaltyOptions , frequencyPenaltyOptions , and presencePenaltyOptions . Furthermore, the type of the chat option stopSequences have been changed from String[] to List<String> . In Azure OpenAI, the type of the chat options frequencyPenalty and presencePenalty has been changed from Double to Float , consistently with all the other implementations. Upgrading to 1.0.0.M1: On our march to release 1.0.0 M1 we have made several breaking changes. Apologies, it is for the best! ChatClient changes: A major change was made that took the 'old' ChatClient and moved the functionality into ChatModel . The 'new' ChatClient now takes an instance of ChatModel . This was done do support a fluent API for creating and executing prompts in a style similar to other client classes in the Spring ecosystem, such as RestClient , WebClient , and JdbcClient . Refer to the [JavaDoc]( docs.spring.io/spring-ai/docs/1.0.0-SNAPSHOT/api/(https://docs.spring.io/spring-ai/docs/1.0.0-SNAPSHOT/api/) ) for more information on the Fluent API, proper reference documentation is coming shortly. We renamed the 'old' ModelClient to Model and renamed implementing classes, for example ImageClient was renamed to ImageModel . The Model implementation represent the portability layer that converts between the Spring AI API and the underlying AI Model API. Adapting to the changes: The ChatClient class is now in the package org.springframework.ai.chat.client Approach 1: Now, instead of getting an Autoconfigured ChatClient instance, you will get a ChatModel instance. The call method signatures after renaming remain the same. To adapt your code should refactor you code to change use of the type ChatClient to ChatModel Here is an example of existing code before the change @RestController public class OldSimpleAiController { private final ChatClient chatClient; public OldSimpleAiController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(""/ai/simple"") Map<String, String> completion(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatClient.call(message)); } } Now after the changes this will be @RestController public class SimpleAiController { private final ChatModel chatModel; public SimpleAiController(ChatModel chatModel) { this.chatModel = chatModel; } @GetMapping(""/ai/simple"") Map<String, String> completion(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of(""generation"", chatModel.call(message)); } } The renaming also applies to the classes * StreamingChatClient → StreamingChatModel * EmbeddingClient → EmbeddingModel * ImageClient → ImageModel * SpeechClient → SpeechModel * and similar for other <XYZ>Client classes Approach 2: In this approach you will use the new fluent API available on the 'new' ChatClient Here is an example of existing code before the change @RestController class OldSimpleAiController { ChatClient chatClient; OldSimpleAiController(ChatClient chatClient) { this.chatClient = chatClient; } @GetMapping(""/ai/simple"") Map<String, String> completion(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of( ""generation"", chatClient.call(message) ); } } Now after the changes this will be @RestController class SimpleAiController { private final ChatClient chatClient; SimpleAiController(ChatClient.Builder builder) { this.chatClient = builder.build(); } @GetMapping(""/ai/simple"") Map<String, String> completion(@RequestParam(value = ""message"", defaultValue = ""Tell me a joke"") String message) { return Map.of( ""generation"", chatClient.prompt().user(message).call().content() ); } } The ChatModel instance is made available to you through autoconfiguration. Approach 3: There is a tag in the GitHub repository called [v1.0.0-SNAPSHOT-before-chatclient-changes]( github.com/spring-projects/spring-ai/tree/v1.0.0-SNAPSHOT-before-chatclient-changes(https://github.com/spring-projects/spring-ai/tree/v1.0.0-SNAPSHOT-before-chatclient-changes) ) that you can checkout and do a local build to avoid updating any of your code until you are ready to migrate your code base. git checkout tags/v1.0.0-SNAPSHOT-before-chatclient-changes ./mvnw clean install -DskipTests Artifact name changes: Renamed POM artifact names: - spring-ai-qdrant → spring-ai-qdrant-store - spring-ai-cassandra → spring-ai-cassandra-store - spring-ai-pinecone → spring-ai-pinecone-store - spring-ai-redis → spring-ai-redis-store - spring-ai-qdrant → spring-ai-qdrant-store - spring-ai-gemfire → spring-ai-gemfire-store - spring-ai-azure-vector-store-spring-boot-starter → spring-ai-azure-store-spring-boot-starter - spring-ai-redis-spring-boot-starter → spring-ai-redis-store-spring-boot-starter Upgrading to 0.8.1: Former spring-ai-vertex-ai has been renamed to spring-ai-vertex-ai-palm2 and spring-ai-vertex-ai-spring-boot-starter has been renamed to spring-ai-vertex-ai-palm2-spring-boot-starter . So, you need to change the dependency from <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai</artifactId> </dependency> To <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-palm2</artifactId> </dependency> and the related Boot starter for the Palm2 model has changed from <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-spring-boot-starter</artifactId> </dependency> to <dependency> <groupId>org.springframework.ai</groupId> <artifactId>spring-ai-vertex-ai-palm2-spring-boot-starter</artifactId> </dependency> Renamed Classes (01.03.2024) VertexAiApi → VertexAiPalm2Api VertexAiClientChat → VertexAiPalm2ChatClient VertexAiEmbeddingClient → VertexAiPalm2EmbeddingClient VertexAiChatOptions → VertexAiPalm2ChatOptions Upgrading to 0.8.0: January 24, 2024 Update: Moving the prompt and messages and metadata packages to subpackages of org.sf.ai.chat New functionality is text to image clients. Classes are OpenAiImageModel and StabilityAiImageModel . See the integration tests for usage, docs are coming soon. A new package model that contains interfaces and base classes to support creating AI Model Clients for any input/output data type combination. At the moment the chat and image model packages implement this. We will be updating the embedding package to this new model soon. A new ""portable options"" design pattern. We wanted to provide as much portability in the ModelCall as possible across different chat based AI Models. There is a common set of generation options and then those that are specific to a model provider. A sort of ""duck typing"" approach is used. ModelOptions in the model package is a marker interface indicating implementations of this class will provide the options for a model. See ImageOptions , a subinterface that defines portable options across all text→image ImageModel implementations. Then StabilityAiImageOptions and OpenAiImageOptions provide the options specific to each model provider. All options classes are created via a fluent API builder all can be passed into the portable ImageModel API. These option data types are using in autoconfiguration/configuration properties for the ImageModel implementations. January 13, 2024 Update: The following OpenAi Autoconfiguration chat properties has changed from spring.ai.openai.model to spring.ai.openai.chat.options.model . from spring.ai.openai.temperature to spring.ai.openai.chat.options.temperature . Find updated documentation about the OpenAi properties: docs.spring.io/spring-ai/reference/api/chat/openai-chat.html(https://docs.spring.io/spring-ai/reference/api/chat/openai-chat.html) December 27, 2023 Update: Merge SimplePersistentVectorStore and InMemoryVectorStore into SimpleVectorStore * Replace InMemoryVectorStore with SimpleVectorStore December 20, 2023 Update: Refactor the Ollama client and related classes and package names Replace the org.springframework.ai.ollama.client.OllamaClient by org.springframework.ai.ollama.OllamaModelCall. The OllamaChatClient method signatures have changed. Rename the org.springframework.ai.autoconfigure.ollama.OllamaProperties into org.springframework.ai.autoconfigure.ollama.OllamaChatProperties and change the suffix to: spring.ai.ollama.chat . Some of the properties have changed as well. December 19, 2023 Update: Renaming of AiClient and related classes and package names Rename AiClient to ChatClient Rename AiResponse to ChatResponse Rename AiStreamClient to StreamingChatClient Rename package org.sf.ai.client to org.sf.ai.chat Rename artifact ID of transformers-embedding to spring-ai-transformers Moved Maven modules from top-level directory and embedding-clients subdirectory to all be under a single models directory. December 1, 2023: We are transitioning the project’s Group ID: FROM : org.springframework.experimental.ai TO : org.springframework.ai Artifacts will still be hosted in the snapshot repository as shown below. The main branch will move to the version 0.8.0-SNAPSHOT . It will be unstable for a week or two. Please use the 0.7.1-SNAPSHOT if you don’t want to be on the bleeding edge. You can access 0.7.1-SNAPSHOT artifacts as before and still access 0.7.1-SNAPSHOT Documentation(https://markpollack.github.io/spring-ai-0.7.1/) . 0.7.1-SNAPSHOT Dependencies: Azure OpenAI <dependency> <groupId>org.springframework.experimental.ai</groupId> <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId> <version>0.7.1-SNAPSHOT</version> </dependency> OpenAI <dependency> <groupId>org.springframework.experimental.ai</groupId> <artifactId>spring-ai-openai-spring-boot-starter</artifactId> <version>0.7.1-SNAPSHOT</version> </dependency> Contribution Guidelines(contribution-guidelines.html)"
