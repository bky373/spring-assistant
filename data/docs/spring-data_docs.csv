"url","content"
"https://docs.spring.io/spring-data/commons/reference/3.3/index.html","Spring Data Commons: © 2008-2024 The original authors. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically. Preface: The Spring Data Commons project applies core Spring concepts to the development of solutions using many relational and non-relational data stores. Project Metadata: Version control: github.com/spring-projects/spring-data-commons(https://github.com/spring-projects/spring-data-commons) Bugtracker: github.com/spring-projects/spring-data-commons/issues(https://github.com/spring-projects/spring-data-commons/issues) Release repository: repo1.maven.org/maven2/(https://repo1.maven.org/maven2/) Milestone repository: repo.spring.io/milestone/(https://repo.spring.io/milestone/) Snapshot repository: repo.spring.io/snapshot/(https://repo.spring.io/snapshot/)"
"https://docs.spring.io/spring-data/commons/reference/3.3/dependencies.html","Dependencies: Due to the different inception dates of individual Spring Data modules, most of them carry different major and minor version numbers. The easiest way to find compatible ones is to rely on the Spring Data Release Train BOM that we ship with the compatible versions defined. In a Maven project, you would declare this dependency in the <dependencyManagement /> section of your POM as follows: Using the Spring Data release train BOM <dependencyManagement> <dependencies> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-bom</artifactId> <version>2024.0.4</version> <scope>import</scope> <type>pom</type> </dependency> </dependencies> </dependencyManagement> The current release train version is 2024.0.4 . The train version uses calver(https://calver.org/) with the pattern YYYY.MINOR.MICRO . The version name follows ${calver} for GA releases and service releases and the following pattern for all other versions: ${calver}-${modifier} , where modifier can be one of the following: SNAPSHOT : Current snapshots M1 , M2 , and so on: Milestones RC1 , RC2 , and so on: Release candidates You can find a working example of using the BOMs in our Spring Data examples repository(https://github.com/spring-projects/spring-data-examples/tree/main/bom) . With that in place, you can declare the Spring Data modules you would like to use without a version in the <dependencies /> block, as follows: Declaring a dependency to a Spring Data module such as JPA <dependencies> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-jpa</artifactId> </dependency> <dependencies> Dependency Management with Spring Boot: Spring Boot selects a recent version of the Spring Data modules for you. If you still want to upgrade to a newer version, set the spring-data-bom.version property to the train version and iteration(#dependencies.train-version) you would like to use. See Spring Boot’s documentation(https://docs.spring.io/spring-boot/docs/current/reference/html/dependency-versions.html#appendix.dependency-versions.properties) (search for ""Spring Data Bom"") for more details. Spring Framework: The current version of Spring Data modules require Spring Framework 6.1.13 or better. The modules might also work with an older bugfix version of that minor version. However, using the most recent version within that generation is highly recommended."
"https://docs.spring.io/spring-data/commons/reference/3.3/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped."
"https://docs.spring.io/spring-data/commons/reference/3.3/object-mapping.html","Object Mapping Fundamentals: This section covers the fundamentals of Spring Data object mapping, object creation, field and property access, mutability and immutability. Note, that this section only applies to Spring Data modules that do not use the object mapping of the underlying data store (like JPA). Also be sure to consult the store-specific sections for store-specific object mapping, like indexes, customizing column or field names or the like. Core responsibility of the Spring Data object mapping is to create instances of domain objects and map the store-native data structures onto those. This means we need two fundamental steps: Instance creation by using one of the constructors exposed. Instance population to materialize all exposed properties. Object creation: Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. The value resolution assumes constructor/factory method argument names to match the property names of the entity, i.e. the resolution will be performed as if the property was to be populated, including all customizations in mapping (different datastore column or field name etc.). This also requires either parameter names information available in the class file or an @ConstructorProperties annotation being present on the constructor. The value resolution can be customized by using Spring Framework’s @Value value annotation using a store-specific SpEL expression. Please consult the section on store specific mappings for further details. Object creation internals To avoid the overhead of reflection, Spring Data object creation uses a factory class generated at runtime by default, which will call the domain classes constructor directly. I.e. for this example type: class Person { Person(String firstname, String lastname) { … } } we will create a factory class semantically equivalent to this one at runtime: class PersonObjectInstantiator implements ObjectInstantiator { Object newInstance(Object... args) { return new Person((String) args[0], (String) args[1]); } } This gives us a roundabout 10% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: it must not be a private class it must not be a non-static inner class it must not be a CGLib proxy class the constructor to be used by Spring Data must not be private If any of these criteria match, Spring Data will fall back to entity instantiation via reflection. Property population: Once an instance of the entity has been created, Spring Data populates all remaining persistent properties of that class. Unless already populated by the entity’s constructor (i.e. consumed through its constructor argument list), the identifier property will be populated first to allow the resolution of cyclic object references. After that, all non-transient properties that have not already been populated by the constructor are set on the entity instance. For that we use the following algorithm: If the property is immutable but exposes a with… method (see below), we use the with… method to create a new entity instance with the new property value. If property access (i.e. access through getters and setters) is defined, we’re invoking the setter method. If the property is mutable we set the field directly. If the property is immutable we’re using the constructor to be used by persistence operations (see Object creation(#mapping.object-creation) ) to create a copy of the instance. By default, we set the field value directly. Property population internals Similarly to our optimizations in object construction(#mapping.object-creation.details) we also use Spring Data runtime generated accessor classes to interact with the entity instance. class Person { private final Long id; private String firstname; private @AccessType(Type.PROPERTY) String lastname; Person() { this.id = null; } Person(Long id, String firstname, String lastname) { // Field assignments } Person withId(Long id) { return new Person(id, this.firstname, this.lastame); } void setLastname(String lastname) { this.lastname = lastname; } } A generated Property Accessor class PersonPropertyAccessor implements PersistentPropertyAccessor { private static final MethodHandle firstname; (2) private Person person; (1) public void setProperty(PersistentProperty property, Object value) { String name = property.getName(); if (""firstname"".equals(name)) { firstname.invoke(person, (String) value); (2) } else if (""id"".equals(name)) { this.person = person.withId((Long) value); (3) } else if (""lastname"".equals(name)) { this.person.setLastname((String) value); (4) } } } 1 PropertyAccessor’s hold a mutable instance of the underlying object. This is, to enable mutations of otherwise immutable properties. 2 By default, Spring Data uses field-access to read and write property values. As per visibility rules of private fields, MethodHandles are used to interact with fields. 3 The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. Calling withId(…) creates a new Person object. All subsequent mutations will take place in the new instance leaving the previous untouched. 4 Using property-access allows direct method invocations without using MethodHandles . This gives us a roundabout 25% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: Types must not reside in the default or under the java package. Types and their constructors must be public Types that are inner classes must be static . The used Java Runtime must allow for declaring classes in the originating ClassLoader . Java 9 and newer impose certain limitations. By default, Spring Data attempts to use generated property accessors and falls back to reflection-based ones if a limitation is detected. Let’s have a look at the following entity: A sample entity class Person { private final @Id Long id; (1) private final String firstname, lastname; (2) private final LocalDate birthday; private final int age; (3) private String comment; (4) private @AccessType(Type.PROPERTY) String remarks; (5) static Person of(String firstname, String lastname, LocalDate birthday) { (6) return new Person(null, firstname, lastname, birthday, Period.between(birthday, LocalDate.now()).getYears()); } Person(Long id, String firstname, String lastname, LocalDate birthday, int age) { (6) this.id = id; this.firstname = firstname; this.lastname = lastname; this.birthday = birthday; this.age = age; } Person withId(Long id) { (1) return new Person(id, this.firstname, this.lastname, this.birthday, this.age); } void setRemarks(String remarks) { (5) this.remarks = remarks; } } 1 The identifier property is final but set to null in the constructor. The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. The original Person instance stays unchanged as a new one is created. The same pattern is usually applied for other properties that are store managed but might have to be changed for persistence operations. The wither method is optional as the persistence constructor (see 6) is effectively a copy constructor and setting the property will be translated into creating a fresh instance with the new identifier value applied. 2 The firstname and lastname properties are ordinary immutable properties potentially exposed through getters. 3 The age property is an immutable but derived one from the birthday property. With the design shown, the database value will trump the defaulting as Spring Data uses the only declared constructor. Even if the intent is that the calculation should be preferred, it’s important that this constructor also takes age as parameter (to potentially ignore it) as otherwise the property population step will attempt to set the age field and fail due to it being immutable and no with… method being present. 4 The comment property is mutable and is populated by setting its field directly. 5 The remarks property is mutable and is populated by invoking the setter method. 6 The class exposes a factory method and a constructor for object creation. The core idea here is to use factory methods instead of additional constructors to avoid the need for constructor disambiguation through @PersistenceCreator . Instead, defaulting of properties is handled within the factory method. If you want Spring Data to use the factory method for object instantiation, annotate it with @PersistenceCreator . General recommendations: Try to stick to immutable objects — Immutable objects are straightforward to create as materializing an object is then a matter of calling its constructor only. Also, this avoids your domain objects to be littered with setter methods that allow client code to manipulate the objects state. If you need those, prefer to make them package protected so that they can only be invoked by a limited amount of co-located types. Constructor-only materialization is up to 30% faster than properties population. Provide an all-args constructor — Even if you cannot or don’t want to model your entities as immutable values, there’s still value in providing a constructor that takes all properties of the entity as arguments, including the mutable ones, as this allows the object mapping to skip the property population for optimal performance. Use factory methods instead of overloaded constructors to avoid @PersistenceCreator — With an all-argument constructor needed for optimal performance, we usually want to expose more application use case specific constructors that omit things like auto-generated identifiers etc. It’s an established pattern to rather use static factory methods to expose these variants of the all-args constructor. Make sure you adhere to the constraints that allow the generated instantiator and property accessor classes to be used — For identifiers to be generated, still use a final field in combination with an all-arguments persistence constructor (preferred) or a with… method — Use Lombok to avoid boilerplate code — As persistence operations usually require a constructor taking all arguments, their declaration becomes a tedious repetition of boilerplate parameter to field assignments that can best be avoided by using Lombok’s @AllArgsConstructor . Overriding Properties: Java’s allows a flexible design of domain classes where a subclass could define a property that is already declared with the same name in its superclass. Consider the following example: public class SuperType { private CharSequence field; public SuperType(CharSequence field) { this.field = field; } public CharSequence getField() { return this.field; } public void setField(CharSequence field) { this.field = field; } } public class SubType extends SuperType { private String field; public SubType(String field) { super(field); this.field = field; } @Override public String getField() { return this.field; } public void setField(String field) { this.field = field; // optional super.setField(field); } } Both classes define a field using assignable types. SubType however shadows SuperType.field . Depending on the class design, using the constructor could be the only default approach to set SuperType.field . Alternatively, calling super.setField(…) in the setter could set the field in SuperType . All these mechanisms create conflicts to some degree because the properties share the same name yet might represent two distinct values. Spring Data skips super-type properties if types are not assignable. That is, the type of the overridden property must be assignable to its super-type property type to be registered as override, otherwise the super-type property is considered transient. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be generally set without making any further assumptions of the setter implementation. Kotlin support: Spring Data adapts specifics of Kotlin to allow object creation and mutation. Kotlin object creation: Kotlin classes are supported to be instantiated, all classes are immutable by default and require explicit property declarations to define mutable properties. Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a constructor that is annotated with @PersistenceCreator , it is used. If the type is a Kotlin data class(#mapping.kotlin) the primary constructor is used. If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. Consider the following data class Person : data class Person(val id: String, val name: String) The class above compiles to a typical class with an explicit constructor. We can customize this class by adding another constructor and annotate it with @PersistenceCreator to indicate a constructor preference: data class Person(var id: String, val name: String) { @PersistenceCreator constructor(id: String) : this(id, ""unknown"") } Kotlin supports parameter optionality by allowing default values to be used if a parameter is not provided. When Spring Data detects a constructor with parameter defaulting, then it leaves these parameters absent if the data store does not provide a value (or simply returns null ) so Kotlin can apply parameter defaulting.Consider the following class that applies parameter defaulting for name data class Person(var id: String, val name: String = ""unknown"") Every time the name parameter is either not part of the result or its value is null , then the name defaults to unknown . Delegated properties are not supported with Spring Data. The mapping metadata filters delegated properties for Kotlin Data classes. In all other cases you can exclude synthetic fields for delegated properties by annotating the property with @delegate:org.springframework.data.annotation.Transient . Property population of Kotlin data classes: In Kotlin, all classes are immutable by default and require explicit property declarations to define mutable properties. Consider the following data class Person : data class Person(val id: String, val name: String) This class is effectively immutable. It allows creating new instances as Kotlin generates a copy(…) method that creates new object instances copying all property values from the existing object and applying property values provided as arguments to the method. Kotlin Overriding Properties: Kotlin allows declaring property overrides(https://kotlinlang.org/docs/inheritance.html#overriding-properties) to alter properties in subclasses. open class SuperType(open var field: Int) class SubType(override var field: Int = 1) : SuperType(field) { } Such an arrangement renders two properties with the name field . Kotlin generates property accessors (getters and setters) for each property in each class. Effectively, the code looks like as follows: public class SuperType { private int field; public SuperType(int field) { this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } public final class SubType extends SuperType { private int field; public SubType(int field) { super(field); this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } Getters and setters on SubType set only SubType.field and not SuperType.field . In such an arrangement, using the constructor is the only default approach to set SuperType.field . Adding a method to SubType to set SuperType.field via this.SuperType.field = … is possible but falls outside of supported conventions. Property overrides create conflicts to some degree because the properties share the same name yet might represent two distinct values. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be set. Kotlin Value Classes: Kotlin Value Classes are designed for a more expressive domain model to make underlying concepts explicit. Spring Data can read and write types that define properties using Value Classes. Consider the following domain model: @JvmInline value class EmailAddress(val theAddress: String) (1) data class Contact(val id: String, val name:String, val emailAddress: EmailAddress) (2) 1 A simple value class with a non-nullable value type. 2 Data class defining a property using the EmailAddress value class. Non-nullable properties using non-primitive value types are flattened in the compiled class to the value type. Nullable primitive value types or nullable value-in-value types are represented with their wrapper type and that affects how value types are represented in the database."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories.html","Working with Spring Data Repositories: The goal of the Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. Spring Data repository documentation and your module This chapter explains the core concepts and interfaces of Spring Data repositories. The information in this chapter is pulled from the Spring Data Commons module. It uses the configuration and code samples for the Jakarta Persistence API (JPA) module. If you want to use XML configuration you should adapt the XML namespace declaration and the types to be extended to the equivalents of the particular module that you use. “ Namespace reference(repositories/namespace-reference.html#repositories.namespace-reference) ” covers XML configuration, which is supported across all Spring Data modules that support the repository API. “ Repository query keywords(repositories/query-keywords-reference.html) ” covers the query method keywords supported by the repository abstraction in general. For detailed information on the specific features of your module, see the chapter on that module of this document."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/data-commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/data-commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/data-commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/data-commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); }"
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/query-methods.html","Query Methods: Standard CRUD functionality repositories usually have queries on the underlying datastore. With Spring Data, declaring those queries becomes a four-step process: Declare an interface extending Repository or one of its subinterfaces and type it to the domain class and ID type that it should handle, as shown in the following example: interface PersonRepository extends Repository<Person, Long> { … } Declare query methods on the interface. interface PersonRepository extends Repository<Person, Long> { List<Person> findByLastname(String lastname); } Set up Spring to create proxy instances for those interfaces, either with JavaConfig(create-instances.html#repositories.create-instances.java-config) or with XML configuration(create-instances.html) . Java XML import org.springframework.data.….repository.config.EnableJpaRepositories; @EnableJpaRepositories class Config { … } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:jpa=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <repositories base-package=""com.acme.repositories""/> </beans> The JPA namespace is used in this example. If you use the repository abstraction for any other store, you need to change this to the appropriate namespace declaration of your store module. In other words, you should exchange jpa in favor of, for example, mongodb . Note that the JavaConfig variant does not configure a package explicitly, because the package of the annotated class is used by default. To customize the package to scan, use one of the basePackage… attributes of the data-store-specific repository’s @EnableJpaRepositories -annotation. Inject the repository instance and use it, as shown in the following example: class SomeClient { private final PersonRepository repository; SomeClient(PersonRepository repository) { this.repository = repository; } void doSomething() { List<Person> persons = repository.findByLastname(""Matthews""); } } The sections that follow explain each step in detail: Defining Repository Interfaces(definition.html) Defining Query Methods(query-methods-details.html) Creating Repository Instances(create-instances.html) Custom Implementations for Spring Data Repositories(custom-implementations.html)"
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/query-methods-details.html","Defining Query Methods: The repository proxy has two ways to derive a store-specific query from the method name: By deriving the query from the method name directly. By using a manually defined query. Available options depend on the actual store. However, there must be a strategy that decides what actual query is created. The next section describes the available options. Query Lookup Strategies: The following strategies are available for the repository infrastructure to resolve the query. With XML configuration, you can configure the strategy at the namespace through the query-lookup-strategy attribute. For Java configuration, you can use the queryLookupStrategy attribute of the EnableJpaRepositories annotation. Some strategies may not be supported for particular datastores. CREATE attempts to construct a store-specific query from the query method name. The general approach is to remove a given set of well known prefixes from the method name and parse the rest of the method. You can read more about query construction in “ Query Creation(#repositories.query-methods.query-creation) ”. USE_DECLARED_QUERY tries to find a declared query and throws an exception if it cannot find one. The query can be defined by an annotation somewhere or declared by other means. See the documentation of the specific store to find available options for that store. If the repository infrastructure does not find a declared query for the method at bootstrap time, it fails. CREATE_IF_NOT_FOUND (the default) combines CREATE and USE_DECLARED_QUERY . It looks up a declared query first, and, if no declared query is found, it creates a custom method name-based query. This is the default lookup strategy and, thus, is used if you do not configure anything explicitly. It allows quick query definition by method names but also custom-tuning of these queries by introducing declared queries as needed. Query Creation: The query builder mechanism built into the Spring Data repository infrastructure is useful for building constraining queries over entities of the repository. The following example shows how to create a number of queries: Query creation from method names interface PersonRepository extends Repository<Person, Long> { List<Person> findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // Enables the distinct flag for the query List<Person> findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List<Person> findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); // Enabling ignoring case for an individual property List<Person> findByLastnameIgnoreCase(String lastname); // Enabling ignoring case for all suitable properties List<Person> findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // Enabling static ORDER BY for a query List<Person> findByLastnameOrderByFirstnameAsc(String lastname); List<Person> findByLastnameOrderByFirstnameDesc(String lastname); } Parsing query method names is divided into subject and predicate. The first part ( find…By , exists…By ) defines the subject of the query, the second part forms the predicate. The introducing clause (subject) can contain further expressions. Any text between find (or other introducing keywords) and By is considered to be descriptive unless using one of the result-limiting keywords such as a Distinct to set a distinct flag on the query to be created or Top/First to limit query results(#repositories.limit-query-result) . The appendix contains the full list of query method subject keywords(query-keywords-reference.html#appendix.query.method.subject) and query method predicate keywords including sorting and letter-casing modifiers(query-keywords-reference.html#appendix.query.method.predicate) . However, the first By acts as a delimiter to indicate the start of the actual criteria predicate. At a very basic level, you can define conditions on entity properties and concatenate them with And and Or . The actual result of parsing the method depends on the persistence store for which you create the query. However, there are some general things to notice: The expressions are usually property traversals combined with operators that can be concatenated. You can combine property expressions with AND and OR . You also get support for operators such as Between , LessThan , GreaterThan , and Like for the property expressions. The supported operators can vary by datastore, so consult the appropriate part of your reference documentation. The method parser supports setting an IgnoreCase flag for individual properties (for example, findByLastnameIgnoreCase(…) ) or for all properties of a type that supports ignoring case (usually String instances — for example, findByLastnameAndFirstnameAllIgnoreCase(…) ). Whether ignoring cases is supported may vary by store, so consult the relevant sections in the reference documentation for the store-specific query method. You can apply static ordering by appending an OrderBy clause to the query method that references a property and by providing a sorting direction ( Asc or Desc ). To create a query method that supports dynamic sorting, see “ Paging, Iterating Large Results, Sorting & Limiting(#repositories.special-parameters) ”. Property Expressions: Property expressions can refer only to a direct property of the managed entity, as shown in the preceding example. At query creation time, you already make sure that the parsed property is a property of the managed domain class. However, you can also define constraints by traversing nested properties. Consider the following method signature: List<Person> findByAddressZipCode(ZipCode zipCode); Assume a Person has an Address with a ZipCode . In that case, the method creates the x.address.zipCode property traversal. The resolution algorithm starts by interpreting the entire part ( AddressZipCode ) as the property and checks the domain class for a property with that name (uncapitalized). If the algorithm succeeds, it uses that property. If not, the algorithm splits up the source at the camel-case parts from the right side into a head and a tail and tries to find the corresponding property — in our example, AddressZip and Code . If the algorithm finds a property with that head, it takes the tail and continues building the tree down from there, splitting the tail up in the way just described. If the first split does not match, the algorithm moves the split point to the left ( Address , ZipCode ) and continues. Although this should work for most cases, it is possible for the algorithm to select the wrong property. Suppose the Person class has an addressZip property as well. The algorithm would match in the first split round already, choose the wrong property, and fail (as the type of addressZip probably has no code property). To resolve this ambiguity you can use _ inside your method name to manually define traversal points. So our method name would be as follows: List<Person> findByAddress_ZipCode(ZipCode zipCode); Because we treat underscores ( _ ) as a reserved character, we strongly advise to follow standard Java naming conventions (that is, not using underscores in property names but applying camel case instead). Field Names starting with underscore: Field names may start with underscores like String _name . Make sure to preserve the _ as in _name and use double _ to split nested paths like user__name . Upper Case Field Names: Field names that are all uppercase can be used as such. Nested paths if applicable require splitting via _ as in USER_name . Field Names with 2nd uppercase letter: Field names that consist of a starting lower case letter followed by an uppercase one like String qCode can be resolved by starting with two upper case letters as in QCode . Please be aware of potential path ambiguities. Path Ambiguities: In the following sample the arrangement of properties qCode and q , with q containing a property called code , creates an ambiguity for the path QCode . record Container(String qCode, Code q) {} record Code(String code) {} Since a direct match on a property is considered first, any potential nested paths will not be considered and the algorithm picks the qCode field. In order to select the code field in q the underscore notation Q_Code is required. Repository Methods Returning Collections or Iterables: Query methods that return multiple results can use standard Java Iterable , List , and Set . Beyond that, we support returning Spring Data’s Streamable , a custom extension of Iterable , as well as collection types provided by Vavr(https://www.vavr.io/) . Refer to the appendix explaining all possible query method return types(query-return-types-reference.html#appendix.query.return.types) . Using Streamable as Query Method Return Type: You can use Streamable as alternative to Iterable or any collection type. It provides convenience methods to access a non-parallel Stream (missing from Iterable ) and the ability to directly ….filter(…) and ….map(…) over the elements and concatenate the Streamable to others: Using Streamable to combine query method results interface PersonRepository extends Repository<Person, Long> { Streamable<Person> findByFirstnameContaining(String firstname); Streamable<Person> findByLastnameContaining(String lastname); } Streamable<Person> result = repository.findByFirstnameContaining(""av"") .and(repository.findByLastnameContaining(""ea"")); Returning Custom Streamable Wrapper Types: Providing dedicated wrapper types for collections is a commonly used pattern to provide an API for a query result that returns multiple elements. Usually, these types are used by invoking a repository method returning a collection-like type and creating an instance of the wrapper type manually. You can avoid that additional step as Spring Data lets you use these wrapper types as query method return types if they meet the following criteria: The type implements Streamable . The type exposes either a constructor or a static factory method named of(…) or valueOf(…) that takes Streamable as an argument. The following listing shows an example: class Product { (1) MonetaryAmount getPrice() { … } } @RequiredArgsConstructor(staticName = ""of"") class Products implements Streamable<Product> { (2) private final Streamable<Product> streamable; public MonetaryAmount getTotal() { (3) return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); } @Override public Iterator<Product> iterator() { (4) return streamable.iterator(); } } interface ProductRepository implements Repository<Product, Long> { Products findAllByDescriptionContaining(String text); (5) } 1 A Product entity that exposes API to access the product’s price. 2 A wrapper type for a Streamable<Product> that can be constructed by using Products.of(…) (factory method created with the Lombok annotation). A standard constructor taking the Streamable<Product> will do as well. 3 The wrapper type exposes an additional API, calculating new values on the Streamable<Product> . 4 Implement the Streamable interface and delegate to the actual result. 5 That wrapper type Products can be used directly as a query method return type. You do not need to return Streamable<Product> and manually wrap it after the query in the repository client. Support for Vavr Collections: Vavr(https://www.vavr.io/) is a library that embraces functional programming concepts in Java. It ships with a custom set of collection types that you can use as query method return types, as the following table shows: Vavr collection type Used Vavr implementation type Valid Java source types io.vavr.collection.Seq io.vavr.collection.List java.util.Iterable io.vavr.collection.Set io.vavr.collection.LinkedHashSet java.util.Iterable io.vavr.collection.Map io.vavr.collection.LinkedHashMap java.util.Map You can use the types in the first column (or subtypes thereof) as query method return types and get the types in the second column used as implementation type, depending on the Java type of the actual query result (third column). Alternatively, you can declare Traversable (the Vavr Iterable equivalent), and we then derive the implementation class from the actual return value. That is, a java.util.List is turned into a Vavr List or Seq , a java.util.Set becomes a Vavr LinkedHashSet Set , and so on. Streaming Query Results: You can process the results of query methods incrementally by using a Java 8 Stream<T> as the return type. Instead of wrapping the query results in a Stream , data store-specific methods are used to perform the streaming, as shown in the following example: Stream the result of a query with Java 8 Stream<T> @Query(""select u from User u"") Stream<User> findAllByCustomQueryAndStream(); Stream<User> readAllByFirstnameNotNull(); @Query(""select u from User u"") Stream<User> streamAllPaged(Pageable pageable); A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example: Working with a Stream<T> result in a try-with-resources block try (Stream<User> stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } Not all Spring Data modules currently support Stream<T> as a return type. Asynchronous Query Results: You can run repository queries asynchronously by using Spring’s asynchronous method running capability(https://docs.spring.io/spring-framework/reference/6.1/integration/scheduling.html) . This means the method returns immediately upon invocation while the actual query occurs in a task that has been submitted to a Spring TaskExecutor . Asynchronous queries differ from reactive queries and should not be mixed. See the store-specific documentation for more details on reactive support. The following example shows a number of asynchronous queries: @Async Future<User> findByFirstname(String firstname); (1) @Async CompletableFuture<User> findOneByFirstname(String firstname); (2) 1 Use java.util.concurrent.Future as the return type. 2 Use a Java 8 java.util.concurrent.CompletableFuture as the return type. Paging, Iterating Large Results, Sorting & Limiting: To handle parameters in your query, define method parameters as already seen in the preceding examples. Besides that, the infrastructure recognizes certain specific types like Pageable , Sort and Limit , to apply pagination, sorting and limiting to your queries dynamically. The following example demonstrates these features: Using Pageable , Slice , Sort and Limit in query methods Page<User> findByLastname(String lastname, Pageable pageable); Slice<User> findByLastname(String lastname, Pageable pageable); List<User> findByLastname(String lastname, Sort sort); List<User> findByLastname(String lastname, Sort sort, Limit limit); List<User> findByLastname(String lastname, Pageable pageable); APIs taking Sort , Pageable and Limit expect non- null values to be handed into methods. If you do not want to apply any sorting or pagination, use Sort.unsorted() , Pageable.unpaged() and Limit.unlimited() . The first method lets you pass an org.springframework.data.domain.Pageable instance to the query method to dynamically add paging to your statically defined query. A Page knows about the total number of elements and pages available. It does so by the infrastructure triggering a count query to calculate the overall number. As this might be expensive (depending on the store used), you can instead return a Slice . A Slice knows only about whether a next Slice is available, which might be sufficient when walking through a larger result set. Sorting options are handled through the Pageable instance, too. If you need only sorting, add an org.springframework.data.domain.Sort parameter to your method. As you can see, returning a List is also possible. In this case, the additional metadata required to build the actual Page instance is not created (which, in turn, means that the additional count query that would have been necessary is not issued). Rather, it restricts the query to look up only the given range of entities. To find out how many pages you get for an entire query, you have to trigger an additional count query. By default, this query is derived from the query you actually trigger. Special parameters may only be used once within a query method. Some special parameters described above are mutually exclusive. Please consider the following list of invalid parameter combinations. Parameters Example Reason Pageable and Sort findBy…​(Pageable page, Sort sort) Pageable already defines Sort Pageable and Limit findBy…​(Pageable page, Limit limit) Pageable already defines a limit. The Top keyword used to limit results can be used to along with Pageable whereas Top defines the total maximum of results, whereas the Pageable parameter may reduce this number. Which Method is Appropriate?: The value provided by the Spring Data abstractions is perhaps best shown by the possible query method return types outlined in the following table below. The table shows which types you can return from a query method Table 1. Consuming Large Query Results Method Amount of Data Fetched Query Structure Constraints List<T>(#repositories.collections-and-iterables) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Streamable<T>(#repositories.collections-and-iterables.streamable) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Stream<T>(#repositories.query-streaming) Chunked (one-by-one or in batches) depending on Stream consumption. Single query using typically cursors. Streams must be closed after usage to avoid resource leaks. Flux<T> Chunked (one-by-one or in batches) depending on Flux consumption. Single query using typically cursors. Store module must provide reactive infrastructure. Slice<T> Pageable.getPageSize() + 1 at Pageable.getOffset() One to many queries fetching data starting at Pageable.getOffset() applying limiting. A Slice can only navigate to the next Slice . Slice provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Window provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Page<T> Pageable.getPageSize() at Pageable.getOffset() One to many queries starting at Pageable.getOffset() applying limiting. Additionally, COUNT(…) query to determine the total number of elements can be required. Often times, COUNT(…) queries are required that are costly. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Paging and Sorting: You can define simple sorting expressions by using property names. You can concatenate expressions to collect multiple criteria into one expression. Defining sort expressions Sort sort = Sort.by(""firstname"").ascending() .and(Sort.by(""lastname"").descending()); For a more type-safe way to define sort expressions, start with the type for which to define the sort expression and use method references to define the properties on which to sort. Defining sort expressions by using the type-safe API TypedSort<Person> person = Sort.sort(Person.class); Sort sort = person.by(Person::getFirstname).ascending() .and(person.by(Person::getLastname).descending()); TypedSort.by(…) makes use of runtime proxies by (typically) using CGlib, which may interfere with native image compilation when using tools such as Graal VM Native. If your store implementation supports Querydsl, you can also use the generated metamodel types to define sort expressions: Defining sort expressions by using the Querydsl API QSort sort = QSort.by(QPerson.firstname.asc()) .and(QSort.by(QPerson.lastname.desc())); Limiting Query Results: In addition to paging it is possible to limit the result size using a dedicated Limit parameter. You can also limit the results of query methods by using the First or Top keywords, which you can use interchangeably but may not be mixed with a Limit parameter. You can append an optional numeric value to Top or First to specify the maximum result size to be returned. If the number is left out, a result size of 1 is assumed. The following example shows how to limit the query size: Limiting the result size of a query with Top and First List<User> findByLastname(Limit limit); User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page<User> queryFirst10ByLastname(String lastname, Pageable pageable); Slice<User> findTop3ByLastname(String lastname, Pageable pageable); List<User> findFirst10ByLastname(String lastname, Sort sort); List<User> findTop10ByLastname(String lastname, Pageable pageable); The limiting expressions also support the Distinct keyword for datastores that support distinct queries. Also, for the queries that limit the result set to one instance, wrapping the result into with the Optional keyword is supported. If pagination or slicing is applied to a limiting query pagination (and the calculation of the number of available pages), it is applied within the limited result. Limiting the results in combination with dynamic sorting by using a Sort parameter lets you express query methods for the 'K' smallest as well as for the 'K' biggest elements."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/create-instances.html","Creating Repository Instances: This section covers how to create instances and bean definitions for the defined repository interfaces. Java Configuration: Use the store-specific @EnableJpaRepositories annotation on a Java configuration class to define a configuration for repository activation. For an introduction to Java-based configuration of the Spring container, see JavaConfig in the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/java.html) . A sample configuration to enable Spring Data repositories resembles the following: Sample annotation-based repository configuration @Configuration @EnableJpaRepositories(""com.acme.repositories"") class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } The preceding example uses the JPA-specific annotation, which you would change according to the store module you actually use. The same applies to the definition of the EntityManagerFactory bean. See the sections covering the store-specific configuration. XML Configuration: Each Spring Data module includes a repositories element that lets you define a base package that Spring scans for you, as shown in the following example: Enabling Spring Data repositories via XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans:beans xmlns:beans=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans:beans> In the preceding example, Spring is instructed to scan com.acme.repositories and all its sub-packages for interfaces extending Repository or one of its sub-interfaces. For each interface found, the infrastructure registers the persistence technology-specific FactoryBean to create the appropriate proxies that handle invocations of the query methods. Each bean is registered under a bean name that is derived from the interface name, so an interface of UserRepository would be registered under userRepository . Bean names for nested repository interfaces are prefixed with their enclosing type name. The base package attribute allows wildcards so that you can define a pattern of scanned packages. Using Filters: By default, the infrastructure picks up every interface that extends the persistence technology-specific Repository sub-interface located under the configured base package and creates a bean instance for it. However, you might want more fine-grained control over which interfaces have bean instances created for them. To do so, use filter elements inside the repository declaration. The semantics are exactly equivalent to the elements in Spring’s component filters. For details, see the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/classpath-scanning.html) for these elements. For example, to exclude certain interfaces from instantiation as repository beans, you could use the following configuration: Using filters Java XML @Configuration @EnableJpaRepositories(basePackages = ""com.acme.repositories"", includeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeRepository"") }, excludeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeOtherRepository"") }) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } <repositories base-package=""com.acme.repositories""> <context:include-filter type=""regex"" expression="".*SomeRepository"" /> <context:exclude-filter type=""regex"" expression="".*SomeOtherRepository"" /> </repositories> The preceding example includes all interfaces ending with SomeRepository and excludes those ending with SomeOtherRepository from being instantiated. Standalone Usage: You can also use the repository infrastructure outside of a Spring container — for example, in CDI environments.You still need some Spring libraries in your classpath, but, generally, you can set up repositories programmatically as well.The Spring Data modules that provide repository support ship with a persistence technology-specific RepositoryFactory that you can use, as follows: Standalone usage of the repository factory RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class);"
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableJpaRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableJpaRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" />"
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/core-extensions.html","Spring Data Extensions: This section documents a set of Spring Data extensions that enable Spring Data usage in a variety of contexts. Currently, most of the integration is targeted towards Spring MVC. Querydsl Extension: Querydsl(http://www.querydsl.com/) is a framework that enables the construction of statically typed SQL-like queries through its fluent API. Several Spring Data modules offer integration with Querydsl through QuerydslPredicateExecutor , as the following example shows: QuerydslPredicateExecutor interface public interface QuerydslPredicateExecutor<T> { Optional<T> findById(Predicate predicate); (1) Iterable<T> findAll(Predicate predicate); (2) long count(Predicate predicate); (3) boolean exists(Predicate predicate); (4) // … more functionality omitted. } 1 Finds and returns a single entity matching the Predicate . 2 Finds and returns all entities matching the Predicate . 3 Returns the number of entities matching the Predicate . 4 Returns whether an entity that matches the Predicate exists. To use the Querydsl support, extend QuerydslPredicateExecutor on your repository interface, as the following example shows: Querydsl integration on repositories interface UserRepository extends CrudRepository<User, Long>, QuerydslPredicateExecutor<User> { } The preceding example lets you write type-safe queries by using Querydsl Predicate instances, as the following example shows: Predicate predicate = user.firstname.equalsIgnoreCase(""dave"") .and(user.lastname.startsWithIgnoreCase(""mathews"")); userRepository.findAll(predicate); Web support: Spring Data modules that support the repository programming model ship with a variety of web support. The web related components require Spring MVC JARs to be on the classpath. Some of them even provide integration with Spring HATEOAS(https://github.com/spring-projects/spring-hateoas) . In general, the integration support is enabled by using the @EnableSpringDataWebSupport annotation in your JavaConfig configuration class, as the following example shows: Enabling Spring Data web support Java XML @Configuration @EnableWebMvc @EnableSpringDataWebSupport class WebConfiguration {} <bean class=""org.springframework.data.web.config.SpringDataWebConfiguration"" /> <!-- If you use Spring HATEOAS, register this one *instead* of the former --> <bean class=""org.springframework.data.web.config.HateoasAwareSpringDataWebConfiguration"" /> The @EnableSpringDataWebSupport annotation registers a few components. We discuss those later in this section. It also detects Spring HATEOAS on the classpath and registers integration components (if present) for it as well. Basic Web Support: Enabling Spring Data web support in XML The configuration shown in the previous section(#core.web) registers a few basic components: A Using the DomainClassConverter Class(#core.web.basic.domain-class-converter) to let Spring MVC resolve instances of repository-managed domain classes from request parameters or path variables. HandlerMethodArgumentResolver(#core.web.basic.paging-and-sorting) implementations to let Spring MVC resolve Pageable and Sort instances from request parameters. Jackson Modules(#core.web.basic.jackson-mappers) to de-/serialize types like Point and Distance , or store specific ones, depending on the Spring Data Module used. Using the DomainClassConverter Class: The DomainClassConverter class lets you use domain types in your Spring MVC controller method signatures directly so that you need not manually lookup the instances through the repository, as the following example shows: A Spring MVC controller using domain types in method signatures @Controller @RequestMapping(""/users"") class UserController { @RequestMapping(""/{id}"") String showUserForm(@PathVariable(""id"") User user, Model model) { model.addAttribute(""user"", user); return ""userForm""; } } The method receives a User instance directly, and no further lookup is necessary. The instance can be resolved by letting Spring MVC convert the path variable into the id type of the domain class first and eventually access the instance through calling findById(…) on the repository instance registered for the domain type. Currently, the repository has to implement CrudRepository to be eligible to be discovered for conversion. HandlerMethodArgumentResolvers for Pageable and Sort: The configuration snippet shown in the previous section(#core.web.basic.domain-class-converter) also registers a PageableHandlerMethodArgumentResolver as well as an instance of SortHandlerMethodArgumentResolver . The registration enables Pageable and Sort as valid controller method arguments, as the following example shows: Using Pageable as a controller method argument @Controller @RequestMapping(""/users"") class UserController { private final UserRepository repository; UserController(UserRepository repository) { this.repository = repository; } @RequestMapping String showUsers(Model model, Pageable pageable) { model.addAttribute(""users"", repository.findAll(pageable)); return ""users""; } } The preceding method signature causes Spring MVC try to derive a Pageable instance from the request parameters by using the following default configuration: Table 1. Request parameters evaluated for Pageable instances page Page you want to retrieve. 0-indexed and defaults to 0. size Size of the page you want to retrieve. Defaults to 20. sort Properties that should be sorted by in the format property,property(,ASC|DESC)(,IgnoreCase) . The default sort direction is case-sensitive ascending. Use multiple sort parameters if you want to switch direction or case sensitivity — for example, ?sort=firstname&sort=lastname,asc&sort=city,ignorecase . To customize this behavior, register a bean that implements the PageableHandlerMethodArgumentResolverCustomizer interface or the SortHandlerMethodArgumentResolverCustomizer interface, respectively. Its customize() method gets called, letting you change settings, as the following example shows: @Bean SortHandlerMethodArgumentResolverCustomizer sortCustomizer() { return s -> s.setPropertyDelimiter(""<-->""); } If setting the properties of an existing MethodArgumentResolver is not sufficient for your purpose, extend either SpringDataWebConfiguration or the HATEOAS-enabled equivalent, override the pageableResolver() or sortResolver() methods, and import your customized configuration file instead of using the @Enable annotation. If you need multiple Pageable or Sort instances to be resolved from the request (for multiple tables, for example), you can use Spring’s @Qualifier annotation to distinguish one from another. The request parameters then have to be prefixed with ${qualifier}_ . The following example shows the resulting method signature: String showUsers(Model model, @Qualifier(""thing1"") Pageable first, @Qualifier(""thing2"") Pageable second) { … } You have to populate thing1_page , thing2_page , and so on. The default Pageable passed into the method is equivalent to a PageRequest.of(0, 20) , but you can customize it by using the @PageableDefault annotation on the Pageable parameter. Creating JSON representations for Page: It’s common for Spring MVC controllers to try to ultimately render a representation of a Spring Data page to clients. While one could simply return Page instances from handler methods to let Jackson render them as is, we strongly recommend against this as the underlying implementation class PageImpl is a domain type. This means we might want or have to change its API for unrelated reasons, and such changes might alter the resulting JSON representation in a breaking way. With Spring Data 3.1, we started hinting at the problem by issuing a warning log describing the problem. We still ultimately recommend to leverage the integration with Spring HATEOAS(#core.web.pageables) for a fully stable and hypermedia-enabled way of rendering pages that easily allow clients to navigate them. But as of version 3.3 Spring Data ships a page rendering mechanism that is convenient to use but does not require the inclusion of Spring HATEOAS. Using Spring Data' PagedModel: At its core, the support consists of a simplified version of Spring HATEOAS' PagedModel (the Spring Data one located in the org.springframework.data.web package). It can be used to wrap Page instances and result in a simplified representation that reflects the structure established by Spring HATEOAS but omits the navigation links. import org.springframework.data.web.PagedModel; @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") PagedModel<?> page(Pageable pageable) { return new PagedModel<>(repository.findAll(pageable)); (1) } } 1 Wraps the Page instance into a PagedModel . This will result in a JSON structure looking like this: { ""content"" : [ … // Page content rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } Note how the document contains a page field exposing the essential pagination metadata. Globally enabling simplified Page rendering: If you don’t want to change all your existing controllers to add the mapping step to return PagedModel instead of Page you can enable the automatic translation of PageImpl instances into PagedModel by tweaking @EnableSpringDataWebSupport as follows: @EnableSpringDataWebSupport(pageSerializationMode = VIA_DTO) class MyConfiguration { } This will allow your controller to still return Page instances and they will automatically be rendered into the simplified representation: @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") Page<?> page(Pageable pageable) { return repository.findAll(pageable); } } Hypermedia Support for Page and Slice: Spring HATEOAS ships with a representation model class ( PagedModel / SlicedModel ) that allows enriching the content of a Page or Slice instance with the necessary Page / Slice metadata as well as links to let the clients easily navigate the pages. The conversion of a Page to a PagedModel is done by an implementation of the Spring HATEOAS RepresentationModelAssembler interface, called the PagedResourcesAssembler . Similarly Slice instances can be converted to a SlicedModel using a SlicedResourcesAssembler . The following example shows how to use a PagedResourcesAssembler as a controller method argument, as the SlicedResourcesAssembler works exactly the same: Using a PagedResourcesAssembler as controller method argument @Controller class PersonController { private final PersonRepository repository; // Constructor omitted @GetMapping(""/people"") HttpEntity<PagedModel<Person>> people(Pageable pageable, PagedResourcesAssembler assembler) { Page<Person> people = repository.findAll(pageable); return ResponseEntity.ok(assembler.toModel(people)); } } Enabling the configuration, as shown in the preceding example, lets the PagedResourcesAssembler be used as a controller method argument. Calling toModel(…) on it has the following effects: The content of the Page becomes the content of the PagedModel instance. The PagedModel object gets a PageMetadata instance attached, and it is populated with information from the Page and the underlying Pageable . The PagedModel may get prev and next links attached, depending on the page’s state. The links point to the URI to which the method maps. The pagination parameters added to the method match the setup of the PageableHandlerMethodArgumentResolver to make sure the links can be resolved later. Assume we have 30 Person instances in the database. You can now trigger a request ( GET localhost:8080/people(http://localhost:8080/people) ) and see output similar to the following: { ""links"" : [ { ""rel"" : ""next"", ""href"" : ""http://localhost:8080/persons?page=1&size=20"" } ], ""content"" : [ … // 20 Person instances rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } The JSON envelope format shown here doesn’t follow any formally specified structure and it’s not guaranteed stable and we might change it at any time. It’s highly recommended to enable the rendering as a hypermedia-enabled, official media type, supported by Spring HATEOAS, like HAL(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#mediatypes.hal) . Those can be activated by using its @EnableHypermediaSupport annotation. Find more information in the Spring HATEOAS reference documentation(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#configuration.at-enable) . The assembler produced the correct URI and also picked up the default configuration to resolve the parameters into a Pageable for an upcoming request. This means that, if you change that configuration, the links automatically adhere to the change. By default, the assembler points to the controller method it was invoked in, but you can customize that by passing a custom Link to be used as base to build the pagination links, which overloads the PagedResourcesAssembler.toModel(…) method. Spring Data Jackson Modules: The core module, and some of the store specific ones, ship with a set of Jackson Modules for types, like org.springframework.data.geo.Distance and org.springframework.data.geo.Point , used by the Spring Data domain. Those Modules are imported once web support(#core.web) is enabled and com.fasterxml.jackson.databind.ObjectMapper is available. During initialization SpringDataJacksonModules , like the SpringDataJacksonConfiguration , get picked up by the infrastructure, so that the declared com.fasterxml.jackson.databind.Module s are made available to the Jackson ObjectMapper . Data binding mixins for the following domain types are registered by the common infrastructure. org.springframework.data.geo.Distance org.springframework.data.geo.Point org.springframework.data.geo.Box org.springframework.data.geo.Circle org.springframework.data.geo.Polygon The individual module may provide additional SpringDataJacksonModules . Please refer to the store specific section for more details. Web Databinding Support: You can use Spring Data projections (described in Projections(projections.html) ) to bind incoming request payloads by using either JSONPath(https://goessner.net/articles/JsonPath/) expressions (requires Jayway JsonPath(https://github.com/json-path/JsonPath) ) or XPath(https://www.w3.org/TR/xpath-31/) expressions (requires XmlBeam(https://xmlbeam.org/) ), as the following example shows: HTTP payload binding using JSONPath or XPath expressions @ProjectedPayload public interface UserPayload { @XBRead(""//firstname"") @JsonPath(""$..firstname"") String getFirstname(); @XBRead(""/lastname"") @JsonPath({ ""$.lastname"", ""$.user.lastname"" }) String getLastname(); } You can use the type shown in the preceding example as a Spring MVC handler method argument or by using ParameterizedTypeReference on one of methods of the RestTemplate . The preceding method declarations would try to find firstname anywhere in the given document. The lastname XML lookup is performed on the top-level of the incoming document. The JSON variant of that tries a top-level lastname first but also tries lastname nested in a user sub-document if the former does not return a value. That way, changes in the structure of the source document can be mitigated easily without having clients calling the exposed methods (usually a drawback of class-based payload binding). Nested projections are supported as described in Projections(projections.html) . If the method returns a complex, non-interface type, a Jackson ObjectMapper is used to map the final value. For Spring MVC, the necessary converters are registered automatically as soon as @EnableSpringDataWebSupport is active and the required dependencies are available on the classpath. For usage with RestTemplate , register a ProjectingJackson2HttpMessageConverter (JSON) or XmlBeamHttpMessageConverter manually. For more information, see the web projection example(https://github.com/spring-projects/spring-data-examples/tree/main/web/projection) in the canonical Spring Data Examples repository(https://github.com/spring-projects/spring-data-examples) . Querydsl Web Support: For those stores that have QueryDSL(http://www.querydsl.com/) integration, you can derive queries from the attributes contained in a Request query string. Consider the following query string: ?firstname=Dave&lastname=Matthews Given the User object from the previous examples, you can resolve a query string to the following value by using the QuerydslPredicateArgumentResolver , as follows: QUser.user.firstname.eq(""Dave"").and(QUser.user.lastname.eq(""Matthews"")) The feature is automatically enabled, along with @EnableSpringDataWebSupport , when Querydsl is found on the classpath. Adding a @QuerydslPredicate to the method signature provides a ready-to-use Predicate , which you can run by using the QuerydslPredicateExecutor . Type information is typically resolved from the method’s return type. Since that information does not necessarily match the domain type, it might be a good idea to use the root attribute of QuerydslPredicate . The following example shows how to use @QuerydslPredicate in a method signature: @Controller class UserController { @Autowired UserRepository repository; @RequestMapping(value = ""/"", method = RequestMethod.GET) String index(Model model, @QuerydslPredicate(root = User.class) Predicate predicate, (1) Pageable pageable, @RequestParam MultiValueMap<String, String> parameters) { model.addAttribute(""users"", repository.findAll(predicate, pageable)); return ""index""; } } 1 Resolve query string arguments to matching Predicate for User . The default binding is as follows: Object on simple properties as eq . Object on collection like properties as contains . Collection on simple properties as in . You can customize those bindings through the bindings attribute of @QuerydslPredicate or by making use of Java 8 default methods and adding the QuerydslBinderCustomizer method to the repository interface, as follows: interface UserRepository extends CrudRepository<User, String>, QuerydslPredicateExecutor<User>, (1) QuerydslBinderCustomizer<QUser> { (2) @Override default void customize(QuerydslBindings bindings, QUser user) { bindings.bind(user.username).first((path, value) -> path.contains(value)) (3) bindings.bind(String.class) .first((StringPath path, String value) -> path.containsIgnoreCase(value)); (4) bindings.excluding(user.password); (5) } } 1 QuerydslPredicateExecutor provides access to specific finder methods for Predicate . 2 QuerydslBinderCustomizer defined on the repository interface is automatically picked up and shortcuts @QuerydslPredicate(bindings=…​) . 3 Define the binding for the username property to be a simple contains binding. 4 Define the default binding for String properties to be a case-insensitive contains match. 5 Exclude the password property from Predicate resolution. You can register a QuerydslBinderCustomizerDefaults bean holding default Querydsl bindings before applying specific bindings from the repository or @QuerydslPredicate . Repository Populators: If you work with the Spring JDBC module, you are probably familiar with the support for populating a DataSource with SQL scripts. A similar abstraction is available on the repositories level, although it does not use SQL as the data definition language because it must be store-independent. Thus, the populators support XML (through Spring’s OXM abstraction) and JSON (through Jackson) to define data with which to populate the repositories. Assume you have a file called data.json with the following content: Data defined in JSON [ { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Dave"", ""lastname"" : ""Matthews"" }, { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Carter"", ""lastname"" : ""Beauford"" } ] You can populate your repositories by using the populator elements of the repository namespace provided in Spring Data Commons. To populate the preceding data to your PersonRepository , declare a populator similar to the following: Declaring a Jackson repository populator <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd""> <repository:jackson2-populator locations=""classpath:data.json"" /> </beans> The preceding declaration causes the data.json file to be read and deserialized by a Jackson ObjectMapper . The type to which the JSON object is unmarshalled is determined by inspecting the _class attribute of the JSON document. The infrastructure eventually selects the appropriate repository to handle the object that was deserialized. To instead use XML to define the data the repositories should be populated with, you can use the unmarshaller-populator element. You configure it to use one of the XML marshaller options available in Spring OXM. See the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/data-access/oxm.html) for details. The following example shows how to unmarshall a repository populator with JAXB: Declaring an unmarshalling repository populator (using JAXB) <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xmlns:oxm=""http://www.springframework.org/schema/oxm"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd http://www.springframework.org/schema/oxm https://www.springframework.org/schema/oxm/spring-oxm.xsd""> <repository:unmarshaller-populator locations=""classpath:data.json"" unmarshaller-ref=""unmarshaller"" /> <oxm:jaxb2-marshaller contextPath=""com.acme"" /> </beans>"
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/scrolling.html","Scrolling: Scrolling is a more fine-grained approach to iterate through larger results set chunks. Scrolling consists of a stable sort, a scroll type (Offset- or Keyset-based scrolling) and result limiting. You can define simple sorting expressions by using property names and define static result limiting using the Top or First keyword(query-methods-details.html#repositories.limit-query-result) through query derivation. You can concatenate expressions to collect multiple criteria into one expression. Scroll queries return a Window<T> that allows obtaining the element’s scroll position to fetch the next Window<T> until your application has consumed the entire query result. Similar to consuming a Java Iterator<List<…>> by obtaining the next batch of results, query result scrolling lets you access the a ScrollPosition through Window.positionAt(…​) . Window<User> users = repository.findFirst10ByLastnameOrderByFirstname(""Doe"", ScrollPosition.offset()); do { for (User u : users) { // consume the user } // obtain the next Scroll users = repository.findFirst10ByLastnameOrderByFirstname(""Doe"", users.positionAt(users.size() - 1)); } while (!users.isEmpty() && users.hasNext()); The ScrollPosition identifies the exact position of an element with the entire query result. Query execution treats the position parameter exclusive , results will start after the given position. ScrollPosition#offset() and ScrollPosition#keyset() as special incarnations of a ScrollPosition indicating the start of a scroll operation. WindowIterator provides a utility to simplify scrolling across Window s by removing the need to check for the presence of a next Window and applying the ScrollPosition . WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(ScrollPosition.offset()); while (users.hasNext()) { User u = users.next(); // consume the user } Scrolling using Offset: Offset scrolling uses similar to pagination, an Offset counter to skip a number of results and let the data source only return results beginning at the given Offset. This simple mechanism avoids large results being sent to the client application. However, most databases require materializing the full query result before your server can return the results. Example 1. Using OffsetScrollPosition with Repository Query Methods interface UserRepository extends Repository<User, Long> { Window<User> findFirst10ByLastnameOrderByFirstname(String lastname, OffsetScrollPosition position); } WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(OffsetScrollPosition.initial()); (1) 1 Start with no offset to include the element at position 0 . There is a difference between ScollPosition.offset() and ScollPosition.offset(0L) . The former indicates the start of scroll operation, pointing to no specific offset whereas the latter identifies the first element (at position 0 ) of the result. Given the exclusive nature of scrolling, using ScollPosition.offset(0) skips the first element and translate to an offset of 1 . Scrolling using Keyset-Filtering: Offset-based requires most databases require materializing the entire result before your server can return the results. So while the client only sees the portion of the requested results, your server needs to build the full result, which causes additional load. Keyset-Filtering approaches result subset retrieval by leveraging built-in capabilities of your database aiming to reduce the computation and I/O requirements for individual queries. This approach maintains a set of keys to resume scrolling by passing keys into the query, effectively amending your filter criteria. The core idea of Keyset-Filtering is to start retrieving results using a stable sorting order. Once you want to scroll to the next chunk, you obtain a ScrollPosition that is used to reconstruct the position within the sorted result. The ScrollPosition captures the keyset of the last entity within the current Window . To run the query, reconstruction rewrites the criteria clause to include all sort fields and the primary key so that the database can leverage potential indexes to run the query. The database needs only constructing a much smaller result from the given keyset position without the need to fully materialize a large result and then skipping results until reaching a particular offset. Keyset-Filtering requires the keyset properties (those used for sorting) to be non-nullable. This limitation applies due to the store specific null value handling of comparison operators as well as the need to run queries against an indexed source. Keyset-Filtering on nullable properties will lead to unexpected results. Using KeysetScrollPosition with Repository Query Methods interface UserRepository extends Repository<User, Long> { Window<User> findFirst10ByLastnameOrderByFirstname(String lastname, KeysetScrollPosition position); } WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(ScrollPosition.keyset()); (1) 1 Start at the very beginning and do not apply additional filtering. Keyset-Filtering works best when your database contains an index that matches the sort fields, hence a static sort works well. Scroll queries applying Keyset-Filtering require to the properties used in the sort order to be returned by the query, and these must be mapped in the returned entity. You can use interface and DTO projections, however make sure to include all properties that you’ve sorted by to avoid keyset extraction failures. When specifying your Sort order, it is sufficient to include sort properties relevant to your query; You do not need to ensure unique query results if you do not want to. The keyset query mechanism amends your sort order by including the primary key (or any remainder of composite primary keys) to ensure each query result is unique."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/null-handling.html","Null Handling of Repository Methods: As of Spring Data 2.0, repository CRUD methods that return an individual aggregate instance use Java 8’s Optional to indicate the potential absence of a value. Besides that, Spring Data supports returning the following wrapper types on query methods: com.google.common.base.Optional scala.Option io.vavr.control.Option Alternatively, query methods can choose not to use a wrapper type at all. The absence of a query result is then indicated by returning null . Repository methods returning collections, collection alternatives, wrappers, and streams are guaranteed never to return null but rather the corresponding empty representation. See “ Repository query return types(query-return-types-reference.html) ” for details. Nullability Annotations: You can express nullability constraints for repository methods by using Spring Framework’s nullability annotations(https://docs.spring.io/spring-framework/reference/6.1/core/null-safety.html) . They provide a tooling-friendly approach and opt-in null checks during runtime, as follows: @NonNullApi(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNullApi.html) : Used on the package level to declare that the default behavior for parameters and return values is, respectively, neither to accept nor to produce null values. @NonNull(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNull.html) : Used on a parameter or return value that must not be null (not needed on a parameter and return value where @NonNullApi applies). @Nullable(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/Nullable.html) : Used on a parameter or return value that can be null . Spring annotations are meta-annotated with JSR 305(https://jcp.org/en/jsr/detail?id=305) annotations (a dormant but widely used JSR). JSR 305 meta-annotations let tooling vendors (such as IDEA(https://www.jetbrains.com/help/idea/nullable-and-notnull-annotations.html) , Eclipse(https://help.eclipse.org/latest/index.jsp?topic=/org.eclipse.jdt.doc.user/tasks/task-using_external_null_annotations.htm) , and Kotlin(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) ) provide null-safety support in a generic way, without having to hard-code support for Spring annotations. To enable runtime checking of nullability constraints for query methods, you need to activate non-nullability on the package level by using Spring’s @NonNullApi in package-info.java , as shown in the following example: Declaring Non-nullability in package-info.java Once non-null defaulting is in place, repository query method invocations get validated at runtime for nullability constraints. If a query result violates the defined constraint, an exception is thrown. This happens when the method would return null but is declared as non-nullable (the default with the annotation defined on the package in which the repository resides). If you want to opt-in to nullable results again, selectively use @Nullable on individual methods. Using the result wrapper types mentioned at the start of this section continues to work as expected: an empty result is translated into the value that represents absence. The following example shows a number of the techniques just described: Using different nullability constraints package com.acme; (1) import org.springframework.lang.Nullable; interface UserRepository extends Repository<User, Long> { User getByEmailAddress(EmailAddress emailAddress); (2) @Nullable User findByEmailAddress(@Nullable EmailAddress emailAdress); (3) Optional<User> findOptionalByEmailAddress(EmailAddress emailAddress); (4) } 1 The repository resides in a package (or sub-package) for which we have defined non-null behavior. 2 Throws an EmptyResultDataAccessException when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . 3 Returns null when the query does not produce a result. Also accepts null as the value for emailAddress . 4 Returns Optional.empty() when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . Nullability in Kotlin-based Repositories: Kotlin has the definition of nullability constraints(https://kotlinlang.org/docs/reference/null-safety.html) baked into the language. Kotlin code compiles to bytecode, which does not express nullability constraints through method signatures but rather through compiled-in metadata. Make sure to include the kotlin-reflect JAR in your project to enable introspection of Kotlin’s nullability constraints. Spring Data repositories use the language mechanism to define those constraints to apply the same runtime checks, as follows: Using nullability constraints on Kotlin repositories interface UserRepository : Repository<User, String> { fun findByUsername(username: String): User (1) fun findByFirstname(firstname: String?): User? (2) } 1 The method defines both the parameter and the result as non-nullable (the Kotlin default). The Kotlin compiler rejects method invocations that pass null to the method. If the query yields an empty result, an EmptyResultDataAccessException is thrown. 2 This method accepts null for the firstname parameter and returns null if the query does not produce a result."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/projections.html","Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> ."
"https://docs.spring.io/spring-data/commons/reference/3.3/query-by-example.html","Query by Example: Introduction: This chapter provides an introduction to Query by Example and explains how to use it. Query by Example (QBE) is a user-friendly querying technique with a simple interface. It allows dynamic query creation and does not require you to write queries that contain field names. In fact, Query by Example does not require you to write queries by using store-specific query languages at all. This chapter explains the core concepts of Query by Example. The information is pulled from the Spring Data Commons module. Depending on your database, String matching support can be limited. Usage: The Query by Example API consists of four parts: Probe: The actual example of a domain object with populated fields. ExampleMatcher : The ExampleMatcher carries details on how to match particular fields. It can be reused across multiple Examples. Example : An Example consists of the probe and the ExampleMatcher . It is used to create the query. FetchableFluentQuery : A FetchableFluentQuery offers a fluent API, that allows further customization of a query derived from an Example . Using the fluent API lets you specify ordering projection and result processing for your query. Query by Example is well suited for several use cases: Querying your data store with a set of static or dynamic constraints. Frequent refactoring of the domain objects without worrying about breaking existing queries. Working independently of the underlying data store API. Query by Example also has several limitations: No support for nested or grouped property constraints, such as firstname = ?0 or (firstname = ?1 and lastname = ?2) . Store-specific support on string matching. Depending on your databases, String matching can support starts/contains/ends/regex for strings. Exact matching for other property types. Before getting started with Query by Example, you need to have a domain object. To get started, create an interface for your repository, as shown in the following example: Sample Person object public class Person { @Id private String id; private String firstname; private String lastname; private Address address; // … getters and setters omitted } The preceding example shows a simple domain object. You can use it to create an Example . By default, fields having null values are ignored, and strings are matched by using the store specific defaults. Inclusion of properties into a Query by Example criteria is based on nullability. Properties using primitive types ( int , double , …) are always included unless the ExampleMatcher ignores the property path(#query-by-example.matchers) . Examples can be built by either using the of factory method or by using ExampleMatcher(#query-by-example.matchers) . Example is immutable. The following listing shows a simple Example: Example 1. Simple Example Person person = new Person(); (1) person.setFirstname(""Dave""); (2) Example<Person> example = Example.of(person); (3) 1 Create a new instance of the domain object. 2 Set the properties to query. 3 Create the Example . You can run the example queries by using repositories. To do so, let your repository interface extend QueryByExampleExecutor<T> . The following listing shows an excerpt from the QueryByExampleExecutor interface: The QueryByExampleExecutor public interface QueryByExampleExecutor<T> { <S extends T> S findOne(Example<S> example); <S extends T> Iterable<S> findAll(Example<S> example); // … more functionality omitted. } Example Matchers: Examples are not limited to default settings. You can specify your own defaults for string matching, null handling, and property-specific settings by using the ExampleMatcher , as shown in the following example: Example 2. Example matcher with customized matching Person person = new Person(); (1) person.setFirstname(""Dave""); (2) ExampleMatcher matcher = ExampleMatcher.matching() (3) .withIgnorePaths(""lastname"") (4) .withIncludeNullValues() (5) .withStringMatcher(StringMatcher.ENDING); (6) Example<Person> example = Example.of(person, matcher); (7) 1 Create a new instance of the domain object. 2 Set properties. 3 Create an ExampleMatcher to expect all values to match. It is usable at this stage even without further configuration. 4 Construct a new ExampleMatcher to ignore the lastname property path. 5 Construct a new ExampleMatcher to ignore the lastname property path and to include null values. 6 Construct a new ExampleMatcher to ignore the lastname property path, to include null values, and to perform suffix string matching. 7 Create a new Example based on the domain object and the configured ExampleMatcher . By default, the ExampleMatcher expects all values set on the probe to match. If you want to get results matching any of the predicates defined implicitly, use ExampleMatcher.matchingAny() . You can specify behavior for individual properties (such as ""firstname"" and ""lastname"" or, for nested properties, ""address.city""). You can tune it with matching options and case sensitivity, as shown in the following example: Configuring matcher options ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", endsWith()) .withMatcher(""lastname"", startsWith().ignoreCase()); } Another way to configure matcher options is to use lambdas (introduced in Java 8). This approach creates a callback that asks the implementor to modify the matcher. You need not return the matcher, because configuration options are held within the matcher instance. The following example shows a matcher that uses lambdas: Configuring matcher options with lambdas ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", match -> match.endsWith()) .withMatcher(""firstname"", match -> match.startsWith()); } Queries created by Example use a merged view of the configuration. Default matching settings can be set at the ExampleMatcher level, while individual settings can be applied to particular property paths. Settings that are set on ExampleMatcher are inherited by property path settings unless they are defined explicitly. Settings on a property patch have higher precedence than default settings. The following table describes the scope of the various ExampleMatcher settings: Table 1. Scope of ExampleMatcher settings Setting Scope Null-handling ExampleMatcher String matching ExampleMatcher and property path Ignoring properties Property path Case sensitivity ExampleMatcher and property path Value transformation Property path Fluent API: QueryByExampleExecutor offers one more method, which we did not mention so far: <S extends T, R> R findBy(Example<S> example, Function<FluentQuery.FetchableFluentQuery<S>, R> queryFunction) . As with other methods, it executes a query derived from an Example . However, with the second argument, you can control aspects of that execution that you cannot dynamically control otherwise. You do so by invoking the various methods of the FetchableFluentQuery in the second argument. sortBy lets you specify an ordering for your result. as lets you specify the type to which you want the result to be transformed. project limits the queried attributes. first , firstValue , one , oneValue , all , page , stream , count , and exists define what kind of result you get and how the query behaves when more than the expected number of results are available. Use the fluent API to get the last of potentially many results, ordered by lastname. Optional<Person> match = repository.findBy(example, q -> q .sortBy(Sort.by(""lastname"").descending()) .first() );"
"https://docs.spring.io/spring-data/commons/reference/3.3/value-expressions.html","Value Expressions Fundamentals: Value Expressions are a combination of Spring Expression Language (SpEL)(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) and Property Placeholder Resolution(https://docs.spring.io/spring-framework/reference/6.1/core/beans/environment.html#beans-placeholder-resolution-in-statements) . They combine powerful evaluation of programmatic expressions with the simplicity to resort to property-placeholder resolution to obtain values from the Environment such as configuration properties. Expressions are expected to be defined by a trusted input such as an annotation value and not to be determined from user input. The following code demonstrates how to use expressions in the context of annotations. Example 1. Annotation Usage @Document(""orders-#{tenantService.getOrderCollection()}-${tenant-config.suffix}"") class Order { // … } Value Expressions can be defined from a sole SpEL Expression, a Property Placeholder or a composite expression mixing various expressions including literals. Example 2. Expression Examples #{tenantService.getOrderCollection()} (1) #{(1+1) + '-hello-world'} (2) ${tenant-config.suffix} (3) orders-${tenant-config.suffix} (4) #{tenantService.getOrderCollection()}-${tenant-config.suffix} (5) 1 Value Expression using a single SpEL Expression. 2 Value Expression using a static SpEL Expression evaluating to 2-hello-world . 3 Value Expression using a single Property Placeholder. 4 Composite expression comprised of the literal orders- and the Property Placeholder ${tenant-config.suffix} . 5 Composite expression using SpEL, Property Placeholders and literals. Using value expressions introduces a lot of flexibility to your code. Doing so requires evaluation of the expression on each usage and, therefore, value expression evaluation has an impact on the performance profile. Parsing and Evaluation: Value Expressions are parsed by the ValueExpressionParser API. Instances of ValueExpression are thread-safe and can be cached for later use to avoid repeated parsing. The following example shows the Value Expression API usage: Parsing and Evaluation Java Kotlin ValueParserConfiguration configuration = SpelExpressionParser::new; ValueEvaluationContext context = ValueEvaluationContext.of(environment, evaluationContext); ValueExpressionParser parser = ValueExpressionParser.create(configuration); ValueExpression expression = parser.parse(""Hello, World""); Object result = expression.evaluate(context); val configuration = ValueParserConfiguration { SpelExpressionParser() } val context = ValueEvaluationContext.of(environment, evaluationContext) val parser = ValueExpressionParser.create(configuration) val expression: ValueExpression = parser.parse(""Hello, World"") val result: Any = expression.evaluate(context) SpEL Expressions: SpEL Expressions(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) follow the Template style where the expression is expected to be enclosed within the #{…} format. Expressions are evaluated using an EvaluationContext that is provided by EvaluationContextProvider . The context itself is a powerful StandardEvaluationContext allowing a wide range of operations, access to static types and context extensions. Make sure to parse and evaluate only expressions from trusted sources such as annotations. Accepting user-provided expressions can create an entry path to exploit the application context and your system resulting in a potential security vulnerability. Extending the Evaluation Context: EvaluationContextProvider and its reactive variant ReactiveEvaluationContextProvider provide access to an EvaluationContext . ExtensionAwareEvaluationContextProvider and its reactive variant ReactiveExtensionAwareEvaluationContextProvider are default implementations that determine context extensions from an application context, specifically ListableBeanFactory . Extensions implement either EvaluationContextExtension or ReactiveEvaluationContextExtension to provide extension support to hydrate EvaluationContext . That are a root object, properties and functions (top-level methods). The following example shows a context extension that provides a root object, properties, functions and an aliased function. Implementing a EvaluationContextExtension Java Kotlin @Component public class MyExtension implements EvaluationContextExtension { @Override public String getExtensionId() { return ""my-extension""; } @Override public Object getRootObject() { return new CustomExtensionRootObject(); } @Override public Map<String, Object> getProperties() { Map<String, Object> properties = new HashMap<>(); properties.put(""key"", ""Hello""); return properties; } @Override public Map<String, Function> getFunctions() { Map<String, Function> functions = new HashMap<>(); try { functions.put(""aliasedMethod"", new Function(getClass().getMethod(""extensionMethod""))); return functions; } catch (Exception o_O) { throw new RuntimeException(o_O); } } public static String extensionMethod() { return ""Hello World""; } public static int add(int i1, int i2) { return i1 + i2; } } public class CustomExtensionRootObject { public boolean rootObjectInstanceMethod() { return true; } } @Component class MyExtension : EvaluationContextExtension { override fun getExtensionId(): String { return ""my-extension"" } override fun getRootObject(): Any? { return CustomExtensionRootObject() } override fun getProperties(): Map<String, Any> { val properties: MutableMap<String, Any> = HashMap() properties[""key""] = ""Hello"" return properties } override fun getFunctions(): Map<String, Function> { val functions: MutableMap<String, Function> = HashMap() try { functions[""aliasedMethod""] = Function(javaClass.getMethod(""extensionMethod"")) return functions } catch (o_O: Exception) { throw RuntimeException(o_O) } } companion object { fun extensionMethod(): String { return ""Hello World"" } fun add(i1: Int, i2: Int): Int { return i1 + i2 } } } class CustomExtensionRootObject { fun rootObjectInstanceMethod(): Boolean { return true } } Once the above shown extension is registered, you can use its exported methods, properties and root object to evaluate SpEL expressions: Example 3. Expression Evaluation Examples #{add(1, 2)} (1) #{extensionMethod()} (2) #{aliasedMethod()} (3) #{key} (4) #{rootObjectInstanceMethod()} (5) 1 Invoke the method add declared by MyExtension resulting in 3 as the method adds both numeric parameters and returns the sum. 2 Invoke the method extensionMethod declared by MyExtension resulting in Hello World . 3 Invoke the method aliasedMethod . The method is exposed as function and redirects into the method extensionMethod declared by MyExtension resulting in Hello World . 4 Evaluate the key property resulting in Hello . 5 Invoke the method rootObjectInstanceMethod on the root object instance CustomExtensionRootObject . You can find real-life context extensions at SecurityEvaluationContextExtension(https://github.com/spring-projects/spring-security/blob/main/data/src/main/java/org/springframework/security/data/repository/query/SecurityEvaluationContextExtension.java) . Property Placeholders: Property placeholders following the form ${…} refer to properties provided typically by a PropertySource through Environment . Properties are useful to resolve against system properties, application configuration files, environment configuration or property sources contributed by secret management systems. You can find more details on the property placeholders in Spring Framework’s documentation on @Value usage(https://docs.spring.io/spring-framework/reference/6.1/core/beans/annotation-config/value-annotations.html#page-title) ."
"https://docs.spring.io/spring-data/commons/reference/3.3/auditing.html","Auditing: Basics: Spring Data provides sophisticated support to transparently keep track of who created or changed an entity and when the change happened.To benefit from that functionality, you have to equip your entity classes with auditing metadata that can be defined either using annotations or by implementing an interface. Additionally, auditing has to be enabled either through Annotation configuration or XML configuration to register the required infrastructure components. Please refer to the store-specific section for configuration samples. Applications that only track creation and modification dates are not required do make their entities implement AuditorAware(#auditing.auditor-aware) . Annotation-based Auditing Metadata: We provide @CreatedBy and @LastModifiedBy to capture the user who created or modified the entity as well as @CreatedDate and @LastModifiedDate to capture when the change happened. An audited entity class Customer { @CreatedBy private User user; @CreatedDate private Instant createdDate; // … further properties omitted } As you can see, the annotations can be applied selectively, depending on which information you want to capture. The annotations, indicating to capture when changes are made, can be used on properties of type JDK8 date and time types, long , Long , and legacy Java Date and Calendar . Auditing metadata does not necessarily need to live in the root level entity but can be added to an embedded one (depending on the actual store in use), as shown in the snippet below. Audit metadata in embedded entity class Customer { private AuditMetadata auditingMetadata; // … further properties omitted } class AuditMetadata { @CreatedBy private User user; @CreatedDate private Instant createdDate; } Interface-based Auditing Metadata: In case you do not want to use annotations to define auditing metadata, you can let your domain class implement the Auditable interface. It exposes setter methods for all of the auditing properties. AuditorAware: In case you use either @CreatedBy or @LastModifiedBy , the auditing infrastructure somehow needs to become aware of the current principal. To do so, we provide an AuditorAware<T> SPI interface that you have to implement to tell the infrastructure who the current user or system interacting with the application is. The generic type T defines what type the properties annotated with @CreatedBy or @LastModifiedBy have to be. The following example shows an implementation of the interface that uses Spring Security’s Authentication object: Implementation of AuditorAware based on Spring Security class SpringSecurityAuditorAware implements AuditorAware<User> { @Override public Optional<User> getCurrentAuditor() { return Optional.ofNullable(SecurityContextHolder.getContext()) .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getPrincipal) .map(User.class::cast); } } The implementation accesses the Authentication object provided by Spring Security and looks up the custom UserDetails instance that you have created in your UserDetailsService implementation. We assume here that you are exposing the domain user through the UserDetails implementation but that, based on the Authentication found, you could also look it up from anywhere. ReactiveAuditorAware: When using reactive infrastructure you might want to make use of contextual information to provide @CreatedBy or @LastModifiedBy information. We provide an ReactiveAuditorAware<T> SPI interface that you have to implement to tell the infrastructure who the current user or system interacting with the application is. The generic type T defines what type the properties annotated with @CreatedBy or @LastModifiedBy have to be. The following example shows an implementation of the interface that uses reactive Spring Security’s Authentication object: Implementation of ReactiveAuditorAware based on Spring Security class SpringSecurityAuditorAware implements ReactiveAuditorAware<User> { @Override public Mono<User> getCurrentAuditor() { return ReactiveSecurityContextHolder.getContext() .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getPrincipal) .map(User.class::cast); } } The implementation accesses the Authentication object provided by Spring Security and looks up the custom UserDetails instance that you have created in your UserDetailsService implementation. We assume here that you are exposing the domain user through the UserDetails implementation but that, based on the Authentication found, you could also look it up from anywhere."
"https://docs.spring.io/spring-data/commons/reference/3.3/custom-conversions.html","Custom Conversions: The following example of a Spring Converter implementation converts from a String to a custom Email value object: @ReadingConverter public class EmailReadConverter implements Converter<String, Email> { public Email convert(String source) { return Email.valueOf(source); } } If you write a Converter whose source and target type are native types, we cannot determine whether we should consider it as a reading or a writing converter. Registering the converter instance as both might lead to unwanted results. For example, a Converter<String, Long> is ambiguous, although it probably does not make sense to try to convert all String instances into Long instances when writing. To let you force the infrastructure to register a converter for only one way, we provide @ReadingConverter and @WritingConverter annotations to be used in the converter implementation. Converters are subject to explicit registration as instances are not picked up from a classpath or container scan to avoid unwanted registration with a conversion service and the side effects resulting from such a registration. Converters are registered with CustomConversions as the central facility that allows registration and querying for registered converters based on source- and target type. CustomConversions ships with a pre-defined set of converter registrations: JSR-310 Converters for conversion between java.time , java.util.Date and String types. Default converters for local temporal types (e.g. LocalDateTime to java.util.Date ) rely on system-default timezone settings to convert between those types. You can override the default converter, by registering your own converter. Converter Disambiguation: Generally, we inspect the Converter implementations for the source and target types they convert from and to. Depending on whether one of those is a type the underlying data access API can handle natively, we register the converter instance as a reading or a writing converter. The following examples show a writing- and a read converter (note the difference is in the order of the qualifiers on Converter ): // Write converter as only the target type is one that can be handled natively class MyConverter implements Converter<Person, String> { … } // Read converter as only the source type is one that can be handled natively class MyConverter implements Converter<String, Person> { … }"
"https://docs.spring.io/spring-data/commons/reference/3.3/entity-callbacks.html","Entity Callbacks: The Spring Data infrastructure provides hooks for modifying an entity before and after certain methods are invoked. Those so called EntityCallback instances provide a convenient way to check and potentially modify an entity in a callback fashioned style. An EntityCallback looks pretty much like a specialized ApplicationListener . Some Spring Data modules publish store specific events (such as BeforeSaveEvent ) that allow modifying the given entity. In some cases, such as when working with immutable types, these events can cause trouble. Also, event publishing relies on ApplicationEventMulticaster . If configuring that with an asynchronous TaskExecutor it can lead to unpredictable outcomes, as event processing can be forked onto a Thread. Entity callbacks provide integration points with both synchronous and reactive APIs to guarantee in-order execution at well-defined checkpoints within the processing chain, returning a potentially modified entity or an reactive wrapper type. Entity callbacks are typically separated by API type. This separation means that a synchronous API considers only synchronous entity callbacks and a reactive implementation considers only reactive entity callbacks. The Entity Callback API has been introduced with Spring Data Commons 2.2. It is the recommended way of applying entity modifications. Existing store specific ApplicationEvents are still published before the invoking potentially registered EntityCallback instances. Implementing Entity Callbacks: An EntityCallback is directly associated with its domain type through its generic type argument. Each Spring Data module typically ships with a set of predefined EntityCallback interfaces covering the entity lifecycle. Anatomy of an EntityCallback @FunctionalInterface public interface BeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked before a domain object is saved. * Can return either the same or a modified instance. * * @return the domain object to be persisted. */ (1) T onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called before an entity is saved. Returns a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Anatomy of a reactive EntityCallback @FunctionalInterface public interface ReactiveBeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked on subscription, before a domain object is saved. * The returned Publisher can emit either the same or a modified instance. * * @return Publisher emitting the domain object to be persisted. */ (1) Publisher<T> onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called on subscription, before an entity is saved. Emits a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Optional entity callback parameters are defined by the implementing Spring Data module and inferred from call site of EntityCallback.callback() . Implement the interface suiting your application needs like shown in the example below: Example BeforeSaveCallback class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { (1) if(collection == ""user"") { return // ... } return // ... } @Override public int getOrder() { return 100; (2) } } 1 Callback implementation according to your requirements. 2 Potentially order the entity callback if multiple ones for the same domain type exist. Ordering follows lowest precedence. Registering Entity Callbacks: EntityCallback beans are picked up by the store specific implementations in case they are registered in the ApplicationContext . Most template APIs already implement ApplicationContextAware and therefore have access to the ApplicationContext The following example explains a collection of valid entity callback registrations: Example EntityCallback Bean registration @Order(1) (1) @Component class First implements BeforeSaveCallback<Person> { @Override public Person onBeforeSave(Person person) { return // ... } } @Component class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { // ... } @Override public int getOrder() { return 100; (2) } } @Configuration public class EntityCallbackConfiguration { @Bean BeforeSaveCallback<Person> unorderedLambdaReceiverCallback() { (3) return (BeforeSaveCallback<Person>) it -> // ... } } @Component class UserCallbacks implements BeforeConvertCallback<User>, BeforeSaveCallback<User> { (4) @Override public Person onBeforeConvert(User user) { return // ... } @Override public Person onBeforeSave(User user) { return // ... } } 1 BeforeSaveCallback receiving its order from the @Order annotation. 2 BeforeSaveCallback receiving its order via the Ordered interface implementation. 3 BeforeSaveCallback using a lambda expression. Unordered by default and invoked last. Note that callbacks implemented by a lambda expression do not expose typing information hence invoking these with a non-assignable entity affects the callback throughput. Use a class or enum to enable type filtering for the callback bean. 4 Combine multiple entity callback interfaces in a single implementation class."
"https://docs.spring.io/spring-data/commons/reference/3.3/is-new-state-detection.html","Entity State Detection Strategies: The following table describes the strategies that Spring Data offers for detecting whether an entity is new: Table 1. Options for detection whether an entity is new in Spring Data @Id -Property inspection (the default) By default, Spring Data inspects the identifier property of the given entity. If the identifier property is null or 0 in case of primitive types, then the entity is assumed to be new. Otherwise, it is assumed to not be new. @Version -Property inspection If a property annotated with @Version is present and null , or in case of a version property of primitive type 0 the entity is considered new. If the version property is present but has a different value, the entity is considered to not be new. If no version property is present Spring Data falls back to inspection of the identifier property. Implementing Persistable If an entity implements Persistable , Spring Data delegates the new detection to the isNew(…) method of the entity. See the Javadoc(https://docs.spring.io/spring-data/data-commons/docs/3.3.4/api//index.html?org/springframework/data/domain/Persistable.html) for details. Note: Properties of Persistable will get detected and persisted if you use AccessType.PROPERTY . To avoid that, use @Transient . Providing a custom EntityInformation implementation You can customize the EntityInformation abstraction used in the repository base implementation by creating a subclass of the module specific repository factory and overriding the getEntityInformation(…) method. You then have to register the custom implementation of module specific repository factory as a Spring bean. Note that this should rarely be necessary."
"https://docs.spring.io/spring-data/commons/reference/3.3/kotlin.html","Kotlin Support: Kotlin(https://kotlinlang.org) is a statically typed language that targets the JVM (and other platforms) which allows writing concise and elegant code while providing excellent interoperability(https://kotlinlang.org/docs/reference/java-interop.html) with existing libraries written in Java. Spring Data provides first-class support for Kotlin and lets developers write Kotlin applications almost as if Spring Data was a Kotlin native framework. The easiest way to build a Spring application with Kotlin is to leverage Spring Boot and its dedicated Kotlin support(https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-kotlin.html) . This comprehensive tutorial(https://spring.io/guides/tutorials/spring-boot-kotlin/) will teach you how to build Spring Boot applications with Kotlin using start.spring.io(https://start.spring.io/#!language=kotlin&type=gradle-project) . Section Summary: Requirements(kotlin/requirements.html) Null Safety(kotlin/null-safety.html) Object Mapping(kotlin/object-mapping.html) Extensions(kotlin/extensions.html) Coroutines(kotlin/coroutines.html)"
"https://docs.spring.io/spring-data/commons/reference/3.3/kotlin/requirements.html","Requirements: Spring Data supports Kotlin 1.3 and requires kotlin-stdlib (or one of its variants, such as kotlin-stdlib-jdk8 ) and kotlin-reflect to be present on the classpath. Those are provided by default if you bootstrap a Kotlin project via start.spring.io(https://start.spring.io/#!language=kotlin&type=gradle-project) ."
"https://docs.spring.io/spring-data/commons/reference/3.3/kotlin/null-safety.html","Null Safety: One of Kotlin’s key features is null safety(https://kotlinlang.org/docs/null-safety.html) , which cleanly deals with null values at compile time. This makes applications safer through nullability declarations and the expression of “value or no value” semantics without paying the cost of wrappers, such as Optional . (Kotlin allows using functional constructs with nullable values. See this comprehensive guide to Kotlin null safety(https://www.baeldung.com/kotlin/null-safety) .) Although Java does not let you express null safety in its type system, Spring Data API is annotated with JSR-305(https://jcp.org/en/jsr/detail?id=305) tooling friendly annotations declared in the org.springframework.lang package. By default, types from Java APIs used in Kotlin are recognized as platform types(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) , for which null checks are relaxed. Kotlin support for JSR-305 annotations(https://kotlinlang.org/docs/reference/java-interop.html#jsr-305-support) and Spring nullability annotations provide null safety for the whole Spring Data API to Kotlin developers, with the advantage of dealing with null related issues at compile time. See Null Handling of Repository Methods(../repositories/null-handling.html) how null safety applies to Spring Data Repositories. You can configure JSR-305 checks by adding the -Xjsr305 compiler flag with the following options: -Xjsr305={strict|warn|ignore} . For Kotlin versions 1.1+, the default behavior is the same as -Xjsr305=warn . The strict value is required take Spring Data API null-safety into account. Kotlin types inferred from Spring API but should be used with the knowledge that Spring API nullability declaration could evolve, even between minor releases and that more checks may be added in the future. Generic type arguments, varargs, and array elements nullability are not supported yet, but should be in an upcoming release."
"https://docs.spring.io/spring-data/commons/reference/3.3/kotlin/object-mapping.html","Object Mapping: See Kotlin support(../object-mapping.html#mapping.kotlin) for details on how Kotlin objects are materialized."
"https://docs.spring.io/spring-data/commons/reference/3.3/kotlin/extensions.html","Extensions: Kotlin extensions(https://kotlinlang.org/docs/reference/extensions.html) provide the ability to extend existing classes with additional functionality. Spring Data Kotlin APIs use these extensions to add new Kotlin-specific conveniences to existing Spring APIs. Keep in mind that Kotlin extensions need to be imported to be used. Similar to static imports, an IDE should automatically suggest the import in most cases. For example, Kotlin reified type parameters(https://kotlinlang.org/docs/reference/inline-functions.html#reified-type-parameters) provide a workaround for JVM generics type erasure(https://docs.oracle.com/javase/tutorial/java/generics/erasure.html) , and Spring Data provides some extensions to take advantage of this feature. This allows for a better Kotlin API."
"https://docs.spring.io/spring-data/commons/reference/3.3/kotlin/coroutines.html","Coroutines: Kotlin Coroutines(https://kotlinlang.org/docs/reference/coroutines-overview.html) are instances of suspendable computations allowing to write non-blocking code imperatively. On language side, suspend functions provides an abstraction for asynchronous operations while on library side kotlinx.coroutines(https://github.com/Kotlin/kotlinx.coroutines) provides functions like async { }(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/async.html) and types like Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) . Spring Data modules provide support for Coroutines on the following scope: Deferred(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/-deferred/index.html) and Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) return values support in Kotlin extensions Dependencies: Coroutines support is enabled when kotlinx-coroutines-core , kotlinx-coroutines-reactive and kotlinx-coroutines-reactor dependencies are in the classpath: Dependencies to add in Maven pom.xml <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-core</artifactId> </dependency> <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-reactive</artifactId> </dependency> <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-reactor</artifactId> </dependency> Supported versions 1.3.0 and above. How Reactive translates to Coroutines?: For return values, the translation from Reactive to Coroutines APIs is the following: fun handler(): Mono<Void> becomes suspend fun handler() fun handler(): Mono<T> becomes suspend fun handler(): T or suspend fun handler(): T? depending on if the Mono can be empty or not (with the advantage of being more statically typed) fun handler(): Flux<T> becomes fun handler(): Flow<T> Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) is Flux equivalent in Coroutines world, suitable for hot or cold stream, finite or infinite streams, with the following main differences: Flow is push-based while Flux is push-pull hybrid Backpressure is implemented via suspending functions Flow has only a single suspending collect method(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/collect.html) and operators are implemented as extensions(https://kotlinlang.org/docs/reference/extensions.html) Operators are easy to implement(https://github.com/Kotlin/kotlinx.coroutines/tree/master/kotlinx-coroutines-core/common/src/flow/operators) thanks to Coroutines Extensions allow adding custom operators to Flow Collect operations are suspending functions map operator(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/map.html) supports asynchronous operation (no need for flatMap ) since it takes a suspending function parameter Read this blog post about Going Reactive with Spring, Coroutines and Kotlin Flow(https://spring.io/blog/2019/04/12/going-reactive-with-spring-coroutines-and-kotlin-flow) for more details, including how to run code concurrently with Coroutines. Repositories: Here is an example of a Coroutines repository: interface CoroutineRepository : CoroutineCrudRepository<User, String> { suspend fun findOne(id: String): User fun findByFirstname(firstname: String): Flow<User> suspend fun findAllByFirstname(id: String): List<User> } Coroutines repositories are built on reactive repositories to expose the non-blocking nature of data access through Kotlin’s Coroutines. Methods on a Coroutines repository can be backed either by a query method or a custom implementation. Invoking a custom implementation method propagates the Coroutines invocation to the actual implementation method if the custom method is suspend -able without requiring the implementation method to return a reactive type such as Mono or Flux . Note that depending on the method declaration the coroutine context may or may not be available. To retain access to the context, either declare your method using suspend or return a type that enables context propagation such as Flow . suspend fun findOne(id: String): User : Retrieve the data once and synchronously by suspending. fun findByFirstname(firstname: String): Flow<User> : Retrieve a stream of data. The Flow is created eagerly while data is fetched upon Flow interaction ( Flow.collect(…) ). fun getUser(): User : Retrieve data once blocking the thread and without context propagation. This should be avoided. Coroutines repositories are only discovered when the repository extends the CoroutineCrudRepository interface."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/namespace-reference.html","Namespace reference: The <repositories /> Element: The <repositories /> element triggers the setup of the Spring Data repository infrastructure. The most important attribute is base-package , which defines the package to scan for Spring Data repository interfaces. See “ XML Configuration(create-instances.html#repositories.create-instances.xml) ”. The following table describes the attributes of the <repositories /> element: Table 1. Attributes Name Description base-package Defines the package to be scanned for repository interfaces that extend *Repository (the actual interface is determined by the specific Spring Data module) in auto-detection mode. All packages below the configured package are scanned, too. Wildcards are allowed. repository-impl-postfix Defines the postfix to autodetect custom repository implementations. Classes whose names end with the configured postfix are considered as candidates. Defaults to Impl . query-lookup-strategy Determines the strategy to be used to create finder queries. See “ Query Lookup Strategies(query-methods-details.html#repositories.query-methods.query-lookup-strategies) ” for details. Defaults to create-if-not-found . named-queries-location Defines the location to search for a Properties file containing externally defined queries. consider-nested-repositories Whether nested repository interface definitions should be considered. Defaults to false ."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/populator-namespace-reference.html","Populators namespace reference: The <populator /> element: The <populator /> element allows to populate a data store via the Spring Data repository infrastructure. [ 1(#_footnotedef_1) ] Table 1. Attributes Name Description locations Where to find the files to read the objects from the repository shall be populated with. 1(#_footnoteref_1) . see XML Configuration(create-instances.html#repositories.create-instances.xml)"
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/commons/reference/3.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/relational/reference/3.3/index.html","Spring Data JDBC and R2DBC: Spring Data JDBC and R2DBC provide repository support for the Java Database Connectivity (JDBC) respective Reactive Relational Database Connectivity (R2DBC) APIs. It eases development of applications with a consistent programming model that need to access SQL data sources. Introduction(repositories/introduction.html) Introduction to Repositories JDBC(jdbc.html) JDBC Object Mapping and Repositories R2DBC(r2dbc.html) R2DBC Object Mapping and Repositories Kotlin(kotlin.html) Kotlin-specific Support Wiki(https://github.com/spring-projects/spring-data-commons/wiki) What’s New, Upgrade Notes, Supported Versions, additional cross-version information. Jens Schauder, Jay Bryant, Mark Paluch, Bastian Wilhelm © 2008-2024 VMware, Inc. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically."
"https://docs.spring.io/spring-data/relational/reference/3.3/commons/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped."
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/introduction.html","Introduction: This chapter explains the basic foundations of Spring Data repositories. Before continuing to the JDBC or R2DBC specifics, make sure you have a sound understanding of the basic concepts explained here. The goal of the Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. Section Summary: Core concepts(core-concepts.html) Defining Repository Interfaces(definition.html) Creating Repository Instances(create-instances.html) Defining Query Methods(query-methods-details.html) Projections(projections.html) Object Mapping Fundamentals(../object-mapping.html) Custom Conversions(../commons/custom-conversions.html) Custom Repository Implementations(custom-implementations.html) Spring Data Extensions(core-extensions.html) Value Expressions Fundamentals(../value-expressions.html) Query by Example(../query-by-example.html) Publishing Events from Aggregate Roots(core-domain-events.html) Entity Callbacks(../commons/entity-callbacks.html) Null Handling of Repository Methods(null-handling.html) Repository query keywords(query-keywords-reference.html) Repository query return types(query-return-types-reference.html)"
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); } Entity State Detection Strategies: The following table describes the strategies that Spring Data offers for detecting whether an entity is new: Table 1. Options for detection whether an entity is new in Spring Data @Id -Property inspection (the default) By default, Spring Data inspects the identifier property of the given entity. If the identifier property is null or 0 in case of primitive types, then the entity is assumed to be new. Otherwise, it is assumed to not be new. @Version -Property inspection If a property annotated with @Version is present and null , or in case of a version property of primitive type 0 the entity is considered new. If the version property is present but has a different value, the entity is considered to not be new. If no version property is present Spring Data falls back to inspection of the identifier property. Implementing Persistable If an entity implements Persistable , Spring Data delegates the new detection to the isNew(…) method of the entity. See the Javadoc(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//index.html?org/springframework/data/domain/Persistable.html) for details. Note: Properties of Persistable will get detected and persisted if you use AccessType.PROPERTY . To avoid that, use @Transient . Providing a custom EntityInformation implementation You can customize the EntityInformation abstraction used in the repository base implementation by creating a subclass of the module specific repository factory and overriding the getEntityInformation(…) method. You then have to register the custom implementation of module specific repository factory as a Spring bean. Note that this should rarely be necessary."
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/create-instances.html","Creating Repository Instances: This section covers how to create instances and bean definitions for the defined repository interfaces. Java Configuration: Use the store-specific @EnableJdbcRepositories annotation on a Java configuration class to define a configuration for repository activation. For an introduction to Java-based configuration of the Spring container, see JavaConfig in the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/java.html) . A sample configuration to enable Spring Data repositories resembles the following: Sample annotation-based repository configuration @Configuration @EnableJpaRepositories(""com.acme.repositories"") class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } The preceding example uses the JPA-specific annotation, which you would change according to the store module you actually use. The same applies to the definition of the EntityManagerFactory bean. See the sections covering the store-specific configuration. XML Configuration: Each Spring Data module includes a repositories element that lets you define a base package that Spring scans for you, as shown in the following example: Enabling Spring Data repositories via XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans:beans xmlns:beans=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans:beans> In the preceding example, Spring is instructed to scan com.acme.repositories and all its sub-packages for interfaces extending Repository or one of its sub-interfaces. For each interface found, the infrastructure registers the persistence technology-specific FactoryBean to create the appropriate proxies that handle invocations of the query methods. Each bean is registered under a bean name that is derived from the interface name, so an interface of UserRepository would be registered under userRepository . Bean names for nested repository interfaces are prefixed with their enclosing type name. The base package attribute allows wildcards so that you can define a pattern of scanned packages. Using Filters: By default, the infrastructure picks up every interface that extends the persistence technology-specific Repository sub-interface located under the configured base package and creates a bean instance for it. However, you might want more fine-grained control over which interfaces have bean instances created for them. To do so, use filter elements inside the repository declaration. The semantics are exactly equivalent to the elements in Spring’s component filters. For details, see the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/classpath-scanning.html) for these elements. For example, to exclude certain interfaces from instantiation as repository beans, you could use the following configuration: Using filters Java XML @Configuration @EnableJdbcRepositories(basePackages = ""com.acme.repositories"", includeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeRepository"") }, excludeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeOtherRepository"") }) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } <repositories base-package=""com.acme.repositories""> <context:include-filter type=""regex"" expression="".*SomeRepository"" /> <context:exclude-filter type=""regex"" expression="".*SomeOtherRepository"" /> </repositories> The preceding example includes all interfaces ending with SomeRepository and excludes those ending with SomeOtherRepository from being instantiated. Standalone Usage: You can also use the repository infrastructure outside of a Spring container — for example, in CDI environments.You still need some Spring libraries in your classpath, but, generally, you can set up repositories programmatically as well.The Spring Data modules that provide repository support ship with a persistence technology-specific RepositoryFactory that you can use, as follows: Standalone usage of the repository factory RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class);"
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/query-methods-details.html","Defining Query Methods: The repository proxy has two ways to derive a store-specific query from the method name: By deriving the query from the method name directly. By using a manually defined query. Available options depend on the actual store. However, there must be a strategy that decides what actual query is created. The next section describes the available options. Query Lookup Strategies: The following strategies are available for the repository infrastructure to resolve the query. With XML configuration, you can configure the strategy at the namespace through the query-lookup-strategy attribute. For Java configuration, you can use the queryLookupStrategy attribute of the EnableJdbcRepositories annotation. Some strategies may not be supported for particular datastores. CREATE attempts to construct a store-specific query from the query method name. The general approach is to remove a given set of well known prefixes from the method name and parse the rest of the method. You can read more about query construction in “ Query Creation(#repositories.query-methods.query-creation) ”. USE_DECLARED_QUERY tries to find a declared query and throws an exception if it cannot find one. The query can be defined by an annotation somewhere or declared by other means. See the documentation of the specific store to find available options for that store. If the repository infrastructure does not find a declared query for the method at bootstrap time, it fails. CREATE_IF_NOT_FOUND (the default) combines CREATE and USE_DECLARED_QUERY . It looks up a declared query first, and, if no declared query is found, it creates a custom method name-based query. This is the default lookup strategy and, thus, is used if you do not configure anything explicitly. It allows quick query definition by method names but also custom-tuning of these queries by introducing declared queries as needed. Query Creation: The query builder mechanism built into the Spring Data repository infrastructure is useful for building constraining queries over entities of the repository. The following example shows how to create a number of queries: Query creation from method names interface PersonRepository extends Repository<Person, Long> { List<Person> findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // Enables the distinct flag for the query List<Person> findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List<Person> findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); // Enabling ignoring case for an individual property List<Person> findByLastnameIgnoreCase(String lastname); // Enabling ignoring case for all suitable properties List<Person> findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // Enabling static ORDER BY for a query List<Person> findByLastnameOrderByFirstnameAsc(String lastname); List<Person> findByLastnameOrderByFirstnameDesc(String lastname); } Parsing query method names is divided into subject and predicate. The first part ( find…By , exists…By ) defines the subject of the query, the second part forms the predicate. The introducing clause (subject) can contain further expressions. Any text between find (or other introducing keywords) and By is considered to be descriptive unless using one of the result-limiting keywords such as a Distinct to set a distinct flag on the query to be created or Top/First to limit query results(#repositories.limit-query-result) . The appendix contains the full list of query method subject keywords(query-keywords-reference.html#appendix.query.method.subject) and query method predicate keywords including sorting and letter-casing modifiers(query-keywords-reference.html#appendix.query.method.predicate) . However, the first By acts as a delimiter to indicate the start of the actual criteria predicate. At a very basic level, you can define conditions on entity properties and concatenate them with And and Or . The actual result of parsing the method depends on the persistence store for which you create the query. However, there are some general things to notice: The expressions are usually property traversals combined with operators that can be concatenated. You can combine property expressions with AND and OR . You also get support for operators such as Between , LessThan , GreaterThan , and Like for the property expressions. The supported operators can vary by datastore, so consult the appropriate part of your reference documentation. The method parser supports setting an IgnoreCase flag for individual properties (for example, findByLastnameIgnoreCase(…) ) or for all properties of a type that supports ignoring case (usually String instances — for example, findByLastnameAndFirstnameAllIgnoreCase(…) ). Whether ignoring cases is supported may vary by store, so consult the relevant sections in the reference documentation for the store-specific query method. You can apply static ordering by appending an OrderBy clause to the query method that references a property and by providing a sorting direction ( Asc or Desc ). To create a query method that supports dynamic sorting, see “ Paging, Iterating Large Results, Sorting & Limiting(#repositories.special-parameters) ”. Property Expressions: Property expressions can refer only to a direct property of the managed entity, as shown in the preceding example. At query creation time, you already make sure that the parsed property is a property of the managed domain class. However, you can also define constraints by traversing nested properties. Consider the following method signature: List<Person> findByAddressZipCode(ZipCode zipCode); Assume a Person has an Address with a ZipCode . In that case, the method creates the x.address.zipCode property traversal. The resolution algorithm starts by interpreting the entire part ( AddressZipCode ) as the property and checks the domain class for a property with that name (uncapitalized). If the algorithm succeeds, it uses that property. If not, the algorithm splits up the source at the camel-case parts from the right side into a head and a tail and tries to find the corresponding property — in our example, AddressZip and Code . If the algorithm finds a property with that head, it takes the tail and continues building the tree down from there, splitting the tail up in the way just described. If the first split does not match, the algorithm moves the split point to the left ( Address , ZipCode ) and continues. Although this should work for most cases, it is possible for the algorithm to select the wrong property. Suppose the Person class has an addressZip property as well. The algorithm would match in the first split round already, choose the wrong property, and fail (as the type of addressZip probably has no code property). To resolve this ambiguity you can use _ inside your method name to manually define traversal points. So our method name would be as follows: List<Person> findByAddress_ZipCode(ZipCode zipCode); Because we treat underscores ( _ ) as a reserved character, we strongly advise to follow standard Java naming conventions (that is, not using underscores in property names but applying camel case instead). Field Names starting with underscore: Field names may start with underscores like String _name . Make sure to preserve the _ as in _name and use double _ to split nested paths like user__name . Upper Case Field Names: Field names that are all uppercase can be used as such. Nested paths if applicable require splitting via _ as in USER_name . Field Names with 2nd uppercase letter: Field names that consist of a starting lower case letter followed by an uppercase one like String qCode can be resolved by starting with two upper case letters as in QCode . Please be aware of potential path ambiguities. Path Ambiguities: In the following sample the arrangement of properties qCode and q , with q containing a property called code , creates an ambiguity for the path QCode . record Container(String qCode, Code q) {} record Code(String code) {} Since a direct match on a property is considered first, any potential nested paths will not be considered and the algorithm picks the qCode field. In order to select the code field in q the underscore notation Q_Code is required. Repository Methods Returning Collections or Iterables: Query methods that return multiple results can use standard Java Iterable , List , and Set . Beyond that, we support returning Spring Data’s Streamable , a custom extension of Iterable , as well as collection types provided by Vavr(https://www.vavr.io/) . Refer to the appendix explaining all possible query method return types(query-return-types-reference.html#appendix.query.return.types) . Using Streamable as Query Method Return Type: You can use Streamable as alternative to Iterable or any collection type. It provides convenience methods to access a non-parallel Stream (missing from Iterable ) and the ability to directly ….filter(…) and ….map(…) over the elements and concatenate the Streamable to others: Using Streamable to combine query method results interface PersonRepository extends Repository<Person, Long> { Streamable<Person> findByFirstnameContaining(String firstname); Streamable<Person> findByLastnameContaining(String lastname); } Streamable<Person> result = repository.findByFirstnameContaining(""av"") .and(repository.findByLastnameContaining(""ea"")); Returning Custom Streamable Wrapper Types: Providing dedicated wrapper types for collections is a commonly used pattern to provide an API for a query result that returns multiple elements. Usually, these types are used by invoking a repository method returning a collection-like type and creating an instance of the wrapper type manually. You can avoid that additional step as Spring Data lets you use these wrapper types as query method return types if they meet the following criteria: The type implements Streamable . The type exposes either a constructor or a static factory method named of(…) or valueOf(…) that takes Streamable as an argument. The following listing shows an example: class Product { (1) MonetaryAmount getPrice() { … } } @RequiredArgsConstructor(staticName = ""of"") class Products implements Streamable<Product> { (2) private final Streamable<Product> streamable; public MonetaryAmount getTotal() { (3) return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); } @Override public Iterator<Product> iterator() { (4) return streamable.iterator(); } } interface ProductRepository implements Repository<Product, Long> { Products findAllByDescriptionContaining(String text); (5) } 1 A Product entity that exposes API to access the product’s price. 2 A wrapper type for a Streamable<Product> that can be constructed by using Products.of(…) (factory method created with the Lombok annotation). A standard constructor taking the Streamable<Product> will do as well. 3 The wrapper type exposes an additional API, calculating new values on the Streamable<Product> . 4 Implement the Streamable interface and delegate to the actual result. 5 That wrapper type Products can be used directly as a query method return type. You do not need to return Streamable<Product> and manually wrap it after the query in the repository client. Support for Vavr Collections: Vavr(https://www.vavr.io/) is a library that embraces functional programming concepts in Java. It ships with a custom set of collection types that you can use as query method return types, as the following table shows: Vavr collection type Used Vavr implementation type Valid Java source types io.vavr.collection.Seq io.vavr.collection.List java.util.Iterable io.vavr.collection.Set io.vavr.collection.LinkedHashSet java.util.Iterable io.vavr.collection.Map io.vavr.collection.LinkedHashMap java.util.Map You can use the types in the first column (or subtypes thereof) as query method return types and get the types in the second column used as implementation type, depending on the Java type of the actual query result (third column). Alternatively, you can declare Traversable (the Vavr Iterable equivalent), and we then derive the implementation class from the actual return value. That is, a java.util.List is turned into a Vavr List or Seq , a java.util.Set becomes a Vavr LinkedHashSet Set , and so on. Streaming Query Results: You can process the results of query methods incrementally by using a Java 8 Stream<T> as the return type. Instead of wrapping the query results in a Stream , data store-specific methods are used to perform the streaming, as shown in the following example: Stream the result of a query with Java 8 Stream<T> @Query(""select u from User u"") Stream<User> findAllByCustomQueryAndStream(); Stream<User> readAllByFirstnameNotNull(); @Query(""select u from User u"") Stream<User> streamAllPaged(Pageable pageable); A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example: Working with a Stream<T> result in a try-with-resources block try (Stream<User> stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } Not all Spring Data modules currently support Stream<T> as a return type. Asynchronous Query Results: You can run repository queries asynchronously by using Spring’s asynchronous method running capability(https://docs.spring.io/spring-framework/reference/6.1/integration/scheduling.html) . This means the method returns immediately upon invocation while the actual query occurs in a task that has been submitted to a Spring TaskExecutor . Asynchronous queries differ from reactive queries and should not be mixed. See the store-specific documentation for more details on reactive support. The following example shows a number of asynchronous queries: @Async Future<User> findByFirstname(String firstname); (1) @Async CompletableFuture<User> findOneByFirstname(String firstname); (2) 1 Use java.util.concurrent.Future as the return type. 2 Use a Java 8 java.util.concurrent.CompletableFuture as the return type. Paging, Iterating Large Results, Sorting & Limiting: To handle parameters in your query, define method parameters as already seen in the preceding examples. Besides that, the infrastructure recognizes certain specific types like Pageable , Sort and Limit , to apply pagination, sorting and limiting to your queries dynamically. The following example demonstrates these features: Using Pageable , Slice , Sort and Limit in query methods Page<User> findByLastname(String lastname, Pageable pageable); Slice<User> findByLastname(String lastname, Pageable pageable); List<User> findByLastname(String lastname, Sort sort); List<User> findByLastname(String lastname, Sort sort, Limit limit); List<User> findByLastname(String lastname, Pageable pageable); APIs taking Sort , Pageable and Limit expect non- null values to be handed into methods. If you do not want to apply any sorting or pagination, use Sort.unsorted() , Pageable.unpaged() and Limit.unlimited() . The first method lets you pass an org.springframework.data.domain.Pageable instance to the query method to dynamically add paging to your statically defined query. A Page knows about the total number of elements and pages available. It does so by the infrastructure triggering a count query to calculate the overall number. As this might be expensive (depending on the store used), you can instead return a Slice . A Slice knows only about whether a next Slice is available, which might be sufficient when walking through a larger result set. Sorting options are handled through the Pageable instance, too. If you need only sorting, add an org.springframework.data.domain.Sort parameter to your method. As you can see, returning a List is also possible. In this case, the additional metadata required to build the actual Page instance is not created (which, in turn, means that the additional count query that would have been necessary is not issued). Rather, it restricts the query to look up only the given range of entities. To find out how many pages you get for an entire query, you have to trigger an additional count query. By default, this query is derived from the query you actually trigger. Special parameters may only be used once within a query method. Some special parameters described above are mutually exclusive. Please consider the following list of invalid parameter combinations. Parameters Example Reason Pageable and Sort findBy…​(Pageable page, Sort sort) Pageable already defines Sort Pageable and Limit findBy…​(Pageable page, Limit limit) Pageable already defines a limit. The Top keyword used to limit results can be used to along with Pageable whereas Top defines the total maximum of results, whereas the Pageable parameter may reduce this number. Which Method is Appropriate?: The value provided by the Spring Data abstractions is perhaps best shown by the possible query method return types outlined in the following table below. The table shows which types you can return from a query method Table 1. Consuming Large Query Results Method Amount of Data Fetched Query Structure Constraints List<T>(#repositories.collections-and-iterables) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Streamable<T>(#repositories.collections-and-iterables.streamable) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Stream<T>(#repositories.query-streaming) Chunked (one-by-one or in batches) depending on Stream consumption. Single query using typically cursors. Streams must be closed after usage to avoid resource leaks. Flux<T> Chunked (one-by-one or in batches) depending on Flux consumption. Single query using typically cursors. Store module must provide reactive infrastructure. Slice<T> Pageable.getPageSize() + 1 at Pageable.getOffset() One to many queries fetching data starting at Pageable.getOffset() applying limiting. A Slice can only navigate to the next Slice . Slice provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Window provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Page<T> Pageable.getPageSize() at Pageable.getOffset() One to many queries starting at Pageable.getOffset() applying limiting. Additionally, COUNT(…) query to determine the total number of elements can be required. Often times, COUNT(…) queries are required that are costly. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Paging and Sorting: You can define simple sorting expressions by using property names. You can concatenate expressions to collect multiple criteria into one expression. Defining sort expressions Sort sort = Sort.by(""firstname"").ascending() .and(Sort.by(""lastname"").descending()); For a more type-safe way to define sort expressions, start with the type for which to define the sort expression and use method references to define the properties on which to sort. Defining sort expressions by using the type-safe API TypedSort<Person> person = Sort.sort(Person.class); Sort sort = person.by(Person::getFirstname).ascending() .and(person.by(Person::getLastname).descending()); TypedSort.by(…) makes use of runtime proxies by (typically) using CGlib, which may interfere with native image compilation when using tools such as Graal VM Native. If your store implementation supports Querydsl, you can also use the generated metamodel types to define sort expressions: Defining sort expressions by using the Querydsl API QSort sort = QSort.by(QPerson.firstname.asc()) .and(QSort.by(QPerson.lastname.desc())); Limiting Query Results: In addition to paging it is possible to limit the result size using a dedicated Limit parameter. You can also limit the results of query methods by using the First or Top keywords, which you can use interchangeably but may not be mixed with a Limit parameter. You can append an optional numeric value to Top or First to specify the maximum result size to be returned. If the number is left out, a result size of 1 is assumed. The following example shows how to limit the query size: Limiting the result size of a query with Top and First List<User> findByLastname(Limit limit); User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page<User> queryFirst10ByLastname(String lastname, Pageable pageable); Slice<User> findTop3ByLastname(String lastname, Pageable pageable); List<User> findFirst10ByLastname(String lastname, Sort sort); List<User> findTop10ByLastname(String lastname, Pageable pageable); The limiting expressions also support the Distinct keyword for datastores that support distinct queries. Also, for the queries that limit the result set to one instance, wrapping the result into with the Optional keyword is supported. If pagination or slicing is applied to a limiting query pagination (and the calculation of the number of available pages), it is applied within the limited result. Limiting the results in combination with dynamic sorting by using a Sort parameter lets you express query methods for the 'K' smallest as well as for the 'K' biggest elements."
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/projections.html","Projections: Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> ."
"https://docs.spring.io/spring-data/relational/reference/3.3/object-mapping.html","Object Mapping Fundamentals: This section covers the fundamentals of Spring Data object mapping, object creation, field and property access, mutability and immutability. Note, that this section only applies to Spring Data modules that do not use the object mapping of the underlying data store (like JPA). Also be sure to consult the store-specific sections for store-specific object mapping, like indexes, customizing column or field names or the like. Core responsibility of the Spring Data object mapping is to create instances of domain objects and map the store-native data structures onto those. This means we need two fundamental steps: Instance creation by using one of the constructors exposed. Instance population to materialize all exposed properties. Object creation: Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. The value resolution assumes constructor/factory method argument names to match the property names of the entity, i.e. the resolution will be performed as if the property was to be populated, including all customizations in mapping (different datastore column or field name etc.). This also requires either parameter names information available in the class file or an @ConstructorProperties annotation being present on the constructor. The value resolution can be customized by using Spring Framework’s @Value value annotation using a store-specific SpEL expression. Please consult the section on store specific mappings for further details. Object creation internals To avoid the overhead of reflection, Spring Data object creation uses a factory class generated at runtime by default, which will call the domain classes constructor directly. I.e. for this example type: class Person { Person(String firstname, String lastname) { … } } we will create a factory class semantically equivalent to this one at runtime: class PersonObjectInstantiator implements ObjectInstantiator { Object newInstance(Object... args) { return new Person((String) args[0], (String) args[1]); } } This gives us a roundabout 10% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: it must not be a private class it must not be a non-static inner class it must not be a CGLib proxy class the constructor to be used by Spring Data must not be private If any of these criteria match, Spring Data will fall back to entity instantiation via reflection. Property population: Once an instance of the entity has been created, Spring Data populates all remaining persistent properties of that class. Unless already populated by the entity’s constructor (i.e. consumed through its constructor argument list), the identifier property will be populated first to allow the resolution of cyclic object references. After that, all non-transient properties that have not already been populated by the constructor are set on the entity instance. For that we use the following algorithm: If the property is immutable but exposes a with… method (see below), we use the with… method to create a new entity instance with the new property value. If property access (i.e. access through getters and setters) is defined, we’re invoking the setter method. If the property is mutable we set the field directly. If the property is immutable we’re using the constructor to be used by persistence operations (see Object creation(#mapping.object-creation) ) to create a copy of the instance. By default, we set the field value directly. Property population internals Similarly to our optimizations in object construction(#mapping.object-creation.details) we also use Spring Data runtime generated accessor classes to interact with the entity instance. class Person { private final Long id; private String firstname; private @AccessType(Type.PROPERTY) String lastname; Person() { this.id = null; } Person(Long id, String firstname, String lastname) { // Field assignments } Person withId(Long id) { return new Person(id, this.firstname, this.lastame); } void setLastname(String lastname) { this.lastname = lastname; } } A generated Property Accessor class PersonPropertyAccessor implements PersistentPropertyAccessor { private static final MethodHandle firstname; (2) private Person person; (1) public void setProperty(PersistentProperty property, Object value) { String name = property.getName(); if (""firstname"".equals(name)) { firstname.invoke(person, (String) value); (2) } else if (""id"".equals(name)) { this.person = person.withId((Long) value); (3) } else if (""lastname"".equals(name)) { this.person.setLastname((String) value); (4) } } } 1 PropertyAccessor’s hold a mutable instance of the underlying object. This is, to enable mutations of otherwise immutable properties. 2 By default, Spring Data uses field-access to read and write property values. As per visibility rules of private fields, MethodHandles are used to interact with fields. 3 The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. Calling withId(…) creates a new Person object. All subsequent mutations will take place in the new instance leaving the previous untouched. 4 Using property-access allows direct method invocations without using MethodHandles . This gives us a roundabout 25% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: Types must not reside in the default or under the java package. Types and their constructors must be public Types that are inner classes must be static . The used Java Runtime must allow for declaring classes in the originating ClassLoader . Java 9 and newer impose certain limitations. By default, Spring Data attempts to use generated property accessors and falls back to reflection-based ones if a limitation is detected. Let’s have a look at the following entity: A sample entity class Person { private final @Id Long id; (1) private final String firstname, lastname; (2) private final LocalDate birthday; private final int age; (3) private String comment; (4) private @AccessType(Type.PROPERTY) String remarks; (5) static Person of(String firstname, String lastname, LocalDate birthday) { (6) return new Person(null, firstname, lastname, birthday, Period.between(birthday, LocalDate.now()).getYears()); } Person(Long id, String firstname, String lastname, LocalDate birthday, int age) { (6) this.id = id; this.firstname = firstname; this.lastname = lastname; this.birthday = birthday; this.age = age; } Person withId(Long id) { (1) return new Person(id, this.firstname, this.lastname, this.birthday, this.age); } void setRemarks(String remarks) { (5) this.remarks = remarks; } } 1 The identifier property is final but set to null in the constructor. The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. The original Person instance stays unchanged as a new one is created. The same pattern is usually applied for other properties that are store managed but might have to be changed for persistence operations. The wither method is optional as the persistence constructor (see 6) is effectively a copy constructor and setting the property will be translated into creating a fresh instance with the new identifier value applied. 2 The firstname and lastname properties are ordinary immutable properties potentially exposed through getters. 3 The age property is an immutable but derived one from the birthday property. With the design shown, the database value will trump the defaulting as Spring Data uses the only declared constructor. Even if the intent is that the calculation should be preferred, it’s important that this constructor also takes age as parameter (to potentially ignore it) as otherwise the property population step will attempt to set the age field and fail due to it being immutable and no with… method being present. 4 The comment property is mutable and is populated by setting its field directly. 5 The remarks property is mutable and is populated by invoking the setter method. 6 The class exposes a factory method and a constructor for object creation. The core idea here is to use factory methods instead of additional constructors to avoid the need for constructor disambiguation through @PersistenceCreator . Instead, defaulting of properties is handled within the factory method. If you want Spring Data to use the factory method for object instantiation, annotate it with @PersistenceCreator . General recommendations: Try to stick to immutable objects — Immutable objects are straightforward to create as materializing an object is then a matter of calling its constructor only. Also, this avoids your domain objects to be littered with setter methods that allow client code to manipulate the objects state. If you need those, prefer to make them package protected so that they can only be invoked by a limited amount of co-located types. Constructor-only materialization is up to 30% faster than properties population. Provide an all-args constructor — Even if you cannot or don’t want to model your entities as immutable values, there’s still value in providing a constructor that takes all properties of the entity as arguments, including the mutable ones, as this allows the object mapping to skip the property population for optimal performance. Use factory methods instead of overloaded constructors to avoid @PersistenceCreator — With an all-argument constructor needed for optimal performance, we usually want to expose more application use case specific constructors that omit things like auto-generated identifiers etc. It’s an established pattern to rather use static factory methods to expose these variants of the all-args constructor. Make sure you adhere to the constraints that allow the generated instantiator and property accessor classes to be used — For identifiers to be generated, still use a final field in combination with an all-arguments persistence constructor (preferred) or a with… method — Use Lombok to avoid boilerplate code — As persistence operations usually require a constructor taking all arguments, their declaration becomes a tedious repetition of boilerplate parameter to field assignments that can best be avoided by using Lombok’s @AllArgsConstructor . Overriding Properties: Java’s allows a flexible design of domain classes where a subclass could define a property that is already declared with the same name in its superclass. Consider the following example: public class SuperType { private CharSequence field; public SuperType(CharSequence field) { this.field = field; } public CharSequence getField() { return this.field; } public void setField(CharSequence field) { this.field = field; } } public class SubType extends SuperType { private String field; public SubType(String field) { super(field); this.field = field; } @Override public String getField() { return this.field; } public void setField(String field) { this.field = field; // optional super.setField(field); } } Both classes define a field using assignable types. SubType however shadows SuperType.field . Depending on the class design, using the constructor could be the only default approach to set SuperType.field . Alternatively, calling super.setField(…) in the setter could set the field in SuperType . All these mechanisms create conflicts to some degree because the properties share the same name yet might represent two distinct values. Spring Data skips super-type properties if types are not assignable. That is, the type of the overridden property must be assignable to its super-type property type to be registered as override, otherwise the super-type property is considered transient. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be generally set without making any further assumptions of the setter implementation. Kotlin support: Spring Data adapts specifics of Kotlin to allow object creation and mutation. Kotlin object creation: Kotlin classes are supported to be instantiated, all classes are immutable by default and require explicit property declarations to define mutable properties. Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a constructor that is annotated with @PersistenceCreator , it is used. If the type is a Kotlin data class(#mapping.kotlin) the primary constructor is used. If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. Consider the following data class Person : data class Person(val id: String, val name: String) The class above compiles to a typical class with an explicit constructor. We can customize this class by adding another constructor and annotate it with @PersistenceCreator to indicate a constructor preference: data class Person(var id: String, val name: String) { @PersistenceCreator constructor(id: String) : this(id, ""unknown"") } Kotlin supports parameter optionality by allowing default values to be used if a parameter is not provided. When Spring Data detects a constructor with parameter defaulting, then it leaves these parameters absent if the data store does not provide a value (or simply returns null ) so Kotlin can apply parameter defaulting.Consider the following class that applies parameter defaulting for name data class Person(var id: String, val name: String = ""unknown"") Every time the name parameter is either not part of the result or its value is null , then the name defaults to unknown . Delegated properties are not supported with Spring Data. The mapping metadata filters delegated properties for Kotlin Data classes. In all other cases you can exclude synthetic fields for delegated properties by annotating the property with @delegate:org.springframework.data.annotation.Transient . Property population of Kotlin data classes: In Kotlin, all classes are immutable by default and require explicit property declarations to define mutable properties. Consider the following data class Person : data class Person(val id: String, val name: String) This class is effectively immutable. It allows creating new instances as Kotlin generates a copy(…) method that creates new object instances copying all property values from the existing object and applying property values provided as arguments to the method. Kotlin Overriding Properties: Kotlin allows declaring property overrides(https://kotlinlang.org/docs/inheritance.html#overriding-properties) to alter properties in subclasses. open class SuperType(open var field: Int) class SubType(override var field: Int = 1) : SuperType(field) { } Such an arrangement renders two properties with the name field . Kotlin generates property accessors (getters and setters) for each property in each class. Effectively, the code looks like as follows: public class SuperType { private int field; public SuperType(int field) { this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } public final class SubType extends SuperType { private int field; public SubType(int field) { super(field); this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } Getters and setters on SubType set only SubType.field and not SuperType.field . In such an arrangement, using the constructor is the only default approach to set SuperType.field . Adding a method to SubType to set SuperType.field via this.SuperType.field = … is possible but falls outside of supported conventions. Property overrides create conflicts to some degree because the properties share the same name yet might represent two distinct values. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be set. Kotlin Value Classes: Kotlin Value Classes are designed for a more expressive domain model to make underlying concepts explicit. Spring Data can read and write types that define properties using Value Classes. Consider the following domain model: @JvmInline value class EmailAddress(val theAddress: String) (1) data class Contact(val id: String, val name:String, val emailAddress: EmailAddress) (2) 1 A simple value class with a non-nullable value type. 2 Data class defining a property using the EmailAddress value class. Non-nullable properties using non-primitive value types are flattened in the compiled class to the value type. Nullable primitive value types or nullable value-in-value types are represented with their wrapper type and that affects how value types are represented in the database."
"https://docs.spring.io/spring-data/relational/reference/3.3/commons/custom-conversions.html","Custom Conversions: The following example of a Spring Converter implementation converts from a String to a custom Email value object: @ReadingConverter public class EmailReadConverter implements Converter<String, Email> { public Email convert(String source) { return Email.valueOf(source); } } If you write a Converter whose source and target type are native types, we cannot determine whether we should consider it as a reading or a writing converter. Registering the converter instance as both might lead to unwanted results. For example, a Converter<String, Long> is ambiguous, although it probably does not make sense to try to convert all String instances into Long instances when writing. To let you force the infrastructure to register a converter for only one way, we provide @ReadingConverter and @WritingConverter annotations to be used in the converter implementation. Converters are subject to explicit registration as instances are not picked up from a classpath or container scan to avoid unwanted registration with a conversion service and the side effects resulting from such a registration. Converters are registered with CustomConversions as the central facility that allows registration and querying for registered converters based on source- and target type. CustomConversions ships with a pre-defined set of converter registrations: JSR-310 Converters for conversion between java.time , java.util.Date and String types. Default converters for local temporal types (e.g. LocalDateTime to java.util.Date ) rely on system-default timezone settings to convert between those types. You can override the default converter, by registering your own converter. Converter Disambiguation: Generally, we inspect the Converter implementations for the source and target types they convert from and to. Depending on whether one of those is a type the underlying data access API can handle natively, we register the converter instance as a reading or a writing converter. The following examples show a writing- and a read converter (note the difference is in the order of the qualifiers on Converter ): // Write converter as only the target type is one that can be handled natively class MyConverter implements Converter<Person, String> { … } // Read converter as only the source type is one that can be handled natively class MyConverter implements Converter<String, Person> { … }"
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableJdbcRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableJdbcRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" />"
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/core-extensions.html","Spring Data Extensions: This section documents a set of Spring Data extensions that enable Spring Data usage in a variety of contexts. Currently, most of the integration is targeted towards Spring MVC. Querydsl Extension: Querydsl(http://www.querydsl.com/) is a framework that enables the construction of statically typed SQL-like queries through its fluent API. Several Spring Data modules offer integration with Querydsl through QuerydslPredicateExecutor , as the following example shows: QuerydslPredicateExecutor interface public interface QuerydslPredicateExecutor<T> { Optional<T> findById(Predicate predicate); (1) Iterable<T> findAll(Predicate predicate); (2) long count(Predicate predicate); (3) boolean exists(Predicate predicate); (4) // … more functionality omitted. } 1 Finds and returns a single entity matching the Predicate . 2 Finds and returns all entities matching the Predicate . 3 Returns the number of entities matching the Predicate . 4 Returns whether an entity that matches the Predicate exists. To use the Querydsl support, extend QuerydslPredicateExecutor on your repository interface, as the following example shows: Querydsl integration on repositories interface UserRepository extends CrudRepository<User, Long>, QuerydslPredicateExecutor<User> { } The preceding example lets you write type-safe queries by using Querydsl Predicate instances, as the following example shows: Predicate predicate = user.firstname.equalsIgnoreCase(""dave"") .and(user.lastname.startsWithIgnoreCase(""mathews"")); userRepository.findAll(predicate); Web support: Spring Data modules that support the repository programming model ship with a variety of web support. The web related components require Spring MVC JARs to be on the classpath. Some of them even provide integration with Spring HATEOAS(https://github.com/spring-projects/spring-hateoas) . In general, the integration support is enabled by using the @EnableSpringDataWebSupport annotation in your JavaConfig configuration class, as the following example shows: Enabling Spring Data web support Java XML @Configuration @EnableWebMvc @EnableSpringDataWebSupport class WebConfiguration {} <bean class=""org.springframework.data.web.config.SpringDataWebConfiguration"" /> <!-- If you use Spring HATEOAS, register this one *instead* of the former --> <bean class=""org.springframework.data.web.config.HateoasAwareSpringDataWebConfiguration"" /> The @EnableSpringDataWebSupport annotation registers a few components. We discuss those later in this section. It also detects Spring HATEOAS on the classpath and registers integration components (if present) for it as well. Basic Web Support: Enabling Spring Data web support in XML The configuration shown in the previous section(#core.web) registers a few basic components: A Using the DomainClassConverter Class(#core.web.basic.domain-class-converter) to let Spring MVC resolve instances of repository-managed domain classes from request parameters or path variables. HandlerMethodArgumentResolver(#core.web.basic.paging-and-sorting) implementations to let Spring MVC resolve Pageable and Sort instances from request parameters. Jackson Modules(#core.web.basic.jackson-mappers) to de-/serialize types like Point and Distance , or store specific ones, depending on the Spring Data Module used. Using the DomainClassConverter Class: The DomainClassConverter class lets you use domain types in your Spring MVC controller method signatures directly so that you need not manually lookup the instances through the repository, as the following example shows: A Spring MVC controller using domain types in method signatures @Controller @RequestMapping(""/users"") class UserController { @RequestMapping(""/{id}"") String showUserForm(@PathVariable(""id"") User user, Model model) { model.addAttribute(""user"", user); return ""userForm""; } } The method receives a User instance directly, and no further lookup is necessary. The instance can be resolved by letting Spring MVC convert the path variable into the id type of the domain class first and eventually access the instance through calling findById(…) on the repository instance registered for the domain type. Currently, the repository has to implement CrudRepository to be eligible to be discovered for conversion. HandlerMethodArgumentResolvers for Pageable and Sort: The configuration snippet shown in the previous section(#core.web.basic.domain-class-converter) also registers a PageableHandlerMethodArgumentResolver as well as an instance of SortHandlerMethodArgumentResolver . The registration enables Pageable and Sort as valid controller method arguments, as the following example shows: Using Pageable as a controller method argument @Controller @RequestMapping(""/users"") class UserController { private final UserRepository repository; UserController(UserRepository repository) { this.repository = repository; } @RequestMapping String showUsers(Model model, Pageable pageable) { model.addAttribute(""users"", repository.findAll(pageable)); return ""users""; } } The preceding method signature causes Spring MVC try to derive a Pageable instance from the request parameters by using the following default configuration: Table 1. Request parameters evaluated for Pageable instances page Page you want to retrieve. 0-indexed and defaults to 0. size Size of the page you want to retrieve. Defaults to 20. sort Properties that should be sorted by in the format property,property(,ASC|DESC)(,IgnoreCase) . The default sort direction is case-sensitive ascending. Use multiple sort parameters if you want to switch direction or case sensitivity — for example, ?sort=firstname&sort=lastname,asc&sort=city,ignorecase . To customize this behavior, register a bean that implements the PageableHandlerMethodArgumentResolverCustomizer interface or the SortHandlerMethodArgumentResolverCustomizer interface, respectively. Its customize() method gets called, letting you change settings, as the following example shows: @Bean SortHandlerMethodArgumentResolverCustomizer sortCustomizer() { return s -> s.setPropertyDelimiter(""<-->""); } If setting the properties of an existing MethodArgumentResolver is not sufficient for your purpose, extend either SpringDataWebConfiguration or the HATEOAS-enabled equivalent, override the pageableResolver() or sortResolver() methods, and import your customized configuration file instead of using the @Enable annotation. If you need multiple Pageable or Sort instances to be resolved from the request (for multiple tables, for example), you can use Spring’s @Qualifier annotation to distinguish one from another. The request parameters then have to be prefixed with ${qualifier}_ . The following example shows the resulting method signature: String showUsers(Model model, @Qualifier(""thing1"") Pageable first, @Qualifier(""thing2"") Pageable second) { … } You have to populate thing1_page , thing2_page , and so on. The default Pageable passed into the method is equivalent to a PageRequest.of(0, 20) , but you can customize it by using the @PageableDefault annotation on the Pageable parameter. Creating JSON representations for Page: It’s common for Spring MVC controllers to try to ultimately render a representation of a Spring Data page to clients. While one could simply return Page instances from handler methods to let Jackson render them as is, we strongly recommend against this as the underlying implementation class PageImpl is a domain type. This means we might want or have to change its API for unrelated reasons, and such changes might alter the resulting JSON representation in a breaking way. With Spring Data 3.1, we started hinting at the problem by issuing a warning log describing the problem. We still ultimately recommend to leverage the integration with Spring HATEOAS(#core.web.pageables) for a fully stable and hypermedia-enabled way of rendering pages that easily allow clients to navigate them. But as of version 3.3 Spring Data ships a page rendering mechanism that is convenient to use but does not require the inclusion of Spring HATEOAS. Using Spring Data' PagedModel: At its core, the support consists of a simplified version of Spring HATEOAS' PagedModel (the Spring Data one located in the org.springframework.data.web package). It can be used to wrap Page instances and result in a simplified representation that reflects the structure established by Spring HATEOAS but omits the navigation links. import org.springframework.data.web.PagedModel; @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") PagedModel<?> page(Pageable pageable) { return new PagedModel<>(repository.findAll(pageable)); (1) } } 1 Wraps the Page instance into a PagedModel . This will result in a JSON structure looking like this: { ""content"" : [ … // Page content rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } Note how the document contains a page field exposing the essential pagination metadata. Globally enabling simplified Page rendering: If you don’t want to change all your existing controllers to add the mapping step to return PagedModel instead of Page you can enable the automatic translation of PageImpl instances into PagedModel by tweaking @EnableSpringDataWebSupport as follows: @EnableSpringDataWebSupport(pageSerializationMode = VIA_DTO) class MyConfiguration { } This will allow your controller to still return Page instances and they will automatically be rendered into the simplified representation: @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") Page<?> page(Pageable pageable) { return repository.findAll(pageable); } } Hypermedia Support for Page and Slice: Spring HATEOAS ships with a representation model class ( PagedModel / SlicedModel ) that allows enriching the content of a Page or Slice instance with the necessary Page / Slice metadata as well as links to let the clients easily navigate the pages. The conversion of a Page to a PagedModel is done by an implementation of the Spring HATEOAS RepresentationModelAssembler interface, called the PagedResourcesAssembler . Similarly Slice instances can be converted to a SlicedModel using a SlicedResourcesAssembler . The following example shows how to use a PagedResourcesAssembler as a controller method argument, as the SlicedResourcesAssembler works exactly the same: Using a PagedResourcesAssembler as controller method argument @Controller class PersonController { private final PersonRepository repository; // Constructor omitted @GetMapping(""/people"") HttpEntity<PagedModel<Person>> people(Pageable pageable, PagedResourcesAssembler assembler) { Page<Person> people = repository.findAll(pageable); return ResponseEntity.ok(assembler.toModel(people)); } } Enabling the configuration, as shown in the preceding example, lets the PagedResourcesAssembler be used as a controller method argument. Calling toModel(…) on it has the following effects: The content of the Page becomes the content of the PagedModel instance. The PagedModel object gets a PageMetadata instance attached, and it is populated with information from the Page and the underlying Pageable . The PagedModel may get prev and next links attached, depending on the page’s state. The links point to the URI to which the method maps. The pagination parameters added to the method match the setup of the PageableHandlerMethodArgumentResolver to make sure the links can be resolved later. Assume we have 30 Person instances in the database. You can now trigger a request ( GET localhost:8080/people(http://localhost:8080/people) ) and see output similar to the following: { ""links"" : [ { ""rel"" : ""next"", ""href"" : ""http://localhost:8080/persons?page=1&size=20"" } ], ""content"" : [ … // 20 Person instances rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } The JSON envelope format shown here doesn’t follow any formally specified structure and it’s not guaranteed stable and we might change it at any time. It’s highly recommended to enable the rendering as a hypermedia-enabled, official media type, supported by Spring HATEOAS, like HAL(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#mediatypes.hal) . Those can be activated by using its @EnableHypermediaSupport annotation. Find more information in the Spring HATEOAS reference documentation(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#configuration.at-enable) . The assembler produced the correct URI and also picked up the default configuration to resolve the parameters into a Pageable for an upcoming request. This means that, if you change that configuration, the links automatically adhere to the change. By default, the assembler points to the controller method it was invoked in, but you can customize that by passing a custom Link to be used as base to build the pagination links, which overloads the PagedResourcesAssembler.toModel(…) method. Spring Data Jackson Modules: The core module, and some of the store specific ones, ship with a set of Jackson Modules for types, like org.springframework.data.geo.Distance and org.springframework.data.geo.Point , used by the Spring Data domain. Those Modules are imported once web support(#core.web) is enabled and com.fasterxml.jackson.databind.ObjectMapper is available. During initialization SpringDataJacksonModules , like the SpringDataJacksonConfiguration , get picked up by the infrastructure, so that the declared com.fasterxml.jackson.databind.Module s are made available to the Jackson ObjectMapper . Data binding mixins for the following domain types are registered by the common infrastructure. org.springframework.data.geo.Distance org.springframework.data.geo.Point org.springframework.data.geo.Box org.springframework.data.geo.Circle org.springframework.data.geo.Polygon The individual module may provide additional SpringDataJacksonModules . Please refer to the store specific section for more details. Web Databinding Support: You can use Spring Data projections (described in Projections(projections.html) ) to bind incoming request payloads by using either JSONPath(https://goessner.net/articles/JsonPath/) expressions (requires Jayway JsonPath(https://github.com/json-path/JsonPath) ) or XPath(https://www.w3.org/TR/xpath-31/) expressions (requires XmlBeam(https://xmlbeam.org/) ), as the following example shows: HTTP payload binding using JSONPath or XPath expressions @ProjectedPayload public interface UserPayload { @XBRead(""//firstname"") @JsonPath(""$..firstname"") String getFirstname(); @XBRead(""/lastname"") @JsonPath({ ""$.lastname"", ""$.user.lastname"" }) String getLastname(); } You can use the type shown in the preceding example as a Spring MVC handler method argument or by using ParameterizedTypeReference on one of methods of the RestTemplate . The preceding method declarations would try to find firstname anywhere in the given document. The lastname XML lookup is performed on the top-level of the incoming document. The JSON variant of that tries a top-level lastname first but also tries lastname nested in a user sub-document if the former does not return a value. That way, changes in the structure of the source document can be mitigated easily without having clients calling the exposed methods (usually a drawback of class-based payload binding). Nested projections are supported as described in Projections(projections.html) . If the method returns a complex, non-interface type, a Jackson ObjectMapper is used to map the final value. For Spring MVC, the necessary converters are registered automatically as soon as @EnableSpringDataWebSupport is active and the required dependencies are available on the classpath. For usage with RestTemplate , register a ProjectingJackson2HttpMessageConverter (JSON) or XmlBeamHttpMessageConverter manually. For more information, see the web projection example(https://github.com/spring-projects/spring-data-examples/tree/main/web/projection) in the canonical Spring Data Examples repository(https://github.com/spring-projects/spring-data-examples) . Querydsl Web Support: For those stores that have QueryDSL(http://www.querydsl.com/) integration, you can derive queries from the attributes contained in a Request query string. Consider the following query string: ?firstname=Dave&lastname=Matthews Given the User object from the previous examples, you can resolve a query string to the following value by using the QuerydslPredicateArgumentResolver , as follows: QUser.user.firstname.eq(""Dave"").and(QUser.user.lastname.eq(""Matthews"")) The feature is automatically enabled, along with @EnableSpringDataWebSupport , when Querydsl is found on the classpath. Adding a @QuerydslPredicate to the method signature provides a ready-to-use Predicate , which you can run by using the QuerydslPredicateExecutor . Type information is typically resolved from the method’s return type. Since that information does not necessarily match the domain type, it might be a good idea to use the root attribute of QuerydslPredicate . The following example shows how to use @QuerydslPredicate in a method signature: @Controller class UserController { @Autowired UserRepository repository; @RequestMapping(value = ""/"", method = RequestMethod.GET) String index(Model model, @QuerydslPredicate(root = User.class) Predicate predicate, (1) Pageable pageable, @RequestParam MultiValueMap<String, String> parameters) { model.addAttribute(""users"", repository.findAll(predicate, pageable)); return ""index""; } } 1 Resolve query string arguments to matching Predicate for User . The default binding is as follows: Object on simple properties as eq . Object on collection like properties as contains . Collection on simple properties as in . You can customize those bindings through the bindings attribute of @QuerydslPredicate or by making use of Java 8 default methods and adding the QuerydslBinderCustomizer method to the repository interface, as follows: interface UserRepository extends CrudRepository<User, String>, QuerydslPredicateExecutor<User>, (1) QuerydslBinderCustomizer<QUser> { (2) @Override default void customize(QuerydslBindings bindings, QUser user) { bindings.bind(user.username).first((path, value) -> path.contains(value)) (3) bindings.bind(String.class) .first((StringPath path, String value) -> path.containsIgnoreCase(value)); (4) bindings.excluding(user.password); (5) } } 1 QuerydslPredicateExecutor provides access to specific finder methods for Predicate . 2 QuerydslBinderCustomizer defined on the repository interface is automatically picked up and shortcuts @QuerydslPredicate(bindings=…​) . 3 Define the binding for the username property to be a simple contains binding. 4 Define the default binding for String properties to be a case-insensitive contains match. 5 Exclude the password property from Predicate resolution. You can register a QuerydslBinderCustomizerDefaults bean holding default Querydsl bindings before applying specific bindings from the repository or @QuerydslPredicate . Repository Populators: If you work with the Spring JDBC module, you are probably familiar with the support for populating a DataSource with SQL scripts. A similar abstraction is available on the repositories level, although it does not use SQL as the data definition language because it must be store-independent. Thus, the populators support XML (through Spring’s OXM abstraction) and JSON (through Jackson) to define data with which to populate the repositories. Assume you have a file called data.json with the following content: Data defined in JSON [ { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Dave"", ""lastname"" : ""Matthews"" }, { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Carter"", ""lastname"" : ""Beauford"" } ] You can populate your repositories by using the populator elements of the repository namespace provided in Spring Data Commons. To populate the preceding data to your PersonRepository , declare a populator similar to the following: Declaring a Jackson repository populator <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd""> <repository:jackson2-populator locations=""classpath:data.json"" /> </beans> The preceding declaration causes the data.json file to be read and deserialized by a Jackson ObjectMapper . The type to which the JSON object is unmarshalled is determined by inspecting the _class attribute of the JSON document. The infrastructure eventually selects the appropriate repository to handle the object that was deserialized. To instead use XML to define the data the repositories should be populated with, you can use the unmarshaller-populator element. You configure it to use one of the XML marshaller options available in Spring OXM. See the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/data-access/oxm.html) for details. The following example shows how to unmarshall a repository populator with JAXB: Declaring an unmarshalling repository populator (using JAXB) <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xmlns:oxm=""http://www.springframework.org/schema/oxm"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd http://www.springframework.org/schema/oxm https://www.springframework.org/schema/oxm/spring-oxm.xsd""> <repository:unmarshaller-populator locations=""classpath:data.json"" unmarshaller-ref=""unmarshaller"" /> <oxm:jaxb2-marshaller contextPath=""com.acme"" /> </beans>"
"https://docs.spring.io/spring-data/relational/reference/3.3/value-expressions.html","Value Expressions Fundamentals: Value Expressions are a combination of Spring Expression Language (SpEL)(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) and Property Placeholder Resolution(https://docs.spring.io/spring-framework/reference/6.1/core/beans/environment.html#beans-placeholder-resolution-in-statements) . They combine powerful evaluation of programmatic expressions with the simplicity to resort to property-placeholder resolution to obtain values from the Environment such as configuration properties. Expressions are expected to be defined by a trusted input such as an annotation value and not to be determined from user input. The following code demonstrates how to use expressions in the context of annotations. Example 1. Annotation Usage @Document(""orders-#{tenantService.getOrderCollection()}-${tenant-config.suffix}"") class Order { // … } Value Expressions can be defined from a sole SpEL Expression, a Property Placeholder or a composite expression mixing various expressions including literals. Example 2. Expression Examples #{tenantService.getOrderCollection()} (1) #{(1+1) + '-hello-world'} (2) ${tenant-config.suffix} (3) orders-${tenant-config.suffix} (4) #{tenantService.getOrderCollection()}-${tenant-config.suffix} (5) 1 Value Expression using a single SpEL Expression. 2 Value Expression using a static SpEL Expression evaluating to 2-hello-world . 3 Value Expression using a single Property Placeholder. 4 Composite expression comprised of the literal orders- and the Property Placeholder ${tenant-config.suffix} . 5 Composite expression using SpEL, Property Placeholders and literals. Using value expressions introduces a lot of flexibility to your code. Doing so requires evaluation of the expression on each usage and, therefore, value expression evaluation has an impact on the performance profile. Parsing and Evaluation: Value Expressions are parsed by the ValueExpressionParser API. Instances of ValueExpression are thread-safe and can be cached for later use to avoid repeated parsing. The following example shows the Value Expression API usage: Parsing and Evaluation Java Kotlin ValueParserConfiguration configuration = SpelExpressionParser::new; ValueEvaluationContext context = ValueEvaluationContext.of(environment, evaluationContext); ValueExpressionParser parser = ValueExpressionParser.create(configuration); ValueExpression expression = parser.parse(""Hello, World""); Object result = expression.evaluate(context); val configuration = ValueParserConfiguration { SpelExpressionParser() } val context = ValueEvaluationContext.of(environment, evaluationContext) val parser = ValueExpressionParser.create(configuration) val expression: ValueExpression = parser.parse(""Hello, World"") val result: Any = expression.evaluate(context) SpEL Expressions: SpEL Expressions(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) follow the Template style where the expression is expected to be enclosed within the #{…} format. Expressions are evaluated using an EvaluationContext that is provided by EvaluationContextProvider . The context itself is a powerful StandardEvaluationContext allowing a wide range of operations, access to static types and context extensions. Make sure to parse and evaluate only expressions from trusted sources such as annotations. Accepting user-provided expressions can create an entry path to exploit the application context and your system resulting in a potential security vulnerability. Extending the Evaluation Context: EvaluationContextProvider and its reactive variant ReactiveEvaluationContextProvider provide access to an EvaluationContext . ExtensionAwareEvaluationContextProvider and its reactive variant ReactiveExtensionAwareEvaluationContextProvider are default implementations that determine context extensions from an application context, specifically ListableBeanFactory . Extensions implement either EvaluationContextExtension or ReactiveEvaluationContextExtension to provide extension support to hydrate EvaluationContext . That are a root object, properties and functions (top-level methods). The following example shows a context extension that provides a root object, properties, functions and an aliased function. Implementing a EvaluationContextExtension Java Kotlin @Component public class MyExtension implements EvaluationContextExtension { @Override public String getExtensionId() { return ""my-extension""; } @Override public Object getRootObject() { return new CustomExtensionRootObject(); } @Override public Map<String, Object> getProperties() { Map<String, Object> properties = new HashMap<>(); properties.put(""key"", ""Hello""); return properties; } @Override public Map<String, Function> getFunctions() { Map<String, Function> functions = new HashMap<>(); try { functions.put(""aliasedMethod"", new Function(getClass().getMethod(""extensionMethod""))); return functions; } catch (Exception o_O) { throw new RuntimeException(o_O); } } public static String extensionMethod() { return ""Hello World""; } public static int add(int i1, int i2) { return i1 + i2; } } public class CustomExtensionRootObject { public boolean rootObjectInstanceMethod() { return true; } } @Component class MyExtension : EvaluationContextExtension { override fun getExtensionId(): String { return ""my-extension"" } override fun getRootObject(): Any? { return CustomExtensionRootObject() } override fun getProperties(): Map<String, Any> { val properties: MutableMap<String, Any> = HashMap() properties[""key""] = ""Hello"" return properties } override fun getFunctions(): Map<String, Function> { val functions: MutableMap<String, Function> = HashMap() try { functions[""aliasedMethod""] = Function(javaClass.getMethod(""extensionMethod"")) return functions } catch (o_O: Exception) { throw RuntimeException(o_O) } } companion object { fun extensionMethod(): String { return ""Hello World"" } fun add(i1: Int, i2: Int): Int { return i1 + i2 } } } class CustomExtensionRootObject { fun rootObjectInstanceMethod(): Boolean { return true } } Once the above shown extension is registered, you can use its exported methods, properties and root object to evaluate SpEL expressions: Example 3. Expression Evaluation Examples #{add(1, 2)} (1) #{extensionMethod()} (2) #{aliasedMethod()} (3) #{key} (4) #{rootObjectInstanceMethod()} (5) 1 Invoke the method add declared by MyExtension resulting in 3 as the method adds both numeric parameters and returns the sum. 2 Invoke the method extensionMethod declared by MyExtension resulting in Hello World . 3 Invoke the method aliasedMethod . The method is exposed as function and redirects into the method extensionMethod declared by MyExtension resulting in Hello World . 4 Evaluate the key property resulting in Hello . 5 Invoke the method rootObjectInstanceMethod on the root object instance CustomExtensionRootObject . You can find real-life context extensions at SecurityEvaluationContextExtension(https://github.com/spring-projects/spring-security/blob/main/data/src/main/java/org/springframework/security/data/repository/query/SecurityEvaluationContextExtension.java) . Property Placeholders: Property placeholders following the form ${…} refer to properties provided typically by a PropertySource through Environment . Properties are useful to resolve against system properties, application configuration files, environment configuration or property sources contributed by secret management systems. You can find more details on the property placeholders in Spring Framework’s documentation on @Value usage(https://docs.spring.io/spring-framework/reference/6.1/core/beans/annotation-config/value-annotations.html#page-title) ."
"https://docs.spring.io/spring-data/relational/reference/3.3/query-by-example.html","Query by Example: Introduction: This chapter provides an introduction to Query by Example and explains how to use it. Query by Example (QBE) is a user-friendly querying technique with a simple interface. It allows dynamic query creation and does not require you to write queries that contain field names. In fact, Query by Example does not require you to write queries by using store-specific query languages at all. This chapter explains the core concepts of Query by Example. The information is pulled from the Spring Data Commons module. Depending on your database, String matching support can be limited. Usage: The Query by Example API consists of four parts: Probe: The actual example of a domain object with populated fields. ExampleMatcher : The ExampleMatcher carries details on how to match particular fields. It can be reused across multiple Examples. Example : An Example consists of the probe and the ExampleMatcher . It is used to create the query. FetchableFluentQuery : A FetchableFluentQuery offers a fluent API, that allows further customization of a query derived from an Example . Using the fluent API lets you specify ordering projection and result processing for your query. Query by Example is well suited for several use cases: Querying your data store with a set of static or dynamic constraints. Frequent refactoring of the domain objects without worrying about breaking existing queries. Working independently of the underlying data store API. Query by Example also has several limitations: No support for nested or grouped property constraints, such as firstname = ?0 or (firstname = ?1 and lastname = ?2) . Store-specific support on string matching. Depending on your databases, String matching can support starts/contains/ends/regex for strings. Exact matching for other property types. Before getting started with Query by Example, you need to have a domain object. To get started, create an interface for your repository, as shown in the following example: Sample Person object public class Person { @Id private String id; private String firstname; private String lastname; private Address address; // … getters and setters omitted } The preceding example shows a simple domain object. You can use it to create an Example . By default, fields having null values are ignored, and strings are matched by using the store specific defaults. Inclusion of properties into a Query by Example criteria is based on nullability. Properties using primitive types ( int , double , …) are always included unless the ExampleMatcher ignores the property path(#query-by-example.matchers) . Examples can be built by either using the of factory method or by using ExampleMatcher(#query-by-example.matchers) . Example is immutable. The following listing shows a simple Example: Example 1. Simple Example Person person = new Person(); (1) person.setFirstname(""Dave""); (2) Example<Person> example = Example.of(person); (3) 1 Create a new instance of the domain object. 2 Set the properties to query. 3 Create the Example . You can run the example queries by using repositories. To do so, let your repository interface extend QueryByExampleExecutor<T> . The following listing shows an excerpt from the QueryByExampleExecutor interface: The QueryByExampleExecutor public interface QueryByExampleExecutor<T> { <S extends T> S findOne(Example<S> example); <S extends T> Iterable<S> findAll(Example<S> example); // … more functionality omitted. } Example Matchers: Examples are not limited to default settings. You can specify your own defaults for string matching, null handling, and property-specific settings by using the ExampleMatcher , as shown in the following example: Example 2. Example matcher with customized matching Person person = new Person(); (1) person.setFirstname(""Dave""); (2) ExampleMatcher matcher = ExampleMatcher.matching() (3) .withIgnorePaths(""lastname"") (4) .withIncludeNullValues() (5) .withStringMatcher(StringMatcher.ENDING); (6) Example<Person> example = Example.of(person, matcher); (7) 1 Create a new instance of the domain object. 2 Set properties. 3 Create an ExampleMatcher to expect all values to match. It is usable at this stage even without further configuration. 4 Construct a new ExampleMatcher to ignore the lastname property path. 5 Construct a new ExampleMatcher to ignore the lastname property path and to include null values. 6 Construct a new ExampleMatcher to ignore the lastname property path, to include null values, and to perform suffix string matching. 7 Create a new Example based on the domain object and the configured ExampleMatcher . By default, the ExampleMatcher expects all values set on the probe to match. If you want to get results matching any of the predicates defined implicitly, use ExampleMatcher.matchingAny() . You can specify behavior for individual properties (such as ""firstname"" and ""lastname"" or, for nested properties, ""address.city""). You can tune it with matching options and case sensitivity, as shown in the following example: Configuring matcher options ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", endsWith()) .withMatcher(""lastname"", startsWith().ignoreCase()); } Another way to configure matcher options is to use lambdas (introduced in Java 8). This approach creates a callback that asks the implementor to modify the matcher. You need not return the matcher, because configuration options are held within the matcher instance. The following example shows a matcher that uses lambdas: Configuring matcher options with lambdas ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", match -> match.endsWith()) .withMatcher(""firstname"", match -> match.startsWith()); } Queries created by Example use a merged view of the configuration. Default matching settings can be set at the ExampleMatcher level, while individual settings can be applied to particular property paths. Settings that are set on ExampleMatcher are inherited by property path settings unless they are defined explicitly. Settings on a property patch have higher precedence than default settings. The following table describes the scope of the various ExampleMatcher settings: Table 1. Scope of ExampleMatcher settings Setting Scope Null-handling ExampleMatcher String matching ExampleMatcher and property path Ignoring properties Property path Case sensitivity ExampleMatcher and property path Value transformation Property path Fluent API: QueryByExampleExecutor offers one more method, which we did not mention so far: <S extends T, R> R findBy(Example<S> example, Function<FluentQuery.FetchableFluentQuery<S>, R> queryFunction) . As with other methods, it executes a query derived from an Example . However, with the second argument, you can control aspects of that execution that you cannot dynamically control otherwise. You do so by invoking the various methods of the FetchableFluentQuery in the second argument. sortBy lets you specify an ordering for your result. as lets you specify the type to which you want the result to be transformed. project limits the queried attributes. first , firstValue , one , oneValue , all , page , stream , count , and exists define what kind of result you get and how the query behaves when more than the expected number of results are available. Use the fluent API to get the last of potentially many results, ordered by lastname. Optional<Person> match = repository.findBy(example, q -> q .sortBy(Sort.by(""lastname"").descending()) .first() ); Here’s an example: Employee employee = new Employee(); (1) employee.name= ""Frodo""; Example<Employee> example = Example.of(employee); (2) repository.findAll(example); (3) // do whatever with the result 1 Create a domain object with the criteria ( null fields will be ignored). 2 Using the domain object, create an Example . 3 Through the repository, execute query (use findOne for a single item). This illustrates how to craft a simple probe using a domain object. In this case, it will query based on the Employee object’s name field being equal to Frodo . null fields are ignored. Employee employee = new Employee(); employee.name = ""Baggins""; employee.role = ""ring bearer""; ExampleMatcher matcher = matching() (1) .withMatcher(""name"", endsWith()) (2) .withIncludeNullValues() (3) .withIgnorePaths(""role""); (4) Example<Employee> example = Example.of(employee, matcher); (5) repository.findAll(example); // do whatever with the result 1 Create a custom ExampleMatcher that matches on ALL fields (use matchingAny() to match on ANY fields) 2 For the name field, use a wildcard that matches against the end of the field 3 Match columns against null (don’t forget that NULL doesn’t equal NULL in relational databases). 4 Ignore the role field when forming the query. 5 Plug the custom ExampleMatcher into the probe. It’s also possible to apply a withTransform() against any property, allowing you to transform a property before forming the query. For example, you can apply a toUpperCase() to a String -based property before the query is created. Query By Example really shines when you don’t know all the fields needed in a query in advance. If you were building a filter on a web page where the user can pick the fields, Query By Example is a great way to flexibly capture that into an efficient query."
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/relational/reference/3.3/commons/entity-callbacks.html","Entity Callbacks: The Spring Data infrastructure provides hooks for modifying an entity before and after certain methods are invoked. Those so called EntityCallback instances provide a convenient way to check and potentially modify an entity in a callback fashioned style. An EntityCallback looks pretty much like a specialized ApplicationListener . Some Spring Data modules publish store specific events (such as BeforeSaveEvent ) that allow modifying the given entity. In some cases, such as when working with immutable types, these events can cause trouble. Also, event publishing relies on ApplicationEventMulticaster . If configuring that with an asynchronous TaskExecutor it can lead to unpredictable outcomes, as event processing can be forked onto a Thread. Entity callbacks provide integration points with both synchronous and reactive APIs to guarantee in-order execution at well-defined checkpoints within the processing chain, returning a potentially modified entity or an reactive wrapper type. Entity callbacks are typically separated by API type. This separation means that a synchronous API considers only synchronous entity callbacks and a reactive implementation considers only reactive entity callbacks. The Entity Callback API has been introduced with Spring Data Commons 2.2. It is the recommended way of applying entity modifications. Existing store specific ApplicationEvents are still published before the invoking potentially registered EntityCallback instances. Implementing Entity Callbacks: An EntityCallback is directly associated with its domain type through its generic type argument. Each Spring Data module typically ships with a set of predefined EntityCallback interfaces covering the entity lifecycle. Anatomy of an EntityCallback @FunctionalInterface public interface BeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked before a domain object is saved. * Can return either the same or a modified instance. * * @return the domain object to be persisted. */ (1) T onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called before an entity is saved. Returns a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Anatomy of a reactive EntityCallback @FunctionalInterface public interface ReactiveBeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked on subscription, before a domain object is saved. * The returned Publisher can emit either the same or a modified instance. * * @return Publisher emitting the domain object to be persisted. */ (1) Publisher<T> onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called on subscription, before an entity is saved. Emits a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Optional entity callback parameters are defined by the implementing Spring Data module and inferred from call site of EntityCallback.callback() . Implement the interface suiting your application needs like shown in the example below: Example BeforeSaveCallback class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { (1) if(collection == ""user"") { return // ... } return // ... } @Override public int getOrder() { return 100; (2) } } 1 Callback implementation according to your requirements. 2 Potentially order the entity callback if multiple ones for the same domain type exist. Ordering follows lowest precedence. Registering Entity Callbacks: EntityCallback beans are picked up by the store specific implementations in case they are registered in the ApplicationContext . Most template APIs already implement ApplicationContextAware and therefore have access to the ApplicationContext The following example explains a collection of valid entity callback registrations: Example EntityCallback Bean registration @Order(1) (1) @Component class First implements BeforeSaveCallback<Person> { @Override public Person onBeforeSave(Person person) { return // ... } } @Component class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { // ... } @Override public int getOrder() { return 100; (2) } } @Configuration public class EntityCallbackConfiguration { @Bean BeforeSaveCallback<Person> unorderedLambdaReceiverCallback() { (3) return (BeforeSaveCallback<Person>) it -> // ... } } @Component class UserCallbacks implements BeforeConvertCallback<User>, BeforeSaveCallback<User> { (4) @Override public Person onBeforeConvert(User user) { return // ... } @Override public Person onBeforeSave(User user) { return // ... } } 1 BeforeSaveCallback receiving its order from the @Order annotation. 2 BeforeSaveCallback receiving its order via the Ordered interface implementation. 3 BeforeSaveCallback using a lambda expression. Unordered by default and invoked last. Note that callbacks implemented by a lambda expression do not expose typing information hence invoking these with a non-assignable entity affects the callback throughput. Use a class or enum to enable type filtering for the callback bean. 4 Combine multiple entity callback interfaces in a single implementation class."
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/null-handling.html","Null Handling of Repository Methods: As of Spring Data 2.0, repository CRUD methods that return an individual aggregate instance use Java 8’s Optional to indicate the potential absence of a value. Besides that, Spring Data supports returning the following wrapper types on query methods: com.google.common.base.Optional scala.Option io.vavr.control.Option Alternatively, query methods can choose not to use a wrapper type at all. The absence of a query result is then indicated by returning null . Repository methods returning collections, collection alternatives, wrappers, and streams are guaranteed never to return null but rather the corresponding empty representation. See “ Repository query return types(query-return-types-reference.html) ” for details. Nullability Annotations: You can express nullability constraints for repository methods by using Spring Framework’s nullability annotations(https://docs.spring.io/spring-framework/reference/6.1/core/null-safety.html) . They provide a tooling-friendly approach and opt-in null checks during runtime, as follows: @NonNullApi(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNullApi.html) : Used on the package level to declare that the default behavior for parameters and return values is, respectively, neither to accept nor to produce null values. @NonNull(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNull.html) : Used on a parameter or return value that must not be null (not needed on a parameter and return value where @NonNullApi applies). @Nullable(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/Nullable.html) : Used on a parameter or return value that can be null . Spring annotations are meta-annotated with JSR 305(https://jcp.org/en/jsr/detail?id=305) annotations (a dormant but widely used JSR). JSR 305 meta-annotations let tooling vendors (such as IDEA(https://www.jetbrains.com/help/idea/nullable-and-notnull-annotations.html) , Eclipse(https://help.eclipse.org/latest/index.jsp?topic=/org.eclipse.jdt.doc.user/tasks/task-using_external_null_annotations.htm) , and Kotlin(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) ) provide null-safety support in a generic way, without having to hard-code support for Spring annotations. To enable runtime checking of nullability constraints for query methods, you need to activate non-nullability on the package level by using Spring’s @NonNullApi in package-info.java , as shown in the following example: Declaring Non-nullability in package-info.java @org.springframework.lang.NonNullApi package com.acme; Once non-null defaulting is in place, repository query method invocations get validated at runtime for nullability constraints. If a query result violates the defined constraint, an exception is thrown. This happens when the method would return null but is declared as non-nullable (the default with the annotation defined on the package in which the repository resides). If you want to opt-in to nullable results again, selectively use @Nullable on individual methods. Using the result wrapper types mentioned at the start of this section continues to work as expected: an empty result is translated into the value that represents absence. The following example shows a number of the techniques just described: Using different nullability constraints package com.acme; (1) import org.springframework.lang.Nullable; interface UserRepository extends Repository<User, Long> { User getByEmailAddress(EmailAddress emailAddress); (2) @Nullable User findByEmailAddress(@Nullable EmailAddress emailAdress); (3) Optional<User> findOptionalByEmailAddress(EmailAddress emailAddress); (4) } 1 The repository resides in a package (or sub-package) for which we have defined non-null behavior. 2 Throws an EmptyResultDataAccessException when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . 3 Returns null when the query does not produce a result. Also accepts null as the value for emailAddress . 4 Returns Optional.empty() when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . Nullability in Kotlin-based Repositories: Kotlin has the definition of nullability constraints(https://kotlinlang.org/docs/reference/null-safety.html) baked into the language. Kotlin code compiles to bytecode, which does not express nullability constraints through method signatures but rather through compiled-in metadata. Make sure to include the kotlin-reflect JAR in your project to enable introspection of Kotlin’s nullability constraints. Spring Data repositories use the language mechanism to define those constraints to apply the same runtime checks, as follows: Using nullability constraints on Kotlin repositories interface UserRepository : Repository<User, String> { fun findByUsername(username: String): User (1) fun findByFirstname(firstname: String?): User? (2) } 1 The method defines both the parameter and the result as non-nullable (the Kotlin default). The Kotlin compiler rejects method invocations that pass null to the method. If the query yields an empty result, an EmptyResultDataAccessException is thrown. 2 This method accepts null for the firstname parameter and returns null if the query does not produce a result."
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/relational/reference/3.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc.html","JDBC: The Spring Data JDBC module applies core Spring concepts to the development of solutions that use JDBC database drivers aligned with Domain-driven design principles(jdbc/domain-driven-design.html) . We provide a “template” as a high-level abstraction for storing and querying aggregates. This document is the reference guide for Spring Data JDBC support. It explains the concepts and semantics and syntax. This chapter points out the specialties for repository support for JDBC. This builds on the core repository support explained in Working with Spring Data Repositories(repositories/introduction.html) . You should have a sound understanding of the basic concepts explained there. Section Summary: Why Spring Data JDBC?(jdbc/why.html) Domain Driven Design and Relational Databases(jdbc/domain-driven-design.html) Getting Started(jdbc/getting-started.html) Persisting Entities(jdbc/entity-persistence.html) Mapping(jdbc/mapping.html) Query Methods(jdbc/query-methods.html) MyBatis Integration(jdbc/mybatis.html) Lifecycle Events(jdbc/events.html) Auditing(jdbc/auditing.html) Transactionality(jdbc/transactions.html) Schema Creation(jdbc/schema-support.html)"
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/why.html","Why Spring Data JDBC?: The main persistence API for relational databases in the Java world is certainly JPA, which has its own Spring Data module. Why is there another one? JPA does a lot of things in order to help the developer. Among other things, it tracks changes to entities. It does lazy loading for you. It lets you map a wide array of object constructs to an equally wide array of database designs. This is great and makes a lot of things really easy. Just take a look at a basic JPA tutorial. But it often gets really confusing as to why JPA does a certain thing. Also, things that are really simple conceptually get rather difficult with JPA. Spring Data JDBC aims to be much simpler conceptually, by embracing the following design decisions: If you load an entity, SQL statements get run. Once this is done, you have a completely loaded entity. No lazy loading or caching is done. If you save an entity, it gets saved. If you do not, it does not. There is no dirty tracking and no session. There is a simple model of how to map entities to tables. It probably only works for rather simple cases. If you do not like that, you should code your own strategy. Spring Data JDBC offers only very limited support for customizing the strategy with annotations."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/domain-driven-design.html","Domain Driven Design and Relational Databases: All Spring Data modules are inspired by the concepts of “repository”, “aggregate”, and “aggregate root” from Domain Driven Design. These are possibly even more important for Spring Data JDBC, because they are, to some extent, contrary to normal practice when working with relational databases. An aggregate is a group of entities that is guaranteed to be consistent between atomic changes to it. A classic example is an Order with OrderItems . A property on Order (for example, numberOfItems is consistent with the actual number of OrderItems ) remains consistent as changes are made. References across aggregates are not guaranteed to be consistent at all times. They are guaranteed to become consistent eventually. Each aggregate has exactly one aggregate root, which is one of the entities of the aggregate. The aggregate gets manipulated only through methods on that aggregate root. These are the atomic changes mentioned earlier. A repository is an abstraction over a persistent store that looks like a collection of all the aggregates of a certain type. For Spring Data in general, this means you want to have one Repository per aggregate root. In addition, for Spring Data JDBC this means that all entities reachable from an aggregate root are considered to be part of that aggregate root. Spring Data JDBC assumes that only the aggregate has a foreign key to a table storing non-root entities of the aggregate and no other entity points toward non-root entities. In the current implementation, entities referenced from an aggregate root are deleted and recreated by Spring Data JDBC. You can overwrite the repository methods with implementations that match your style of working and designing your database."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/getting-started.html","Getting Started: An easy way to bootstrap setting up a working environment is to create a Spring-based project in Spring Tools(https://spring.io/tools) or from Spring Initializr(https://start.spring.io) . First, you need to set up a running database server. Refer to your vendor documentation on how to configure your database for JDBC access. Requirements: Spring Data JDBC requires Spring Framework(https://docs.spring.io/spring-framework/reference/6.1) 6.1.13 and above. In terms of databases, Spring Data JDBC requires a dialect(#jdbc.dialects) to abstract common SQL functionality over vendor-specific flavours. Spring Data JDBC includes direct support for the following databases: DB2 H2 HSQLDB MariaDB Microsoft SQL Server MySQL Oracle Postgres If you use a different database then your application won’t start up. The dialect(#jdbc.dialects) section contains further detail on how to proceed in such case. Hello World: To create a Spring project in STS: Go to File → New → Spring Template Project → Simple Spring Utility Project, and press Yes when prompted. Then enter a project and a package name, such as org.spring.jdbc.example . Add the following to the pom.xml files dependencies element: <dependencies> <!-- other dependency elements omitted --> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-jdbc</artifactId> <version>3.3.4</version> </dependency> </dependencies> Change the version of Spring in the pom.xml to be <spring.version>6.1.13</spring.version> Add the following location of the Spring Milestone repository for Maven to your pom.xml such that it is at the same level as your <dependencies/> element: <repositories> <repository> <id>spring-milestone</id> <name>Spring Maven MILESTONE Repository</name> <url>https://repo.spring.io/milestone</url> </repository> </repositories> The repository is also browseable here(https://repo.spring.io/milestone/org/springframework/data/) . Logging: Spring Data JDBC does little to no logging on its own. Instead, the mechanics of JdbcTemplate to issue SQL statements provide logging. Thus, if you want to inspect what SQL statements are run, activate logging for Spring’s NamedParameterJdbcTemplate(https://docs.spring.io/spring-framework/reference/6.1/data-access.html#jdbc-JdbcTemplate) or MyBatis(https://www.mybatis.org/mybatis-3/logging.html) . You may also want to set the logging level to DEBUG to see some additional information. To do so, edit the application.properties file to have the following content: logging.level.org.springframework.jdbc=DEBUG Examples Repository: There is a GitHub repository with several examples(https://github.com/spring-projects/spring-data-examples) that you can download and play around with to get a feel for how the library works. Configuration: The Spring Data JDBC repositories support can be activated by an annotation through Java configuration, as the following example shows: Spring Data JDBC repositories using Java configuration @Configuration @EnableJdbcRepositories (1) class ApplicationConfig extends AbstractJdbcConfiguration { (2) @Bean DataSource dataSource() { (3) EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); return builder.setType(EmbeddedDatabaseType.HSQL).build(); } @Bean NamedParameterJdbcOperations namedParameterJdbcOperations(DataSource dataSource) { (4) return new NamedParameterJdbcTemplate(dataSource); } @Bean TransactionManager transactionManager(DataSource dataSource) { (5) return new DataSourceTransactionManager(dataSource); } } 1 @EnableJdbcRepositories creates implementations for interfaces derived from Repository 2 AbstractJdbcConfiguration(../api/java/org/springframework/data/jdbc/repository/config/AbstractJdbcConfiguration.html) provides various default beans required by Spring Data JDBC 3 Creates a DataSource connecting to a database. This is required by the following two bean methods. 4 Creates the NamedParameterJdbcOperations used by Spring Data JDBC to access the database. 5 Spring Data JDBC utilizes the transaction management provided by Spring JDBC. The configuration class in the preceding example sets up an embedded HSQL database by using the EmbeddedDatabaseBuilder API of spring-jdbc . The DataSource is then used to set up NamedParameterJdbcOperations and a TransactionManager . We finally activate Spring Data JDBC repositories by using the @EnableJdbcRepositories . If no base package is configured, it uses the package in which the configuration class resides. Extending AbstractJdbcConfiguration(../api/java/org/springframework/data/jdbc/repository/config/AbstractJdbcConfiguration.html) ensures various beans get registered. Overwriting its methods can be used to customize the setup (see below). This configuration can be further simplified by using Spring Boot. With Spring Boot a DataSource is sufficient once the starter spring-boot-starter-data-jdbc is included in the dependencies. Everything else is done by Spring Boot. There are a couple of things one might want to customize in this setup. Dialects: Spring Data JDBC uses implementations of the interface Dialect to encapsulate behavior that is specific to a database or its JDBC driver. By default, the AbstractJdbcConfiguration(../api/java/org/springframework/data/jdbc/repository/config/AbstractJdbcConfiguration.html) attempts to determine the dialect from the database configuration by obtaining a connection and registering the correct Dialect . You override AbstractJdbcConfiguration.jdbcDialect(NamedParameterJdbcOperations) to customize dialect selection. If you use a database for which no dialect is available, then your application won’t start up. In that case, you’ll have to ask your vendor to provide a Dialect implementation. Alternatively, you can implement your own Dialect . Dialects are resolved by DialectResolver(../api/java/org/springframework/data/jdbc/repository/config/DialectResolver.html) from a JdbcOperations instance, typically by inspecting Connection.getMetaData() . + You can let Spring auto-discover your JdbcDialect(../api/java/org/springframework/data/jdbc/core/dialect/JdbcDialect.html) by registering a class that implements org.springframework.data.jdbc.repository.config.DialectResolver$JdbcDialectProvider through META-INF/spring.factories . DialectResolver discovers dialect provider implementations from the class path using Spring’s SpringFactoriesLoader . To do so: Implement your own Dialect . Implement a JdbcDialectProvider returning the Dialect . Register the provider by creating a spring.factories resource under META-INF and perform the registration by adding a line org.springframework.data.jdbc.repository.config.DialectResolver$JdbcDialectProvider=<fully qualified name of your JdbcDialectProvider>"
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/entity-persistence.html","Persisting Entities: Saving an aggregate can be performed with the CrudRepository.save(…) method. If the aggregate is new, this results in an insert for the aggregate root, followed by insert statements for all directly or indirectly referenced entities. If the aggregate root is not new, all referenced entities get deleted, the aggregate root gets updated, and all referenced entities get inserted again. Note that whether an instance is new is part of the instance’s state. This approach has some obvious downsides. If only few of the referenced entities have been actually changed, the deletion and insertion is wasteful. While this process could and probably will be improved, there are certain limitations to what Spring Data JDBC can offer. It does not know the previous state of an aggregate. So any update process always has to take whatever it finds in the database and make sure it converts it to whatever is the state of the entity passed to the save method. See also Entity State Detection(../repositories/core-concepts.html#is-new-state-detection) for further details. Loading Aggregates: Spring Data JDBC offers two ways how it can load aggregates: The traditional and before version 3.2 the only way is really simple: Each query loads the aggregate roots, independently if the query is based on a CrudRepository method, a derived query or a annotated query. If the aggregate root references other entities those are loaded with separate statements. Spring Data JDBC 3.2 allows the use of Single Query Loading . With this an arbitrary number of aggregates can be fully loaded with a single SQL query. This should be significantly more efficient, especially for complex aggregates, consisting of many entities. Currently, Single Query Loading is restricted in different ways: The aggregate must not have nested collections, this includes Map . The plan is to remove this constraint in the future. The aggregate must not use AggregateReference or embedded entities. The plan is to remove this constraint in the future. The database dialect must support it.Of the dialects provided by Spring Data JDBC all but H2 and HSQL support this. H2 and HSQL don’t support analytic functions (aka windowing functions). It only works for the find methods in CrudRepository , not for derived queries and not for annotated queries. The plan is to remove this constraint in the future. Single Query Loading needs to be enabled in the JdbcMappingContext , by calling setSingleQueryLoadingEnabled(true) If any condition is not fulfilled Spring Data JDBC falls back to the default approach of loading aggregates. Single Query Loading is to be considered experimental. We appreciate feedback on how it works for you. While Single Query Loading can be abbreviated as SQL, but we highly discourage doing so since confusion with Structured Query Language is almost guaranteed. ID Generation: Spring Data uses the identifer property to identify entities. The ID of an entity must be annotated with Spring Data’s @Id(https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/annotation/Id.html) annotation. When your database has an auto-increment column for the ID column, the generated value gets set in the entity after inserting it into the database. Spring Data does not attempt to insert values of identifier columns when the entity is new and the identifier value defaults to its initial value. That is 0 for primitive types and null if the identifier property uses a numeric wrapper type such as Long . Entity State Detection(../repositories/core-concepts.html#is-new-state-detection) explains in detail the strategies to detect whether an entity is new or whether it is expected to exist in your database. One important constraint is that, after saving an entity, the entity must not be new anymore. Note that whether an entity is new is part of the entity’s state. With auto-increment columns, this happens automatically, because the ID gets set by Spring Data with the value from the ID column. Template API: As an alternative to repositories Spring Data JDBC offers the JdbcAggregateTemplate(../api/java/org/springframework/data/jdbc/core/JdbcAggregateTemplate.html) as a more direct means to load and persist entities in a relational database. To a large extent, repositories use JdbcAggregateTemplate to implement their features. This section highlights only the most interesting parts of the JdbcAggregateTemplate . For a more complete overview, see the JavaDoc of JdbcAggregateTemplate . Accessing the JdbcAggregateTemplate: JdbcAggregateTemplate is intended to be used as a Spring bean. If you have set up your application to include Spring Data JDBC, you can configure a dependency on JdbcAggregateTemplate in any Spring bean, and the Spring Framework injects a properly configured instance. This includes fragments you use to implement custom methods for your Spring Data Repositories, letting you to use JdbcAggregateTemplate to customize and extend your repositories. Persisting: JdbcAggregateTemplate offers three types of methods for persisting entities: `save , insert , and update . Each comes in two flavors: Operating on single aggregates, named exactly as mentioned above, and with an All suffix operation on an Iterable . save does the same as the method of same name in a repository. insert and update skip the test if the entity is new and assume a new or existing aggregate as indicated by their name. Querying: JdbcAggregateTemplate offers a considerable array of methods for querying aggregates and about collections of aggregates. There is one type of method that requires special attention. That’s the methods taking a Query as an argument. They allow the execution of programmatically constructed queries, as follows: template.findOne(query(where(""name"").is(""Gandalf"")), Person.class); The Query(../api/java/org/springframework/data/relational/core/query/Query.html) returned by the query method defines the list of columns to select, a where clause (through a CriteriaDefinition), and specification of limit and offset clauses. For details of the Query class, see its JavaDoc. The Criteria(../api/java/org/springframework/data/relational/core/query/Criteria.html) class, of which where is a static member, provides implementations of org.springframework.data.relational.core.query.CriteriaDefinition[], which represent the where-clause of the query. Methods for the Criteria Class: The Criteria class provides the following methods, all of which correspond to SQL operators: Criteria and (String column) : Adds a chained Criteria with the specified property to the current Criteria and returns the newly created one. Criteria or (String column) : Adds a chained Criteria with the specified property to the current Criteria and returns the newly created one. Criteria greaterThan (Object o) : Creates a criterion by using the > operator. Criteria greaterThanOrEquals (Object o) : Creates a criterion by using the >= operator. Criteria in (Object…​ o) : Creates a criterion by using the IN operator for a varargs argument. Criteria in (Collection<?> collection) : Creates a criterion by using the IN operator using a collection. Criteria is (Object o) : Creates a criterion by using column matching ( property = value ). Criteria isNull () : Creates a criterion by using the IS NULL operator. Criteria isNotNull () : Creates a criterion by using the IS NOT NULL operator. Criteria lessThan (Object o) : Creates a criterion by using the < operator. Criteria lessThanOrEquals (Object o) : Creates a criterion by using the ⇐ operator. Criteria like (Object o) : Creates a criterion by using the LIKE operator without escape character processing. Criteria not (Object o) : Creates a criterion by using the != operator. Criteria notIn (Object…​ o) : Creates a criterion by using the NOT IN operator for a varargs argument. Criteria notIn (Collection<?> collection) : Creates a criterion by using the NOT IN operator using a collection. Optimistic Locking: Spring Data supports optimistic locking by means of a numeric attribute that is annotated with @Version(https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/annotation/Version.html) on the aggregate root. Whenever Spring Data saves an aggregate with such a version attribute two things happen: The update statement for the aggregate root will contain a where clause checking that the version stored in the database is actually unchanged. If this isn’t the case an OptimisticLockingFailureException will be thrown. Also, the version attribute gets increased both in the entity and in the database so a concurrent action will notice the change and throw an OptimisticLockingFailureException if applicable as described above. This process also applies to inserting new aggregates, where a null or 0 version indicates a new instance and the increased instance afterwards marks the instance as not new anymore, making this work rather nicely with cases where the id is generated during object construction for example when UUIDs are used. During deletes the version check also applies but no version is increased."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/mapping.html","Mapping: Rich mapping support is provided by the BasicJdbcConverter . BasicJdbcConverter has a rich metadata model that allows mapping domain objects to a data row. The mapping metadata model is populated by using annotations on your domain objects. However, the infrastructure is not limited to using annotations as the only source of metadata information. The BasicJdbcConverter also lets you map objects to rows without providing any additional metadata, by following a set of conventions. This section describes the features of the BasicJdbcConverter , including how to use conventions for mapping objects to rows and how to override those conventions with annotation-based mapping metadata. Read on the basics about Object Mapping Fundamentals(../object-mapping.html) before continuing with this chapter. Convention-based Mapping: BasicJdbcConverter has a few conventions for mapping objects to rows when no additional mapping metadata is provided. The conventions are: The short Java class name is mapped to the table name in the following manner. The com.bigbank.SavingsAccount class maps to the SAVINGS_ACCOUNT table name. The same name mapping is applied for mapping fields to column names. For example, the firstName field maps to the FIRST_NAME column. You can control this mapping by providing a custom NamingStrategy . See Mapping Configuration(#mapping.configuration) for more detail. Table and column names that are derived from property or class names are used in SQL statements without quotes by default. You can control this behavior by setting RelationalMappingContext.setForceQuote(true) . The converter uses any Spring Converters registered with CustomConversions to override the default mapping of object properties to row columns and values. The fields of an object are used to convert to and from columns in the row. Public JavaBean properties are not used. If you have a single non-zero-argument constructor whose constructor argument names match top-level column names of the row, that constructor is used. Otherwise, the zero-argument constructor is used. If there is more than one non-zero-argument constructor, an exception is thrown. Refer to Object Creation(../object-mapping.html#mapping.object-creation) for further details. Supported Types in Your Entity: The properties of the following types are currently supported: All primitive types and their boxed types ( int , float , Integer , Float , and so on) Enums get mapped to their name. String java.util.Date , java.time.LocalDate , java.time.LocalDateTime , and java.time.LocalTime Arrays and Collections of the types mentioned above can be mapped to columns of array type if your database supports that. Anything your database driver accepts. References to other entities. They are considered a one-to-one relationship, or an embedded type. It is optional for one-to-one relationship entities to have an id attribute. The table of the referenced entity is expected to have an additional column with a name based on the referencing entity see Back References(#jdbc.entity-persistence.types.backrefs) . Embedded entities do not need an id . If one is present it gets mapped as a normal attribute without any special meaning. Set<some entity> is considered a one-to-many relationship. The table of the referenced entity is expected to have an additional column with a name based on the referencing entity see Back References(#jdbc.entity-persistence.types.backrefs) . Map<simple type, some entity> is considered a qualified one-to-many relationship. The table of the referenced entity is expected to have two additional columns: One named based on the referencing entity for the foreign key (see Back References(#jdbc.entity-persistence.types.backrefs) ) and one with the same name and an additional _key suffix for the map key. List<some entity> is mapped as a Map<Integer, some entity> . The same additional columns are expected and the names used can be customized in the same way. For List , Set , and Map naming of the back reference can be controlled by implementing NamingStrategy.getReverseColumnName(RelationalPersistentEntity<?> owner) and NamingStrategy.getKeyColumn(RelationalPersistentProperty property) , respectively. Alternatively you may annotate the attribute with @MappedCollection(idColumn=""your_column_name"", keyColumn=""your_key_column_name"") . Specifying a key column for a Set has no effect. Mapping Annotation Overview: The RelationalConverter can use metadata to drive the mapping of objects to rows. The following annotations are available: @Id : Applied at the field level to mark the primary key. @Table : Applied at the class level to indicate this class is a candidate for mapping to the database. You can specify the name of the table where the database is stored. @Transient : By default, all fields are mapped to the row. This annotation excludes the field where it is applied from being stored in the database. Transient properties cannot be used within a persistence constructor as the converter cannot materialize a value for the constructor argument. @PersistenceCreator : Marks a given constructor or static factory method — even a package protected one — to use when instantiating the object from the database. Constructor arguments are mapped by name to the values in the retrieved row. @Value : This annotation is part of the Spring Framework. Within the mapping framework it can be applied to constructor arguments. This lets you use a Spring Expression Language statement to transform a key’s value retrieved in the database before it is used to construct a domain object. In order to reference a column of a given row one has to use expressions like: @Value(""#root.myProperty"") where root refers to the root of the given Row . @Column : Applied at the field level to describe the name of the column as it is represented in the row, letting the name be different from the field name of the class. Names specified with a @Column annotation are always quoted when used in SQL statements. For most databases, this means that these names are case-sensitive. It also means that you can use special characters in these names. However, this is not recommended, since it may cause problems with other tools. @Version : Applied at field level is used for optimistic locking and checked for modification on save operations. The value is null ( zero for primitive types) is considered as marker for entities to be new. The initially stored value is zero ( one for primitive types). The version gets incremented automatically on every update. See Optimistic Locking(entity-persistence.html#jdbc.entity-persistence.optimistic-locking) for further reference. The mapping metadata infrastructure is defined in the separate spring-data-commons project that is technology-agnostic. Specific subclasses are used in the JDBC support to support annotation based metadata. Other strategies can also be put in place (if there is demand). Referenced Entities: The handling of referenced entities is limited. This is based on the idea of aggregate roots as described above. If you reference another entity, that entity is, by definition, part of your aggregate. So, if you remove the reference, the previously referenced entity gets deleted. This also means references are 1-1 or 1-n, but not n-1 or n-m. If you have n-1 or n-m references, you are, by definition, dealing with two separate aggregates. References between those may be encoded as simple id values, which map properly with Spring Data JDBC. A better way to encode these, is to make them instances of AggregateReference . An AggregateReference is a wrapper around an id value which marks that value as a reference to a different aggregate. Also, the type of that aggregate is encoded in a type parameter. Back References: All references in an aggregate result in a foreign key relationship in the opposite direction in the database. By default, the name of the foreign key column is the table name of the referencing entity. Alternatively you may choose to have them named by the entity name of the referencing entity ignoring @Table annotations. You activate this behaviour by calling setForeignKeyNaming(ForeignKeyNaming.IGNORE_RENAMING) on the RelationalMappingContext . For List and Map references an additional column is required for holding the list index or map key. It is based on the foreign key column with an additional _KEY suffix. If you want a completely different way of naming these back references you may implement NamingStrategy.getReverseColumnName(RelationalPersistentEntity<?> owner) in a way that fits your needs. Declaring and setting an AggregateReference class Person { @Id long id; AggregateReference<Person, Long> bestFriend; } // ... Person p1, p2 = // some initialization p1.bestFriend = AggregateReference.to(p2.id); You should not include attributes in your entities to hold the actual value of a back reference, nor of the key column of maps or lists. If you want these value to be available in your domain model we recommend to do this in a AfterConvertCallback and store the values in transient values. Types for which you registered suitable () . Naming Strategy: By convention, Spring Data applies a NamingStrategy to determine table, column, and schema names defaulting to snake case(https://en.wikipedia.org/wiki/Snake_case) . An object property named firstName becomes first_name . You can tweak that by providing a NamingStrategy(../api/java/org/springframework/data/relational/core/mapping/NamingStrategy.html) in your application context. Override table names: When the table naming strategy does not match your database table names, you can override the table name with the Table(../api/java/org/springframework/data/relational/core/mapping/Table.html) annotation. The element value of this annotation provides the custom table name. The following example maps the MyEntity class to the CUSTOM_TABLE_NAME table in the database: @Table(""CUSTOM_TABLE_NAME"") class MyEntity { @Id Integer id; String name; } You may use Spring Data’s SpEL support(../value-expressions.html) to dynamically create the table name. Once generated the table name will be cached, so it is dynamic per mapping context only. Override column names: When the column naming strategy does not match your database table names, you can override the table name with the Column(../api/java/org/springframework/data/relational/core/mapping/Column.html) annotation. The element value of this annotation provides the custom column name. The following example maps the name property of the MyEntity class to the CUSTOM_COLUMN_NAME column in the database: class MyEntity { @Id Integer id; @Column(""CUSTOM_COLUMN_NAME"") String name; } The MappedCollection(../api/java/org/springframework/data/relational/core/mapping/MappedCollection.html) annotation can be used on a reference type (one-to-one relationship) or on Sets, Lists, and Maps (one-to-many relationship). idColumn element of the annotation provides a custom name for the foreign key column referencing the id column in the other table. In the following example the corresponding table for the MySubEntity class has a NAME column, and the CUSTOM_MY_ENTITY_ID_COLUMN_NAME column of the MyEntity id for relationship reasons: class MyEntity { @Id Integer id; @MappedCollection(idColumn = ""CUSTOM_MY_ENTITY_ID_COLUMN_NAME"") Set<MySubEntity> subEntities; } class MySubEntity { String name; } When using List and Map you must have an additional column for the position of a dataset in the List or the key value of the entity in the Map . This additional column name may be customized with the keyColumn Element of the MappedCollection(../api/java/org/springframework/data/relational/core/mapping/MappedCollection.html) annotation: class MyEntity { @Id Integer id; @MappedCollection(idColumn = ""CUSTOM_COLUMN_NAME"", keyColumn = ""CUSTOM_KEY_COLUMN_NAME"") List<MySubEntity> name; } class MySubEntity { String name; } You may use Spring Data’s SpEL support(../value-expressions.html) to dynamically create column names. Once generated the names will be cached, so it is dynamic per mapping context only. Embedded entities: Embedded entities are used to have value objects in your java data model, even if there is only one table in your database. In the following example you see, that MyEntity is mapped with the @Embedded annotation. The consequence of this is, that in the database a table my_entity with the two columns id and name (from the EmbeddedEntity class) is expected. However, if the name column is actually null within the result set, the entire property embeddedEntity will be set to null according to the onEmpty of @Embedded , which null s objects when all nested properties are null . Opposite to this behavior USE_EMPTY tries to create a new instance using either a default constructor or one that accepts nullable parameter values from the result set. Example 1. Sample Code of embedding objects class MyEntity { @Id Integer id; @Embedded(onEmpty = USE_NULL) (1) EmbeddedEntity embeddedEntity; } class EmbeddedEntity { String name; } 1 Null s embeddedEntity if name in null . Use USE_EMPTY to instantiate embeddedEntity with a potential null value for the name property. If you need a value object multiple times in an entity, this can be achieved with the optional prefix element of the @Embedded annotation. This element represents a prefix and is prepend for each column name in the embedded object. Make use of the shortcuts @Embedded.Nullable & @Embedded.Empty for @Embedded(onEmpty = USE_NULL) and @Embedded(onEmpty = USE_EMPTY) to reduce verbosity and simultaneously set JSR-305 @javax.annotation.Nonnull accordingly. class MyEntity { @Id Integer id; @Embedded.Nullable (1) EmbeddedEntity embeddedEntity; } 1 Shortcut for @Embedded(onEmpty = USE_NULL) . Embedded entities containing a Collection or a Map will always be considered non-empty since they will at least contain the empty collection or map. Such an entity will therefore never be null even when using @Embedded(onEmpty = USE_NULL). Read Only Properties: Attributes annotated with @ReadOnlyProperty will not be written to the database by Spring Data, but they will be read when an entity gets loaded. Spring Data will not automatically reload an entity after writing it. Therefore, you have to reload it explicitly if you want to see data that was generated in the database for such columns. If the annotated attribute is an entity or collection of entities, it is represented by one or more separate rows in separate tables. Spring Data will not perform any insert, delete or update for these rows. Insert Only Properties: Attributes annotated with @InsertOnlyProperty will only be written to the database by Spring Data during insert operations. For updates these properties will be ignored. @InsertOnlyProperty is only supported for the aggregate root. Customized Object Construction: The mapping subsystem allows the customization of the object construction by annotating a constructor with the @PersistenceConstructor annotation.The values to be used for the constructor parameters are resolved in the following way: If a parameter is annotated with the @Value annotation, the given expression is evaluated, and the result is used as the parameter value. If the Java type has a property whose name matches the given field of the input row, then its property information is used to select the appropriate constructor parameter to which to pass the input field value. This works only if the parameter name information is present in the Java .class files, which you can achieve by compiling the source with debug information or using the -parameters command-line switch for javac in Java 8. Otherwise, a MappingException is thrown to indicate that the given constructor parameter could not be bound. class OrderItem { private @Id final String id; private final int quantity; private final double unitPrice; OrderItem(String id, int quantity, double unitPrice) { this.id = id; this.quantity = quantity; this.unitPrice = unitPrice; } // getters/setters omitted } Overriding Mapping with Explicit Converters: Spring Data allows registration of custom converters to influence how values are mapped in the database. Currently, converters are only applied on property-level, i.e. you can only convert single values in your domain to single values in the database and back. Conversion between complex objects and multiple columns isn’t supported. Writing a Property by Using a Registered Spring Converter: The following example shows an implementation of a Converter that converts from a Boolean object to a String value: import org.springframework.core.convert.converter.Converter; @WritingConverter public class BooleanToStringConverter implements Converter<Boolean, String> { @Override public String convert(Boolean source) { return source != null && source ? ""T"" : ""F""; } } There are a couple of things to notice here: Boolean and String are both simple types hence Spring Data requires a hint in which direction this converter should apply (reading or writing). By annotating this converter with @WritingConverter you instruct Spring Data to write every Boolean property as String in the database. Reading by Using a Spring Converter: The following example shows an implementation of a Converter that converts from a String to a Boolean value: @ReadingConverter public class StringToBooleanConverter implements Converter<String, Boolean> { @Override public Boolean convert(String source) { return source != null && source.equalsIgnoreCase(""T"") ? Boolean.TRUE : Boolean.FALSE; } } There are a couple of things to notice here: String and Boolean are both simple types hence Spring Data requires a hint in which direction this converter should apply (reading or writing). By annotating this converter with @ReadingConverter you instruct Spring Data to convert every String value from the database that should be assigned to a Boolean property. Registering Spring Converters with the JdbcConverter: class MyJdbcConfiguration extends AbstractJdbcConfiguration { // … @Override protected List<?> userConverters() { return Arrays.asList(new BooleanToStringConverter(), new StringToBooleanConverter()); } } In previous versions of Spring Data JDBC it was recommended to directly overwrite AbstractJdbcConfiguration.jdbcCustomConversions() . This is no longer necessary or even recommended, since that method assembles conversions intended for all databases, conversions registered by the Dialect used and conversions registered by the user. If you are migrating from an older version of Spring Data JDBC and have AbstractJdbcConfiguration.jdbcCustomConversions() overwritten conversions from your Dialect will not get registered. If you want to rely on Spring Boot(https://spring.io/projects/spring-boot) to bootstrap Spring Data JDBC, but still want to override certain aspects of the configuration, you may want to expose beans of that type. For custom conversions you may e.g. choose to register a bean of type JdbcCustomConversions that will be picked up the by the Boot infrastructure. To learn more about this please make sure to read the Spring Boot Reference Documentation(https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#data.sql.jdbc) . JdbcValue: Value conversion uses JdbcValue to enrich values propagated to JDBC operations with a java.sql.Types type. Register a custom write converter if you need to specify a JDBC-specific type instead of using type derivation. This converter should convert the value to JdbcValue which has a field for the value and for the actual JDBCType ."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/query-methods.html","Query Methods: This section offers some specific information about the implementation and use of Spring Data JDBC. Most of the data access operations you usually trigger on a repository result in a query being run against the databases. Defining such a query is a matter of declaring a method on the repository interface, as the following example shows: PersonRepository with query methods interface PersonRepository extends PagingAndSortingRepository<Person, String> { List<Person> findByFirstname(String firstname); (1) List<Person> findByFirstnameOrderByLastname(String firstname, Pageable pageable); (2) Slice<Person> findByLastname(String lastname, Pageable pageable); (3) Page<Person> findByLastname(String lastname, Pageable pageable); (4) Person findByFirstnameAndLastname(String firstname, String lastname); (5) Person findFirstByLastname(String lastname); (6) @Query(""SELECT * FROM person WHERE lastname = :lastname"") List<Person> findByLastname(String lastname); (7) @Query(""SELECT * FROM person WHERE lastname = :lastname"") Stream<Person> streamByLastname(String lastname); (8) @Query(""SELECT * FROM person WHERE username = :#{ principal?.username }"") Person findActiveUser(); (9) } 1 The method shows a query for all people with the given firstname . The query is derived by parsing the method name for constraints that can be concatenated with And and Or . Thus, the method name results in a query expression of SELECT … FROM person WHERE firstname = :firstname . 2 Use Pageable to pass offset and sorting parameters to the database. 3 Return a Slice<Person> .Selects LIMIT+1 rows to determine whether there’s more data to consume. ResultSetExtractor customization is not supported. 4 Run a paginated query returning Page<Person> .Selects only data within the given page bounds and potentially a count query to determine the total count. ResultSetExtractor customization is not supported. 5 Find a single entity for the given criteria. It completes with IncorrectResultSizeDataAccessException on non-unique results. 6 In contrast to <3>, the first entity is always emitted even if the query yields more result documents. 7 The findByLastname method shows a query for all people with the given lastname . 8 The streamByLastname method returns a Stream , which makes values possible as soon as they are returned from the database. 9 You can use the Spring Expression Language to dynamically resolve parameters. In the sample, Spring Security is used to resolve the username of the current user. The following table shows the keywords that are supported for query methods: Table 1. Supported keywords for query methods Keyword Sample Logical result After findByBirthdateAfter(Date date) birthdate > date GreaterThan findByAgeGreaterThan(int age) age > age GreaterThanEqual findByAgeGreaterThanEqual(int age) age >= age Before findByBirthdateBefore(Date date) birthdate < date LessThan findByAgeLessThan(int age) age < age LessThanEqual findByAgeLessThanEqual(int age) age <= age Between findByAgeBetween(int from, int to) age BETWEEN from AND to NotBetween findByAgeNotBetween(int from, int to) age NOT BETWEEN from AND to In findByAgeIn(Collection<Integer> ages) age IN (age1, age2, ageN) NotIn findByAgeNotIn(Collection ages) age NOT IN (age1, age2, ageN) IsNotNull , NotNull findByFirstnameNotNull() firstname IS NOT NULL IsNull , Null findByFirstnameNull() firstname IS NULL Like , StartingWith , EndingWith findByFirstnameLike(String name) firstname LIKE name NotLike , IsNotLike findByFirstnameNotLike(String name) firstname NOT LIKE name Containing on String findByFirstnameContaining(String name) firstname LIKE '%' + name + '%' NotContaining on String findByFirstnameNotContaining(String name) firstname NOT LIKE '%' + name + '%' (No keyword) findByFirstname(String name) firstname = name Not findByFirstnameNot(String name) firstname != name IsTrue , True findByActiveIsTrue() active IS TRUE IsFalse , False findByActiveIsFalse() active IS FALSE Query derivation is limited to properties that can be used in a WHERE clause without using joins. Query Lookup Strategies: The JDBC module supports defining a query manually as a String in a @Query annotation or as named query in a property file. Deriving a query from the name of the method is is currently limited to simple properties, that means properties present in the aggregate root directly. Also, only select queries are supported by this approach. Using @Query: The following example shows how to use @Query to declare a query method: Declare a query method by using @Query interface UserRepository extends CrudRepository<User, Long> { @Query(""select firstName, lastName from User u where u.emailAddress = :email"") User findByEmailAddress(@Param(""email"") String email); } For converting the query result into entities the same RowMapper is used by default as for the queries Spring Data JDBC generates itself. The query you provide must match the format the RowMapper expects. Columns for all properties that are used in the constructor of an entity must be provided. Columns for properties that get set via setter, wither or field access are optional. Properties that don’t have a matching column in the result will not be set. The query is used for populating the aggregate root, embedded entities and one-to-one relationships including arrays of primitive types which get stored and loaded as SQL-array-types. Separate queries are generated for maps, lists, sets and arrays of entities. Properties one-to-one relationships must have there name prefixed by the name of the relationship plus _ . For example if the User from the example above has an address with the property city the column for that city must be labeled address_city . Note that String-based queries do not support pagination nor accept Sort , PageRequest , and Limit as a query parameter as for these queries the query would be required to be rewritten. If you want to apply limiting, please express this intent using SQL and bind the appropriate parameters to the query yourself. Queries may contain SpEL expressions where bind variables are allowed. Such a SpEL expression will get replaced with a bind variable and the variable gets bound to the result of the SpEL expression. Use a SpEL in a query @Query(""SELECT * FROM person WHERE id = :#{#person.id}"") Person findWithSpEL(PersonRef person); This can be used to access members of a parameter, as demonstrated in the example above. For more involved use cases an EvaluationContextExtension can be made available in the application context, which in turn can make any object available in to the SpEL. Spring fully supports Java 8’s parameter name discovery based on the -parameters compiler flag. By using this flag in your build as an alternative to debug information, you can omit the @Param annotation for named parameters. Spring Data JDBC supports only named parameters. Named Queries: If no query is given in an annotation as described in the previous section Spring Data JDBC will try to locate a named query. There are two ways how the name of the query can be determined. The default is to take the domain class of the query, i.e. the aggregate root of the repository, take its simple name and append the name of the method separated by a . . Alternatively the @Query annotation has a name attribute which can be used to specify the name of a query to be looked up. Named queries are expected to be provided in the property file META-INF/jdbc-named-queries.properties on the classpath. The location of that file may be changed by setting a value to @EnableJdbcRepositories.namedQueriesLocation . Named queries are handled in the same way as queries provided by annotation. Customizing Query Methods: Streaming Results: When you specify Stream as the return type of a query method, Spring Data JDBC returns elements as soon as they become available. When dealing with large amounts of data this is suitable for reducing latency and memory requirements. The stream contains an open connection to the database. To avoid memory leaks, that connection needs to be closed eventually, by closing the stream. The recommended way to do that is a try-with-resource clause . It also means that, once the connection to the database is closed, the stream cannot obtain further elements and likely throws an exception. Custom RowMapper or ResultSetExtractor: The @Query annotation allows you to specify a custom RowMapper or ResultSetExtractor to use. The attributes rowMapperClass and resultSetExtractorClass allow you to specify classes to use, which will get instantiated using a default constructor. Alternatively you may set rowMapperClassRef or resultSetExtractorClassRef to a bean name from your Spring application context. If you want to use a certain RowMapper not just for a single method but for all methods with custom queries returning a certain type, you may register a RowMapperMap bean and registering a RowMapper per method return type. The following example shows how to register DefaultQueryMappingConfiguration : @Bean QueryMappingConfiguration rowMappers() { return new DefaultQueryMappingConfiguration() .register(Person.class, new PersonRowMapper()) .register(Address.class, new AddressRowMapper()); } When determining which RowMapper to use for a method, the following steps are followed, based on the return type of the method: If the type is a simple type, no RowMapper is used. Instead, the query is expected to return a single row with a single column, and a conversion to the return type is applied to that value. The entity classes in the QueryMappingConfiguration are iterated until one is found that is a superclass or interface of the return type in question. The RowMapper registered for that class is used. Iterating happens in the order of registration, so make sure to register more general types after specific ones. If applicable, wrapper types such as collections or Optional are unwrapped. Thus, a return type of Optional<Person> uses the Person type in the preceding process. Using a custom RowMapper through QueryMappingConfiguration , @Query(rowMapperClass=…) , or a custom ResultSetExtractor disables Entity Callbacks and Lifecycle Events as the result mapping can issue its own events/callbacks if needed. Modifying Query: You can mark a query as being a modifying query by using the @Modifying on query method, as the following example shows: @Modifying @Query(""UPDATE DUMMYENTITY SET name = :name WHERE id = :id"") boolean updateName(@Param(""id"") Long id, @Param(""name"") String name); You can specify the following return types: void int (updated record count) boolean (whether a record was updated) Modifying queries are executed directly against the database. No events or callbacks get called. Therefore also fields with auditing annotations do not get updated if they don’t get updated in the annotated query."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/mybatis.html","MyBatis Integration: The CRUD operations and query methods can be delegated to MyBatis. This section describes how to configure Spring Data JDBC to integrate with MyBatis and which conventions to follow to hand over the running of the queries as well as the mapping to the library. Configuration: The easiest way to properly plug MyBatis into Spring Data JDBC is by importing MyBatisJdbcConfiguration into you application configuration: @Configuration @EnableJdbcRepositories @Import(MyBatisJdbcConfiguration.class) class Application { @Bean SqlSessionFactoryBean sqlSessionFactoryBean() { // Configure MyBatis here } } As you can see, all you need to declare is a SqlSessionFactoryBean as MyBatisJdbcConfiguration relies on a SqlSession bean to be available in the ApplicationContext eventually. Usage conventions: For each operation in CrudRepository , Spring Data JDBC runs multiple statements. If there is a SqlSessionFactory(https://github.com/mybatis/mybatis-3/blob/master/src/main/java/org/apache/ibatis/session/SqlSessionFactory.java) in the application context, Spring Data checks, for each step, whether the SessionFactory offers a statement. If one is found, that statement (including its configured mapping to an entity) is used. The name of the statement is constructed by concatenating the fully qualified name of the entity type with Mapper. and a String determining the kind of statement. For example, if an instance of org.example.User is to be inserted, Spring Data JDBC looks for a statement named org.example.UserMapper.insert . When the statement is run, an instance of [ MyBatisContext ] gets passed as an argument, which makes various arguments available to the statement. The following table describes the available MyBatis statements: Name Purpose CrudRepository methods that might trigger this statement Attributes available in the MyBatisContext insert Inserts a single entity. This also applies for entities referenced by the aggregate root. save , saveAll . getInstance : the instance to be saved getDomainType : The type of the entity to be saved. get(<key>) : ID of the referencing entity, where <key> is the name of the back reference column provided by the NamingStrategy . update Updates a single entity. This also applies for entities referenced by the aggregate root. save , saveAll . getInstance : The instance to be saved getDomainType : The type of the entity to be saved. delete Deletes a single entity. delete , deleteById . getId : The ID of the instance to be deleted getDomainType : The type of the entity to be deleted. deleteAll-<propertyPath> Deletes all entities referenced by any aggregate root of the type used as prefix with the given property path. Note that the type used for prefixing the statement name is the name of the aggregate root, not the one of the entity to be deleted. deleteAll . getDomainType : The types of the entities to be deleted. deleteAll Deletes all aggregate roots of the type used as the prefix deleteAll . getDomainType : The type of the entities to be deleted. delete-<propertyPath> Deletes all entities referenced by an aggregate root with the given propertyPath deleteById . getId : The ID of the aggregate root for which referenced entities are to be deleted. getDomainType : The type of the entities to be deleted. findById Selects an aggregate root by ID findById . getId : The ID of the entity to load. getDomainType : The type of the entity to load. findAll Select all aggregate roots findAll . getDomainType : The type of the entity to load. findAllById Select a set of aggregate roots by ID values findAllById . getId : A list of ID values of the entities to load. getDomainType : The type of the entity to load. findAllByProperty-<propertyName> Select a set of entities that is referenced by another entity. The type of the referencing entity is used for the prefix. The referenced entities type is used as the suffix. This method is deprecated. Use findAllByPath instead All find* methods. If no query is defined for findAllByPath getId : The ID of the entity referencing the entities to be loaded. getDomainType : The type of the entity to load. findAllByPath-<propertyPath> Select a set of entities that is referenced by another entity via a property path. All find* methods. getIdentifier : The Identifier holding the id of the aggregate root plus the keys and list indexes of all path elements. getDomainType : The type of the entity to load. findAllSorted Select all aggregate roots, sorted findAll(Sort) . getSort : The sorting specification. findAllPaged Select a page of aggregate roots, optionally sorted findAll(Page) . getPageable : The paging specification. count Count the number of aggregate root of the type used as prefix count getDomainType : The type of aggregate roots to count."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/events.html","Lifecycle Events: Spring Data JDBC publishes lifecycle events to ApplicationListener objects, typically beans in the application context. Events are notifications about a certain lifecycle phase. In contrast to entity callbacks, events are intended for notification. Transactional listeners will receive events when the transaction completes. Events and callbacks get only triggered for aggregate roots. If you want to process non-root entities, you need to do that through a listener for the containing aggregate root. Entity lifecycle events can be costly, and you may notice a change in the performance profile when loading large result sets. You can disable lifecycle events on Template API(../api/java/org/springframework/data/jdbc/core/JdbcAggregateTemplate.html#setEntityLifecycleEventsEnabled(boolean)) . For example, the following listener gets invoked before an aggregate gets saved: @Bean ApplicationListener<BeforeSaveEvent<Object>> loggingSaves() { return event -> { Object entity = event.getEntity(); LOG.info(""{} is getting saved."", entity); }; } If you want to handle events only for a specific domain type you may derive your listener from AbstractRelationalEventListener and overwrite one or more of the onXXX methods, where XXX stands for an event type. Callback methods will only get invoked for events related to the domain type and their subtypes, therefore you don’t require further casting. class PersonLoadListener extends AbstractRelationalEventListener<Person> { @Override protected void onAfterLoad(AfterLoadEvent<Person> personLoad) { LOG.info(personLoad.getEntity()); } } The following table describes the available events.For more details about the exact relation between process steps see the description of available callbacks(#jdbc.entity-callbacks) which map 1:1 to events. Table 1. Available events Event When It Is Published BeforeDeleteEvent(../api/java/org/springframework/data/relational/core/mapping/event/BeforeDeleteEvent.html) Before an aggregate root gets deleted. AfterDeleteEvent(../api/java/org/springframework/data/relational/core/mapping/event/AfterDeleteEvent.html) After an aggregate root gets deleted. BeforeConvertEvent(../api/java/org/springframework/data/relational/core/mapping/event/BeforeConvertEvent.html) Before an aggregate root gets converted into a plan for executing SQL statements, but after the decision was made if the aggregate is new or not, i.e. if an update or an insert is in order. BeforeSaveEvent(../api/java/org/springframework/data/relational/core/mapping/event/BeforeSaveEvent.html) Before an aggregate root gets saved (that is, inserted or updated but after the decision about whether if it gets inserted or updated was made). AfterSaveEvent(../api/java/org/springframework/data/relational/core/mapping/event/AfterSaveEvent.html) After an aggregate root gets saved (that is, inserted or updated). AfterConvertEvent(../api/java/org/springframework/data/relational/core/mapping/event/AfterConvertEvent.html) After an aggregate root gets created from a database ResultSet and all its properties get set. Lifecycle events depend on an ApplicationEventMulticaster , which in case of the SimpleApplicationEventMulticaster can be configured with a TaskExecutor , and therefore gives no guarantees when an Event is processed. Store-specific EntityCallbacks: Spring Data JDBC uses the EntityCallback API(../commons/entity-callbacks.html) for its auditing support and reacts on the callbacks listed in the following table. Table 2. Process Steps and Callbacks of the Different Processes performed by Spring Data JDBC. Process EntityCallback / Process Step Comment Delete BeforeDeleteCallback(../api/java/org/springframework/data/relational/core/mapping/event/BeforeDeleteCallback.html) Before the actual deletion. The aggregate root and all the entities of that aggregate get removed from the database. AfterDeleteCallback(../api/java/org/springframework/data/relational/core/mapping/event/AfterDeleteCallback.html) After an aggregate gets deleted. Save Determine if an insert or an update of the aggregate is to be performed dependent on if it is new or not. BeforeConvertCallback(../api/java/org/springframework/data/relational/core/mapping/event/BeforeConvertCallback.html) This is the correct callback if you want to set an id programmatically. In the previous step new aggregates got detected as such and a Id generated in this step would be used in the following step. Convert the aggregate to a aggregate change, it is a sequence of SQL statements to be executed against the database. In this step the decision is made if an Id is provided by the aggregate or if the Id is still empty and is expected to be generated by the database. BeforeSaveCallback(../api/java/org/springframework/data/relational/core/mapping/event/BeforeSaveCallback.html) Changes made to the aggregate root may get considered, but the decision if an id value will be sent to the database is already made in the previous step. Do not use this for creating Ids for new aggregates. Use BeforeConvertCallback instead. The SQL statements determined above get executed against the database. AfterSaveCallback(../api/java/org/springframework/data/relational/core/mapping/event/AfterSaveCallback.html) After an aggregate root gets saved (that is, inserted or updated). Load Load the aggregate using 1 or more SQL queries. Construct the aggregate from the resultset. AfterConvertCallback(../api/java/org/springframework/data/relational/core/mapping/event/AfterConvertCallback.html) We encourage the use of callbacks over events since they support the use of immutable classes and therefore are more powerful and versatile than events."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/auditing.html","Auditing: In order to activate auditing, add @EnableJdbcAuditing to your configuration, as the following example shows: Activating auditing with Java configuration @Configuration @EnableJdbcAuditing class Config { @Bean AuditorAware<AuditableUser> auditorProvider() { return new AuditorAwareImpl(); } } If you expose a bean of type AuditorAware to the ApplicationContext , the auditing infrastructure automatically picks it up and uses it to determine the current user to be set on domain types. If you have multiple implementations registered in the ApplicationContext , you can select the one to be used by explicitly setting the auditorAwareRef attribute of @EnableJdbcAuditing ."
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/transactions.html","Transactionality: The methods of CrudRepository instances are transactional by default. For reading operations, the transaction configuration readOnly flag is set to true . All others are configured with a plain @Transactional annotation so that default transaction configuration applies. For details, see the Javadoc of SimpleJdbcRepository(../api/java/org/springframework/data/jdbc/repository/support/SimpleJdbcRepository.html) . If you need to tweak transaction configuration for one of the methods declared in a repository, redeclare the method in your repository interface, as follows: Custom transaction configuration for CRUD interface UserRepository extends CrudRepository<User, Long> { @Override @Transactional(timeout = 10) List<User> findAll(); // Further query method declarations } The preceding causes the findAll() method to be run with a timeout of 10 seconds and without the readOnly flag. Another way to alter transactional behavior is by using a facade or service implementation that typically covers more than one repository. Its purpose is to define transactional boundaries for non-CRUD operations. The following example shows how to create such a facade: Using a facade to define transactions for multiple repository calls @Service public class UserManagementImpl implements UserManagement { private final UserRepository userRepository; private final RoleRepository roleRepository; UserManagementImpl(UserRepository userRepository, RoleRepository roleRepository) { this.userRepository = userRepository; this.roleRepository = roleRepository; } @Transactional public void addRoleToAllUsers(String roleName) { Role role = roleRepository.findByName(roleName); for (User user : userRepository.findAll()) { user.addRole(role); userRepository.save(user); } } The preceding example causes calls to addRoleToAllUsers(…) to run inside a transaction (participating in an existing one or creating a new one if none are already running). The transaction configuration for the repositories is neglected, as the outer transaction configuration determines the actual repository to be used. Note that you have to explicitly activate <tx:annotation-driven /> or use @EnableTransactionManagement to get annotation-based configuration for facades working. Note that the preceding example assumes you use component scanning. Transactional Query Methods: To let your query methods be transactional, use @Transactional at the repository interface you define, as the following example shows: Using @Transactional at query methods @Transactional(readOnly = true) interface UserRepository extends CrudRepository<User, Long> { List<User> findByLastname(String lastname); @Modifying @Transactional @Query(""delete from User u where u.active = false"") void deleteInactiveUsers(); } Typically, you want the readOnly flag to be set to true, because most of the query methods only read data. In contrast to that, deleteInactiveUsers() uses the @Modifying annotation and overrides the transaction configuration. Thus, the method is with the readOnly flag set to false . It is highly recommended to make query methods transactional. These methods might execute more than one query in order to populate an entity. Without a common transaction Spring Data JDBC executes the queries in different connections. This may put excessive strain on the connection pool and might even lead to dead locks when multiple methods request a fresh connection while holding on to one. It is definitely reasonable to mark read-only queries as such by setting the readOnly flag. This does not, however, act as a check that you do not trigger a manipulating query (although some databases reject INSERT and UPDATE statements inside a read-only transaction). Instead, the readOnly flag is propagated as a hint to the underlying JDBC driver for performance optimizations. JDBC Locking: Spring Data JDBC supports locking on derived query methods. To enable locking on a given derived query method inside a repository, you annotate it with @Lock . The required value of type LockMode offers two values: PESSIMISTIC_READ which guarantees that the data you are reading doesn’t get modified, and PESSIMISTIC_WRITE which obtains a lock to modify the data. Some databases do not make this distinction. In that cases both modes are equivalent of PESSIMISTIC_WRITE . Using @Lock on derived query method interface UserRepository extends CrudRepository<User, Long> { @Lock(LockMode.PESSIMISTIC_READ) List<User> findByLastname(String lastname); } As you can see above, the method findByLastname(String lastname) will be executed with a pessimistic read lock. If you are using a databse with the MySQL Dialect this will result for example in the following query: Resulting Sql query for MySQL dialect Select * from user u where u.lastname = lastname LOCK IN SHARE MODE"
"https://docs.spring.io/spring-data/relational/reference/3.3/jdbc/schema-support.html","Schema Creation: When working with SQL databases, the schema is an essential part. Spring Data JDBC supports a wide range of schema options yet when starting with a domain model it can be challenging to come up with an initial domain model. To assist you with a code-first approach, Spring Data JDBC ships with an integration to create database change sets using Liquibase(https://www.liquibase.org/) . Consider the following domain entity: @Table class Person { @Id long id; String firstName; String lastName; LocalDate birthday; boolean active; } Rendering the initial ChangeSet through the following code: RelationalMappingContext context = … // The context contains the Person entity, ideally initialized through initialEntitySet LiquibaseChangeSetWriter writer = new LiquibaseChangeSetWriter(context); writer.writeChangeSet(new FileSystemResource(new File(…))); yields the following change log: databaseChangeLog: - changeSet: id: '1685969572426' author: Spring Data Relational objectQuotingStrategy: LEGACY changes: - createTable: columns: - column: autoIncrement: true constraints: nullable: false primaryKey: true name: id type: BIGINT - column: constraints: nullable: true name: first_name type: VARCHAR(255 BYTE) - column: constraints: nullable: true name: last_name type: VARCHAR(255 BYTE) - column: constraints: nullable: true name: birthday type: DATE - column: constraints: nullable: false name: active type: TINYINT tableName: person Column types are computed from an object implementing the SqlTypeMapping strategy interface. Nullability is inferred from the type and set to false if a property type use primitive Java types. Schema support can assist you throughout the application development lifecycle. In differential mode, you provide an existing Liquibase Database to the schema writer instance and the schema writer compares existing tables to mapped entities and derives from the difference which tables and columns to create/to drop. By default, no tables and no columns are dropped unless you configure dropTableFilter and dropColumnFilter . Both filter predicate provide the table name respective column name so your code can computer which tables and columns can be dropped. writer.setDropTableFilter(tableName -> …); writer.setDropColumnFilter((tableName, columnName) -> …); Schema support can only identify additions and removals in the sense of removing tables/columns that are not mapped or adding columns that do not exist in the database. Columns cannot be renamed nor data cannot be migrated because entity mapping does not provide details of how the schema has evolved."
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc.html","R2DBC: The Spring Data R2DBC module applies core Spring concepts to the development of solutions that use R2DBC database drivers aligned with Domain-driven design principles(jdbc/domain-driven-design.html) . We provide a “template” as a high-level abstraction for storing and querying aggregates. This document is the reference guide for Spring Data R2DBC support. It explains the concepts and semantics and syntax. This chapter points out the specialties for repository support for JDBC. This builds on the core repository support explained in Working with Spring Data Repositories(repositories/introduction.html) . You should have a sound understanding of the basic concepts explained there. R2DBC contains a wide range of features: Spring configuration support with Java-based @Configuration(r2dbc/getting-started.html#r2dbc.connectionfactory) classes for an R2DBC driver instance. R2dbcEntityTemplate(r2dbc/entity-persistence.html) as central class for entity-bound operations that increases productivity when performing common R2DBC operations with integrated object mapping between rows and POJOs. Feature-rich object mapping(r2dbc/mapping.html) integrated with Spring’s Conversion Service. Annotation-based mapping metadata(r2dbc/mapping.html#mapping.usage.annotations) that is extensible to support other metadata formats. Automatic implementation of Repository interfaces(r2dbc/repositories.html) , including support for custom query methods(repositories/custom-implementations.html) . For most tasks, you should use R2dbcEntityTemplate or the repository support, which both use the rich mapping functionality. R2dbcEntityTemplate is the place to look for accessing functionality such as ad-hoc CRUD operations. Section Summary: Getting Started(r2dbc/getting-started.html) Persisting Entities(r2dbc/entity-persistence.html) Mapping(r2dbc/mapping.html) R2DBC Repositories(r2dbc/repositories.html) Query Methods(r2dbc/query-methods.html) EntityCallbacks(r2dbc/entity-callbacks.html) Auditing(r2dbc/auditing.html) Kotlin(r2dbc/kotlin.html) Migration Guide(r2dbc/migration-guide.html)"
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/getting-started.html","Getting Started: An easy way to bootstrap setting up a working environment is to create a Spring-based project in Spring Tools(https://spring.io/tools) or from Spring Initializr(https://start.spring.io) . First, you need to set up a running database server. Refer to your vendor documentation on how to configure your database for R2DBC access. Requirements: Spring Data R2DBC requires Spring Framework(https://docs.spring.io/spring-framework/reference/6.1) 6.1.13 and above. In terms of databases, Spring Data R2DBC requires a driver(#r2dbc.drivers) to abstract common SQL functionality over vendor-specific flavours. Spring Data R2DBC includes direct support for the following databases: H2(https://github.com/r2dbc/r2dbc-h2) ( io.r2dbc:r2dbc-h2 ) MariaDB(https://github.com/mariadb-corporation/mariadb-connector-r2dbc) ( org.mariadb:r2dbc-mariadb ) Microsoft SQL Server(https://github.com/r2dbc/r2dbc-mssql) ( io.r2dbc:r2dbc-mssql ) MySQL(https://github.com/asyncer-io/r2dbc-mysql) ( io.asyncer:r2dbc-mysql ) jasync-sql MySQL(https://github.com/jasync-sql/jasync-sql) ( com.github.jasync-sql:jasync-r2dbc-mysql ) Postgres(https://github.com/r2dbc/r2dbc-postgresql) ( io.r2dbc:r2dbc-postgresql ) Oracle(https://github.com/oracle/oracle-r2dbc) ( com.oracle.database.r2dbc:oracle-r2dbc ) If you use a different database then your application won’t start up. The dialect(#r2dbc.dialects) section contains further detail on how to proceed in such case. Hello World: To create a Spring project in STS: Go to File → New → Spring Template Project → Simple Spring Utility Project, and press Yes when prompted. Then enter a project and a package name, such as org.spring.r2dbc.example . Add the following to the pom.xml files dependencies element: Add the following to the pom.xml files dependencies element: <dependencies> <!-- other dependency elements omitted --> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-r2dbc</artifactId> <version>3.3.4</version> </dependency> <!-- a R2DBC driver --> <dependency> <groupId>io.r2dbc</groupId> <artifactId>r2dbc-h2</artifactId> <version>x.y.z</version> </dependency> </dependencies> Change the version of Spring in the pom.xml to be <spring.version>6.1.13</spring.version> Add the following location of the Spring Milestone repository for Maven to your pom.xml such that it is at the same level as your <dependencies/> element: <repositories> <repository> <id>spring-milestone</id> <name>Spring Maven MILESTONE Repository</name> <url>https://repo.spring.io/milestone</url> </repository> </repositories> The repository is also browseable here(https://repo.spring.io/milestone/org/springframework/data/) . You may also want to set the logging level to DEBUG to see some additional information. To do so, edit the application.properties file to have the following content: logging.level.org.springframework.r2dbc=DEBUG Then you can, for example, create a Person class to persist, as follows: public class Person { private final String id; private final String name; private final int age; public Person(String id, String name, int age) { this.id = id; this.name = name; this.age = age; } public String getId() { return id; } public String getName() { return name; } public int getAge() { return age; } @Override public String toString() { return ""Person [id="" + id + "", name="" + name + "", age="" + age + ""]""; } } Next, you need to create a table structure in your database, as follows: CREATE TABLE person (id VARCHAR(255) PRIMARY KEY, name VARCHAR(255), age INT); You also need a main application to run, as follows: import io.r2dbc.spi.ConnectionFactories; import io.r2dbc.spi.ConnectionFactory; import reactor.test.StepVerifier; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import org.springframework.data.r2dbc.core.R2dbcEntityTemplate; public class R2dbcApp { private static final Log log = LogFactory.getLog(R2dbcApp.class); public static void main(String[] args) { ConnectionFactory connectionFactory = ConnectionFactories.get(""r2dbc:h2:mem:///test?options=DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE""); R2dbcEntityTemplate template = new R2dbcEntityTemplate(connectionFactory); template.getDatabaseClient().sql(""CREATE TABLE person"" + ""(id VARCHAR(255) PRIMARY KEY,"" + ""name VARCHAR(255),"" + ""age INT)"") .fetch() .rowsUpdated() .as(StepVerifier::create) .expectNextCount(1) .verifyComplete(); template.insert(Person.class) .using(new Person(""joe"", ""Joe"", 34)) .as(StepVerifier::create) .expectNextCount(1) .verifyComplete(); template.select(Person.class) .first() .doOnNext(it -> log.info(it)) .as(StepVerifier::create) .expectNextCount(1) .verifyComplete(); } } When you run the main program, the preceding examples produce output similar to the following: 2018-11-28 10:47:03,893 DEBUG amework.core.r2dbc.DefaultDatabaseClient: 310 - Executing SQL statement [CREATE TABLE person (id VARCHAR(255) PRIMARY KEY, name VARCHAR(255), age INT)] 2018-11-28 10:47:04,074 DEBUG amework.core.r2dbc.DefaultDatabaseClient: 908 - Executing SQL statement [INSERT INTO person (id, name, age) VALUES($1, $2, $3)] 2018-11-28 10:47:04,092 DEBUG amework.core.r2dbc.DefaultDatabaseClient: 575 - Executing SQL statement [SELECT id, name, age FROM person] 2018-11-28 10:47:04,436 INFO org.spring.r2dbc.example.R2dbcApp: 43 - Person [id='joe', name='Joe', age=34] Even in this simple example, there are few things to notice: You can create an instance of the central helper class in Spring Data R2DBC ( R2dbcEntityTemplate ) by using a standard io.r2dbc.spi.ConnectionFactory object. The mapper works against standard POJO objects without the need for any additional metadata (though you can, optionally, provide that information — see here(mapping.html) .). Mapping conventions can use field access.Notice that the Person class has only getters. If the constructor argument names match the column names of the stored row, they are used to instantiate the object. Examples Repository: There is a GitHub repository with several examples(https://github.com/spring-projects/spring-data-examples) that you can download and play around with to get a feel for how the library works. Connecting to a Relational Database with Spring: One of the first tasks when using relational databases and Spring is to create a io.r2dbc.spi.ConnectionFactory object by using the IoC container. Make sure to use a supported database and driver(#requirements) . Registering a ConnectionFactory Instance using Java Configuration: The following example shows an example of using Java-based bean metadata to register an instance of io.r2dbc.spi.ConnectionFactory : Registering a io.r2dbc.spi.ConnectionFactory object using Java Configuration @Configuration public class ApplicationConfiguration extends AbstractR2dbcConfiguration { @Override @Bean public ConnectionFactory connectionFactory() { return … } } This approach lets you use the standard io.r2dbc.spi.ConnectionFactory instance, with the container using Spring’s AbstractR2dbcConfiguration .As compared to registering a ConnectionFactory instance directly, the configuration support has the added advantage of also providing the container with an ExceptionTranslator implementation that translates R2DBC exceptions to exceptions in Spring’s portable DataAccessException hierarchy for data access classes annotated with the @Repository annotation.This hierarchy and the use of @Repository is described in Spring’s DAO support features(https://docs.spring.io/spring-framework/reference/6.1/data-access.html) . AbstractR2dbcConfiguration also registers DatabaseClient , which is required for database interaction and for Repository implementation. Dialects: Spring Data R2DBC uses a Dialect to encapsulate behavior that is specific to a database or its driver. Spring Data R2DBC reacts to database specifics by inspecting the ConnectionFactory and selects the appropriate database dialect accordingly. If you use a database for which no dialect is available, then your application won’t start up. In that case, you’ll have to ask your vendor to provide a Dialect implementation. Alternatively, you can implement your own Dialect . Dialects are resolved by DialectResolver(https://docs.spring.io/spring-data/r2dbc/docs/3.3.4/api//org/springframework/data/r2dbc/dialect/DialectResolver.html) from a ConnectionFactory , typically by inspecting ConnectionFactoryMetadata . + You can let Spring auto-discover your R2dbcDialect by registering a class that implements org.springframework.data.r2dbc.dialect.DialectResolver$R2dbcDialectProvider through META-INF/spring.factories . DialectResolver discovers dialect provider implementations from the class path using Spring’s SpringFactoriesLoader . To do so: Implement your own Dialect . Implement a R2dbcDialectProvider returning the Dialect . Register the provider by creating a spring.factories resource under META-INF and perform the registration by adding a line org.springframework.data.r2dbc.dialect.DialectResolver$R2dbcDialectProvider=<fully qualified name of your R2dbcDialectProvider>"
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/entity-persistence.html","Persisting Entities: R2dbcEntityTemplate is the central entrypoint for Spring Data R2DBC. It provides direct entity-oriented methods and a more narrow, fluent interface for typical ad-hoc use-cases, such as querying, inserting, updating, and deleting data. The entry points ( insert() , select() , update() , and others) follow a natural naming schema based on the operation to be run. Moving on from the entry point, the API is designed to offer only context-dependent methods that lead to a terminating method that creates and runs a SQL statement. Spring Data R2DBC uses a R2dbcDialect abstraction to determine bind markers, pagination support and the data types natively supported by the underlying driver. All terminal methods return always a Publisher type that represents the desired operation. The actual statements are sent to the database upon subscription. Methods for Inserting and Updating Entities: There are several convenient methods on R2dbcEntityTemplate for saving and inserting your objects. To have more fine-grained control over the conversion process, you can register Spring converters with R2dbcCustomConversions — for example Converter<Person, OutboundRow> and Converter<Row, Person> . The simple case of using the save operation is to save a POJO. In this case, the table name is determined by name (not fully qualified) of the class. You may also call the save operation with a specific collection name. You can use mapping metadata to override the collection in which to store the object. When inserting or saving, if the Id property is not set, the assumption is that its value will be auto-generated by the database. Consequently, for auto-generation the type of the Id property or field in your class must be a Long , or Integer . The following example shows how to insert a row and retrieving its contents: Inserting and retrieving entities using the R2dbcEntityTemplate Person person = new Person(""John"", ""Doe""); Mono<Person> saved = template.insert(person); Mono<Person> loaded = template.selectOne(query(where(""firstname"").is(""John"")), Person.class); The following insert and update operations are available: A similar set of insert operations is also available: Mono<T> insert (T objectToSave) : Insert the object to the default table. Mono<T> update (T objectToSave) : Insert the object to the default table. Table names can be customized by using the fluent API. Selecting Data: The select(…) and selectOne(…) methods on R2dbcEntityTemplate are used to select data from a table. Both methods take a Query(#r2dbc.datbaseclient.fluent-api.criteria) object that defines the field projection, the WHERE clause, the ORDER BY clause and limit/offset pagination. Limit/offset functionality is transparent to the application regardless of the underlying database. This functionality is supported by the R2dbcDialect abstraction(getting-started.html#r2dbc.dialects) to cater for differences between the individual SQL flavors. Selecting entities using the R2dbcEntityTemplate Flux<Person> loaded = template.select(query(where(""firstname"").is(""John"")), Person.class); Fluent API: This section explains the fluent API usage. Consider the following simple query: Flux<Person> people = template.select(Person.class) (1) .all(); (2) 1 Using Person with the select(…) method maps tabular results on Person result objects. 2 Fetching all() rows returns a Flux<Person> without limiting results. The following example declares a more complex query that specifies the table name by name, a WHERE condition, and an ORDER BY clause: Mono<Person> first = template.select(Person.class) (1) .from(""other_person"") .matching(query(where(""firstname"").is(""John"") (2) .and(""lastname"").in(""Doe"", ""White"")) .sort(by(desc(""id"")))) (3) .one(); (4) 1 Selecting from a table by name returns row results using the given domain type. 2 The issued query declares a WHERE condition on firstname and lastname columns to filter results. 3 Results can be ordered by individual column names, resulting in an ORDER BY clause. 4 Selecting the one result fetches only a single row. This way of consuming rows expects the query to return exactly a single result. Mono emits a IncorrectResultSizeDataAccessException if the query yields more than a single result. You can directly apply Projections(../repositories/projections.html) to results by providing the target type via select(Class<?>) . You can switch between retrieving a single entity and retrieving multiple entities through the following terminating methods: first() : Consume only the first row, returning a Mono . The returned Mono completes without emitting an object if the query returns no results. one() : Consume exactly one row, returning a Mono . The returned Mono completes without emitting an object if the query returns no results. If the query returns more than one row, Mono completes exceptionally emitting IncorrectResultSizeDataAccessException . all() : Consume all returned rows returning a Flux . count() : Apply a count projection returning Mono<Long> . exists() : Return whether the query yields any rows by returning Mono<Boolean> . You can use the select() entry point to express your SELECT queries. The resulting SELECT queries support the commonly used clauses ( WHERE and ORDER BY ) and support pagination. The fluent API style let you chain together multiple methods while having easy-to-understand code. To improve readability, you can use static imports that let you avoid using the 'new' keyword for creating Criteria instances. Methods for the Criteria Class: The Criteria class provides the following methods, all of which correspond to SQL operators: Criteria and (String column) : Adds a chained Criteria with the specified property to the current Criteria and returns the newly created one. Criteria or (String column) : Adds a chained Criteria with the specified property to the current Criteria and returns the newly created one. Criteria greaterThan (Object o) : Creates a criterion by using the > operator. Criteria greaterThanOrEquals (Object o) : Creates a criterion by using the >= operator. Criteria in (Object…​ o) : Creates a criterion by using the IN operator for a varargs argument. Criteria in (Collection<?> collection) : Creates a criterion by using the IN operator using a collection. Criteria is (Object o) : Creates a criterion by using column matching ( property = value ). Criteria isNull () : Creates a criterion by using the IS NULL operator. Criteria isNotNull () : Creates a criterion by using the IS NOT NULL operator. Criteria lessThan (Object o) : Creates a criterion by using the < operator. Criteria lessThanOrEquals (Object o) : Creates a criterion by using the ⇐ operator. Criteria like (Object o) : Creates a criterion by using the LIKE operator without escape character processing. Criteria not (Object o) : Creates a criterion by using the != operator. Criteria notIn (Object…​ o) : Creates a criterion by using the NOT IN operator for a varargs argument. Criteria notIn (Collection<?> collection) : Creates a criterion by using the NOT IN operator using a collection. You can use Criteria with SELECT , UPDATE , and DELETE queries. Inserting Data: You can use the insert() entry point to insert data. Consider the following simple typed insert operation: Mono<Person> insert = template.insert(Person.class) (1) .using(new Person(""John"", ""Doe"")); (2) 1 Using Person with the into(…) method sets the INTO table, based on mapping metadata. It also prepares the insert statement to accept Person objects for inserting. 2 Provide a scalar Person object. Alternatively, you can supply a Publisher to run a stream of INSERT statements. This method extracts all non- null values and inserts them. Updating Data: You can use the update() entry point to update rows. Updating data starts by specifying the table to update by accepting Update specifying assignments. It also accepts Query to create a WHERE clause. Consider the following simple typed update operation: Person modified = … Mono<Long> update = template.update(Person.class) (1) .inTable(""other_table"") (2) .matching(query(where(""firstname"").is(""John""))) (3) .apply(update(""age"", 42)); (4) 1 Update Person objects and apply mapping based on mapping metadata. 2 Set a different table name by calling the inTable(…) method. 3 Specify a query that translates into a WHERE clause. 4 Apply the Update object. Set in this case age to 42 and return the number of affected rows. Deleting Data: You can use the delete() entry point to delete rows. Removing data starts with a specification of the table to delete from and, optionally, accepts a Criteria to create a WHERE clause. Consider the following simple insert operation: Mono<Long> delete = template.delete(Person.class) (1) .from(""other_table"") (2) .matching(query(where(""firstname"").is(""John""))) (3) .all(); (4) 1 Delete Person objects and apply mapping based on mapping metadata. 2 Set a different table name by calling the from(…) method. 3 Specify a query that translates into a WHERE clause. 4 Apply the delete operation and return the number of affected rows. Using Repositories, saving an entity can be performed with the ReactiveCrudRepository.save(…) method. If the entity is new, this results in an insert for the entity. If the entity is not new, it gets updated. Note that whether an instance is new is part of the instance’s state. This approach has some obvious downsides. If only few of the referenced entities have been actually changed, the deletion and insertion is wasteful. While this process could and probably will be improved, there are certain limitations to what Spring Data R2DBC can offer. It does not know the previous state of an aggregate. So any update process always has to take whatever it finds in the database and make sure it converts it to whatever is the state of the entity passed to the save method. ID Generation: Spring Data uses the identifer property to identify entities. The ID of an entity must be annotated with Spring Data’s @Id(https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/annotation/Id.html) annotation. When your database has an auto-increment column for the ID column, the generated value gets set in the entity after inserting it into the database. Spring Data does not attempt to insert values of identifier columns when the entity is new and the identifier value defaults to its initial value. That is 0 for primitive types and null if the identifier property uses a numeric wrapper type such as Long . Entity State Detection(../repositories/core-concepts.html#is-new-state-detection) explains in detail the strategies to detect whether an entity is new or whether it is expected to exist in your database. One important constraint is that, after saving an entity, the entity must not be new anymore. Note that whether an entity is new is part of the entity’s state. With auto-increment columns, this happens automatically, because the ID gets set by Spring Data with the value from the ID column. Optimistic Locking: Spring Data supports optimistic locking by means of a numeric attribute that is annotated with @Version(https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/annotation/Version.html) on the aggregate root. Whenever Spring Data saves an aggregate with such a version attribute two things happen: The update statement for the aggregate root will contain a where clause checking that the version stored in the database is actually unchanged. If this isn’t the case an OptimisticLockingFailureException will be thrown. Also, the version attribute gets increased both in the entity and in the database so a concurrent action will notice the change and throw an OptimisticLockingFailureException if applicable as described above. This process also applies to inserting new aggregates, where a null or 0 version indicates a new instance and the increased instance afterwards marks the instance as not new anymore, making this work rather nicely with cases where the id is generated during object construction for example when UUIDs are used. During deletes the version check also applies but no version is increased. @Table class Person { @Id Long id; String firstname; String lastname; @Version Long version; } R2dbcEntityTemplate template = …; Mono<Person> daenerys = template.insert(new Person(""Daenerys"")); (1) Person other = template.select(Person.class) .matching(query(where(""id"").is(daenerys.getId()))) .first().block(); (2) daenerys.setLastname(""Targaryen""); template.update(daenerys); (3) template.update(other).subscribe(); // emits OptimisticLockingFailureException (4) 1 Initially insert row. version is set to 0 . 2 Load the just inserted row. version is still 0 . 3 Update the row with version = 0 .Set the lastname and bump version to 1 . 4 Try to update the previously loaded row that still has version = 0 .The operation fails with an OptimisticLockingFailureException , as the current version is 1 ."
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/mapping.html","Mapping: Rich mapping support is provided by the MappingR2dbcConverter . MappingR2dbcConverter has a rich metadata model that allows mapping domain objects to a data row. The mapping metadata model is populated by using annotations on your domain objects. However, the infrastructure is not limited to using annotations as the only source of metadata information. The MappingR2dbcConverter also lets you map objects to rows without providing any additional metadata, by following a set of conventions. This section describes the features of the MappingR2dbcConverter , including how to use conventions for mapping objects to rows and how to override those conventions with annotation-based mapping metadata. Read on the basics about Object Mapping Fundamentals(../object-mapping.html) before continuing with this chapter. Convention-based Mapping: MappingR2dbcConverter has a few conventions for mapping objects to rows when no additional mapping metadata is provided. The conventions are: The short Java class name is mapped to the table name in the following manner. The com.bigbank.SavingsAccount class maps to the SAVINGS_ACCOUNT table name. The same name mapping is applied for mapping fields to column names. For example, the firstName field maps to the FIRST_NAME column. You can control this mapping by providing a custom NamingStrategy . See Mapping Configuration(#mapping.configuration) for more detail. Table and column names that are derived from property or class names are used in SQL statements without quotes by default. You can control this behavior by setting RelationalMappingContext.setForceQuote(true) . Nested objects are not supported. The converter uses any Spring Converters registered with CustomConversions to override the default mapping of object properties to row columns and values. The fields of an object are used to convert to and from columns in the row. Public JavaBean properties are not used. If you have a single non-zero-argument constructor whose constructor argument names match top-level column names of the row, that constructor is used. Otherwise, the zero-argument constructor is used. If there is more than one non-zero-argument constructor, an exception is thrown. Refer to Object Creation(../object-mapping.html#mapping.object-creation) for further details. Mapping Configuration: By default, (unless explicitly configured) an instance of MappingR2dbcConverter is created when you create a DatabaseClient . You can create your own instance of the MappingR2dbcConverter . By creating your own instance, you can register Spring converters to map specific classes to and from the database. You can configure the MappingR2dbcConverter as well as DatabaseClient and ConnectionFactory by using Java-based metadata. The following example uses Spring’s Java-based configuration: If you set setForceQuote of the R2dbcMappingContext to true, table and column names derived from classes and properties are used with database specific quotes. This means that it is OK to use reserved SQL words (such as order) in these names. You can do so by overriding r2dbcMappingContext(Optional<NamingStrategy>) of AbstractR2dbcConfiguration . Spring Data converts the letter casing of such a name to that form which is also used by the configured database when no quoting is used. Therefore, you can use unquoted names when creating tables, as long as you do not use keywords or special characters in your names. For databases that adhere to the SQL standard, this means that names are converted to upper case. The quoting character and the way names get capitalized is controlled by the used Dialect . See R2DBC Drivers(getting-started.html#r2dbc.dialects) for how to configure custom dialects. @Configuration class to configure R2DBC mapping support @Configuration public class MyAppConfig extends AbstractR2dbcConfiguration { public ConnectionFactory connectionFactory() { return ConnectionFactories.get(""r2dbc:…""); } // the following are optional @Override protected List<Object> getCustomConverters() { return List.of(new PersonReadConverter(), new PersonWriteConverter()); } } AbstractR2dbcConfiguration requires you to implement a method that defines a ConnectionFactory . You can add additional converters to the converter by overriding the r2dbcCustomConversions method. You can configure a custom NamingStrategy by registering it as a bean. The NamingStrategy controls how the names of classes and properties get converted to the names of tables and columns. AbstractR2dbcConfiguration creates a DatabaseClient instance and registers it with the container under the name of databaseClient . Metadata-based Mapping: To take full advantage of the object mapping functionality inside the Spring Data R2DBC support, you should annotate your mapped objects with the @Table annotation. Although it is not necessary for the mapping framework to have this annotation (your POJOs are mapped correctly, even without any annotations), it lets the classpath scanner find and pre-process your domain objects to extract the necessary metadata. If you do not use this annotation, your application takes a slight performance hit the first time you store a domain object, because the mapping framework needs to build up its internal metadata model so that it knows about the properties of your domain object and how to persist them. The following example shows a domain object: Example domain object package com.mycompany.domain; @Table public class Person { @Id private Long id; private Integer ssn; private String firstName; private String lastName; } The @Id annotation tells the mapper which property you want to use as the primary key. Default Type Mapping: The following table explains how property types of an entity affect mapping: Source Type Target Type Remarks Primitive types and wrapper types Passthru Can be customized using Explicit Converters(#mapping.explicit.converters) . JSR-310 Date/Time types Passthru Can be customized using Explicit Converters(#mapping.explicit.converters) . String , BigInteger , BigDecimal , and UUID Passthru Can be customized using Explicit Converters(#mapping.explicit.converters) . Enum String Can be customized by registering Explicit Converters(#mapping.explicit.converters) . Blob and Clob Passthru Can be customized using Explicit Converters(#mapping.explicit.converters) . byte[] , ByteBuffer Passthru Considered a binary payload. Collection<T> Array of T Conversion to Array type if supported by the configured driver(getting-started.html#requirements) , not supported otherwise. Arrays of primitive types, wrapper types and String Array of wrapper type (e.g. int[] → Integer[] ) Conversion to Array type if supported by the configured driver(getting-started.html#requirements) , not supported otherwise. Driver-specific types Passthru Contributed as a simple type by the used R2dbcDialect . Complex objects Target type depends on registered Converter . Requires a Explicit Converters(#mapping.explicit.converters) , not supported otherwise. The native data type for a column depends on the R2DBC driver type mapping. Drivers can contribute additional simple types such as Geometry types. Mapping Annotation Overview: The RelationalConverter can use metadata to drive the mapping of objects to rows. The following annotations are available: @Id : Applied at the field level to mark the primary key. @Table : Applied at the class level to indicate this class is a candidate for mapping to the database. You can specify the name of the table where the database is stored. @Transient : By default, all fields are mapped to the row. This annotation excludes the field where it is applied from being stored in the database. Transient properties cannot be used within a persistence constructor as the converter cannot materialize a value for the constructor argument. @PersistenceCreator : Marks a given constructor or static factory method — even a package protected one — to use when instantiating the object from the database. Constructor arguments are mapped by name to the values in the retrieved row. @Value : This annotation is part of the Spring Framework. Within the mapping framework it can be applied to constructor arguments. This lets you use a Spring Expression Language statement to transform a key’s value retrieved in the database before it is used to construct a domain object. In order to reference a column of a given row one has to use expressions like: @Value(""#root.myProperty"") where root refers to the root of the given Row . @Column : Applied at the field level to describe the name of the column as it is represented in the row, letting the name be different from the field name of the class. Names specified with a @Column annotation are always quoted when used in SQL statements. For most databases, this means that these names are case-sensitive. It also means that you can use special characters in these names. However, this is not recommended, since it may cause problems with other tools. @Version : Applied at field level is used for optimistic locking and checked for modification on save operations. The value is null ( zero for primitive types) is considered as marker for entities to be new. The initially stored value is zero ( one for primitive types). The version gets incremented automatically on every update. See Optimistic Locking(entity-persistence.html#r2dbc.entity-persistence.optimistic-locking) for further reference. The mapping metadata infrastructure is defined in the separate spring-data-commons project that is technology-agnostic. Specific subclasses are used in the R2DBC support to support annotation based metadata. Other strategies can also be put in place (if there is demand). Naming Strategy: By convention, Spring Data applies a NamingStrategy to determine table, column, and schema names defaulting to snake case(https://en.wikipedia.org/wiki/Snake_case) . An object property named firstName becomes first_name . You can tweak that by providing a NamingStrategy(../api/java/org/springframework/data/relational/core/mapping/NamingStrategy.html) in your application context. Override table names: When the table naming strategy does not match your database table names, you can override the table name with the Table(../api/java/org/springframework/data/relational/core/mapping/Table.html) annotation. The element value of this annotation provides the custom table name. The following example maps the MyEntity class to the CUSTOM_TABLE_NAME table in the database: @Table(""CUSTOM_TABLE_NAME"") class MyEntity { @Id Integer id; String name; } You may use Spring Data’s SpEL support(../value-expressions.html) to dynamically create the table name. Once generated the table name will be cached, so it is dynamic per mapping context only. Override column names: When the column naming strategy does not match your database table names, you can override the table name with the Column(../api/java/org/springframework/data/relational/core/mapping/Column.html) annotation. The element value of this annotation provides the custom column name. The following example maps the name property of the MyEntity class to the CUSTOM_COLUMN_NAME column in the database: class MyEntity { @Id Integer id; @Column(""CUSTOM_COLUMN_NAME"") String name; } You may use Spring Data’s SpEL support(../value-expressions.html) to dynamically create column names. Once generated the names will be cached, so it is dynamic per mapping context only. Read Only Properties: Attributes annotated with @ReadOnlyProperty will not be written to the database by Spring Data, but they will be read when an entity gets loaded. Spring Data will not automatically reload an entity after writing it. Therefore, you have to reload it explicitly if you want to see data that was generated in the database for such columns. If the annotated attribute is an entity or collection of entities, it is represented by one or more separate rows in separate tables. Spring Data will not perform any insert, delete or update for these rows. Insert Only Properties: Attributes annotated with @InsertOnlyProperty will only be written to the database by Spring Data during insert operations. For updates these properties will be ignored. @InsertOnlyProperty is only supported for the aggregate root. Customized Object Construction: The mapping subsystem allows the customization of the object construction by annotating a constructor with the @PersistenceConstructor annotation.The values to be used for the constructor parameters are resolved in the following way: If a parameter is annotated with the @Value annotation, the given expression is evaluated, and the result is used as the parameter value. If the Java type has a property whose name matches the given field of the input row, then its property information is used to select the appropriate constructor parameter to which to pass the input field value. This works only if the parameter name information is present in the Java .class files, which you can achieve by compiling the source with debug information or using the -parameters command-line switch for javac in Java 8. Otherwise, a MappingException is thrown to indicate that the given constructor parameter could not be bound. class OrderItem { private @Id final String id; private final int quantity; private final double unitPrice; OrderItem(String id, int quantity, double unitPrice) { this.id = id; this.quantity = quantity; this.unitPrice = unitPrice; } // getters/setters omitted } Overriding Mapping with Explicit Converters: When storing and querying your objects, it is often convenient to have a R2dbcConverter instance to handle the mapping of all Java types to OutboundRow instances. However, you may sometimes want the R2dbcConverter instances to do most of the work but let you selectively handle the conversion for a particular type — perhaps to optimize performance. To selectively handle the conversion yourself, register one or more one or more org.springframework.core.convert.converter.Converter instances with the R2dbcConverter . You can use the r2dbcCustomConversions method in AbstractR2dbcConfiguration to configure converters. The examples at the beginning of this chapter(#mapping.configuration) show how to perform the configuration with Java. Custom top-level entity conversion requires asymmetric types for conversion. Inbound data is extracted from R2DBC’s Row . Outbound data (to be used with INSERT / UPDATE statements) is represented as OutboundRow and later assembled to a statement. The following example of a Spring Converter implementation converts from a Row to a Person POJO: @ReadingConverter public class PersonReadConverter implements Converter<Row, Person> { public Person convert(Row source) { Person p = new Person(source.get(""id"", String.class),source.get(""name"", String.class)); p.setAge(source.get(""age"", Integer.class)); return p; } } Please note that converters get applied on singular properties. Collection properties (e.g. Collection<Person> ) are iterated and converted element-wise. Collection converters (e.g. Converter<List<Person>>, OutboundRow ) are not supported. R2DBC uses boxed primitives ( Integer.class instead of int.class ) to return primitive values. The following example converts from a Person to a OutboundRow : @WritingConverter public class PersonWriteConverter implements Converter<Person, OutboundRow> { public OutboundRow convert(Person source) { OutboundRow row = new OutboundRow(); row.put(""id"", Parameter.from(source.getId())); row.put(""name"", Parameter.from(source.getFirstName())); row.put(""age"", Parameter.from(source.getAge())); return row; } } Overriding Enum Mapping with Explicit Converters: Some databases, such as Postgres(https://github.com/pgjdbc/r2dbc-postgresql#postgres-enum-types) , can natively write enum values using their database-specific enumerated column type. Spring Data converts Enum values by default to String values for maximum portability. To retain the actual enum value, register a @Writing converter whose source and target types use the actual enum type to avoid using Enum.name() conversion. Additionally, you need to configure the enum type on the driver level so that the driver is aware how to represent the enum type. The following example shows the involved components to read and write Color enum values natively: enum Color { Grey, Blue } class ColorConverter extends EnumWriteSupport<Color> { } class Product { @Id long id; Color color; // … }"
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/repositories.html","R2DBC Repositories: This chapter points out the specialties for repository support for R2DBC. This builds on the core repository support explained in Working with Spring Data Repositories(../repositories/introduction.html) . Before reading this chapter, you should have a sound understanding of the basic concepts explained there. Usage: To access domain entities stored in a relational database, you can use our sophisticated repository support that eases implementation quite significantly. To do so, create an interface for your repository. Consider the following Person class: Sample Person entity public class Person { @Id private Long id; private String firstname; private String lastname; // … getters and setters omitted } The following example shows a repository interface for the preceding Person class: Basic repository interface to persist Person entities public interface PersonRepository extends ReactiveCrudRepository<Person, Long> { // additional custom query methods go here } To configure R2DBC repositories, you can use the @EnableR2dbcRepositories annotation. If no base package is configured, the infrastructure scans the package of the annotated configuration class. The following example shows how to use Java configuration for a repository: Java configuration for repositories @Configuration @EnableR2dbcRepositories class ApplicationConfig extends AbstractR2dbcConfiguration { @Override public ConnectionFactory connectionFactory() { return … } } Because our domain repository extends ReactiveCrudRepository , it provides you with reactive CRUD operations to access the entities. On top of ReactiveCrudRepository , there is also ReactiveSortingRepository , which adds additional sorting functionality similar to that of PagingAndSortingRepository . Working with the repository instance is merely a matter of dependency injecting it into a client. Consequently, you can retrieve all Person objects with the following code: Paging access to Person entities @ExtendWith(SpringExtension.class) @ContextConfiguration class PersonRepositoryTests { @Autowired PersonRepository repository; @Test void readsAllEntitiesCorrectly() { repository.findAll() .as(StepVerifier::create) .expectNextCount(1) .verifyComplete(); } @Test void readsEntitiesByNameCorrectly() { repository.findByFirstname(""Hello World"") .as(StepVerifier::create) .expectNextCount(1) .verifyComplete(); } } The preceding example creates an application context with Spring’s unit test support, which performs annotation-based dependency injection into test cases. Inside the test method, we use the repository to query the database. We use StepVerifier as a test aid to verify our expectations against the results. Result Mapping: A query method returning an Interface- or DTO projection is backed by results produced by the actual query. Interface projections generally rely on mapping results onto the domain type first to consider potential @Column type mappings and the actual projection proxy uses a potentially partially materialized entity to expose projection data. Result mapping for DTO projections depends on the actual query type. Derived queries use the domain type to map results, and Spring Data creates DTO instances solely from properties available on the domain type. Declaring properties in your DTO that are not available on the domain type is not supported. String-based queries use a different approach since the actual query, specifically the field projection, and result type declaration are close together. DTO projections used with query methods annotated with @Query map query results directly into the DTO type. Field mappings on the domain type are not considered. Using the DTO type directly, your query method can benefit from a more dynamic projection that isn’t restricted to the domain model. Working with multiple Databases: When working with multiple, potentially different databases, your application will require a different approach to configuration. The provided AbstractR2dbcConfiguration support class assumes a single ConnectionFactory from which the Dialect gets derived. That being said, you need to define a few beans yourself to configure Spring Data R2DBC to work with multiple databases. R2DBC repositories require R2dbcEntityOperations to implement repositories. A simple configuration to scan for repositories without using AbstractR2dbcConfiguration looks like: @Configuration @EnableR2dbcRepositories(basePackages = ""com.acme.mysql"", entityOperationsRef = ""mysqlR2dbcEntityOperations"") static class MySQLConfiguration { @Bean @Qualifier(""mysql"") public ConnectionFactory mysqlConnectionFactory() { return … } @Bean public R2dbcEntityOperations mysqlR2dbcEntityOperations(@Qualifier(""mysql"") ConnectionFactory connectionFactory) { DatabaseClient databaseClient = DatabaseClient.create(connectionFactory); return new R2dbcEntityTemplate(databaseClient, MySqlDialect.INSTANCE); } } Note that @EnableR2dbcRepositories allows configuration either through databaseClientRef or entityOperationsRef . Using various DatabaseClient beans is useful when connecting to multiple databases of the same type. When using different database systems that differ in their dialect, use @EnableR2dbcRepositories (entityOperationsRef = …)` instead."
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/query-methods.html","Query Methods: Most of the data access operations you usually trigger on a repository result in a query being run against the databases. Defining such a query is a matter of declaring a method on the repository interface, as the following example shows: Example 1. PersonRepository with query methods interface ReactivePersonRepository extends ReactiveSortingRepository<Person, Long> { Flux<Person> findByFirstname(String firstname); (1) Flux<Person> findByFirstname(Publisher<String> firstname); (2) Flux<Person> findByFirstnameOrderByLastname(String firstname, Pageable pageable); (3) Mono<Person> findByFirstnameAndLastname(String firstname, String lastname); (4) Mono<Person> findFirstByLastname(String lastname); (5) @Query(""SELECT * FROM person WHERE lastname = :lastname"") Flux<Person> findByLastname(String lastname); (6) @Query(""SELECT firstname, lastname FROM person WHERE lastname = $1"") Mono<Person> findFirstByLastname(String lastname); (7) } 1 The method shows a query for all people with the given firstname . The query is derived by parsing the method name for constraints that can be concatenated with And and Or . Thus, the method name results in a query expression of SELECT … FROM person WHERE firstname = :firstname . 2 The method shows a query for all people with the given firstname once the firstname is emitted by the given Publisher . 3 Use Pageable to pass offset and sorting parameters to the database. 4 Find a single entity for the given criteria. It completes with IncorrectResultSizeDataAccessException on non-unique results. 5 Unless <4>, the first entity is always emitted even if the query yields more result rows. 6 The findByLastname method shows a query for all people with the given last name. 7 A query for a single Person entity projecting only firstname and lastname columns. The annotated query uses native bind markers, which are Postgres bind markers in this example. Note that the columns of a select statement used in a @Query annotation must match the names generated by the NamingStrategy for the respective property. If a select statement does not include a matching column, that property is not set. If that property is required by the persistence constructor, either null or (for primitive types) the default value is provided. The following table shows the keywords that are supported for query methods: Table 1. Supported keywords for query methods Keyword Sample Logical result After findByBirthdateAfter(Date date) birthdate > date GreaterThan findByAgeGreaterThan(int age) age > age GreaterThanEqual findByAgeGreaterThanEqual(int age) age >= age Before findByBirthdateBefore(Date date) birthdate < date LessThan findByAgeLessThan(int age) age < age LessThanEqual findByAgeLessThanEqual(int age) age <= age Between findByAgeBetween(int from, int to) age BETWEEN from AND to NotBetween findByAgeNotBetween(int from, int to) age NOT BETWEEN from AND to In findByAgeIn(Collection<Integer> ages) age IN (age1, age2, ageN) NotIn findByAgeNotIn(Collection ages) age NOT IN (age1, age2, ageN) IsNotNull , NotNull findByFirstnameNotNull() firstname IS NOT NULL IsNull , Null findByFirstnameNull() firstname IS NULL Like , StartingWith , EndingWith findByFirstnameLike(String name) firstname LIKE name NotLike , IsNotLike findByFirstnameNotLike(String name) firstname NOT LIKE name Containing on String findByFirstnameContaining(String name) firstname LIKE '%' + name +'%' NotContaining on String findByFirstnameNotContaining(String name) firstname NOT LIKE '%' + name +'%' (No keyword) findByFirstname(String name) firstname = name Not findByFirstnameNot(String name) firstname != name IsTrue , True findByActiveIsTrue() active IS TRUE IsFalse , False findByActiveIsFalse() active IS FALSE Modifying Queries: The previous sections describe how to declare queries to access a given entity or collection of entities. Using keywords from the preceding table can be used in conjunction with delete…By or remove…By to create derived queries that delete matching rows. Example 2. Delete…By Query interface ReactivePersonRepository extends ReactiveSortingRepository<Person, String> { Mono<Integer> deleteByLastname(String lastname); (1) Mono<Void> deletePersonByLastname(String lastname); (2) Mono<Boolean> deletePersonByLastname(String lastname); (3) } 1 Using a return type of Mono<Integer> returns the number of affected rows. 2 Using Void just reports whether the rows were successfully deleted without emitting a result value. 3 Using Boolean reports whether at least one row was removed. As this approach is feasible for comprehensive custom functionality, you can modify queries that only need parameter binding by annotating the query method with @Modifying , as shown in the following example: @Modifying @Query(""UPDATE person SET firstname = :firstname where lastname = :lastname"") Mono<Integer> setFixedFirstnameFor(String firstname, String lastname); The result of a modifying query can be: Void (or Kotlin Unit ) to discard update count and await completion. Integer or another numeric type emitting the affected rows count. Boolean to emit whether at least one row was updated. The @Modifying annotation is only relevant in combination with the @Query annotation. Derived custom methods do not require this annotation. Modifying queries are executed directly against the database. No events or callbacks get called. Therefore also fields with auditing annotations do not get updated if they don’t get updated in the annotated query. Alternatively, you can add custom modifying behavior by using the facilities described in Custom Implementations for Spring Data Repositories(../repositories/custom-implementations.html) . Using @Query: The following example shows how to use @Query to declare a query method: Declare a query method by using @Query interface UserRepository extends ReactiveCrudRepository<User, Long> { @Query(""select firstName, lastName from User u where u.emailAddress = :email"") Flux<User> findByEmailAddress(@Param(""email"") String email); } Note that String-based queries do not support pagination nor accept Sort , PageRequest , and Limit as a query parameter as for these queries the query would be required to be rewritten. If you want to apply limiting, please express this intent using SQL and bind the appropriate parameters to the query yourself. Spring fully supports Java 8’s parameter name discovery based on the -parameters compiler flag. By using this flag in your build as an alternative to debug information, you can omit the @Param annotation for named parameters. Queries with SpEL Expressions: Query string definitions can be used together with SpEL expressions to create dynamic queries at runtime. SpEL expressions can provide predicate values which are evaluated right before running the query. Expressions expose method arguments through an array that contains all the arguments. The following query uses [0] to declare the predicate value for lastname (which is equivalent to the :lastname parameter binding): @Query(""SELECT * FROM person WHERE lastname = :#{[0]}"") Flux<Person> findByQueryWithExpression(String lastname); SpEL in query strings can be a powerful way to enhance queries. However, they can also accept a broad range of unwanted arguments. You should make sure to sanitize strings before passing them to the query to avoid unwanted changes to your query. Expression support is extensible through the Query SPI: org.springframework.data.spel.spi.EvaluationContextExtension . The Query SPI can contribute properties and functions and can customize the root object. Extensions are retrieved from the application context at the time of SpEL evaluation when the query is built. When using SpEL expressions in combination with plain parameters, use named parameter notation instead of native bind markers to ensure a proper binding order."
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/entity-callbacks.html","EntityCallbacks: Spring Data R2DBC uses the EntityCallback API(../commons/entity-callbacks.html) for its auditing support and reacts on the following callbacks. Table 1. Supported Entity Callbacks Callback Method Description Order BeforeConvertCallback onBeforeConvert(T entity, SqlIdentifier table) Invoked before a domain object is converted to OutboundRow . Ordered.LOWEST_PRECEDENCE AfterConvertCallback onAfterConvert(T entity, SqlIdentifier table) Invoked after a domain object is loaded. Can modify the domain object after reading it from a row. Ordered.LOWEST_PRECEDENCE AuditingEntityCallback onBeforeConvert(T entity, SqlIdentifier table) Marks an auditable entity created or modified 100 BeforeSaveCallback onBeforeSave(T entity, OutboundRow row, SqlIdentifier table) Invoked before a domain object is saved. Can modify the target, to be persisted, OutboundRow containing all mapped entity information. Ordered.LOWEST_PRECEDENCE AfterSaveCallback onAfterSave(T entity, OutboundRow row, SqlIdentifier table) Invoked after a domain object is saved. Can modify the domain object, to be returned after save, OutboundRow containing all mapped entity information. Ordered.LOWEST_PRECEDENCE"
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/auditing.html","Auditing: Since Spring Data R2DBC 1.2, auditing can be enabled by annotating a configuration class with the @EnableR2dbcAuditing annotation, as the following example shows: Activating auditing using JavaConfig @Configuration @EnableR2dbcAuditing class Config { @Bean public ReactiveAuditorAware<AuditableUser> myAuditorProvider() { return new AuditorAwareImpl(); } } If you expose a bean of type ReactiveAuditorAware to the ApplicationContext , the auditing infrastructure picks it up automatically and uses it to determine the current user to be set on domain types. If you have multiple implementations registered in the ApplicationContext , you can select the one to be used by explicitly setting the auditorAwareRef attribute of @EnableR2dbcAuditing ."
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/kotlin.html","Kotlin: This part of the reference documentation explains the specific Kotlin functionality offered by Spring Data R2DBC. See Kotlin Support(../kotlin.html) for the general functionality provided by Spring Data. To retrieve a list of SWCharacter objects in Java, you would normally write the following: Flux<SWCharacter> characters = client.select().from(SWCharacter.class).fetch().all(); With Kotlin and the Spring Data extensions, you can instead write the following: val characters = client.select().from<SWCharacter>().fetch().all() // or (both are equivalent) val characters : Flux<SWCharacter> = client.select().from().fetch().all() As in Java, characters in Kotlin is strongly typed, but Kotlin’s clever type inference allows for shorter syntax. Spring Data R2DBC provides the following extensions: Reified generics support for DatabaseClient and Criteria . Coroutines(../kotlin/coroutines.html) extensions for DatabaseClient ."
"https://docs.spring.io/spring-data/relational/reference/3.3/r2dbc/migration-guide.html","Migration Guide: The following sections explain how to migrate to a newer version of Spring Data R2DBC. Upgrading from 1.1.x to 1.2.x: Spring Data R2DBC was developed with the intent to evaluate how well R2DBC can integrate with Spring applications. One of the main aspects was to move core support into Spring Framework once R2DBC support has proven useful. Spring Framework 5.3 ships with a new module: Spring R2DBC ( spring-r2dbc ). spring-r2dbc ships core R2DBC functionality (a slim variant of DatabaseClient , Transaction Manager, Connection Factory initialization, Exception translation) that was initially provided by Spring Data R2DBC. The 1.2.0 release aligns with what’s provided in Spring R2DBC by making several changes outlined in the following sections. Spring R2DBC’s DatabaseClient is a more lightweight implementation that encapsulates a pure SQL-oriented interface. You will notice that the method to run SQL statements changed from DatabaseClient.execute(…) to DatabaseClient.sql(…) . The fluent API for CRUD operations has moved into R2dbcEntityTemplate . If you use logging of SQL statements through the logger prefix org.springframework.data.r2dbc , make sure to update it to org.springframework.r2dbc (that is removing .data ) to point to Spring R2DBC components. Deprecations: Deprecation of o.s.d.r2dbc.core.DatabaseClient and its support classes ConnectionAccessor , FetchSpec , SqlProvider and a few more. Named parameter support classes such as NamedParameterExpander are encapsulated by Spring R2DBC’s DatabaseClient implementation hence we’re not providing replacements as this was internal API in the first place. Use o.s.r2dbc.core.DatabaseClient and their Spring R2DBC replacements available from org.springframework.r2dbc.core . Entity-based methods ( select / insert / update / delete ) methods are available through R2dbcEntityTemplate which was introduced with version 1.1. Deprecation of o.s.d.r2dbc.connectionfactory , o.s.d.r2dbc.connectionfactory.init , and o.s.d.r2dbc.connectionfactory.lookup packages. Use Spring R2DBC’s variant which you can find at o.s.r2dbc.connection . Deprecation of o.s.d.r2dbc.convert.ColumnMapRowMapper . Use o.s.r2dbc.core.ColumnMapRowMapper instead. Deprecation of binding support classes o.s.d.r2dbc.dialect.Bindings , BindMarker , BindMarkers , BindMarkersFactory and related types. Use replacements from org.springframework.r2dbc.core.binding . Deprecation of BadSqlGrammarException , UncategorizedR2dbcException and exception translation at o.s.d.r2dbc.support . Spring R2DBC provides a slim exception translation variant without an SPI for now available through o.s.r2dbc.connection.ConnectionFactoryUtils#convertR2dbcException . Usage of replacements provided by Spring R2DBC: To ease migration, several deprecated types are now subtypes of their replacements provided by Spring R2DBC. Spring Data R2DBC has changes several methods or introduced new methods accepting Spring R2DBC types. Specifically the following classes are changed: R2dbcEntityTemplate R2dbcDialect Types in org.springframework.data.r2dbc.query We recommend that you review and update your imports if you work with these types directly. Breaking Changes: OutboundRow and statement mappers switched from using SettableValue to Parameter Repository factory support requires o.s.r2dbc.core.DatabaseClient instead of o.s.data.r2dbc.core.DatabaseClient . Dependency Changes: To make use of Spring R2DBC, make sure to include the following dependency: org.springframework:spring-r2dbc"
"https://docs.spring.io/spring-data/relational/reference/3.3/kotlin.html","Kotlin Support: Kotlin(https://kotlinlang.org) is a statically typed language that targets the JVM (and other platforms) which allows writing concise and elegant code while providing excellent interoperability(https://kotlinlang.org/docs/reference/java-interop.html) with existing libraries written in Java. Spring Data provides first-class support for Kotlin and lets developers write Kotlin applications almost as if Spring Data was a Kotlin native framework. The easiest way to build a Spring application with Kotlin is to leverage Spring Boot and its dedicated Kotlin support(https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-kotlin.html) . This comprehensive tutorial(https://spring.io/guides/tutorials/spring-boot-kotlin/) will teach you how to build Spring Boot applications with Kotlin using start.spring.io(https://start.spring.io/#!language=kotlin&type=gradle-project) . Section Summary: Requirements(kotlin/requirements.html) Null Safety(kotlin/null-safety.html) Object Mapping(kotlin/object-mapping.html) Extensions(kotlin/extensions.html) Coroutines(kotlin/coroutines.html)"
"https://docs.spring.io/spring-data/relational/reference/3.3/kotlin/requirements.html","Requirements: Spring Data supports Kotlin 1.3 and requires kotlin-stdlib (or one of its variants, such as kotlin-stdlib-jdk8 ) and kotlin-reflect to be present on the classpath. Those are provided by default if you bootstrap a Kotlin project via start.spring.io(https://start.spring.io/#!language=kotlin&type=gradle-project) ."
"https://docs.spring.io/spring-data/relational/reference/3.3/kotlin/null-safety.html","Null Safety: One of Kotlin’s key features is null safety(https://kotlinlang.org/docs/null-safety.html) , which cleanly deals with null values at compile time. This makes applications safer through nullability declarations and the expression of “value or no value” semantics without paying the cost of wrappers, such as Optional . (Kotlin allows using functional constructs with nullable values. See this comprehensive guide to Kotlin null safety(https://www.baeldung.com/kotlin/null-safety) .) Although Java does not let you express null safety in its type system, Spring Data API is annotated with JSR-305(https://jcp.org/en/jsr/detail?id=305) tooling friendly annotations declared in the org.springframework.lang package. By default, types from Java APIs used in Kotlin are recognized as platform types(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) , for which null checks are relaxed. Kotlin support for JSR-305 annotations(https://kotlinlang.org/docs/reference/java-interop.html#jsr-305-support) and Spring nullability annotations provide null safety for the whole Spring Data API to Kotlin developers, with the advantage of dealing with null related issues at compile time. See Null Handling of Repository Methods(../repositories/null-handling.html) how null safety applies to Spring Data Repositories. You can configure JSR-305 checks by adding the -Xjsr305 compiler flag with the following options: -Xjsr305={strict|warn|ignore} . For Kotlin versions 1.1+, the default behavior is the same as -Xjsr305=warn . The strict value is required take Spring Data API null-safety into account. Kotlin types inferred from Spring API but should be used with the knowledge that Spring API nullability declaration could evolve, even between minor releases and that more checks may be added in the future. Generic type arguments, varargs, and array elements nullability are not supported yet, but should be in an upcoming release."
"https://docs.spring.io/spring-data/relational/reference/3.3/kotlin/object-mapping.html","Object Mapping: See Kotlin support(../object-mapping.html#mapping.kotlin) for details on how Kotlin objects are materialized."
"https://docs.spring.io/spring-data/relational/reference/3.3/kotlin/extensions.html","Extensions: Kotlin extensions(https://kotlinlang.org/docs/reference/extensions.html) provide the ability to extend existing classes with additional functionality. Spring Data Kotlin APIs use these extensions to add new Kotlin-specific conveniences to existing Spring APIs. Keep in mind that Kotlin extensions need to be imported to be used. Similar to static imports, an IDE should automatically suggest the import in most cases. For example, Kotlin reified type parameters(https://kotlinlang.org/docs/reference/inline-functions.html#reified-type-parameters) provide a workaround for JVM generics type erasure(https://docs.oracle.com/javase/tutorial/java/generics/erasure.html) , and Spring Data provides some extensions to take advantage of this feature. This allows for a better Kotlin API."
"https://docs.spring.io/spring-data/relational/reference/3.3/kotlin/coroutines.html","Coroutines: Kotlin Coroutines(https://kotlinlang.org/docs/reference/coroutines-overview.html) are instances of suspendable computations allowing to write non-blocking code imperatively. On language side, suspend functions provides an abstraction for asynchronous operations while on library side kotlinx.coroutines(https://github.com/Kotlin/kotlinx.coroutines) provides functions like async { }(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/async.html) and types like Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) . Spring Data modules provide support for Coroutines on the following scope: Deferred(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/-deferred/index.html) and Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) return values support in Kotlin extensions Dependencies: Coroutines support is enabled when kotlinx-coroutines-core , kotlinx-coroutines-reactive and kotlinx-coroutines-reactor dependencies are in the classpath: Dependencies to add in Maven pom.xml <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-core</artifactId> </dependency> <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-reactive</artifactId> </dependency> <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-reactor</artifactId> </dependency> Supported versions 1.3.0 and above. How Reactive translates to Coroutines?: For return values, the translation from Reactive to Coroutines APIs is the following: fun handler(): Mono<Void> becomes suspend fun handler() fun handler(): Mono<T> becomes suspend fun handler(): T or suspend fun handler(): T? depending on if the Mono can be empty or not (with the advantage of being more statically typed) fun handler(): Flux<T> becomes fun handler(): Flow<T> Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) is Flux equivalent in Coroutines world, suitable for hot or cold stream, finite or infinite streams, with the following main differences: Flow is push-based while Flux is push-pull hybrid Backpressure is implemented via suspending functions Flow has only a single suspending collect method(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/collect.html) and operators are implemented as extensions(https://kotlinlang.org/docs/reference/extensions.html) Operators are easy to implement(https://github.com/Kotlin/kotlinx.coroutines/tree/master/kotlinx-coroutines-core/common/src/flow/operators) thanks to Coroutines Extensions allow adding custom operators to Flow Collect operations are suspending functions map operator(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/map.html) supports asynchronous operation (no need for flatMap ) since it takes a suspending function parameter Read this blog post about Going Reactive with Spring, Coroutines and Kotlin Flow(https://spring.io/blog/2019/04/12/going-reactive-with-spring-coroutines-and-kotlin-flow) for more details, including how to run code concurrently with Coroutines. Repositories: Here is an example of a Coroutines repository: interface CoroutineRepository : CoroutineCrudRepository<User, String> { suspend fun findOne(id: String): User fun findByFirstname(firstname: String): Flow<User> suspend fun findAllByFirstname(id: String): List<User> } Coroutines repositories are built on reactive repositories to expose the non-blocking nature of data access through Kotlin’s Coroutines. Methods on a Coroutines repository can be backed either by a query method or a custom implementation. Invoking a custom implementation method propagates the Coroutines invocation to the actual implementation method if the custom method is suspend -able without requiring the implementation method to return a reactive type such as Mono or Flux . Note that depending on the method declaration the coroutine context may or may not be available. To retain access to the context, either declare your method using suspend or return a type that enables context propagation such as Flow . suspend fun findOne(id: String): User : Retrieve the data once and synchronously by suspending. fun findByFirstname(firstname: String): Flow<User> : Retrieve a stream of data. The Flow is created eagerly while data is fetched upon Flow interaction ( Flow.collect(…) ). fun getUser(): User : Retrieve data once blocking the thread and without context propagation. This should be avoided. Coroutines repositories are only discovered when the repository extends the CoroutineCrudRepository interface."
"https://docs.spring.io/spring-data/jpa/reference/3.3/index.html","Spring Data JPA: Spring Data JPA provides repository support for the Jakarta Persistence API (JPA). It eases development of applications with a consistent programming model that need to access JPA data sources. JPA(jpa.html) JPA and JPA Repositories Envers(envers.html) Support for Envers Revision Repositories Wiki(https://github.com/spring-projects/spring-data-commons/wiki) What’s New, Upgrade Notes, Supported Versions, additional cross-version information. Oliver Gierke, Thomas Darimont, Christoph Strobl, Mark Paluch, Jay Bryant, Greg Turnquist © 2008-2024 VMware, Inc. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically."
"https://docs.spring.io/spring-data/jpa/reference/3.3/commons/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa.html","JPA: This chapter points out the specialties for repository support for JPA. This builds on the core repository support explained in Working with Spring Data Repositories(repositories.html) . Make sure you have a sound understanding of the basic concepts explained there. Section Summary: Getting Started(jpa/getting-started.html) Core concepts(repositories/core-concepts.html) Defining Repository Interfaces(repositories/definition.html) Configuration(repositories/create-instances.html) Persisting Entities(jpa/entity-persistence.html) Defining Query Methods(repositories/query-methods-details.html) JPA Query Methods(jpa/query-methods.html) Projections(repositories/projections.html) Stored Procedures(jpa/stored-procedures.html) Specifications(jpa/specifications.html) Query by Example(repositories/query-by-example.html) Transactionality(jpa/transactions.html) Locking(jpa/locking.html) Auditing(auditing.html) Merging persistence units(jpa/misc-merging-persistence-units.html) CDI Integration(jpa/jpd-misc-cdi-integration.html) Custom Repository Implementations(repositories/custom-implementations.html) Publishing Events from Aggregate Roots(repositories/core-domain-events.html) Null Handling of Repository Methods(repositories/null-handling.html) Spring Data Extensions(repositories/core-extensions.html) Repository query keywords(repositories/query-keywords-reference.html) Repository query return types(repositories/query-return-types-reference.html) Frequently Asked Questions(jpa/faq.html) Glossary(jpa/glossary.html)"
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/getting-started.html","Getting Started: An easy way to bootstrap setting up a working environment is to create a Spring-based project via start.spring.io(https://start.spring.io/#!type=maven-project&dependencies=h2,data-jpa) or create a Spring project in Spring Tools(https://spring.io/tools) . Examples Repository: The GitHub spring-data-examples repository(https://github.com/spring-projects/spring-data-examples) hosts several examples that you can download and play around with to get a feel for how the library works. Hello World: Let’s start with a simple entity and its corresponding repository: @Entity class Person { @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String name; // getters and setters omitted for brevity } interface PersonRepository extends Repository<Person, Long> { Person save(Person person); Optional<Person> findById(long id); } Create the main application to run, as the following example shows: @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } @Bean CommandLineRunner runner(PersonRepository repository) { return args -> { Person person = new Person(); person.setName(""John""); repository.save(person); Person saved = repository.findById(person.getId()).orElseThrow(NoSuchElementException::new); }; } } Even in this simple example, there are a few notable things to point out: Repository instances are automatically implemented. When used as parameters of @Bean methods, these will be autowired without further need for annotations. The basic repository extends Repository . We suggest to consider how much API surface you want to expose towards your application. More complex repository interfaces are ListCrudRepository or JpaRepository ."
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); }"
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/create-instances.html","Configuration: This section describes configuring Spring Data JPA through either: “ Annotation-based Configuration(#jpa.java-config) ” (Java configuration) “ Spring Namespace(#repositories.create-instances.xml) ” (XML configuration) Annotation-based Configuration: The Spring Data JPA repositories support can be activated through both JavaConfig as well as a custom XML namespace, as shown in the following example: Example 1. Spring Data JPA repositories using JavaConfig @Configuration @EnableJpaRepositories @EnableTransactionManagement class ApplicationConfig { @Bean public DataSource dataSource() { EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); return builder.setType(EmbeddedDatabaseType.HSQL).build(); } @Bean public LocalContainerEntityManagerFactoryBean entityManagerFactory() { HibernateJpaVendorAdapter vendorAdapter = new HibernateJpaVendorAdapter(); vendorAdapter.setGenerateDdl(true); LocalContainerEntityManagerFactoryBean factory = new LocalContainerEntityManagerFactoryBean(); factory.setJpaVendorAdapter(vendorAdapter); factory.setPackagesToScan(""com.acme.domain""); factory.setDataSource(dataSource()); return factory; } @Bean public PlatformTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) { JpaTransactionManager txManager = new JpaTransactionManager(); txManager.setEntityManagerFactory(entityManagerFactory); return txManager; } } You must create LocalContainerEntityManagerFactoryBean and not EntityManagerFactory directly, since the former also participates in exception translation mechanisms in addition to creating EntityManagerFactory . The preceding configuration class sets up an embedded HSQL database by using the EmbeddedDatabaseBuilder API of spring-jdbc . Spring Data then sets up an EntityManagerFactory and uses Hibernate as the sample persistence provider. The last infrastructure component declared here is the JpaTransactionManager . Finally, the example activates Spring Data JPA repositories by using the @EnableJpaRepositories annotation, which essentially carries the same attributes as the XML namespace. If no base package is configured, it uses the one in which the configuration class resides. Spring Namespace: The JPA module of Spring Data contains a custom namespace that allows defining repository beans. It also contains certain features and element attributes that are special to JPA. Generally, the JPA repositories can be set up by using the repositories element, as shown in the following example: Example 2. Setting up JPA repositories by using the namespace <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:jpa=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans> Which is better, JavaConfig or XML? XML is how Spring was configured long ago. In today’s era of fast-growing Java, record types, annotations, and more, new projects typically use as much pure Java as possible. While there is no immediate plan to remove XML support, some of the newest features MAY not be available through XML. Using the repositories element it activates persistence exception translation for all beans annotated with @Repository , to let exceptions being thrown by the JPA persistence providers be converted into Spring’s DataAccessException hierarchy. Custom Namespace Attributes: Beyond the default attributes of the repositories element, the JPA namespace offers additional attributes to let you gain more detailed control over the setup of the repositories: Table 1. Custom JPA-specific attributes of the repositories element entity-manager-factory-ref Explicitly wire the EntityManagerFactory to be used with the repositories being detected by the repositories element. Usually used if multiple EntityManagerFactory beans are used within the application. If not configured, Spring Data automatically looks up the EntityManagerFactory bean with the name entityManagerFactory in the ApplicationContext . transaction-manager-ref Explicitly wire the PlatformTransactionManager to be used with the repositories being detected by the repositories element. Usually only necessary if multiple transaction managers or EntityManagerFactory beans have been configured. Default to a single defined PlatformTransactionManager inside the current ApplicationContext . Spring Data JPA requires a PlatformTransactionManager bean named transactionManager to be present if no explicit transaction-manager-ref is defined. Bootstrap Mode: By default, Spring Data JPA repositories are default Spring beans. They are singleton scoped and eagerly initialized. During startup, they already interact with the JPA EntityManager for verification and metadata analysis purposes. Spring Framework supports the initialization of the JPA EntityManagerFactory in a background thread because that process usually takes up a significant amount of startup time in a Spring application. To make use of that background initialization effectively, we need to make sure that JPA repositories are initialized as late as possible. As of Spring Data JPA 2.1 you can now configure a BootstrapMode (either via the @EnableJpaRepositories annotation or the XML namespace) that takes the following values: DEFAULT (default) — Repositories are instantiated eagerly unless explicitly annotated with @Lazy . The lazification only has effect if no client bean needs an instance of the repository as that will require the initialization of the repository bean. LAZY — Implicitly declares all repository beans lazy and also causes lazy initialization proxies to be created to be injected into client beans. That means, that repositories will not get instantiated if the client bean is simply storing the instance in a field and not making use of the repository during initialization. Repository instances will be initialized and verified upon first interaction with the repository. DEFERRED — Fundamentally the same mode of operation as LAZY , but triggering repository initialization in response to an ContextRefreshedEvent so that repositories are verified before the application has completely started. Recommendations: If you’re not using asynchronous JPA bootstrap stick with the default bootstrap mode. In case you bootstrap JPA asynchronously, DEFERRED is a reasonable default as it will make sure the Spring Data JPA bootstrap only waits for the EntityManagerFactory setup if that itself takes longer than initializing all other application components. Still, it makes sure that repositories are properly initialized and validated before the application signals it’s up. LAZY is a decent choice for testing scenarios and local development. Once you are pretty sure that repositories can properly bootstrap, or in cases where you are testing other parts of the application, running verification for all repositories might unnecessarily increase the startup time. The same applies to local development in which you only access parts of the application that might need to have a single repository initialized."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/entity-persistence.html","Persisting Entities: This section describes how to persist (save) entities with Spring Data JPA. Saving Entities: Saving an entity can be performed with the CrudRepository.save(…) method. It persists or merges the given entity by using the underlying JPA EntityManager . If the entity has not yet been persisted, Spring Data JPA saves the entity with a call to the entityManager.persist(…) method. Otherwise, it calls the entityManager.merge(…) method. Entity State-detection Strategies: Spring Data JPA offers the following strategies to detect whether an entity is new or not: Version-Property and Id-Property inspection ( default ): By default Spring Data JPA inspects first if there is a Version-property of non-primitive type. If there is, the entity is considered new if the value of that property is null . Without such a Version-property Spring Data JPA inspects the identifier property of the given entity. If the identifier property is null , then the entity is assumed to be new. Otherwise, it is assumed to be not new. Implementing Persistable : If an entity implements Persistable , Spring Data JPA delegates the new detection to the isNew(…) method of the entity. See the JavaDoc(https://docs.spring.io/spring-data/data-commons/docs/current/api/index.html?org/springframework/data/domain/Persistable.html) for details. Implementing EntityInformation : You can customize the EntityInformation abstraction used in the SimpleJpaRepository implementation by creating a subclass of JpaRepositoryFactory and overriding the getEntityInformation(…) method accordingly. You then have to register the custom implementation of JpaRepositoryFactory as a Spring bean. Note that this should be rarely necessary. See the JavaDoc(../api/java/org/springframework/data/jpa/repository/support/JpaRepositoryFactory.html) for details. Option 1 is not an option for entities that use manually assigned identifiers and no version attribute as with those the identifier will always be non- null . A common pattern in that scenario is to use a common base class with a transient flag defaulting to indicate a new instance and using JPA lifecycle callbacks to flip that flag on persistence operations: Example 1. A base class for entities with manually assigned identifiers @MappedSuperclass public abstract class AbstractEntity<ID> implements Persistable<ID> { @Transient private boolean isNew = true; (1) @Override public boolean isNew() { return isNew; (2) } @PrePersist (3) @PostLoad void markNotNew() { this.isNew = false; } // More code… } 1 Declare a flag to hold the new state. Transient so that it’s not persisted to the database. 2 Return the flag in the implementation of Persistable.isNew() so that Spring Data repositories know whether to call EntityManager.persist() or ….merge() . 3 Declare a method using JPA entity callbacks so that the flag is switched to indicate an existing entity after a repository call to save(…) or an instance creation by the persistence provider."
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/query-methods-details.html","Defining Query Methods: The repository proxy has two ways to derive a store-specific query from the method name: By deriving the query from the method name directly. By using a manually defined query. Available options depend on the actual store. However, there must be a strategy that decides what actual query is created. The next section describes the available options. Query Lookup Strategies: The following strategies are available for the repository infrastructure to resolve the query. With XML configuration, you can configure the strategy at the namespace through the query-lookup-strategy attribute. For Java configuration, you can use the queryLookupStrategy attribute of the EnableJpaRepositories annotation. Some strategies may not be supported for particular datastores. CREATE attempts to construct a store-specific query from the query method name. The general approach is to remove a given set of well known prefixes from the method name and parse the rest of the method. You can read more about query construction in “ Query Creation(#repositories.query-methods.query-creation) ”. USE_DECLARED_QUERY tries to find a declared query and throws an exception if it cannot find one. The query can be defined by an annotation somewhere or declared by other means. See the documentation of the specific store to find available options for that store. If the repository infrastructure does not find a declared query for the method at bootstrap time, it fails. CREATE_IF_NOT_FOUND (the default) combines CREATE and USE_DECLARED_QUERY . It looks up a declared query first, and, if no declared query is found, it creates a custom method name-based query. This is the default lookup strategy and, thus, is used if you do not configure anything explicitly. It allows quick query definition by method names but also custom-tuning of these queries by introducing declared queries as needed. Query Creation: The query builder mechanism built into the Spring Data repository infrastructure is useful for building constraining queries over entities of the repository. The following example shows how to create a number of queries: Query creation from method names interface PersonRepository extends Repository<Person, Long> { List<Person> findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // Enables the distinct flag for the query List<Person> findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List<Person> findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); // Enabling ignoring case for an individual property List<Person> findByLastnameIgnoreCase(String lastname); // Enabling ignoring case for all suitable properties List<Person> findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // Enabling static ORDER BY for a query List<Person> findByLastnameOrderByFirstnameAsc(String lastname); List<Person> findByLastnameOrderByFirstnameDesc(String lastname); } Parsing query method names is divided into subject and predicate. The first part ( find…By , exists…By ) defines the subject of the query, the second part forms the predicate. The introducing clause (subject) can contain further expressions. Any text between find (or other introducing keywords) and By is considered to be descriptive unless using one of the result-limiting keywords such as a Distinct to set a distinct flag on the query to be created or Top/First to limit query results(#repositories.limit-query-result) . The appendix contains the full list of query method subject keywords(query-keywords-reference.html#appendix.query.method.subject) and query method predicate keywords including sorting and letter-casing modifiers(query-keywords-reference.html#appendix.query.method.predicate) . However, the first By acts as a delimiter to indicate the start of the actual criteria predicate. At a very basic level, you can define conditions on entity properties and concatenate them with And and Or . The actual result of parsing the method depends on the persistence store for which you create the query. However, there are some general things to notice: The expressions are usually property traversals combined with operators that can be concatenated. You can combine property expressions with AND and OR . You also get support for operators such as Between , LessThan , GreaterThan , and Like for the property expressions. The supported operators can vary by datastore, so consult the appropriate part of your reference documentation. The method parser supports setting an IgnoreCase flag for individual properties (for example, findByLastnameIgnoreCase(…) ) or for all properties of a type that supports ignoring case (usually String instances — for example, findByLastnameAndFirstnameAllIgnoreCase(…) ). Whether ignoring cases is supported may vary by store, so consult the relevant sections in the reference documentation for the store-specific query method. You can apply static ordering by appending an OrderBy clause to the query method that references a property and by providing a sorting direction ( Asc or Desc ). To create a query method that supports dynamic sorting, see “ Paging, Iterating Large Results, Sorting & Limiting(#repositories.special-parameters) ”. Property Expressions: Property expressions can refer only to a direct property of the managed entity, as shown in the preceding example. At query creation time, you already make sure that the parsed property is a property of the managed domain class. However, you can also define constraints by traversing nested properties. Consider the following method signature: List<Person> findByAddressZipCode(ZipCode zipCode); Assume a Person has an Address with a ZipCode . In that case, the method creates the x.address.zipCode property traversal. The resolution algorithm starts by interpreting the entire part ( AddressZipCode ) as the property and checks the domain class for a property with that name (uncapitalized). If the algorithm succeeds, it uses that property. If not, the algorithm splits up the source at the camel-case parts from the right side into a head and a tail and tries to find the corresponding property — in our example, AddressZip and Code . If the algorithm finds a property with that head, it takes the tail and continues building the tree down from there, splitting the tail up in the way just described. If the first split does not match, the algorithm moves the split point to the left ( Address , ZipCode ) and continues. Although this should work for most cases, it is possible for the algorithm to select the wrong property. Suppose the Person class has an addressZip property as well. The algorithm would match in the first split round already, choose the wrong property, and fail (as the type of addressZip probably has no code property). To resolve this ambiguity you can use _ inside your method name to manually define traversal points. So our method name would be as follows: List<Person> findByAddress_ZipCode(ZipCode zipCode); Because we treat underscores ( _ ) as a reserved character, we strongly advise to follow standard Java naming conventions (that is, not using underscores in property names but applying camel case instead). Field Names starting with underscore: Field names may start with underscores like String _name . Make sure to preserve the _ as in _name and use double _ to split nested paths like user__name . Upper Case Field Names: Field names that are all uppercase can be used as such. Nested paths if applicable require splitting via _ as in USER_name . Field Names with 2nd uppercase letter: Field names that consist of a starting lower case letter followed by an uppercase one like String qCode can be resolved by starting with two upper case letters as in QCode . Please be aware of potential path ambiguities. Path Ambiguities: In the following sample the arrangement of properties qCode and q , with q containing a property called code , creates an ambiguity for the path QCode . record Container(String qCode, Code q) {} record Code(String code) {} Since a direct match on a property is considered first, any potential nested paths will not be considered and the algorithm picks the qCode field. In order to select the code field in q the underscore notation Q_Code is required. Repository Methods Returning Collections or Iterables: Query methods that return multiple results can use standard Java Iterable , List , and Set . Beyond that, we support returning Spring Data’s Streamable , a custom extension of Iterable , as well as collection types provided by Vavr(https://www.vavr.io/) . Refer to the appendix explaining all possible query method return types(query-return-types-reference.html#appendix.query.return.types) . Using Streamable as Query Method Return Type: You can use Streamable as alternative to Iterable or any collection type. It provides convenience methods to access a non-parallel Stream (missing from Iterable ) and the ability to directly ….filter(…) and ….map(…) over the elements and concatenate the Streamable to others: Using Streamable to combine query method results interface PersonRepository extends Repository<Person, Long> { Streamable<Person> findByFirstnameContaining(String firstname); Streamable<Person> findByLastnameContaining(String lastname); } Streamable<Person> result = repository.findByFirstnameContaining(""av"") .and(repository.findByLastnameContaining(""ea"")); Returning Custom Streamable Wrapper Types: Providing dedicated wrapper types for collections is a commonly used pattern to provide an API for a query result that returns multiple elements. Usually, these types are used by invoking a repository method returning a collection-like type and creating an instance of the wrapper type manually. You can avoid that additional step as Spring Data lets you use these wrapper types as query method return types if they meet the following criteria: The type implements Streamable . The type exposes either a constructor or a static factory method named of(…) or valueOf(…) that takes Streamable as an argument. The following listing shows an example: class Product { (1) MonetaryAmount getPrice() { … } } @RequiredArgsConstructor(staticName = ""of"") class Products implements Streamable<Product> { (2) private final Streamable<Product> streamable; public MonetaryAmount getTotal() { (3) return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); } @Override public Iterator<Product> iterator() { (4) return streamable.iterator(); } } interface ProductRepository implements Repository<Product, Long> { Products findAllByDescriptionContaining(String text); (5) } 1 A Product entity that exposes API to access the product’s price. 2 A wrapper type for a Streamable<Product> that can be constructed by using Products.of(…) (factory method created with the Lombok annotation). A standard constructor taking the Streamable<Product> will do as well. 3 The wrapper type exposes an additional API, calculating new values on the Streamable<Product> . 4 Implement the Streamable interface and delegate to the actual result. 5 That wrapper type Products can be used directly as a query method return type. You do not need to return Streamable<Product> and manually wrap it after the query in the repository client. Support for Vavr Collections: Vavr(https://www.vavr.io/) is a library that embraces functional programming concepts in Java. It ships with a custom set of collection types that you can use as query method return types, as the following table shows: Vavr collection type Used Vavr implementation type Valid Java source types io.vavr.collection.Seq io.vavr.collection.List java.util.Iterable io.vavr.collection.Set io.vavr.collection.LinkedHashSet java.util.Iterable io.vavr.collection.Map io.vavr.collection.LinkedHashMap java.util.Map You can use the types in the first column (or subtypes thereof) as query method return types and get the types in the second column used as implementation type, depending on the Java type of the actual query result (third column). Alternatively, you can declare Traversable (the Vavr Iterable equivalent), and we then derive the implementation class from the actual return value. That is, a java.util.List is turned into a Vavr List or Seq , a java.util.Set becomes a Vavr LinkedHashSet Set , and so on. Streaming Query Results: You can process the results of query methods incrementally by using a Java 8 Stream<T> as the return type. Instead of wrapping the query results in a Stream , data store-specific methods are used to perform the streaming, as shown in the following example: Stream the result of a query with Java 8 Stream<T> @Query(""select u from User u"") Stream<User> findAllByCustomQueryAndStream(); Stream<User> readAllByFirstnameNotNull(); @Query(""select u from User u"") Stream<User> streamAllPaged(Pageable pageable); A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example: Working with a Stream<T> result in a try-with-resources block try (Stream<User> stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } Not all Spring Data modules currently support Stream<T> as a return type. Asynchronous Query Results: You can run repository queries asynchronously by using Spring’s asynchronous method running capability(https://docs.spring.io/spring-framework/reference/6.1/integration/scheduling.html) . This means the method returns immediately upon invocation while the actual query occurs in a task that has been submitted to a Spring TaskExecutor . Asynchronous queries differ from reactive queries and should not be mixed. See the store-specific documentation for more details on reactive support. The following example shows a number of asynchronous queries: @Async Future<User> findByFirstname(String firstname); (1) @Async CompletableFuture<User> findOneByFirstname(String firstname); (2) 1 Use java.util.concurrent.Future as the return type. 2 Use a Java 8 java.util.concurrent.CompletableFuture as the return type. Paging, Iterating Large Results, Sorting & Limiting: To handle parameters in your query, define method parameters as already seen in the preceding examples. Besides that, the infrastructure recognizes certain specific types like Pageable , Sort and Limit , to apply pagination, sorting and limiting to your queries dynamically. The following example demonstrates these features: Using Pageable , Slice , Sort and Limit in query methods Page<User> findByLastname(String lastname, Pageable pageable); Slice<User> findByLastname(String lastname, Pageable pageable); List<User> findByLastname(String lastname, Sort sort); List<User> findByLastname(String lastname, Sort sort, Limit limit); List<User> findByLastname(String lastname, Pageable pageable); APIs taking Sort , Pageable and Limit expect non- null values to be handed into methods. If you do not want to apply any sorting or pagination, use Sort.unsorted() , Pageable.unpaged() and Limit.unlimited() . The first method lets you pass an org.springframework.data.domain.Pageable instance to the query method to dynamically add paging to your statically defined query. A Page knows about the total number of elements and pages available. It does so by the infrastructure triggering a count query to calculate the overall number. As this might be expensive (depending on the store used), you can instead return a Slice . A Slice knows only about whether a next Slice is available, which might be sufficient when walking through a larger result set. Sorting options are handled through the Pageable instance, too. If you need only sorting, add an org.springframework.data.domain.Sort parameter to your method. As you can see, returning a List is also possible. In this case, the additional metadata required to build the actual Page instance is not created (which, in turn, means that the additional count query that would have been necessary is not issued). Rather, it restricts the query to look up only the given range of entities. To find out how many pages you get for an entire query, you have to trigger an additional count query. By default, this query is derived from the query you actually trigger. Special parameters may only be used once within a query method. Some special parameters described above are mutually exclusive. Please consider the following list of invalid parameter combinations. Parameters Example Reason Pageable and Sort findBy…​(Pageable page, Sort sort) Pageable already defines Sort Pageable and Limit findBy…​(Pageable page, Limit limit) Pageable already defines a limit. The Top keyword used to limit results can be used to along with Pageable whereas Top defines the total maximum of results, whereas the Pageable parameter may reduce this number. Which Method is Appropriate?: The value provided by the Spring Data abstractions is perhaps best shown by the possible query method return types outlined in the following table below. The table shows which types you can return from a query method Table 1. Consuming Large Query Results Method Amount of Data Fetched Query Structure Constraints List<T>(#repositories.collections-and-iterables) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Streamable<T>(#repositories.collections-and-iterables.streamable) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Stream<T>(#repositories.query-streaming) Chunked (one-by-one or in batches) depending on Stream consumption. Single query using typically cursors. Streams must be closed after usage to avoid resource leaks. Flux<T> Chunked (one-by-one or in batches) depending on Flux consumption. Single query using typically cursors. Store module must provide reactive infrastructure. Slice<T> Pageable.getPageSize() + 1 at Pageable.getOffset() One to many queries fetching data starting at Pageable.getOffset() applying limiting. A Slice can only navigate to the next Slice . Slice provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Window provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Page<T> Pageable.getPageSize() at Pageable.getOffset() One to many queries starting at Pageable.getOffset() applying limiting. Additionally, COUNT(…) query to determine the total number of elements can be required. Often times, COUNT(…) queries are required that are costly. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Paging and Sorting: You can define simple sorting expressions by using property names. You can concatenate expressions to collect multiple criteria into one expression. Defining sort expressions Sort sort = Sort.by(""firstname"").ascending() .and(Sort.by(""lastname"").descending()); For a more type-safe way to define sort expressions, start with the type for which to define the sort expression and use method references to define the properties on which to sort. Defining sort expressions by using the type-safe API TypedSort<Person> person = Sort.sort(Person.class); Sort sort = person.by(Person::getFirstname).ascending() .and(person.by(Person::getLastname).descending()); TypedSort.by(…) makes use of runtime proxies by (typically) using CGlib, which may interfere with native image compilation when using tools such as Graal VM Native. If your store implementation supports Querydsl, you can also use the generated metamodel types to define sort expressions: Defining sort expressions by using the Querydsl API QSort sort = QSort.by(QPerson.firstname.asc()) .and(QSort.by(QPerson.lastname.desc())); Limiting Query Results: In addition to paging it is possible to limit the result size using a dedicated Limit parameter. You can also limit the results of query methods by using the First or Top keywords, which you can use interchangeably but may not be mixed with a Limit parameter. You can append an optional numeric value to Top or First to specify the maximum result size to be returned. If the number is left out, a result size of 1 is assumed. The following example shows how to limit the query size: Limiting the result size of a query with Top and First List<User> findByLastname(Limit limit); User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page<User> queryFirst10ByLastname(String lastname, Pageable pageable); Slice<User> findTop3ByLastname(String lastname, Pageable pageable); List<User> findFirst10ByLastname(String lastname, Sort sort); List<User> findTop10ByLastname(String lastname, Pageable pageable); The limiting expressions also support the Distinct keyword for datastores that support distinct queries. Also, for the queries that limit the result set to one instance, wrapping the result into with the Optional keyword is supported. If pagination or slicing is applied to a limiting query pagination (and the calculation of the number of available pages), it is applied within the limited result. Limiting the results in combination with dynamic sorting by using a Sort parameter lets you express query methods for the 'K' smallest as well as for the 'K' biggest elements."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/query-methods.html","JPA Query Methods: This section describes the various ways to create a query with Spring Data JPA. Query Lookup Strategies: The JPA module supports defining a query manually as a String or having it being derived from the method name. Derived queries with the predicates IsStartingWith , StartingWith , StartsWith , IsEndingWith , EndingWith , EndsWith , IsNotContaining , NotContaining , NotContains , IsContaining , Containing , Contains the respective arguments for these queries will get sanitized. This means if the arguments actually contain characters recognized by LIKE as wildcards these will get escaped so they match only as literals. The escape character used can be configured by setting the escapeCharacter of the @EnableJpaRepositories annotation. Compare with Using SpEL Expressions(#jpa.query.spel-expressions) . Declared Queries: Although getting a query derived from the method name is quite convenient, one might face the situation in which either the method name parser does not support the keyword one wants to use or the method name would get unnecessarily ugly. So you can either use JPA named queries through a naming convention (see Using JPA Named Queries(#jpa.query-methods.named-queries) for more information) or rather annotate your query method with @Query (see Using @Query(#jpa.query-methods.at-query) for details). Query Creation: Generally, the query creation mechanism for JPA works as described in Query Methods(https://docs.spring.io/spring-data/commons/reference/repositories/query-methods.html) . The following example shows what a JPA query method translates into: Example 1. Query creation from method names public interface UserRepository extends Repository<User, Long> { List<User> findByEmailAddressAndLastname(String emailAddress, String lastname); } We create a query using the JPA criteria API from this, but, essentially, this translates into the following query: select u from User u where u.emailAddress = ?1 and u.lastname = ?2 . Spring Data JPA does a property check and traverses nested properties, as described in Property Expressions(../repositories/query-methods-details.html#repositories.query-methods.query-property-expressions) . The following table describes the keywords supported for JPA and what a method containing that keyword translates to: Table 1. Supported keywords inside method names Keyword Sample JPQL snippet Distinct findDistinctByLastnameAndFirstname select distinct …​ where x.lastname = ?1 and x.firstname = ?2 And findByLastnameAndFirstname … where x.lastname = ?1 and x.firstname = ?2 Or findByLastnameOrFirstname … where x.lastname = ?1 or x.firstname = ?2 Is , Equals findByFirstname , findByFirstnameIs , findByFirstnameEquals … where x.firstname = ?1 Between findByStartDateBetween … where x.startDate between ?1 and ?2 LessThan findByAgeLessThan … where x.age < ?1 LessThanEqual findByAgeLessThanEqual … where x.age <= ?1 GreaterThan findByAgeGreaterThan … where x.age > ?1 GreaterThanEqual findByAgeGreaterThanEqual … where x.age >= ?1 After findByStartDateAfter … where x.startDate > ?1 Before findByStartDateBefore … where x.startDate < ?1 IsNull , Null findByAge(Is)Null … where x.age is null IsNotNull , NotNull findByAge(Is)NotNull … where x.age is not null Like findByFirstnameLike … where x.firstname like ?1 NotLike findByFirstnameNotLike … where x.firstname not like ?1 StartingWith findByFirstnameStartingWith … where x.firstname like ?1 (parameter bound with appended % ) EndingWith findByFirstnameEndingWith … where x.firstname like ?1 (parameter bound with prepended % ) Containing findByFirstnameContaining … where x.firstname like ?1 (parameter bound wrapped in % ) OrderBy findByAgeOrderByLastnameDesc … where x.age = ?1 order by x.lastname desc Not findByLastnameNot … where x.lastname <> ?1 In findByAgeIn(Collection<Age> ages) … where x.age in ?1 NotIn findByAgeNotIn(Collection<Age> ages) … where x.age not in ?1 True findByActiveTrue() … where x.active = true False findByActiveFalse() … where x.active = false IgnoreCase findByFirstnameIgnoreCase … where UPPER(x.firstname) = UPPER(?1) In and NotIn also take any subclass of Collection as a parameter as well as arrays or varargs. For other syntactical versions of the same logical operator, check Repository query keywords(../repositories/query-keywords-reference.html) . DISTINCT can be tricky and not always producing the results you expect. For example, select distinct u from User u will produce a complete different result than select distinct u.lastname from User u . In the first case, since you are including User.id , nothing will duplicated, hence you’ll get the whole table, and it would be of User objects. However, that latter query would narrow the focus to just User.lastname and find all unique last names for that table. This would also yield a List<String> result set instead of a List<User> result set. countDistinctByLastname(String lastname) can also produce unexpected results. Spring Data JPA will derive select count(distinct u.id) from User u where u.lastname = ?1 . Again, since u.id won’t hit any duplicates, this query will count up all the users that had the binding last name. Which would the same as countByLastname(String lastname) ! What is the point of this query anyway? To find the number of people with a given last name? To find the number of distinct people with that binding last name? To find the number of distinct last names ? (That last one is an entirely different query!) Using distinct sometimes requires writing the query by hand and using @Query to best capture the information you seek, since you also may be needing a projection to capture the result set. Annotation-based Configuration: Annotation-based configuration has the advantage of not needing another configuration file to be edited, lowering maintenance effort. You pay for that benefit by the need to recompile your domain class for every new query declaration. Example 2. Annotation-based named query configuration @Entity @NamedQuery(name = ""User.findByEmailAddress"", query = ""select u from User u where u.emailAddress = ?1"") public class User { } Using JPA Named Queries: The examples use the <named-query /> element and @NamedQuery annotation. The queries for these configuration elements have to be defined in the JPA query language. Of course, you can use <named-native-query /> or @NamedNativeQuery too. These elements let you define the query in native SQL by losing the database platform independence. XML Named Query Definition: To use XML configuration, add the necessary <named-query /> element to the orm.xml JPA configuration file located in the META-INF folder of your classpath. Automatic invocation of named queries is enabled by using some defined naming convention. For more details, see below. Example 3. XML named query configuration <named-query name=""User.findByLastname""> <query>select u from User u where u.lastname = ?1</query> </named-query> The query has a special name that is used to resolve it at runtime. Declaring Interfaces: To allow these named queries, specify the UserRepository as follows: Example 4. Query method declaration in UserRepository public interface UserRepository extends JpaRepository<User, Long> { List<User> findByLastname(String lastname); User findByEmailAddress(String emailAddress); } Spring Data tries to resolve a call to these methods to a named query, starting with the simple name of the configured domain class, followed by the method name separated by a dot. So the preceding example would use the named queries defined earlier instead of trying to create a query from the method name. Using @Query: Using named queries to declare queries for entities is a valid approach and works fine for a small number of queries. As the queries themselves are tied to the Java method that runs them, you can actually bind them directly by using the Spring Data JPA @Query annotation rather than annotating them to the domain class. This frees the domain class from persistence specific information and co-locates the query to the repository interface. Queries annotated to the query method take precedence over queries defined using @NamedQuery or named queries declared in orm.xml . The following example shows a query created with the @Query annotation: Example 5. Declare query at the query method using @Query public interface UserRepository extends JpaRepository<User, Long> { @Query(""select u from User u where u.emailAddress = ?1"") User findByEmailAddress(String emailAddress); } Applying a QueryRewriter: Sometimes, no matter how many features you try to apply, it seems impossible to get Spring Data JPA to apply every thing you’d like to a query before it is sent to the EntityManager . You have the ability to get your hands on the query, right before it’s sent to the EntityManager and ""rewrite"" it. That is, you can make any alterations at the last moment. Example 6. Declare a QueryRewriter using @Query public interface MyRepository extends JpaRepository<User, Long> { @Query(value = ""select original_user_alias.* from SD_USER original_user_alias"", nativeQuery = true, queryRewriter = MyQueryRewriter.class) List<User> findByNativeQuery(String param); @Query(value = ""select original_user_alias from User original_user_alias"", queryRewriter = MyQueryRewriter.class) List<User> findByNonNativeQuery(String param); } This example shows both a native (pure SQL) rewriter as well as a JPQL query, both leveraging the same QueryRewriter . In this scenario, Spring Data JPA will look for a bean registered in the application context of the corresponding type. You can write a query rewriter like this: Example 7. Example QueryRewriter public class MyQueryRewriter implements QueryRewriter { @Override public String rewrite(String query, Sort sort) { return query.replaceAll(""original_user_alias"", ""rewritten_user_alias""); } } You have to ensure your QueryRewriter is registered in the application context, whether it’s by applying one of Spring Framework’s @Component -based annotations, or having it as part of a @Bean method inside an @Configuration class. Another option is to have the repository itself implement the interface. Example 8. Repository that provides the QueryRewriter public interface MyRepository extends JpaRepository<User, Long>, QueryRewriter { @Query(value = ""select original_user_alias.* from SD_USER original_user_alias"", nativeQuery = true, queryRewriter = MyRepository.class) List<User> findByNativeQuery(String param); @Query(value = ""select original_user_alias from User original_user_alias"", queryRewriter = MyRepository.class) List<User> findByNonNativeQuery(String param); @Override default String rewrite(String query, Sort sort) { return query.replaceAll(""original_user_alias"", ""rewritten_user_alias""); } } Depending on what you’re doing with your QueryRewriter , it may be advisable to have more than one, each registered with the application context. In a CDI-based environment, Spring Data JPA will search the BeanManager for instances of your implementation of QueryRewriter . Using Advanced LIKE Expressions: The query running mechanism for manually defined queries created with @Query allows the definition of advanced LIKE expressions inside the query definition, as shown in the following example: Example 9. Advanced like expressions in @Query public interface UserRepository extends JpaRepository<User, Long> { @Query(""select u from User u where u.firstname like %?1"") List<User> findByFirstnameEndsWith(String firstname); } In the preceding example, the LIKE delimiter character ( % ) is recognized, and the query is transformed into a valid JPQL query (removing the % ). Upon running the query, the parameter passed to the method call gets augmented with the previously recognized LIKE pattern. Native Queries: The @Query annotation allows for running native queries by setting the nativeQuery flag to true, as shown in the following example: Example 10. Declare a native query at the query method using @Query public interface UserRepository extends JpaRepository<User, Long> { @Query(value = ""SELECT * FROM USERS WHERE EMAIL_ADDRESS = ?1"", nativeQuery = true) User findByEmailAddress(String emailAddress); } Spring Data JPA does not currently support dynamic sorting for native queries, because it would have to manipulate the actual query declared, which it cannot do reliably for native SQL. You can, however, use native queries for pagination by specifying the count query yourself, as shown in the following example: Example 11. Declare native count queries for pagination at the query method by using @Query public interface UserRepository extends JpaRepository<User, Long> { @Query(value = ""SELECT * FROM USERS WHERE LASTNAME = ?1"", countQuery = ""SELECT count(*) FROM USERS WHERE LASTNAME = ?1"", nativeQuery = true) Page<User> findByLastname(String lastname, Pageable pageable); } A similar approach also works with named native queries, by adding the .count suffix to a copy of your query. You probably need to register a result set mapping for your count query, though. Using Sort: Sorting can be done by either providing a PageRequest or by using Sort directly. The properties actually used within the Order instances of Sort need to match your domain model, which means they need to resolve to either a property or an alias used within the query. The JPQL defines this as a state field path expression. Using any non-referenceable path expression leads to an Exception . However, using Sort together with @Query(#jpa.query-methods.at-query) lets you sneak in non-path-checked Order instances containing functions within the ORDER BY clause. This is possible because the Order is appended to the given query string. By default, Spring Data JPA rejects any Order instance containing function calls, but you can use JpaSort.unsafe to add potentially unsafe ordering. The following example uses Sort and JpaSort , including an unsafe option on JpaSort : Example 12. Using Sort and JpaSort public interface UserRepository extends JpaRepository<User, Long> { @Query(""select u from User u where u.lastname like ?1%"") List<User> findByAndSort(String lastname, Sort sort); @Query(""select u.id, LENGTH(u.firstname) as fn_len from User u where u.lastname like ?1%"") List<Object[]> findByAsArrayAndSort(String lastname, Sort sort); } repo.findByAndSort(""lannister"", Sort.by(""firstname"")); (1) repo.findByAndSort(""stark"", Sort.by(""LENGTH(firstname)"")); (2) repo.findByAndSort(""targaryen"", JpaSort.unsafe(""LENGTH(firstname)"")); (3) repo.findByAsArrayAndSort(""bolton"", Sort.by(""fn_len"")); (4) 1 Valid Sort expression pointing to property in domain model. 2 Invalid Sort containing function call. Throws Exception. 3 Valid Sort containing explicitly unsafe Order . 4 Valid Sort expression pointing to aliased function. Scrolling Large Query Results: When working with large data sets, scrolling(#repositories.scrolling) can help to process those results efficiently without loading all results into memory. You have multiple options to consume large query results: Paging(../repositories/query-methods-details.html#repositories.paging-and-sorting) . You have learned in the previous chapter about Pageable and PageRequest . Offset-based scrolling(#repositories.scrolling.offset) . This is a lighter variant than paging because it does not require the total result count. Keyset-baset scrolling(#repositories.scrolling.keyset) . This method avoids the shortcomings of offset-based result retrieval by leveraging database indexes(https://use-the-index-luke.com/no-offset) . Read more on which method to use best(#repositories.scrolling.guidance) for your particular arrangement. You can use the Scroll API with query methods, Query-by-Example(../repositories/query-by-example.html) , and Querydsl(../repositories/core-extensions.html#core.extensions.querydsl) . Scrolling with String-based query methods is not yet supported. Scrolling is also not supported using stored @Procedure query methods. Using Named Parameters: By default, Spring Data JPA uses position-based parameter binding, as described in all the preceding examples. This makes query methods a little error-prone when refactoring regarding the parameter position. To solve this issue, you can use @Param annotation to give a method parameter a concrete name and bind the name in the query, as shown in the following example: Example 13. Using named parameters public interface UserRepository extends JpaRepository<User, Long> { @Query(""select u from User u where u.firstname = :firstname or u.lastname = :lastname"") User findByLastnameOrFirstname(@Param(""lastname"") String lastname, @Param(""firstname"") String firstname); } The method parameters are switched according to their order in the defined query. As of version 4, Spring fully supports Java 8’s parameter name discovery based on the -parameters compiler flag. By using this flag in your build as an alternative to debug information, you can omit the @Param annotation for named parameters. Using SpEL Expressions: As of Spring Data JPA release 1.4, we support the usage of restricted SpEL template expressions in manually defined queries that are defined with @Query . Upon the query being run, these expressions are evaluated against a predefined set of variables. Spring Data JPA supports a variable called entityName . Its usage is select x from #{#entityName} x . It inserts the entityName of the domain type associated with the given repository. The entityName is resolved as follows: If the domain type has set the name property on the @Entity annotation, it is used. Otherwise, the simple class-name of the domain type is used. The following example demonstrates one use case for the #{#entityName} expression in a query string where you want to define a repository interface with a query method and a manually defined query: Example 14. Using SpEL expressions in repository query methods - entityName @Entity public class User { @Id @GeneratedValue Long id; String lastname; } public interface UserRepository extends JpaRepository<User,Long> { @Query(""select u from #{#entityName} u where u.lastname = ?1"") List<User> findByLastname(String lastname); } To avoid stating the actual entity name in the query string of a @Query annotation, you can use the #{#entityName} variable. The entityName can be customized by using the @Entity annotation. Customizations in orm.xml are not supported for the SpEL expressions. Of course, you could have just used User in the query declaration directly, but that would require you to change the query as well. The reference to #entityName picks up potential future remappings of the User class to a different entity name (for example, by using @Entity(name = ""MyUser"") . Another use case for the #{#entityName} expression in a query string is if you want to define a generic repository interface with specialized repository interfaces for a concrete domain type. To not repeat the definition of custom query methods on the concrete interfaces, you can use the entity name expression in the query string of the @Query annotation in the generic repository interface, as shown in the following example: Example 15. Using SpEL expressions in repository query methods - entityName with inheritance @MappedSuperclass public abstract class AbstractMappedType { … String attribute; } @Entity public class ConcreteType extends AbstractMappedType { … } @NoRepositoryBean public interface MappedTypeRepository<T extends AbstractMappedType> extends Repository<T, Long> { @Query(""select t from #{#entityName} t where t.attribute = ?1"") List<T> findAllByAttribute(String attribute); } public interface ConcreteRepository extends MappedTypeRepository<ConcreteType> { … } In the preceding example, the MappedTypeRepository interface is the common parent interface for a few domain types extending AbstractMappedType . It also defines the generic findAllByAttribute(…) method, which can be used on instances of the specialized repository interfaces. If you now invoke findByAllAttribute(…) on ConcreteRepository , the query becomes select t from ConcreteType t where t.attribute = ?1 . SpEL expressions to manipulate arguments may also be used to manipulate method arguments. In these SpEL expressions the entity name is not available, but the arguments are. They can be accessed by name or index as demonstrated in the following example. Example 16. Using SpEL expressions in repository query methods - accessing arguments. @Query(""select u from User u where u.firstname = ?1 and u.firstname=?#{[0]} and u.emailAddress = ?#{principal.emailAddress}"") List<User> findByFirstnameAndCurrentUserWithCustomQuery(String firstname); For like -conditions one often wants to append % to the beginning or the end of a String valued parameter. This can be done by appending or prefixing a bind parameter marker or a SpEL expression with % . Again the following example demonstrates this. Example 17. Using SpEL expressions in repository query methods - wildcard shortcut. @Query(""select u from User u where u.lastname like %:#{[0]}% and u.lastname like %:lastname%"") List<User> findByLastnameWithSpelExpression(@Param(""lastname"") String lastname); When using like -conditions with values that are coming from a not secure source the values should be sanitized so they can’t contain any wildcards and thereby allow attackers to select more data than they should be able to. For this purpose the escape(String) method is made available in the SpEL context. It prefixes all instances of _ and % in the first argument with the single character from the second argument. In combination with the escape clause of the like expression available in JPQL and standard SQL this allows easy cleaning of bind parameters. Example 18. Using SpEL expressions in repository query methods - sanitizing input values. @Query(""select u from User u where u.firstname like %?#{escape([0])}% escape ?#{escapeCharacter()}"") List<User> findContainingEscaped(String namePart); Given this method declaration in a repository interface findContainingEscaped(""Peter_"") will find Peter_Parker but not Peter Parker . The escape character used can be configured by setting the escapeCharacter of the @EnableJpaRepositories annotation. Note that the method escape(String) available in the SpEL context will only escape the SQL and JPQL standard wildcards _ and % . If the underlying database or the JPA implementation supports additional wildcards these will not get escaped. Other Methods: Spring Data JPA offers many ways to build queries. But sometimes, your query may simply be too complicated for the techniques offered. In that situation, consider: If you haven’t already, simply write the query yourself using @Query(#jpa.query-methods.at-query) . If that doesn’t fit your needs, consider implementing a custom implementation(../repositories/custom-implementations.html) . This lets you register a method in your repository while leaving the implementation completely up to you. This gives you the ability to: Talk directly to the EntityManager (writing pure HQL/JPQL/EQL/native SQL or using the Criteria API ) Leverage Spring Framework’s JdbcTemplate (native SQL) Use another 3rd-party database toolkit. Another option is putting your query inside the database and then using either Spring Data JPA’s @StoredProcedure annotation(stored-procedures.html) or if it’s a database function using the @Query annotation(#jpa.query-methods.at-query) and invoking it with a CALL . These tactics may be most effective when you need maximum control of your query, while still letting Spring Data JPA provide resource management. Modifying Queries: All the previous sections describe how to declare queries to access a given entity or collection of entities. You can add custom modifying behavior by using the custom method facilities described in Custom Implementations for Spring Data Repositories(https://docs.spring.io/spring-data/commons/reference/repositories/custom-implementations.html) . As this approach is feasible for comprehensive custom functionality, you can modify queries that only need parameter binding by annotating the query method with @Modifying , as shown in the following example: Example 19. Declaring manipulating queries @Modifying @Query(""update User u set u.firstname = ?1 where u.lastname = ?2"") int setFixedFirstnameFor(String firstname, String lastname); Doing so triggers the query annotated to the method as an updating query instead of a selecting one. As the EntityManager might contain outdated entities after the execution of the modifying query, we do not automatically clear it (see the JavaDoc(https://jakarta.ee/specifications/persistence/2.2/apidocs/javax/persistence/entitymanager) of EntityManager.clear() for details), since this effectively drops all non-flushed changes still pending in the EntityManager . If you wish the EntityManager to be cleared automatically, you can set the @Modifying annotation’s clearAutomatically attribute to true . The @Modifying annotation is only relevant in combination with the @Query annotation. Derived query methods or custom methods do not require this annotation. Derived Delete Queries: Spring Data JPA also supports derived delete queries that let you avoid having to declare the JPQL query explicitly, as shown in the following example: Example 20. Using a derived delete query interface UserRepository extends Repository<User, Long> { void deleteByRoleId(long roleId); @Modifying @Query(""delete from User u where u.role.id = ?1"") void deleteInBulkByRoleId(long roleId); } Although the deleteByRoleId(…) method looks like it basically produces the same result as the deleteInBulkByRoleId(…) , there is an important difference between the two method declarations in terms of the way they are run. As the name suggests, the latter method issues a single JPQL query (the one defined in the annotation) against the database. This means even currently loaded instances of User do not see lifecycle callbacks invoked. To make sure lifecycle queries are actually invoked, an invocation of deleteByRoleId(…) runs a query and then deletes the returned instances one by one, so that the persistence provider can actually invoke @PreRemove callbacks on those entities. In fact, a derived delete query is a shortcut for running the query and then calling CrudRepository.delete(Iterable<User> users) on the result and keeping behavior in sync with the implementations of other delete(…) methods in CrudRepository . Applying Query Hints: To apply JPA query hints to the queries declared in your repository interface, you can use the @QueryHints annotation. It takes an array of JPA @QueryHint annotations plus a boolean flag to potentially disable the hints applied to the additional count query triggered when applying pagination, as shown in the following example: Example 21. Using QueryHints with a repository method public interface UserRepository extends Repository<User, Long> { @QueryHints(value = { @QueryHint(name = ""name"", value = ""value"")}, forCounting = false) Page<User> findByLastname(String lastname, Pageable pageable); } The preceding declaration would apply the configured @QueryHint for that actually query but omit applying it to the count query triggered to calculate the total number of pages. Adding Comments to Queries: Sometimes, you need to debug a query based upon database performance. The query your database administrator shows you may look VERY different than what you wrote using @Query , or it may look nothing like what you presume Spring Data JPA has generated regarding a custom finder or if you used query by example. To make this process easier, you can insert custom comments into almost any JPA operation, whether its a query or other operation by applying the @Meta annotation. Example 22. Apply @Meta annotation to repository operations public interface RoleRepository extends JpaRepository<Role, Integer> { @Meta(comment = ""find roles by name"") List<Role> findByName(String name); @Override @Meta(comment = ""find roles using QBE"") <S extends Role> List<S> findAll(Example<S> example); @Meta(comment = ""count roles for a given name"") long countByName(String name); @Override @Meta(comment = ""exists based on QBE"") <S extends Role> boolean exists(Example<S> example); } This sample repository has a mixture of custom finders as well as overriding the inherited operations from JpaRepository . Either way, the @Meta annotation lets you add a comment that will be inserted into queries before they are sent to the database. It’s also important to note that this feature isn’t confined solely to queries. It extends to the count and exists operations. And while not shown, it also extends to certain delete operations. While we have attempted to apply this feature everywhere possible, some operations of the underlying EntityManager don’t support comments. For example, entityManager.createQuery() is clearly documented as supporting comments, but entityManager.find() operations do not. Neither JPQL logging nor SQL logging is a standard in JPA, so each provider requires custom configuration, as shown the sections below. Activating Hibernate comments: To activate query comments in Hibernate, you must set hibernate.use_sql_comments to true . If you are using Java-based configuration settings, this can be done like this: Example 23. Java-based JPA configuration @Bean public Properties jpaProperties() { Properties properties = new Properties(); properties.setProperty(""hibernate.use_sql_comments"", ""true""); return properties; } If you have a persistence.xml file, you can apply it there: Example 24. persistence.xml -based configuration <persistence-unit name=""my-persistence-unit""> ...registered classes... <properties> <property name=""hibernate.use_sql_comments"" value=""true"" /> </properties> </persistence-unit> Finally, if you are using Spring Boot, then you can set it up inside your application.properties file: Example 25. Spring Boot property-based configuration spring.jpa.properties.hibernate.use_sql_comments=true Activating EclipseLink comments: To activate query comments in EclipseLink, you must set eclipselink.logging.level.sql to FINE . If you are using Java-based configuration settings, this can be done like this: Example 26. Java-based JPA configuration @Bean public Properties jpaProperties() { Properties properties = new Properties(); properties.setProperty(""eclipselink.logging.level.sql"", ""FINE""); return properties; } If you have a persistence.xml file, you can apply it there: Example 27. persistence.xml -based configuration <persistence-unit name=""my-persistence-unit""> ...registered classes... <properties> <property name=""eclipselink.logging.level.sql"" value=""FINE"" /> </properties> </persistence-unit> Finally, if you are using Spring Boot, then you can set it up inside your application.properties file: Example 28. Spring Boot property-based configuration spring.jpa.properties.eclipselink.logging.level.sql=FINE Configuring Fetch- and LoadGraphs: The JPA 2.1 specification introduced support for specifying Fetch- and LoadGraphs that we also support with the @EntityGraph annotation, which lets you reference a @NamedEntityGraph definition. You can use that annotation on an entity to configure the fetch plan of the resulting query. The type ( Fetch or Load ) of the fetching can be configured by using the type attribute on the @EntityGraph annotation. See the JPA 2.1 Spec 3.7.4 for further reference. The following example shows how to define a named entity graph on an entity: Example 29. Defining a named entity graph on an entity. @Entity @NamedEntityGraph(name = ""GroupInfo.detail"", attributeNodes = @NamedAttributeNode(""members"")) public class GroupInfo { // default fetch mode is lazy. @ManyToMany List<GroupMember> members = new ArrayList<GroupMember>(); … } The following example shows how to reference a named entity graph on a repository query method: Example 30. Referencing a named entity graph definition on a repository query method. public interface GroupRepository extends CrudRepository<GroupInfo, String> { @EntityGraph(value = ""GroupInfo.detail"", type = EntityGraphType.LOAD) GroupInfo getByGroupName(String name); } It is also possible to define ad hoc entity graphs by using @EntityGraph . The provided attributePaths are translated into the according EntityGraph without needing to explicitly add @NamedEntityGraph to your domain types, as shown in the following example: Example 31. Using AD-HOC entity graph definition on an repository query method. public interface GroupRepository extends CrudRepository<GroupInfo, String> { @EntityGraph(attributePaths = { ""members"" }) GroupInfo getByGroupName(String name); } Scrolling: Scrolling is a more fine-grained approach to iterate through larger results set chunks. Scrolling consists of a stable sort, a scroll type (Offset- or Keyset-based scrolling) and result limiting. You can define simple sorting expressions by using property names and define static result limiting using the Top or First keyword(../repositories/query-methods-details.html#repositories.limit-query-result) through query derivation. You can concatenate expressions to collect multiple criteria into one expression. Scroll queries return a Window<T> that allows obtaining the element’s scroll position to fetch the next Window<T> until your application has consumed the entire query result. Similar to consuming a Java Iterator<List<…>> by obtaining the next batch of results, query result scrolling lets you access the a ScrollPosition through Window.positionAt(…​) . Window<User> users = repository.findFirst10ByLastnameOrderByFirstname(""Doe"", ScrollPosition.offset()); do { for (User u : users) { // consume the user } // obtain the next Scroll users = repository.findFirst10ByLastnameOrderByFirstname(""Doe"", users.positionAt(users.size() - 1)); } while (!users.isEmpty() && users.hasNext()); The ScrollPosition identifies the exact position of an element with the entire query result. Query execution treats the position parameter exclusive , results will start after the given position. ScrollPosition#offset() and ScrollPosition#keyset() as special incarnations of a ScrollPosition indicating the start of a scroll operation. WindowIterator provides a utility to simplify scrolling across Window s by removing the need to check for the presence of a next Window and applying the ScrollPosition . WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(ScrollPosition.offset()); while (users.hasNext()) { User u = users.next(); // consume the user } Scrolling using Offset: Offset scrolling uses similar to pagination, an Offset counter to skip a number of results and let the data source only return results beginning at the given Offset. This simple mechanism avoids large results being sent to the client application. However, most databases require materializing the full query result before your server can return the results. Example 32. Using OffsetScrollPosition with Repository Query Methods interface UserRepository extends Repository<User, Long> { Window<User> findFirst10ByLastnameOrderByFirstname(String lastname, OffsetScrollPosition position); } WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(OffsetScrollPosition.initial()); (1) 1 Start with no offset to include the element at position 0 . There is a difference between ScollPosition.offset() and ScollPosition.offset(0L) . The former indicates the start of scroll operation, pointing to no specific offset whereas the latter identifies the first element (at position 0 ) of the result. Given the exclusive nature of scrolling, using ScollPosition.offset(0) skips the first element and translate to an offset of 1 . Scrolling using Keyset-Filtering: Offset-based requires most databases require materializing the entire result before your server can return the results. So while the client only sees the portion of the requested results, your server needs to build the full result, which causes additional load. Keyset-Filtering approaches result subset retrieval by leveraging built-in capabilities of your database aiming to reduce the computation and I/O requirements for individual queries. This approach maintains a set of keys to resume scrolling by passing keys into the query, effectively amending your filter criteria. The core idea of Keyset-Filtering is to start retrieving results using a stable sorting order. Once you want to scroll to the next chunk, you obtain a ScrollPosition that is used to reconstruct the position within the sorted result. The ScrollPosition captures the keyset of the last entity within the current Window . To run the query, reconstruction rewrites the criteria clause to include all sort fields and the primary key so that the database can leverage potential indexes to run the query. The database needs only constructing a much smaller result from the given keyset position without the need to fully materialize a large result and then skipping results until reaching a particular offset. Keyset-Filtering requires the keyset properties (those used for sorting) to be non-nullable. This limitation applies due to the store specific null value handling of comparison operators as well as the need to run queries against an indexed source. Keyset-Filtering on nullable properties will lead to unexpected results. Using KeysetScrollPosition with Repository Query Methods interface UserRepository extends Repository<User, Long> { Window<User> findFirst10ByLastnameOrderByFirstname(String lastname, KeysetScrollPosition position); } WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(ScrollPosition.keyset()); (1) 1 Start at the very beginning and do not apply additional filtering. Keyset-Filtering works best when your database contains an index that matches the sort fields, hence a static sort works well. Scroll queries applying Keyset-Filtering require to the properties used in the sort order to be returned by the query, and these must be mapped in the returned entity. You can use interface and DTO projections, however make sure to include all properties that you’ve sorted by to avoid keyset extraction failures. When specifying your Sort order, it is sufficient to include sort properties relevant to your query; You do not need to ensure unique query results if you do not want to. The keyset query mechanism amends your sort order by including the primary key (or any remainder of composite primary keys) to ensure each query result is unique."
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/projections.html","Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. It is important to note that Class-based projections(#projections.dtos) with JPQL is limited to constructor expressions in your JPQL expression, e.g. SELECT new com.example.NamesOnly(u.firstname, u.lastname) from User u . (Note the usage of a FQDN for the DTO type!) This JPQL expression can be used in @Query annotations as well where you define any named queries. And it’s important to point out that class-based projections do not work with native queries AT ALL. As a workaround you may use named queries with ResultSetMapping or the Hibernate-specific ResultListTransformer(https://docs.jboss.org/hibernate/orm/6.6/javadocs/org/hibernate/query/ResultListTransformer.html) Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> ."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/stored-procedures.html","Stored Procedures: The JPA 2.1 specification introduced support for calling stored procedures by using the JPA criteria query API. We Introduced the @Procedure annotation for declaring stored procedure metadata on a repository method. The examples to follow use the following stored procedure: Example 1. The definition of the plus1inout procedure in HSQL DB. /; DROP procedure IF EXISTS plus1inout /; CREATE procedure plus1inout (IN arg int, OUT res int) BEGIN ATOMIC set res = arg + 1; END /; Metadata for stored procedures can be configured by using the NamedStoredProcedureQuery annotation on an entity type. Example 2. StoredProcedure metadata definitions on an entity. @Entity @NamedStoredProcedureQuery(name = ""User.plus1"", procedureName = ""plus1inout"", parameters = { @StoredProcedureParameter(mode = ParameterMode.IN, name = ""arg"", type = Integer.class), @StoredProcedureParameter(mode = ParameterMode.OUT, name = ""res"", type = Integer.class) }) public class User {} Note that @NamedStoredProcedureQuery has two different names for the stored procedure. name is the name JPA uses. procedureName is the name the stored procedure has in the database. You can reference stored procedures from a repository method in multiple ways. The stored procedure to be called can either be defined directly by using the value or procedureName attribute of the @Procedure annotation. This refers directly to the stored procedure in the database and ignores any configuration via @NamedStoredProcedureQuery . Alternatively you may specify the @NamedStoredProcedureQuery.name attribute as the @Procedure.name attribute. If neither value , procedureName nor name is configured, the name of the repository method is used as the name attribute. The following example shows how to reference an explicitly mapped procedure: Example 3. Referencing explicitly mapped procedure with name ""plus1inout"" in database. @Procedure(""plus1inout"") Integer explicitlyNamedPlus1inout(Integer arg); The following example is equivalent to the previous one but uses the procedureName alias: Example 4. Referencing implicitly mapped procedure with name ""plus1inout"" in database via procedureName alias. @Procedure(procedureName = ""plus1inout"") Integer callPlus1InOut(Integer arg); The following is again equivalent to the previous two but using the method name instead of an explicite annotation attribute. Example 5. Referencing implicitly mapped named stored procedure ""User.plus1"" in EntityManager by using the method name. @Procedure Integer plus1inout(@Param(""arg"") Integer arg); The following example shows how to reference a stored procedure by referencing the @NamedStoredProcedureQuery.name attribute. Example 6. Referencing explicitly mapped named stored procedure ""User.plus1IO"" in EntityManager . @Procedure(name = ""User.plus1IO"") Integer entityAnnotatedCustomNamedProcedurePlus1IO(@Param(""arg"") Integer arg); If the stored procedure getting called has a single out parameter that parameter may be returned as the return value of the method. If there are multiple out parameters specified in a @NamedStoredProcedureQuery annotation those can be returned as a Map with the key being the parameter name given in the @NamedStoredProcedureQuery annotation."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/specifications.html","Specifications: JPA 2 introduces a criteria API that you can use to build queries programmatically. By writing a criteria , you define the where clause of a query for a domain class. Taking another step back, these criteria can be regarded as a predicate over the entity that is described by the JPA criteria API constraints. Spring Data JPA takes the concept of a specification from Eric Evans' book, “Domain Driven Design”, following the same semantics and providing an API to define such specifications with the JPA criteria API. To support specifications, you can extend your repository interface with the JpaSpecificationExecutor interface, as follows: public interface CustomerRepository extends CrudRepository<Customer, Long>, JpaSpecificationExecutor<Customer> { … } The additional interface has methods that let you run specifications in a variety of ways. For example, the findAll method returns all entities that match the specification, as shown in the following example: List<T> findAll(Specification<T> spec); The Specification interface is defined as follows: public interface Specification<T> { Predicate toPredicate(Root<T> root, CriteriaQuery<?> query, CriteriaBuilder builder); } Specifications can easily be used to build an extensible set of predicates on top of an entity that then can be combined and used with JpaRepository without the need to declare a query (method) for every needed combination, as shown in the following example: Example 1. Specifications for a Customer public class CustomerSpecs { public static Specification<Customer> isLongTermCustomer() { return (root, query, builder) -> { LocalDate date = LocalDate.now().minusYears(2); return builder.lessThan(root.get(Customer_.createdAt), date); }; } public static Specification<Customer> hasSalesOfMoreThan(MonetaryAmount value) { return (root, query, builder) -> { // build query here }; } } The Customer_ type is a metamodel type generated using the JPA Metamodel generator (see the Hibernate implementation’s documentation for an example(https://docs.jboss.org/hibernate/jpamodelgen/1.0/reference/en-US/html_single/#whatisit) ). So the expression, Customer_.createdAt , assumes the Customer has a createdAt attribute of type Date . Besides that, we have expressed some criteria on a business requirement abstraction level and created executable Specifications . So a client might use a Specification as follows: Example 2. Using a simple Specification List<Customer> customers = customerRepository.findAll(isLongTermCustomer()); Why not create a query for this kind of data access? Using a single Specification does not gain a lot of benefit over a plain query declaration. The power of specifications really shines when you combine them to create new Specification objects. You can achieve this through the default methods of Specification we provide to build expressions similar to the following: Example 3. Combined Specifications MonetaryAmount amount = new MonetaryAmount(200.0, Currencies.DOLLAR); List<Customer> customers = customerRepository.findAll( isLongTermCustomer().or(hasSalesOfMoreThan(amount))); Specification offers some “glue-code” default methods to chain and combine Specification instances. These methods let you extend your data access layer by creating new Specification implementations and combining them with already existing implementations. And with JPA 2.1, the CriteriaBuilder API introduced CriteriaDelete . This is provided through JpaSpecificationExecutor’s `delete(Specification) API. Example 4. Using a Specification to delete entries. Specification<User> ageLessThan18 = (root, query, cb) -> cb.lessThan(root.get(""age"").as(Integer.class), 18) userRepository.delete(ageLessThan18); The Specification builds up a criteria where the age field (cast as an integer) is less than 18 . Passed on to the userRepository , it will use JPA’s CriteriaDelete feature to generate the right DELETE operation. It then returns the number of entities deleted."
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/query-by-example.html","Query by Example: Introduction: This chapter provides an introduction to Query by Example and explains how to use it. Query by Example (QBE) is a user-friendly querying technique with a simple interface. It allows dynamic query creation and does not require you to write queries that contain field names. In fact, Query by Example does not require you to write queries by using store-specific query languages at all. This chapter explains the core concepts of Query by Example. The information is pulled from the Spring Data Commons module. Depending on your database, String matching support can be limited. Usage: The Query by Example API consists of four parts: Probe: The actual example of a domain object with populated fields. ExampleMatcher : The ExampleMatcher carries details on how to match particular fields. It can be reused across multiple Examples. Example : An Example consists of the probe and the ExampleMatcher . It is used to create the query. FetchableFluentQuery : A FetchableFluentQuery offers a fluent API, that allows further customization of a query derived from an Example . Using the fluent API lets you specify ordering projection and result processing for your query. Query by Example is well suited for several use cases: Querying your data store with a set of static or dynamic constraints. Frequent refactoring of the domain objects without worrying about breaking existing queries. Working independently of the underlying data store API. Query by Example also has several limitations: No support for nested or grouped property constraints, such as firstname = ?0 or (firstname = ?1 and lastname = ?2) . Store-specific support on string matching. Depending on your databases, String matching can support starts/contains/ends/regex for strings. Exact matching for other property types. Before getting started with Query by Example, you need to have a domain object. To get started, create an interface for your repository, as shown in the following example: Sample Person object public class Person { @Id private String id; private String firstname; private String lastname; private Address address; // … getters and setters omitted } The preceding example shows a simple domain object. You can use it to create an Example . By default, fields having null values are ignored, and strings are matched by using the store specific defaults. Inclusion of properties into a Query by Example criteria is based on nullability. Properties using primitive types ( int , double , …) are always included unless the ExampleMatcher ignores the property path(#query-by-example.matchers) . Examples can be built by either using the of factory method or by using ExampleMatcher(#query-by-example.matchers) . Example is immutable. The following listing shows a simple Example: Example 1. Simple Example Person person = new Person(); (1) person.setFirstname(""Dave""); (2) Example<Person> example = Example.of(person); (3) 1 Create a new instance of the domain object. 2 Set the properties to query. 3 Create the Example . You can run the example queries by using repositories. To do so, let your repository interface extend QueryByExampleExecutor<T> . The following listing shows an excerpt from the QueryByExampleExecutor interface: The QueryByExampleExecutor public interface QueryByExampleExecutor<T> { <S extends T> S findOne(Example<S> example); <S extends T> Iterable<S> findAll(Example<S> example); // … more functionality omitted. } Example Matchers: Examples are not limited to default settings. You can specify your own defaults for string matching, null handling, and property-specific settings by using the ExampleMatcher , as shown in the following example: Example 2. Example matcher with customized matching Person person = new Person(); (1) person.setFirstname(""Dave""); (2) ExampleMatcher matcher = ExampleMatcher.matching() (3) .withIgnorePaths(""lastname"") (4) .withIncludeNullValues() (5) .withStringMatcher(StringMatcher.ENDING); (6) Example<Person> example = Example.of(person, matcher); (7) 1 Create a new instance of the domain object. 2 Set properties. 3 Create an ExampleMatcher to expect all values to match. It is usable at this stage even without further configuration. 4 Construct a new ExampleMatcher to ignore the lastname property path. 5 Construct a new ExampleMatcher to ignore the lastname property path and to include null values. 6 Construct a new ExampleMatcher to ignore the lastname property path, to include null values, and to perform suffix string matching. 7 Create a new Example based on the domain object and the configured ExampleMatcher . By default, the ExampleMatcher expects all values set on the probe to match. If you want to get results matching any of the predicates defined implicitly, use ExampleMatcher.matchingAny() . You can specify behavior for individual properties (such as ""firstname"" and ""lastname"" or, for nested properties, ""address.city""). You can tune it with matching options and case sensitivity, as shown in the following example: Configuring matcher options ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", endsWith()) .withMatcher(""lastname"", startsWith().ignoreCase()); } Another way to configure matcher options is to use lambdas (introduced in Java 8). This approach creates a callback that asks the implementor to modify the matcher. You need not return the matcher, because configuration options are held within the matcher instance. The following example shows a matcher that uses lambdas: Configuring matcher options with lambdas ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", match -> match.endsWith()) .withMatcher(""firstname"", match -> match.startsWith()); } Queries created by Example use a merged view of the configuration. Default matching settings can be set at the ExampleMatcher level, while individual settings can be applied to particular property paths. Settings that are set on ExampleMatcher are inherited by property path settings unless they are defined explicitly. Settings on a property patch have higher precedence than default settings. The following table describes the scope of the various ExampleMatcher settings: Table 1. Scope of ExampleMatcher settings Setting Scope Null-handling ExampleMatcher String matching ExampleMatcher and property path Ignoring properties Property path Case sensitivity ExampleMatcher and property path Value transformation Property path Fluent API: QueryByExampleExecutor offers one more method, which we did not mention so far: <S extends T, R> R findBy(Example<S> example, Function<FluentQuery.FetchableFluentQuery<S>, R> queryFunction) . As with other methods, it executes a query derived from an Example . However, with the second argument, you can control aspects of that execution that you cannot dynamically control otherwise. You do so by invoking the various methods of the FetchableFluentQuery in the second argument. sortBy lets you specify an ordering for your result. as lets you specify the type to which you want the result to be transformed. project limits the queried attributes. first , firstValue , one , oneValue , all , page , stream , count , and exists define what kind of result you get and how the query behaves when more than the expected number of results are available. Use the fluent API to get the last of potentially many results, ordered by lastname. Optional<Person> match = repository.findBy(example, q -> q .sortBy(Sort.by(""lastname"").descending()) .first() ); In Spring Data JPA, you can use Query by Example with Repositories, as shown in the following example: Example 3. Query by Example using a Repository public interface PersonRepository extends JpaRepository<Person, String> { … } public class PersonService { @Autowired PersonRepository personRepository; public List<Person> findPeople(Person probe) { return personRepository.findAll(Example.of(probe)); } } Currently, only SingularAttribute properties can be used for property matching. The property specifier accepts property names (such as firstname and lastname ). You can navigate by chaining properties together with dots ( address.city ). You can also tune it with matching options and case sensitivity. The following table shows the various StringMatcher options that you can use and the result of using them on a field named firstname : Table 2. StringMatcher options Matching Logical result DEFAULT (case-sensitive) firstname = ?0 DEFAULT (case-insensitive) LOWER(firstname) = LOWER(?0) EXACT (case-sensitive) firstname = ?0 EXACT (case-insensitive) LOWER(firstname) = LOWER(?0) STARTING (case-sensitive) firstname like ?0 + '%' STARTING (case-insensitive) LOWER(firstname) like LOWER(?0) + '%' ENDING (case-sensitive) firstname like '%' + ?0 ENDING (case-insensitive) LOWER(firstname) like '%' + LOWER(?0) CONTAINING (case-sensitive) firstname like '%' + ?0 + '%' CONTAINING (case-insensitive) LOWER(firstname) like '%' + LOWER(?0) + '%' Regex-matching is not supported by JPA."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/transactions.html","Transactionality: By default, methods inherited from CrudRepository inherit the transactional configuration from SimpleJpaRepository(../api/java/org/springframework/data/jpa/repository/support/SimpleJpaRepository.html) . For read operations, the transaction configuration readOnly flag is set to true . All others are configured with a plain @Transactional so that default transaction configuration applies. Repository methods that are backed by transactional repository fragments inherit the transactional attributes from the actual fragment method. If you need to tweak transaction configuration for one of the methods declared in a repository, redeclare the method in your repository interface, as follows: Example 1. Custom transaction configuration for CRUD public interface UserRepository extends CrudRepository<User, Long> { @Override @Transactional(timeout = 10) public List<User> findAll(); // Further query method declarations } Doing so causes the findAll() method to run with a timeout of 10 seconds and without the readOnly flag. Another way to alter transactional behaviour is to use a facade or service implementation that (typically) covers more than one repository. Its purpose is to define transactional boundaries for non-CRUD operations. The following example shows how to use such a facade for more than one repository: Example 2. Using a facade to define transactions for multiple repository calls @Service public class UserManagementImpl implements UserManagement { private final UserRepository userRepository; private final RoleRepository roleRepository; public UserManagementImpl(UserRepository userRepository, RoleRepository roleRepository) { this.userRepository = userRepository; this.roleRepository = roleRepository; } @Transactional public void addRoleToAllUsers(String roleName) { Role role = roleRepository.findByName(roleName); for (User user : userRepository.findAll()) { user.addRole(role); userRepository.save(user); } } } This example causes call to addRoleToAllUsers(…) to run inside a transaction (participating in an existing one or creating a new one if none are already running). The transaction configuration at the repositories is then neglected, as the outer transaction configuration determines the actual one used. Note that you must activate <tx:annotation-driven /> or use @EnableTransactionManagement explicitly to get annotation-based configuration of facades to work. This example assumes you use component scanning. Note that the call to save is not strictly necessary from a JPA point of view, but should still be there in order to stay consistent to the repository abstraction offered by Spring Data. Transactional query methods: Declared query methods (including default methods) do not get any transaction configuration applied by default. To run those methods transactionally, use @Transactional at the repository interface you define, as shown in the following example: Example 3. Using @Transactional at query methods @Transactional(readOnly = true) interface UserRepository extends JpaRepository<User, Long> { List<User> findByLastname(String lastname); @Modifying @Transactional @Query(""delete from User u where u.active = false"") void deleteInactiveUsers(); } Typically, you want the readOnly flag to be set to true , as most of the query methods only read data. In contrast to that, deleteInactiveUsers() makes use of the @Modifying annotation and overrides the transaction configuration. Thus, the method runs with the readOnly flag set to false . You can use transactions for read-only queries and mark them as such by setting the readOnly flag. Doing so does not, however, act as a check that you do not trigger a manipulating query (although some databases reject INSERT and UPDATE statements inside a read-only transaction). The readOnly flag is instead propagated as a hint to the underlying JDBC driver for performance optimizations. Furthermore, Spring performs some optimizations on the underlying JPA provider. For example, when used with Hibernate, the flush mode is set to NEVER when you configure a transaction as readOnly , which causes Hibernate to skip dirty checks (a noticeable improvement on large object trees)."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/locking.html","Locking: To specify the lock mode to be used, you can use the @Lock annotation on query methods, as shown in the following example: Example 1. Defining lock metadata on query methods interface UserRepository extends Repository<User, Long> { // Plain query method @Lock(LockModeType.READ) List<User> findByLastname(String lastname); } This method declaration causes the query being triggered to be equipped with a LockModeType of READ . You can also define locking for CRUD methods by redeclaring them in your repository interface and adding the @Lock annotation, as shown in the following example: Example 2. Defining lock metadata on CRUD methods interface UserRepository extends Repository<User, Long> { // Redeclaration of a CRUD method @Lock(LockModeType.READ) List<User> findAll(); }"
"https://docs.spring.io/spring-data/jpa/reference/3.3/auditing.html","Auditing: Basics: Spring Data provides sophisticated support to transparently keep track of who created or changed an entity and when the change happened.To benefit from that functionality, you have to equip your entity classes with auditing metadata that can be defined either using annotations or by implementing an interface. Additionally, auditing has to be enabled either through Annotation configuration or XML configuration to register the required infrastructure components. Please refer to the store-specific section for configuration samples. Applications that only track creation and modification dates are not required do make their entities implement AuditorAware(#auditing.auditor-aware) . Annotation-based Auditing Metadata: We provide @CreatedBy and @LastModifiedBy to capture the user who created or modified the entity as well as @CreatedDate and @LastModifiedDate to capture when the change happened. An audited entity class Customer { @CreatedBy private User user; @CreatedDate private Instant createdDate; // … further properties omitted } As you can see, the annotations can be applied selectively, depending on which information you want to capture. The annotations, indicating to capture when changes are made, can be used on properties of type JDK8 date and time types, long , Long , and legacy Java Date and Calendar . Auditing metadata does not necessarily need to live in the root level entity but can be added to an embedded one (depending on the actual store in use), as shown in the snippet below. Audit metadata in embedded entity class Customer { private AuditMetadata auditingMetadata; // … further properties omitted } class AuditMetadata { @CreatedBy private User user; @CreatedDate private Instant createdDate; } Interface-based Auditing Metadata: In case you do not want to use annotations to define auditing metadata, you can let your domain class implement the Auditable interface. It exposes setter methods for all of the auditing properties. AuditorAware: In case you use either @CreatedBy or @LastModifiedBy , the auditing infrastructure somehow needs to become aware of the current principal. To do so, we provide an AuditorAware<T> SPI interface that you have to implement to tell the infrastructure who the current user or system interacting with the application is. The generic type T defines what type the properties annotated with @CreatedBy or @LastModifiedBy have to be. The following example shows an implementation of the interface that uses Spring Security’s Authentication object: Implementation of AuditorAware based on Spring Security class SpringSecurityAuditorAware implements AuditorAware<User> { @Override public Optional<User> getCurrentAuditor() { return Optional.ofNullable(SecurityContextHolder.getContext()) .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getPrincipal) .map(User.class::cast); } } The implementation accesses the Authentication object provided by Spring Security and looks up the custom UserDetails instance that you have created in your UserDetailsService implementation. We assume here that you are exposing the domain user through the UserDetails implementation but that, based on the Authentication found, you could also look it up from anywhere. ReactiveAuditorAware: When using reactive infrastructure you might want to make use of contextual information to provide @CreatedBy or @LastModifiedBy information. We provide an ReactiveAuditorAware<T> SPI interface that you have to implement to tell the infrastructure who the current user or system interacting with the application is. The generic type T defines what type the properties annotated with @CreatedBy or @LastModifiedBy have to be. The following example shows an implementation of the interface that uses reactive Spring Security’s Authentication object: Implementation of ReactiveAuditorAware based on Spring Security class SpringSecurityAuditorAware implements ReactiveAuditorAware<User> { @Override public Mono<User> getCurrentAuditor() { return ReactiveSecurityContextHolder.getContext() .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getPrincipal) .map(User.class::cast); } } The implementation accesses the Authentication object provided by Spring Security and looks up the custom UserDetails instance that you have created in your UserDetailsService implementation. We assume here that you are exposing the domain user through the UserDetails implementation but that, based on the Authentication found, you could also look it up from anywhere. There is also a convenience base class, AbstractAuditable , which you can extend to avoid the need to manually implement the interface methods. Doing so increases the coupling of your domain classes to Spring Data, which might be something you want to avoid. Usually, the annotation-based way of defining auditing metadata is preferred as it is less invasive and more flexible. General Auditing Configuration: Spring Data JPA ships with an entity listener that can be used to trigger the capturing of auditing information. First, you must register the AuditingEntityListener to be used for all entities in your persistence contexts inside your orm.xml file, as shown in the following example: Example 1. Auditing configuration orm.xml <persistence-unit-metadata> <persistence-unit-defaults> <entity-listeners> <entity-listener class=""….data.jpa.domain.support.AuditingEntityListener"" /> </entity-listeners> </persistence-unit-defaults> </persistence-unit-metadata> You can also enable the AuditingEntityListener on a per-entity basis by using the @EntityListeners annotation, as follows: @Entity @EntityListeners(AuditingEntityListener.class) public class MyEntity { } The auditing feature requires spring-aspects.jar to be on the classpath. With orm.xml suitably modified and spring-aspects.jar on the classpath, activating auditing functionality is a matter of adding the Spring Data JPA auditing namespace element to your configuration, as follows: Example 2. Activating auditing using XML configuration <jpa:auditing auditor-aware-ref=""yourAuditorAwareBean"" /> As of Spring Data JPA 1.5, you can enable auditing by annotating a configuration class with the @EnableJpaAuditing annotation. You must still modify the orm.xml file and have spring-aspects.jar on the classpath. The following example shows how to use the @EnableJpaAuditing annotation: Example 3. Activating auditing with Java configuration @Configuration @EnableJpaAuditing class Config { @Bean public AuditorAware<AuditableUser> auditorProvider() { return new AuditorAwareImpl(); } } If you expose a bean of type AuditorAware to the ApplicationContext , the auditing infrastructure automatically picks it up and uses it to determine the current user to be set on domain types. If you have multiple implementations registered in the ApplicationContext , you can select the one to be used by explicitly setting the auditorAwareRef attribute of @EnableJpaAuditing ."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/misc-merging-persistence-units.html","Merging persistence units: Spring supports having multiple persistence units. Sometimes, however, you might want to modularize your application but still make sure that all these modules run inside a single persistence unit. To enable that behavior, Spring Data JPA offers a PersistenceUnitManager implementation that automatically merges persistence units based on their name, as shown in the following example: Example 1. Using MergingPersistenceUnitmanager <bean class=""….LocalContainerEntityManagerFactoryBean""> <property name=""persistenceUnitManager""> <bean class=""….MergingPersistenceUnitManager"" /> </property> </bean> Classpath Scanning for @Entity Classes and JPA Mapping Files: A plain JPA setup requires all annotation-mapped entity classes to be listed in orm.xml . The same applies to XML mapping files. Spring Data JPA provides a ClasspathScanningPersistenceUnitPostProcessor that gets a base package configured and optionally takes a mapping filename pattern. It then scans the given package for classes annotated with @Entity or @MappedSuperclass , loads the configuration files that match the filename pattern, and hands them to the JPA configuration. The post-processor must be configured as follows: Example 2. Using ClasspathScanningPersistenceUnitPostProcessor <bean class=""….LocalContainerEntityManagerFactoryBean""> <property name=""persistenceUnitPostProcessors""> <list> <bean class=""org.springframework.data.jpa.support.ClasspathScanningPersistenceUnitPostProcessor""> <constructor-arg value=""com.acme.domain"" /> <property name=""mappingFileNamePattern"" value=""**/*Mapping.xml"" /> </bean> </list> </property> </bean> As of Spring 3.1, a package to scan can be configured on the LocalContainerEntityManagerFactoryBean directly to enable classpath scanning for entity classes. See the JavaDoc(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/orm/jpa/LocalContainerEntityManagerFactoryBean.html#setPackagesToScan(java.lang.String…​)$$) for details."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/jpd-misc-cdi-integration.html","CDI Integration: Instances of the repository interfaces are usually created by a container, for which Spring is the most natural choice when working with Spring Data. Spring offers sophisticated support for creating bean instances, as documented in Creating Repository Instances(https://docs.spring.io/spring-data/commons/reference/repositories/create-instances.html) . As of version 1.1.0, Spring Data JPA ships with a custom CDI extension that allows using the repository abstraction in CDI environments. The extension is part of the JAR. To activate it, include the Spring Data JPA JAR on your classpath. You can now set up the infrastructure by implementing a CDI Producer for the EntityManagerFactory and EntityManager , as shown in the following example: class EntityManagerFactoryProducer { @Produces @ApplicationScoped public EntityManagerFactory createEntityManagerFactory() { return Persistence.createEntityManagerFactory(""my-persistence-unit""); } public void close(@Disposes EntityManagerFactory entityManagerFactory) { entityManagerFactory.close(); } @Produces @RequestScoped public EntityManager createEntityManager(EntityManagerFactory entityManagerFactory) { return entityManagerFactory.createEntityManager(); } public void close(@Disposes EntityManager entityManager) { entityManager.close(); } } The necessary setup can vary depending on the JavaEE environment. You may need to do nothing more than redeclare a EntityManager as a CDI bean, as follows: class CdiConfig { @Produces @RequestScoped @PersistenceContext public EntityManager entityManager; } In the preceding example, the container has to be capable of creating JPA EntityManagers itself. All the configuration does is re-export the JPA EntityManager as a CDI bean. The Spring Data JPA CDI extension picks up all available EntityManager instances as CDI beans and creates a proxy for a Spring Data repository whenever a bean of a repository type is requested by the container. Thus, obtaining an instance of a Spring Data repository is a matter of declaring an @Inject property, as shown in the following example: class RepositoryClient { @Inject PersonRepository repository; public void businessMethod() { List<Person> people = repository.findAll(); } }"
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableJpaRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableJpaRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" /> Using JpaContext in Custom Implementations: When working with multiple EntityManager instances and custom repository implementations(#repositories.custom-implementations) , you need to wire the correct EntityManager into the repository implementation class. You can do so by explicitly naming the EntityManager in the @PersistenceContext annotation or, if the EntityManager is @Autowired , by using @Qualifier . As of Spring Data JPA 1.9, Spring Data JPA includes a class called JpaContext that lets you obtain the EntityManager by managed domain class, assuming it is managed by only one of the EntityManager instances in the application. The following example shows how to use JpaContext in a custom repository: Example 5. Using JpaContext in a custom repository implementation class UserRepositoryImpl implements UserRepositoryCustom { private final EntityManager em; @Autowired public UserRepositoryImpl(JpaContext context) { this.em = context.getEntityManagerByManagedType(User.class); } … } The advantage of this approach is that, if the domain type gets assigned to a different persistence unit, the repository does not have to be touched to alter the reference to the persistence unit."
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/null-handling.html","Null Handling of Repository Methods: As of Spring Data 2.0, repository CRUD methods that return an individual aggregate instance use Java 8’s Optional to indicate the potential absence of a value. Besides that, Spring Data supports returning the following wrapper types on query methods: com.google.common.base.Optional scala.Option io.vavr.control.Option Alternatively, query methods can choose not to use a wrapper type at all. The absence of a query result is then indicated by returning null . Repository methods returning collections, collection alternatives, wrappers, and streams are guaranteed never to return null but rather the corresponding empty representation. See “ Repository query return types(query-return-types-reference.html) ” for details. Nullability Annotations: You can express nullability constraints for repository methods by using Spring Framework’s nullability annotations(https://docs.spring.io/spring-framework/reference/6.1/core/null-safety.html) . They provide a tooling-friendly approach and opt-in null checks during runtime, as follows: @NonNullApi(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNullApi.html) : Used on the package level to declare that the default behavior for parameters and return values is, respectively, neither to accept nor to produce null values. @NonNull(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNull.html) : Used on a parameter or return value that must not be null (not needed on a parameter and return value where @NonNullApi applies). @Nullable(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/Nullable.html) : Used on a parameter or return value that can be null . Spring annotations are meta-annotated with JSR 305(https://jcp.org/en/jsr/detail?id=305) annotations (a dormant but widely used JSR). JSR 305 meta-annotations let tooling vendors (such as IDEA(https://www.jetbrains.com/help/idea/nullable-and-notnull-annotations.html) , Eclipse(https://help.eclipse.org/latest/index.jsp?topic=/org.eclipse.jdt.doc.user/tasks/task-using_external_null_annotations.htm) , and Kotlin(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) ) provide null-safety support in a generic way, without having to hard-code support for Spring annotations. To enable runtime checking of nullability constraints for query methods, you need to activate non-nullability on the package level by using Spring’s @NonNullApi in package-info.java , as shown in the following example: Declaring Non-nullability in package-info.java @org.springframework.lang.NonNullApi package com.acme; Once non-null defaulting is in place, repository query method invocations get validated at runtime for nullability constraints. If a query result violates the defined constraint, an exception is thrown. This happens when the method would return null but is declared as non-nullable (the default with the annotation defined on the package in which the repository resides). If you want to opt-in to nullable results again, selectively use @Nullable on individual methods. Using the result wrapper types mentioned at the start of this section continues to work as expected: an empty result is translated into the value that represents absence. The following example shows a number of the techniques just described: Using different nullability constraints package com.acme; (1) import org.springframework.lang.Nullable; interface UserRepository extends Repository<User, Long> { User getByEmailAddress(EmailAddress emailAddress); (2) @Nullable User findByEmailAddress(@Nullable EmailAddress emailAdress); (3) Optional<User> findOptionalByEmailAddress(EmailAddress emailAddress); (4) } 1 The repository resides in a package (or sub-package) for which we have defined non-null behavior. 2 Throws an EmptyResultDataAccessException when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . 3 Returns null when the query does not produce a result. Also accepts null as the value for emailAddress . 4 Returns Optional.empty() when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . Nullability in Kotlin-based Repositories: Kotlin has the definition of nullability constraints(https://kotlinlang.org/docs/reference/null-safety.html) baked into the language. Kotlin code compiles to bytecode, which does not express nullability constraints through method signatures but rather through compiled-in metadata. Make sure to include the kotlin-reflect JAR in your project to enable introspection of Kotlin’s nullability constraints. Spring Data repositories use the language mechanism to define those constraints to apply the same runtime checks, as follows: Using nullability constraints on Kotlin repositories interface UserRepository : Repository<User, String> { fun findByUsername(username: String): User (1) fun findByFirstname(firstname: String?): User? (2) } 1 The method defines both the parameter and the result as non-nullable (the Kotlin default). The Kotlin compiler rejects method invocations that pass null to the method. If the query yields an empty result, an EmptyResultDataAccessException is thrown. 2 This method accepts null for the firstname parameter and returns null if the query does not produce a result."
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/core-extensions.html","Spring Data Extensions: This section documents a set of Spring Data extensions that enable Spring Data usage in a variety of contexts. Currently, most of the integration is targeted towards Spring MVC. Querydsl Extension: Querydsl(http://www.querydsl.com/) is a framework that enables the construction of statically typed SQL-like queries through its fluent API. Several Spring Data modules offer integration with Querydsl through QuerydslPredicateExecutor , as the following example shows: QuerydslPredicateExecutor interface public interface QuerydslPredicateExecutor<T> { Optional<T> findById(Predicate predicate); (1) Iterable<T> findAll(Predicate predicate); (2) long count(Predicate predicate); (3) boolean exists(Predicate predicate); (4) // … more functionality omitted. } 1 Finds and returns a single entity matching the Predicate . 2 Finds and returns all entities matching the Predicate . 3 Returns the number of entities matching the Predicate . 4 Returns whether an entity that matches the Predicate exists. To use the Querydsl support, extend QuerydslPredicateExecutor on your repository interface, as the following example shows: Querydsl integration on repositories interface UserRepository extends CrudRepository<User, Long>, QuerydslPredicateExecutor<User> { } The preceding example lets you write type-safe queries by using Querydsl Predicate instances, as the following example shows: Predicate predicate = user.firstname.equalsIgnoreCase(""dave"") .and(user.lastname.startsWithIgnoreCase(""mathews"")); userRepository.findAll(predicate); Web support: Spring Data modules that support the repository programming model ship with a variety of web support. The web related components require Spring MVC JARs to be on the classpath. Some of them even provide integration with Spring HATEOAS(https://github.com/spring-projects/spring-hateoas) . In general, the integration support is enabled by using the @EnableSpringDataWebSupport annotation in your JavaConfig configuration class, as the following example shows: Enabling Spring Data web support Java XML @Configuration @EnableWebMvc @EnableSpringDataWebSupport class WebConfiguration {} <bean class=""org.springframework.data.web.config.SpringDataWebConfiguration"" /> <!-- If you use Spring HATEOAS, register this one *instead* of the former --> <bean class=""org.springframework.data.web.config.HateoasAwareSpringDataWebConfiguration"" /> The @EnableSpringDataWebSupport annotation registers a few components. We discuss those later in this section. It also detects Spring HATEOAS on the classpath and registers integration components (if present) for it as well. Basic Web Support: Enabling Spring Data web support in XML The configuration shown in the previous section(#core.web) registers a few basic components: A Using the DomainClassConverter Class(#core.web.basic.domain-class-converter) to let Spring MVC resolve instances of repository-managed domain classes from request parameters or path variables. HandlerMethodArgumentResolver(#core.web.basic.paging-and-sorting) implementations to let Spring MVC resolve Pageable and Sort instances from request parameters. Jackson Modules(#core.web.basic.jackson-mappers) to de-/serialize types like Point and Distance , or store specific ones, depending on the Spring Data Module used. Using the DomainClassConverter Class: The DomainClassConverter class lets you use domain types in your Spring MVC controller method signatures directly so that you need not manually lookup the instances through the repository, as the following example shows: A Spring MVC controller using domain types in method signatures @Controller @RequestMapping(""/users"") class UserController { @RequestMapping(""/{id}"") String showUserForm(@PathVariable(""id"") User user, Model model) { model.addAttribute(""user"", user); return ""userForm""; } } The method receives a User instance directly, and no further lookup is necessary. The instance can be resolved by letting Spring MVC convert the path variable into the id type of the domain class first and eventually access the instance through calling findById(…) on the repository instance registered for the domain type. Currently, the repository has to implement CrudRepository to be eligible to be discovered for conversion. HandlerMethodArgumentResolvers for Pageable and Sort: The configuration snippet shown in the previous section(#core.web.basic.domain-class-converter) also registers a PageableHandlerMethodArgumentResolver as well as an instance of SortHandlerMethodArgumentResolver . The registration enables Pageable and Sort as valid controller method arguments, as the following example shows: Using Pageable as a controller method argument @Controller @RequestMapping(""/users"") class UserController { private final UserRepository repository; UserController(UserRepository repository) { this.repository = repository; } @RequestMapping String showUsers(Model model, Pageable pageable) { model.addAttribute(""users"", repository.findAll(pageable)); return ""users""; } } The preceding method signature causes Spring MVC try to derive a Pageable instance from the request parameters by using the following default configuration: Table 1. Request parameters evaluated for Pageable instances page Page you want to retrieve. 0-indexed and defaults to 0. size Size of the page you want to retrieve. Defaults to 20. sort Properties that should be sorted by in the format property,property(,ASC|DESC)(,IgnoreCase) . The default sort direction is case-sensitive ascending. Use multiple sort parameters if you want to switch direction or case sensitivity — for example, ?sort=firstname&sort=lastname,asc&sort=city,ignorecase . To customize this behavior, register a bean that implements the PageableHandlerMethodArgumentResolverCustomizer interface or the SortHandlerMethodArgumentResolverCustomizer interface, respectively. Its customize() method gets called, letting you change settings, as the following example shows: @Bean SortHandlerMethodArgumentResolverCustomizer sortCustomizer() { return s -> s.setPropertyDelimiter(""<-->""); } If setting the properties of an existing MethodArgumentResolver is not sufficient for your purpose, extend either SpringDataWebConfiguration or the HATEOAS-enabled equivalent, override the pageableResolver() or sortResolver() methods, and import your customized configuration file instead of using the @Enable annotation. If you need multiple Pageable or Sort instances to be resolved from the request (for multiple tables, for example), you can use Spring’s @Qualifier annotation to distinguish one from another. The request parameters then have to be prefixed with ${qualifier}_ . The following example shows the resulting method signature: String showUsers(Model model, @Qualifier(""thing1"") Pageable first, @Qualifier(""thing2"") Pageable second) { … } You have to populate thing1_page , thing2_page , and so on. The default Pageable passed into the method is equivalent to a PageRequest.of(0, 20) , but you can customize it by using the @PageableDefault annotation on the Pageable parameter. Creating JSON representations for Page: It’s common for Spring MVC controllers to try to ultimately render a representation of a Spring Data page to clients. While one could simply return Page instances from handler methods to let Jackson render them as is, we strongly recommend against this as the underlying implementation class PageImpl is a domain type. This means we might want or have to change its API for unrelated reasons, and such changes might alter the resulting JSON representation in a breaking way. With Spring Data 3.1, we started hinting at the problem by issuing a warning log describing the problem. We still ultimately recommend to leverage the integration with Spring HATEOAS(#core.web.pageables) for a fully stable and hypermedia-enabled way of rendering pages that easily allow clients to navigate them. But as of version 3.3 Spring Data ships a page rendering mechanism that is convenient to use but does not require the inclusion of Spring HATEOAS. Using Spring Data' PagedModel: At its core, the support consists of a simplified version of Spring HATEOAS' PagedModel (the Spring Data one located in the org.springframework.data.web package). It can be used to wrap Page instances and result in a simplified representation that reflects the structure established by Spring HATEOAS but omits the navigation links. import org.springframework.data.web.PagedModel; @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") PagedModel<?> page(Pageable pageable) { return new PagedModel<>(repository.findAll(pageable)); (1) } } 1 Wraps the Page instance into a PagedModel . This will result in a JSON structure looking like this: { ""content"" : [ … // Page content rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } Note how the document contains a page field exposing the essential pagination metadata. Globally enabling simplified Page rendering: If you don’t want to change all your existing controllers to add the mapping step to return PagedModel instead of Page you can enable the automatic translation of PageImpl instances into PagedModel by tweaking @EnableSpringDataWebSupport as follows: @EnableSpringDataWebSupport(pageSerializationMode = VIA_DTO) class MyConfiguration { } This will allow your controller to still return Page instances and they will automatically be rendered into the simplified representation: @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") Page<?> page(Pageable pageable) { return repository.findAll(pageable); } } Hypermedia Support for Page and Slice: Spring HATEOAS ships with a representation model class ( PagedModel / SlicedModel ) that allows enriching the content of a Page or Slice instance with the necessary Page / Slice metadata as well as links to let the clients easily navigate the pages. The conversion of a Page to a PagedModel is done by an implementation of the Spring HATEOAS RepresentationModelAssembler interface, called the PagedResourcesAssembler . Similarly Slice instances can be converted to a SlicedModel using a SlicedResourcesAssembler . The following example shows how to use a PagedResourcesAssembler as a controller method argument, as the SlicedResourcesAssembler works exactly the same: Using a PagedResourcesAssembler as controller method argument @Controller class PersonController { private final PersonRepository repository; // Constructor omitted @GetMapping(""/people"") HttpEntity<PagedModel<Person>> people(Pageable pageable, PagedResourcesAssembler assembler) { Page<Person> people = repository.findAll(pageable); return ResponseEntity.ok(assembler.toModel(people)); } } Enabling the configuration, as shown in the preceding example, lets the PagedResourcesAssembler be used as a controller method argument. Calling toModel(…) on it has the following effects: The content of the Page becomes the content of the PagedModel instance. The PagedModel object gets a PageMetadata instance attached, and it is populated with information from the Page and the underlying Pageable . The PagedModel may get prev and next links attached, depending on the page’s state. The links point to the URI to which the method maps. The pagination parameters added to the method match the setup of the PageableHandlerMethodArgumentResolver to make sure the links can be resolved later. Assume we have 30 Person instances in the database. You can now trigger a request ( GET localhost:8080/people(http://localhost:8080/people) ) and see output similar to the following: { ""links"" : [ { ""rel"" : ""next"", ""href"" : ""http://localhost:8080/persons?page=1&size=20"" } ], ""content"" : [ … // 20 Person instances rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } The JSON envelope format shown here doesn’t follow any formally specified structure and it’s not guaranteed stable and we might change it at any time. It’s highly recommended to enable the rendering as a hypermedia-enabled, official media type, supported by Spring HATEOAS, like HAL(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#mediatypes.hal) . Those can be activated by using its @EnableHypermediaSupport annotation. Find more information in the Spring HATEOAS reference documentation(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#configuration.at-enable) . The assembler produced the correct URI and also picked up the default configuration to resolve the parameters into a Pageable for an upcoming request. This means that, if you change that configuration, the links automatically adhere to the change. By default, the assembler points to the controller method it was invoked in, but you can customize that by passing a custom Link to be used as base to build the pagination links, which overloads the PagedResourcesAssembler.toModel(…) method. Spring Data Jackson Modules: The core module, and some of the store specific ones, ship with a set of Jackson Modules for types, like org.springframework.data.geo.Distance and org.springframework.data.geo.Point , used by the Spring Data domain. Those Modules are imported once web support(#core.web) is enabled and com.fasterxml.jackson.databind.ObjectMapper is available. During initialization SpringDataJacksonModules , like the SpringDataJacksonConfiguration , get picked up by the infrastructure, so that the declared com.fasterxml.jackson.databind.Module s are made available to the Jackson ObjectMapper . Data binding mixins for the following domain types are registered by the common infrastructure. org.springframework.data.geo.Distance org.springframework.data.geo.Point org.springframework.data.geo.Box org.springframework.data.geo.Circle org.springframework.data.geo.Polygon The individual module may provide additional SpringDataJacksonModules . Please refer to the store specific section for more details. Web Databinding Support: You can use Spring Data projections (described in Projections(projections.html) ) to bind incoming request payloads by using either JSONPath(https://goessner.net/articles/JsonPath/) expressions (requires Jayway JsonPath(https://github.com/json-path/JsonPath) ) or XPath(https://www.w3.org/TR/xpath-31/) expressions (requires XmlBeam(https://xmlbeam.org/) ), as the following example shows: HTTP payload binding using JSONPath or XPath expressions @ProjectedPayload public interface UserPayload { @XBRead(""//firstname"") @JsonPath(""$..firstname"") String getFirstname(); @XBRead(""/lastname"") @JsonPath({ ""$.lastname"", ""$.user.lastname"" }) String getLastname(); } You can use the type shown in the preceding example as a Spring MVC handler method argument or by using ParameterizedTypeReference on one of methods of the RestTemplate . The preceding method declarations would try to find firstname anywhere in the given document. The lastname XML lookup is performed on the top-level of the incoming document. The JSON variant of that tries a top-level lastname first but also tries lastname nested in a user sub-document if the former does not return a value. That way, changes in the structure of the source document can be mitigated easily without having clients calling the exposed methods (usually a drawback of class-based payload binding). Nested projections are supported as described in Projections(projections.html) . If the method returns a complex, non-interface type, a Jackson ObjectMapper is used to map the final value. For Spring MVC, the necessary converters are registered automatically as soon as @EnableSpringDataWebSupport is active and the required dependencies are available on the classpath. For usage with RestTemplate , register a ProjectingJackson2HttpMessageConverter (JSON) or XmlBeamHttpMessageConverter manually. For more information, see the web projection example(https://github.com/spring-projects/spring-data-examples/tree/main/web/projection) in the canonical Spring Data Examples repository(https://github.com/spring-projects/spring-data-examples) . Querydsl Web Support: For those stores that have QueryDSL(http://www.querydsl.com/) integration, you can derive queries from the attributes contained in a Request query string. Consider the following query string: ?firstname=Dave&lastname=Matthews Given the User object from the previous examples, you can resolve a query string to the following value by using the QuerydslPredicateArgumentResolver , as follows: QUser.user.firstname.eq(""Dave"").and(QUser.user.lastname.eq(""Matthews"")) The feature is automatically enabled, along with @EnableSpringDataWebSupport , when Querydsl is found on the classpath. Adding a @QuerydslPredicate to the method signature provides a ready-to-use Predicate , which you can run by using the QuerydslPredicateExecutor . Type information is typically resolved from the method’s return type. Since that information does not necessarily match the domain type, it might be a good idea to use the root attribute of QuerydslPredicate . The following example shows how to use @QuerydslPredicate in a method signature: @Controller class UserController { @Autowired UserRepository repository; @RequestMapping(value = ""/"", method = RequestMethod.GET) String index(Model model, @QuerydslPredicate(root = User.class) Predicate predicate, (1) Pageable pageable, @RequestParam MultiValueMap<String, String> parameters) { model.addAttribute(""users"", repository.findAll(predicate, pageable)); return ""index""; } } 1 Resolve query string arguments to matching Predicate for User . The default binding is as follows: Object on simple properties as eq . Object on collection like properties as contains . Collection on simple properties as in . You can customize those bindings through the bindings attribute of @QuerydslPredicate or by making use of Java 8 default methods and adding the QuerydslBinderCustomizer method to the repository interface, as follows: interface UserRepository extends CrudRepository<User, String>, QuerydslPredicateExecutor<User>, (1) QuerydslBinderCustomizer<QUser> { (2) @Override default void customize(QuerydslBindings bindings, QUser user) { bindings.bind(user.username).first((path, value) -> path.contains(value)) (3) bindings.bind(String.class) .first((StringPath path, String value) -> path.containsIgnoreCase(value)); (4) bindings.excluding(user.password); (5) } } 1 QuerydslPredicateExecutor provides access to specific finder methods for Predicate . 2 QuerydslBinderCustomizer defined on the repository interface is automatically picked up and shortcuts @QuerydslPredicate(bindings=…​) . 3 Define the binding for the username property to be a simple contains binding. 4 Define the default binding for String properties to be a case-insensitive contains match. 5 Exclude the password property from Predicate resolution. You can register a QuerydslBinderCustomizerDefaults bean holding default Querydsl bindings before applying specific bindings from the repository or @QuerydslPredicate . Repository Populators: If you work with the Spring JDBC module, you are probably familiar with the support for populating a DataSource with SQL scripts. A similar abstraction is available on the repositories level, although it does not use SQL as the data definition language because it must be store-independent. Thus, the populators support XML (through Spring’s OXM abstraction) and JSON (through Jackson) to define data with which to populate the repositories. Assume you have a file called data.json with the following content: Data defined in JSON [ { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Dave"", ""lastname"" : ""Matthews"" }, { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Carter"", ""lastname"" : ""Beauford"" } ] You can populate your repositories by using the populator elements of the repository namespace provided in Spring Data Commons. To populate the preceding data to your PersonRepository , declare a populator similar to the following: Declaring a Jackson repository populator <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd""> <repository:jackson2-populator locations=""classpath:data.json"" /> </beans> The preceding declaration causes the data.json file to be read and deserialized by a Jackson ObjectMapper . The type to which the JSON object is unmarshalled is determined by inspecting the _class attribute of the JSON document. The infrastructure eventually selects the appropriate repository to handle the object that was deserialized. To instead use XML to define the data the repositories should be populated with, you can use the unmarshaller-populator element. You configure it to use one of the XML marshaller options available in Spring OXM. See the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/data-access/oxm.html) for details. The following example shows how to unmarshall a repository populator with JAXB: Declaring an unmarshalling repository populator (using JAXB) <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xmlns:oxm=""http://www.springframework.org/schema/oxm"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd http://www.springframework.org/schema/oxm https://www.springframework.org/schema/oxm/spring-oxm.xsd""> <repository:unmarshaller-populator locations=""classpath:data.json"" unmarshaller-ref=""unmarshaller"" /> <oxm:jaxb2-marshaller contextPath=""com.acme"" /> </beans>"
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/jpa/reference/3.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/faq.html","Frequently Asked Questions: Common: I’d like to get more detailed logging information on what methods are called inside JpaRepository for example. How can I gain them? You can make use of CustomizableTraceInterceptor provided by Spring, as shown in the following example: <bean id=""customizableTraceInterceptor"" class="" org.springframework.aop.interceptor.CustomizableTraceInterceptor""> <property name=""enterMessage"" value=""Entering $[methodName]($[arguments])""/> <property name=""exitMessage"" value=""Leaving $[methodName](): $[returnValue]""/> </bean> <aop:config> <aop:advisor advice-ref=""customizableTraceInterceptor"" pointcut=""execution(public * org.springframework.data.jpa.repository.JpaRepository+.*(..))""/> </aop:config> Auditing: I want to use Spring Data JPA auditing capabilities but have my database already configured to set modification and creation date on entities. How can I prevent Spring Data from setting the date programmatically? Set the set-dates attribute of the auditing namespace element to false ."
"https://docs.spring.io/spring-data/jpa/reference/3.3/jpa/glossary.html","Glossary: AOP Aspect oriented programming Commons DBCP Commons DataBase Connection Pools - a library from the Apache foundation that offers pooling implementations of the DataSource interface. CRUD Create, Read, Update, Delete - Basic persistence operations. DAO Data Access Object - Pattern to separate persisting logic from the object to be persisted Dependency Injection Pattern to hand a component’s dependency to the component from outside, freeing the component to lookup the dependent itself. For more information, see (https://en.wikipedia.org/wiki/Dependency_Injection) en.wikipedia.org/wiki/Dependency_Injection(https://en.wikipedia.org/wiki/Dependency_Injection) . EclipseLink Object relational mapper implementing JPA - (https://www.eclipse.org/eclipselink/) www.eclipse.org/eclipselink/(https://www.eclipse.org/eclipselink/) Hibernate Object relational mapper implementing JPA - (https://hibernate.org/) hibernate.org/(https://hibernate.org/) JPA Jakarta Persistence API Spring Java application framework - (https://spring.io/projects/spring-framework/) spring.io/projects/spring-framework(https://spring.io/projects/spring-framework)"
"https://docs.spring.io/spring-data/jpa/reference/3.3/envers.html","Envers: This chapter points out the specialties for repository support for Envers. This builds on the core repository support explained earlier. Make sure you have a sound understanding of the basic concepts explained there. Section Summary: Introduction(envers/introduction.html) Configuration(envers/configuration.html) Usage(envers/usage.html)"
"https://docs.spring.io/spring-data/jpa/reference/3.3/envers/introduction.html","Introduction: What is Spring Data Envers?: Spring Data Envers makes typical Envers queries available in repositories for Spring Data JPA. It differs from other Spring Data modules in that it is always used in combination with another Spring Data Module: Spring Data JPA. What is Envers?: Envers is a Hibernate module(https://hibernate.org/orm/envers/) that adds auditing capabilities to JPA entities. This documentation assumes you are familiar with Envers, just as Spring Data Envers relies on Envers being properly configured."
"https://docs.spring.io/spring-data/jpa/reference/3.3/envers/configuration.html","Configuration: As a starting point for using Spring Data Envers, you need a project with Spring Data JPA on the classpath and an additional spring-data-envers dependency: <dependencies> <!-- other dependency elements omitted --> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-envers</artifactId> <version>3.3.4</version> </dependency> </dependencies> This also brings hibernate-envers into the project as a transient dependency. To enable Spring Data Envers and Spring Data JPA, we need to configure two beans and a special repositoryFactoryBeanClass : @Configuration @EnableEnversRepositories @EnableTransactionManagement public class EnversDemoConfiguration { @Bean public DataSource dataSource() { EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); return builder.setType(EmbeddedDatabaseType.HSQL).build(); } @Bean public LocalContainerEntityManagerFactoryBean entityManagerFactory() { HibernateJpaVendorAdapter vendorAdapter = new HibernateJpaVendorAdapter(); vendorAdapter.setGenerateDdl(true); LocalContainerEntityManagerFactoryBean factory = new LocalContainerEntityManagerFactoryBean(); factory.setJpaVendorAdapter(vendorAdapter); factory.setPackagesToScan(""example.springdata.jpa.envers""); factory.setDataSource(dataSource()); return factory; } @Bean public PlatformTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) { JpaTransactionManager txManager = new JpaTransactionManager(); txManager.setEntityManagerFactory(entityManagerFactory); return txManager; } } To actually use Spring Data Envers, make one or more repositories into a RevisionRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/history/RevisionRepository.html) by adding it as an extended interface: interface PersonRepository extends CrudRepository<Person, Long>, RevisionRepository<Person, Long, Long> (1) {} 1 The first type parameter ( Person ) denotes the entity type, the second ( Long ) denotes the type of the id property, and the last one ( Long ) is the type of the revision number. For Envers in default configuration, the revision number parameter should be Integer or Long . The entity for that repository must be an entity with Envers auditing enabled (that is, it must have an @Audited annotation): @Entity @Audited class Person { @Id @GeneratedValue Long id; String name; @Version Long version; }"
"https://docs.spring.io/spring-data/jpa/reference/3.3/envers/usage.html","Usage: You can now use the methods from RevisionRepository to query the revisions of the entity, as the following test case shows: @ExtendWith(SpringExtension.class) @Import(EnversDemoConfiguration.class) (1) class EnversIntegrationTests { final PersonRepository repository; final TransactionTemplate tx; EnversIntegrationTests(@Autowired PersonRepository repository, @Autowired PlatformTransactionManager tm) { this.repository = repository; this.tx = new TransactionTemplate(tm); } @Test void testRepository() { Person updated = preparePersonHistory(); Revisions<Long, Person> revisions = repository.findRevisions(updated.id); Iterator<Revision<Long, Person>> revisionIterator = revisions.iterator(); checkNextRevision(revisionIterator, ""John"", RevisionType.INSERT); checkNextRevision(revisionIterator, ""Jonny"", RevisionType.UPDATE); checkNextRevision(revisionIterator, null, RevisionType.DELETE); assertThat(revisionIterator.hasNext()).isFalse(); } /** * Checks that the next element in the iterator is a Revision entry referencing a Person * with the given name after whatever change brought that Revision into existence. * <p> * As a side effect the Iterator gets advanced by one element. * * @param revisionIterator the iterator to be tested. * @param name the expected name of the Person referenced by the Revision. * @param revisionType the type of the revision denoting if it represents an insert, update or delete. */ private void checkNextRevision(Iterator<Revision<Long, Person>> revisionIterator, String name, RevisionType revisionType) { assertThat(revisionIterator.hasNext()).isTrue(); Revision<Long, Person> revision = revisionIterator.next(); assertThat(revision.getEntity().name).isEqualTo(name); assertThat(revision.getMetadata().getRevisionType()).isEqualTo(revisionType); } /** * Creates a Person with a couple of changes so it has a non-trivial revision history. * @return the created Person. */ private Person preparePersonHistory() { Person john = new Person(); john.setName(""John""); // create Person saved = tx.execute(__ -> repository.save(john)); assertThat(saved).isNotNull(); saved.setName(""Jonny""); // update Person updated = tx.execute(__ -> repository.save(saved)); assertThat(updated).isNotNull(); // delete tx.executeWithoutResult(__ -> repository.delete(updated)); return updated; } } 1 This references the application context configuration presented earlier (in the Configuration(../envers.html#envers.configuration) section). Further Resources: You can download the Spring Data Envers example in the Spring Data Examples repository(https://github.com/spring-projects/spring-data-examples) and play around with to get a feel for how the library works. You should also check out the Javadoc for RevisionRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/history/RevisionRepository.html) and related classes. You can ask questions at Stackoverflow by using the spring-data-envers tag(https://stackoverflow.com/questions/tagged/spring-data-envers) . The source code and issue tracker for Spring Data Envers is hosted at GitHub(https://github.com/spring-projects/spring-data-jpa) (as a module of Spring Data JPA)."
"https://docs.spring.io/spring-data/ldap/reference/3.3/index.html","Spring Data LDAP: Spring Data LDAP provides repository support for Lightweight Directory Access Protocol (LDAP). It eases development of applications with a consistent programming model that need to access LDAP data sources. Introduction(repositories/introduction.html) Introduction to Spring Data Repositories LDAP(ldap.html) LDAP and LDAP Repositories Wiki(https://github.com/spring-projects/spring-data-commons/wiki) What’s New, Upgrade Notes, Supported Versions, additional cross-version information. Mattias Hellborg Arthursson, Ulrik Sandberg, Eric Dalquist, Keith Barlow, Rob Winch, Mark Paluch, Jay Bryant © 2008-2024 VMware, Inc. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically."
"https://docs.spring.io/spring-data/ldap/reference/3.3/commons/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped."
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/introduction.html","Introduction: This chapter explains the basic foundations of Spring Data repositories. Before continuing to the LDAP specifics, make sure you have a sound understanding of the basic concepts explained here. Working with Spring Data Repositories: The goal of the Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. Spring Data repository documentation and your module This chapter explains the core concepts and interfaces of Spring Data repositories. The information in this chapter is pulled from the Spring Data Commons module. It uses the configuration and code samples for the Jakarta Persistence API (JPA) module. If you want to use XML configuration you should adapt the XML namespace declaration and the types to be extended to the equivalents of the particular module that you use. “ Namespace reference(namespace-reference.html#repositories.namespace-reference) ” covers XML configuration, which is supported across all Spring Data modules that support the repository API. “ Repository query keywords(query-keywords-reference.html) ” covers the query method keywords supported by the repository abstraction in general. For detailed information on the specific features of your module, see the chapter on that module of this document. Section Summary: Core concepts(core-concepts.html) Defining Repository Interfaces(definition.html) Creating Repository Instances(create-instances.html) Defining Query Methods(query-methods-details.html) Projections(projections.html) Custom Repository Implementations(custom-implementations.html) Publishing Events from Aggregate Roots(core-domain-events.html) Spring Data Extensions(core-extensions.html) Null Handling of Repository Methods(null-handling.html) Namespace reference(namespace-reference.html) Repository query keywords(query-keywords-reference.html) Repository query return types(query-return-types-reference.html)"
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); }"
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/create-instances.html","Creating Repository Instances: This section covers how to create instances and bean definitions for the defined repository interfaces. Java Configuration: Use the store-specific @EnableLdapRepositories annotation on a Java configuration class to define a configuration for repository activation. For an introduction to Java-based configuration of the Spring container, see JavaConfig in the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/java.html) . A sample configuration to enable Spring Data repositories resembles the following: Sample annotation-based repository configuration @Configuration @EnableJpaRepositories(""com.acme.repositories"") class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } The preceding example uses the JPA-specific annotation, which you would change according to the store module you actually use. The same applies to the definition of the EntityManagerFactory bean. See the sections covering the store-specific configuration. XML Configuration: Each Spring Data module includes a repositories element that lets you define a base package that Spring scans for you, as shown in the following example: Enabling Spring Data repositories via XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans:beans xmlns:beans=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans:beans> In the preceding example, Spring is instructed to scan com.acme.repositories and all its sub-packages for interfaces extending Repository or one of its sub-interfaces. For each interface found, the infrastructure registers the persistence technology-specific FactoryBean to create the appropriate proxies that handle invocations of the query methods. Each bean is registered under a bean name that is derived from the interface name, so an interface of UserRepository would be registered under userRepository . Bean names for nested repository interfaces are prefixed with their enclosing type name. The base package attribute allows wildcards so that you can define a pattern of scanned packages. Using Filters: By default, the infrastructure picks up every interface that extends the persistence technology-specific Repository sub-interface located under the configured base package and creates a bean instance for it. However, you might want more fine-grained control over which interfaces have bean instances created for them. To do so, use filter elements inside the repository declaration. The semantics are exactly equivalent to the elements in Spring’s component filters. For details, see the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/classpath-scanning.html) for these elements. For example, to exclude certain interfaces from instantiation as repository beans, you could use the following configuration: Using filters Java XML @Configuration @EnableLdapRepositories(basePackages = ""com.acme.repositories"", includeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeRepository"") }, excludeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeOtherRepository"") }) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } <repositories base-package=""com.acme.repositories""> <context:include-filter type=""regex"" expression="".*SomeRepository"" /> <context:exclude-filter type=""regex"" expression="".*SomeOtherRepository"" /> </repositories> The preceding example includes all interfaces ending with SomeRepository and excludes those ending with SomeOtherRepository from being instantiated. Standalone Usage: You can also use the repository infrastructure outside of a Spring container — for example, in CDI environments.You still need some Spring libraries in your classpath, but, generally, you can set up repositories programmatically as well.The Spring Data modules that provide repository support ship with a persistence technology-specific RepositoryFactory that you can use, as follows: Standalone usage of the repository factory RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class);"
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/query-methods-details.html","Defining Query Methods: The repository proxy has two ways to derive a store-specific query from the method name: By deriving the query from the method name directly. By using a manually defined query. Available options depend on the actual store. However, there must be a strategy that decides what actual query is created. The next section describes the available options. Query Lookup Strategies: The following strategies are available for the repository infrastructure to resolve the query. With XML configuration, you can configure the strategy at the namespace through the query-lookup-strategy attribute. For Java configuration, you can use the queryLookupStrategy attribute of the EnableLdapRepositories annotation. Some strategies may not be supported for particular datastores. CREATE attempts to construct a store-specific query from the query method name. The general approach is to remove a given set of well known prefixes from the method name and parse the rest of the method. You can read more about query construction in “ Query Creation(#repositories.query-methods.query-creation) ”. USE_DECLARED_QUERY tries to find a declared query and throws an exception if it cannot find one. The query can be defined by an annotation somewhere or declared by other means. See the documentation of the specific store to find available options for that store. If the repository infrastructure does not find a declared query for the method at bootstrap time, it fails. CREATE_IF_NOT_FOUND (the default) combines CREATE and USE_DECLARED_QUERY . It looks up a declared query first, and, if no declared query is found, it creates a custom method name-based query. This is the default lookup strategy and, thus, is used if you do not configure anything explicitly. It allows quick query definition by method names but also custom-tuning of these queries by introducing declared queries as needed. Query Creation: The query builder mechanism built into the Spring Data repository infrastructure is useful for building constraining queries over entities of the repository. The following example shows how to create a number of queries: Query creation from method names interface PersonRepository extends Repository<Person, Long> { List<Person> findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // Enables the distinct flag for the query List<Person> findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List<Person> findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); // Enabling ignoring case for an individual property List<Person> findByLastnameIgnoreCase(String lastname); // Enabling ignoring case for all suitable properties List<Person> findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // Enabling static ORDER BY for a query List<Person> findByLastnameOrderByFirstnameAsc(String lastname); List<Person> findByLastnameOrderByFirstnameDesc(String lastname); } Parsing query method names is divided into subject and predicate. The first part ( find…By , exists…By ) defines the subject of the query, the second part forms the predicate. The introducing clause (subject) can contain further expressions. Any text between find (or other introducing keywords) and By is considered to be descriptive unless using one of the result-limiting keywords such as a Distinct to set a distinct flag on the query to be created or Top/First to limit query results(#repositories.limit-query-result) . The appendix contains the full list of query method subject keywords(query-keywords-reference.html#appendix.query.method.subject) and query method predicate keywords including sorting and letter-casing modifiers(query-keywords-reference.html#appendix.query.method.predicate) . However, the first By acts as a delimiter to indicate the start of the actual criteria predicate. At a very basic level, you can define conditions on entity properties and concatenate them with And and Or . The actual result of parsing the method depends on the persistence store for which you create the query. However, there are some general things to notice: The expressions are usually property traversals combined with operators that can be concatenated. You can combine property expressions with AND and OR . You also get support for operators such as Between , LessThan , GreaterThan , and Like for the property expressions. The supported operators can vary by datastore, so consult the appropriate part of your reference documentation. The method parser supports setting an IgnoreCase flag for individual properties (for example, findByLastnameIgnoreCase(…) ) or for all properties of a type that supports ignoring case (usually String instances — for example, findByLastnameAndFirstnameAllIgnoreCase(…) ). Whether ignoring cases is supported may vary by store, so consult the relevant sections in the reference documentation for the store-specific query method. You can apply static ordering by appending an OrderBy clause to the query method that references a property and by providing a sorting direction ( Asc or Desc ). To create a query method that supports dynamic sorting, see “ Paging, Iterating Large Results, Sorting & Limiting(#repositories.special-parameters) ”. Property Expressions: Property expressions can refer only to a direct property of the managed entity, as shown in the preceding example. At query creation time, you already make sure that the parsed property is a property of the managed domain class. However, you can also define constraints by traversing nested properties. Consider the following method signature: List<Person> findByAddressZipCode(ZipCode zipCode); Assume a Person has an Address with a ZipCode . In that case, the method creates the x.address.zipCode property traversal. The resolution algorithm starts by interpreting the entire part ( AddressZipCode ) as the property and checks the domain class for a property with that name (uncapitalized). If the algorithm succeeds, it uses that property. If not, the algorithm splits up the source at the camel-case parts from the right side into a head and a tail and tries to find the corresponding property — in our example, AddressZip and Code . If the algorithm finds a property with that head, it takes the tail and continues building the tree down from there, splitting the tail up in the way just described. If the first split does not match, the algorithm moves the split point to the left ( Address , ZipCode ) and continues. Although this should work for most cases, it is possible for the algorithm to select the wrong property. Suppose the Person class has an addressZip property as well. The algorithm would match in the first split round already, choose the wrong property, and fail (as the type of addressZip probably has no code property). To resolve this ambiguity you can use _ inside your method name to manually define traversal points. So our method name would be as follows: List<Person> findByAddress_ZipCode(ZipCode zipCode); Because we treat underscores ( _ ) as a reserved character, we strongly advise to follow standard Java naming conventions (that is, not using underscores in property names but applying camel case instead). Field Names starting with underscore: Field names may start with underscores like String _name . Make sure to preserve the _ as in _name and use double _ to split nested paths like user__name . Upper Case Field Names: Field names that are all uppercase can be used as such. Nested paths if applicable require splitting via _ as in USER_name . Field Names with 2nd uppercase letter: Field names that consist of a starting lower case letter followed by an uppercase one like String qCode can be resolved by starting with two upper case letters as in QCode . Please be aware of potential path ambiguities. Path Ambiguities: In the following sample the arrangement of properties qCode and q , with q containing a property called code , creates an ambiguity for the path QCode . record Container(String qCode, Code q) {} record Code(String code) {} Since a direct match on a property is considered first, any potential nested paths will not be considered and the algorithm picks the qCode field. In order to select the code field in q the underscore notation Q_Code is required. Repository Methods Returning Collections or Iterables: Query methods that return multiple results can use standard Java Iterable , List , and Set . Beyond that, we support returning Spring Data’s Streamable , a custom extension of Iterable , as well as collection types provided by Vavr(https://www.vavr.io/) . Refer to the appendix explaining all possible query method return types(query-return-types-reference.html#appendix.query.return.types) . Using Streamable as Query Method Return Type: You can use Streamable as alternative to Iterable or any collection type. It provides convenience methods to access a non-parallel Stream (missing from Iterable ) and the ability to directly ….filter(…) and ….map(…) over the elements and concatenate the Streamable to others: Using Streamable to combine query method results interface PersonRepository extends Repository<Person, Long> { Streamable<Person> findByFirstnameContaining(String firstname); Streamable<Person> findByLastnameContaining(String lastname); } Streamable<Person> result = repository.findByFirstnameContaining(""av"") .and(repository.findByLastnameContaining(""ea"")); Returning Custom Streamable Wrapper Types: Providing dedicated wrapper types for collections is a commonly used pattern to provide an API for a query result that returns multiple elements. Usually, these types are used by invoking a repository method returning a collection-like type and creating an instance of the wrapper type manually. You can avoid that additional step as Spring Data lets you use these wrapper types as query method return types if they meet the following criteria: The type implements Streamable . The type exposes either a constructor or a static factory method named of(…) or valueOf(…) that takes Streamable as an argument. The following listing shows an example: class Product { (1) MonetaryAmount getPrice() { … } } @RequiredArgsConstructor(staticName = ""of"") class Products implements Streamable<Product> { (2) private final Streamable<Product> streamable; public MonetaryAmount getTotal() { (3) return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); } @Override public Iterator<Product> iterator() { (4) return streamable.iterator(); } } interface ProductRepository implements Repository<Product, Long> { Products findAllByDescriptionContaining(String text); (5) } 1 A Product entity that exposes API to access the product’s price. 2 A wrapper type for a Streamable<Product> that can be constructed by using Products.of(…) (factory method created with the Lombok annotation). A standard constructor taking the Streamable<Product> will do as well. 3 The wrapper type exposes an additional API, calculating new values on the Streamable<Product> . 4 Implement the Streamable interface and delegate to the actual result. 5 That wrapper type Products can be used directly as a query method return type. You do not need to return Streamable<Product> and manually wrap it after the query in the repository client. Support for Vavr Collections: Vavr(https://www.vavr.io/) is a library that embraces functional programming concepts in Java. It ships with a custom set of collection types that you can use as query method return types, as the following table shows: Vavr collection type Used Vavr implementation type Valid Java source types io.vavr.collection.Seq io.vavr.collection.List java.util.Iterable io.vavr.collection.Set io.vavr.collection.LinkedHashSet java.util.Iterable io.vavr.collection.Map io.vavr.collection.LinkedHashMap java.util.Map You can use the types in the first column (or subtypes thereof) as query method return types and get the types in the second column used as implementation type, depending on the Java type of the actual query result (third column). Alternatively, you can declare Traversable (the Vavr Iterable equivalent), and we then derive the implementation class from the actual return value. That is, a java.util.List is turned into a Vavr List or Seq , a java.util.Set becomes a Vavr LinkedHashSet Set , and so on. Streaming Query Results: You can process the results of query methods incrementally by using a Java 8 Stream<T> as the return type. Instead of wrapping the query results in a Stream , data store-specific methods are used to perform the streaming, as shown in the following example: Stream the result of a query with Java 8 Stream<T> @Query(""select u from User u"") Stream<User> findAllByCustomQueryAndStream(); Stream<User> readAllByFirstnameNotNull(); @Query(""select u from User u"") Stream<User> streamAllPaged(Pageable pageable); A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example: Working with a Stream<T> result in a try-with-resources block try (Stream<User> stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } Not all Spring Data modules currently support Stream<T> as a return type. Asynchronous Query Results: You can run repository queries asynchronously by using Spring’s asynchronous method running capability(https://docs.spring.io/spring-framework/reference/6.1/integration/scheduling.html) . This means the method returns immediately upon invocation while the actual query occurs in a task that has been submitted to a Spring TaskExecutor . Asynchronous queries differ from reactive queries and should not be mixed. See the store-specific documentation for more details on reactive support. The following example shows a number of asynchronous queries: @Async Future<User> findByFirstname(String firstname); (1) @Async CompletableFuture<User> findOneByFirstname(String firstname); (2) 1 Use java.util.concurrent.Future as the return type. 2 Use a Java 8 java.util.concurrent.CompletableFuture as the return type. Paging, Iterating Large Results, Sorting & Limiting: To handle parameters in your query, define method parameters as already seen in the preceding examples. Besides that, the infrastructure recognizes certain specific types like Pageable , Sort and Limit , to apply pagination, sorting and limiting to your queries dynamically. The following example demonstrates these features: Using Pageable , Slice , Sort and Limit in query methods Page<User> findByLastname(String lastname, Pageable pageable); Slice<User> findByLastname(String lastname, Pageable pageable); List<User> findByLastname(String lastname, Sort sort); List<User> findByLastname(String lastname, Sort sort, Limit limit); List<User> findByLastname(String lastname, Pageable pageable); APIs taking Sort , Pageable and Limit expect non- null values to be handed into methods. If you do not want to apply any sorting or pagination, use Sort.unsorted() , Pageable.unpaged() and Limit.unlimited() . The first method lets you pass an org.springframework.data.domain.Pageable instance to the query method to dynamically add paging to your statically defined query. A Page knows about the total number of elements and pages available. It does so by the infrastructure triggering a count query to calculate the overall number. As this might be expensive (depending on the store used), you can instead return a Slice . A Slice knows only about whether a next Slice is available, which might be sufficient when walking through a larger result set. Sorting options are handled through the Pageable instance, too. If you need only sorting, add an org.springframework.data.domain.Sort parameter to your method. As you can see, returning a List is also possible. In this case, the additional metadata required to build the actual Page instance is not created (which, in turn, means that the additional count query that would have been necessary is not issued). Rather, it restricts the query to look up only the given range of entities. To find out how many pages you get for an entire query, you have to trigger an additional count query. By default, this query is derived from the query you actually trigger. Special parameters may only be used once within a query method. Some special parameters described above are mutually exclusive. Please consider the following list of invalid parameter combinations. Parameters Example Reason Pageable and Sort findBy…​(Pageable page, Sort sort) Pageable already defines Sort Pageable and Limit findBy…​(Pageable page, Limit limit) Pageable already defines a limit. The Top keyword used to limit results can be used to along with Pageable whereas Top defines the total maximum of results, whereas the Pageable parameter may reduce this number. Which Method is Appropriate?: The value provided by the Spring Data abstractions is perhaps best shown by the possible query method return types outlined in the following table below. The table shows which types you can return from a query method Table 1. Consuming Large Query Results Method Amount of Data Fetched Query Structure Constraints List<T>(#repositories.collections-and-iterables) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Streamable<T>(#repositories.collections-and-iterables.streamable) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Stream<T>(#repositories.query-streaming) Chunked (one-by-one or in batches) depending on Stream consumption. Single query using typically cursors. Streams must be closed after usage to avoid resource leaks. Flux<T> Chunked (one-by-one or in batches) depending on Flux consumption. Single query using typically cursors. Store module must provide reactive infrastructure. Slice<T> Pageable.getPageSize() + 1 at Pageable.getOffset() One to many queries fetching data starting at Pageable.getOffset() applying limiting. A Slice can only navigate to the next Slice . Slice provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Window provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Page<T> Pageable.getPageSize() at Pageable.getOffset() One to many queries starting at Pageable.getOffset() applying limiting. Additionally, COUNT(…) query to determine the total number of elements can be required. Often times, COUNT(…) queries are required that are costly. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Paging and Sorting: You can define simple sorting expressions by using property names. You can concatenate expressions to collect multiple criteria into one expression. Defining sort expressions Sort sort = Sort.by(""firstname"").ascending() .and(Sort.by(""lastname"").descending()); For a more type-safe way to define sort expressions, start with the type for which to define the sort expression and use method references to define the properties on which to sort. Defining sort expressions by using the type-safe API TypedSort<Person> person = Sort.sort(Person.class); Sort sort = person.by(Person::getFirstname).ascending() .and(person.by(Person::getLastname).descending()); TypedSort.by(…) makes use of runtime proxies by (typically) using CGlib, which may interfere with native image compilation when using tools such as Graal VM Native. If your store implementation supports Querydsl, you can also use the generated metamodel types to define sort expressions: Defining sort expressions by using the Querydsl API QSort sort = QSort.by(QPerson.firstname.asc()) .and(QSort.by(QPerson.lastname.desc())); Limiting Query Results: In addition to paging it is possible to limit the result size using a dedicated Limit parameter. You can also limit the results of query methods by using the First or Top keywords, which you can use interchangeably but may not be mixed with a Limit parameter. You can append an optional numeric value to Top or First to specify the maximum result size to be returned. If the number is left out, a result size of 1 is assumed. The following example shows how to limit the query size: Limiting the result size of a query with Top and First List<User> findByLastname(Limit limit); User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page<User> queryFirst10ByLastname(String lastname, Pageable pageable); Slice<User> findTop3ByLastname(String lastname, Pageable pageable); List<User> findFirst10ByLastname(String lastname, Sort sort); List<User> findTop10ByLastname(String lastname, Pageable pageable); The limiting expressions also support the Distinct keyword for datastores that support distinct queries. Also, for the queries that limit the result set to one instance, wrapping the result into with the Optional keyword is supported. If pagination or slicing is applied to a limiting query pagination (and the calculation of the number of available pages), it is applied within the limited result. Limiting the results in combination with dynamic sorting by using a Sort parameter lets you express query methods for the 'K' smallest as well as for the 'K' biggest elements."
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/projections.html","Projections: Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> ."
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableLdapRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableLdapRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" />"
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/core-extensions.html","Spring Data Extensions: This section documents a set of Spring Data extensions that enable Spring Data usage in a variety of contexts. Currently, most of the integration is targeted towards Spring MVC. Querydsl Extension: Querydsl(http://www.querydsl.com/) is a framework that enables the construction of statically typed SQL-like queries through its fluent API. Several Spring Data modules offer integration with Querydsl through QuerydslPredicateExecutor , as the following example shows: QuerydslPredicateExecutor interface public interface QuerydslPredicateExecutor<T> { Optional<T> findById(Predicate predicate); (1) Iterable<T> findAll(Predicate predicate); (2) long count(Predicate predicate); (3) boolean exists(Predicate predicate); (4) // … more functionality omitted. } 1 Finds and returns a single entity matching the Predicate . 2 Finds and returns all entities matching the Predicate . 3 Returns the number of entities matching the Predicate . 4 Returns whether an entity that matches the Predicate exists. To use the Querydsl support, extend QuerydslPredicateExecutor on your repository interface, as the following example shows: Querydsl integration on repositories interface UserRepository extends CrudRepository<User, Long>, QuerydslPredicateExecutor<User> { } The preceding example lets you write type-safe queries by using Querydsl Predicate instances, as the following example shows: Predicate predicate = user.firstname.equalsIgnoreCase(""dave"") .and(user.lastname.startsWithIgnoreCase(""mathews"")); userRepository.findAll(predicate); Web support: Spring Data modules that support the repository programming model ship with a variety of web support. The web related components require Spring MVC JARs to be on the classpath. Some of them even provide integration with Spring HATEOAS(https://github.com/spring-projects/spring-hateoas) . In general, the integration support is enabled by using the @EnableSpringDataWebSupport annotation in your JavaConfig configuration class, as the following example shows: Enabling Spring Data web support Java XML @Configuration @EnableWebMvc @EnableSpringDataWebSupport class WebConfiguration {} <bean class=""org.springframework.data.web.config.SpringDataWebConfiguration"" /> <!-- If you use Spring HATEOAS, register this one *instead* of the former --> <bean class=""org.springframework.data.web.config.HateoasAwareSpringDataWebConfiguration"" /> The @EnableSpringDataWebSupport annotation registers a few components. We discuss those later in this section. It also detects Spring HATEOAS on the classpath and registers integration components (if present) for it as well. Basic Web Support: Enabling Spring Data web support in XML The configuration shown in the previous section(#core.web) registers a few basic components: A Using the DomainClassConverter Class(#core.web.basic.domain-class-converter) to let Spring MVC resolve instances of repository-managed domain classes from request parameters or path variables. HandlerMethodArgumentResolver(#core.web.basic.paging-and-sorting) implementations to let Spring MVC resolve Pageable and Sort instances from request parameters. Jackson Modules(#core.web.basic.jackson-mappers) to de-/serialize types like Point and Distance , or store specific ones, depending on the Spring Data Module used. Using the DomainClassConverter Class: The DomainClassConverter class lets you use domain types in your Spring MVC controller method signatures directly so that you need not manually lookup the instances through the repository, as the following example shows: A Spring MVC controller using domain types in method signatures @Controller @RequestMapping(""/users"") class UserController { @RequestMapping(""/{id}"") String showUserForm(@PathVariable(""id"") User user, Model model) { model.addAttribute(""user"", user); return ""userForm""; } } The method receives a User instance directly, and no further lookup is necessary. The instance can be resolved by letting Spring MVC convert the path variable into the id type of the domain class first and eventually access the instance through calling findById(…) on the repository instance registered for the domain type. Currently, the repository has to implement CrudRepository to be eligible to be discovered for conversion. HandlerMethodArgumentResolvers for Pageable and Sort: The configuration snippet shown in the previous section(#core.web.basic.domain-class-converter) also registers a PageableHandlerMethodArgumentResolver as well as an instance of SortHandlerMethodArgumentResolver . The registration enables Pageable and Sort as valid controller method arguments, as the following example shows: Using Pageable as a controller method argument @Controller @RequestMapping(""/users"") class UserController { private final UserRepository repository; UserController(UserRepository repository) { this.repository = repository; } @RequestMapping String showUsers(Model model, Pageable pageable) { model.addAttribute(""users"", repository.findAll(pageable)); return ""users""; } } The preceding method signature causes Spring MVC try to derive a Pageable instance from the request parameters by using the following default configuration: Table 1. Request parameters evaluated for Pageable instances page Page you want to retrieve. 0-indexed and defaults to 0. size Size of the page you want to retrieve. Defaults to 20. sort Properties that should be sorted by in the format property,property(,ASC|DESC)(,IgnoreCase) . The default sort direction is case-sensitive ascending. Use multiple sort parameters if you want to switch direction or case sensitivity — for example, ?sort=firstname&sort=lastname,asc&sort=city,ignorecase . To customize this behavior, register a bean that implements the PageableHandlerMethodArgumentResolverCustomizer interface or the SortHandlerMethodArgumentResolverCustomizer interface, respectively. Its customize() method gets called, letting you change settings, as the following example shows: @Bean SortHandlerMethodArgumentResolverCustomizer sortCustomizer() { return s -> s.setPropertyDelimiter(""<-->""); } If setting the properties of an existing MethodArgumentResolver is not sufficient for your purpose, extend either SpringDataWebConfiguration or the HATEOAS-enabled equivalent, override the pageableResolver() or sortResolver() methods, and import your customized configuration file instead of using the @Enable annotation. If you need multiple Pageable or Sort instances to be resolved from the request (for multiple tables, for example), you can use Spring’s @Qualifier annotation to distinguish one from another. The request parameters then have to be prefixed with ${qualifier}_ . The following example shows the resulting method signature: String showUsers(Model model, @Qualifier(""thing1"") Pageable first, @Qualifier(""thing2"") Pageable second) { … } You have to populate thing1_page , thing2_page , and so on. The default Pageable passed into the method is equivalent to a PageRequest.of(0, 20) , but you can customize it by using the @PageableDefault annotation on the Pageable parameter. Creating JSON representations for Page: It’s common for Spring MVC controllers to try to ultimately render a representation of a Spring Data page to clients. While one could simply return Page instances from handler methods to let Jackson render them as is, we strongly recommend against this as the underlying implementation class PageImpl is a domain type. This means we might want or have to change its API for unrelated reasons, and such changes might alter the resulting JSON representation in a breaking way. With Spring Data 3.1, we started hinting at the problem by issuing a warning log describing the problem. We still ultimately recommend to leverage the integration with Spring HATEOAS(#core.web.pageables) for a fully stable and hypermedia-enabled way of rendering pages that easily allow clients to navigate them. But as of version 3.3 Spring Data ships a page rendering mechanism that is convenient to use but does not require the inclusion of Spring HATEOAS. Using Spring Data' PagedModel: At its core, the support consists of a simplified version of Spring HATEOAS' PagedModel (the Spring Data one located in the org.springframework.data.web package). It can be used to wrap Page instances and result in a simplified representation that reflects the structure established by Spring HATEOAS but omits the navigation links. import org.springframework.data.web.PagedModel; @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") PagedModel<?> page(Pageable pageable) { return new PagedModel<>(repository.findAll(pageable)); (1) } } 1 Wraps the Page instance into a PagedModel . This will result in a JSON structure looking like this: { ""content"" : [ … // Page content rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } Note how the document contains a page field exposing the essential pagination metadata. Globally enabling simplified Page rendering: If you don’t want to change all your existing controllers to add the mapping step to return PagedModel instead of Page you can enable the automatic translation of PageImpl instances into PagedModel by tweaking @EnableSpringDataWebSupport as follows: @EnableSpringDataWebSupport(pageSerializationMode = VIA_DTO) class MyConfiguration { } This will allow your controller to still return Page instances and they will automatically be rendered into the simplified representation: @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") Page<?> page(Pageable pageable) { return repository.findAll(pageable); } } Hypermedia Support for Page and Slice: Spring HATEOAS ships with a representation model class ( PagedModel / SlicedModel ) that allows enriching the content of a Page or Slice instance with the necessary Page / Slice metadata as well as links to let the clients easily navigate the pages. The conversion of a Page to a PagedModel is done by an implementation of the Spring HATEOAS RepresentationModelAssembler interface, called the PagedResourcesAssembler . Similarly Slice instances can be converted to a SlicedModel using a SlicedResourcesAssembler . The following example shows how to use a PagedResourcesAssembler as a controller method argument, as the SlicedResourcesAssembler works exactly the same: Using a PagedResourcesAssembler as controller method argument @Controller class PersonController { private final PersonRepository repository; // Constructor omitted @GetMapping(""/people"") HttpEntity<PagedModel<Person>> people(Pageable pageable, PagedResourcesAssembler assembler) { Page<Person> people = repository.findAll(pageable); return ResponseEntity.ok(assembler.toModel(people)); } } Enabling the configuration, as shown in the preceding example, lets the PagedResourcesAssembler be used as a controller method argument. Calling toModel(…) on it has the following effects: The content of the Page becomes the content of the PagedModel instance. The PagedModel object gets a PageMetadata instance attached, and it is populated with information from the Page and the underlying Pageable . The PagedModel may get prev and next links attached, depending on the page’s state. The links point to the URI to which the method maps. The pagination parameters added to the method match the setup of the PageableHandlerMethodArgumentResolver to make sure the links can be resolved later. Assume we have 30 Person instances in the database. You can now trigger a request ( GET localhost:8080/people(http://localhost:8080/people) ) and see output similar to the following: { ""links"" : [ { ""rel"" : ""next"", ""href"" : ""http://localhost:8080/persons?page=1&size=20"" } ], ""content"" : [ … // 20 Person instances rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } The JSON envelope format shown here doesn’t follow any formally specified structure and it’s not guaranteed stable and we might change it at any time. It’s highly recommended to enable the rendering as a hypermedia-enabled, official media type, supported by Spring HATEOAS, like HAL(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#mediatypes.hal) . Those can be activated by using its @EnableHypermediaSupport annotation. Find more information in the Spring HATEOAS reference documentation(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#configuration.at-enable) . The assembler produced the correct URI and also picked up the default configuration to resolve the parameters into a Pageable for an upcoming request. This means that, if you change that configuration, the links automatically adhere to the change. By default, the assembler points to the controller method it was invoked in, but you can customize that by passing a custom Link to be used as base to build the pagination links, which overloads the PagedResourcesAssembler.toModel(…) method. Spring Data Jackson Modules: The core module, and some of the store specific ones, ship with a set of Jackson Modules for types, like org.springframework.data.geo.Distance and org.springframework.data.geo.Point , used by the Spring Data domain. Those Modules are imported once web support(#core.web) is enabled and com.fasterxml.jackson.databind.ObjectMapper is available. During initialization SpringDataJacksonModules , like the SpringDataJacksonConfiguration , get picked up by the infrastructure, so that the declared com.fasterxml.jackson.databind.Module s are made available to the Jackson ObjectMapper . Data binding mixins for the following domain types are registered by the common infrastructure. org.springframework.data.geo.Distance org.springframework.data.geo.Point org.springframework.data.geo.Box org.springframework.data.geo.Circle org.springframework.data.geo.Polygon The individual module may provide additional SpringDataJacksonModules . Please refer to the store specific section for more details. Web Databinding Support: You can use Spring Data projections (described in Projections(projections.html) ) to bind incoming request payloads by using either JSONPath(https://goessner.net/articles/JsonPath/) expressions (requires Jayway JsonPath(https://github.com/json-path/JsonPath) ) or XPath(https://www.w3.org/TR/xpath-31/) expressions (requires XmlBeam(https://xmlbeam.org/) ), as the following example shows: HTTP payload binding using JSONPath or XPath expressions @ProjectedPayload public interface UserPayload { @XBRead(""//firstname"") @JsonPath(""$..firstname"") String getFirstname(); @XBRead(""/lastname"") @JsonPath({ ""$.lastname"", ""$.user.lastname"" }) String getLastname(); } You can use the type shown in the preceding example as a Spring MVC handler method argument or by using ParameterizedTypeReference on one of methods of the RestTemplate . The preceding method declarations would try to find firstname anywhere in the given document. The lastname XML lookup is performed on the top-level of the incoming document. The JSON variant of that tries a top-level lastname first but also tries lastname nested in a user sub-document if the former does not return a value. That way, changes in the structure of the source document can be mitigated easily without having clients calling the exposed methods (usually a drawback of class-based payload binding). Nested projections are supported as described in Projections(projections.html) . If the method returns a complex, non-interface type, a Jackson ObjectMapper is used to map the final value. For Spring MVC, the necessary converters are registered automatically as soon as @EnableSpringDataWebSupport is active and the required dependencies are available on the classpath. For usage with RestTemplate , register a ProjectingJackson2HttpMessageConverter (JSON) or XmlBeamHttpMessageConverter manually. For more information, see the web projection example(https://github.com/spring-projects/spring-data-examples/tree/main/web/projection) in the canonical Spring Data Examples repository(https://github.com/spring-projects/spring-data-examples) . Querydsl Web Support: For those stores that have QueryDSL(http://www.querydsl.com/) integration, you can derive queries from the attributes contained in a Request query string. Consider the following query string: ?firstname=Dave&lastname=Matthews Given the User object from the previous examples, you can resolve a query string to the following value by using the QuerydslPredicateArgumentResolver , as follows: QUser.user.firstname.eq(""Dave"").and(QUser.user.lastname.eq(""Matthews"")) The feature is automatically enabled, along with @EnableSpringDataWebSupport , when Querydsl is found on the classpath. Adding a @QuerydslPredicate to the method signature provides a ready-to-use Predicate , which you can run by using the QuerydslPredicateExecutor . Type information is typically resolved from the method’s return type. Since that information does not necessarily match the domain type, it might be a good idea to use the root attribute of QuerydslPredicate . The following example shows how to use @QuerydslPredicate in a method signature: @Controller class UserController { @Autowired UserRepository repository; @RequestMapping(value = ""/"", method = RequestMethod.GET) String index(Model model, @QuerydslPredicate(root = User.class) Predicate predicate, (1) Pageable pageable, @RequestParam MultiValueMap<String, String> parameters) { model.addAttribute(""users"", repository.findAll(predicate, pageable)); return ""index""; } } 1 Resolve query string arguments to matching Predicate for User . The default binding is as follows: Object on simple properties as eq . Object on collection like properties as contains . Collection on simple properties as in . You can customize those bindings through the bindings attribute of @QuerydslPredicate or by making use of Java 8 default methods and adding the QuerydslBinderCustomizer method to the repository interface, as follows: interface UserRepository extends CrudRepository<User, String>, QuerydslPredicateExecutor<User>, (1) QuerydslBinderCustomizer<QUser> { (2) @Override default void customize(QuerydslBindings bindings, QUser user) { bindings.bind(user.username).first((path, value) -> path.contains(value)) (3) bindings.bind(String.class) .first((StringPath path, String value) -> path.containsIgnoreCase(value)); (4) bindings.excluding(user.password); (5) } } 1 QuerydslPredicateExecutor provides access to specific finder methods for Predicate . 2 QuerydslBinderCustomizer defined on the repository interface is automatically picked up and shortcuts @QuerydslPredicate(bindings=…​) . 3 Define the binding for the username property to be a simple contains binding. 4 Define the default binding for String properties to be a case-insensitive contains match. 5 Exclude the password property from Predicate resolution. You can register a QuerydslBinderCustomizerDefaults bean holding default Querydsl bindings before applying specific bindings from the repository or @QuerydslPredicate . Repository Populators: If you work with the Spring JDBC module, you are probably familiar with the support for populating a DataSource with SQL scripts. A similar abstraction is available on the repositories level, although it does not use SQL as the data definition language because it must be store-independent. Thus, the populators support XML (through Spring’s OXM abstraction) and JSON (through Jackson) to define data with which to populate the repositories. Assume you have a file called data.json with the following content: Data defined in JSON [ { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Dave"", ""lastname"" : ""Matthews"" }, { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Carter"", ""lastname"" : ""Beauford"" } ] You can populate your repositories by using the populator elements of the repository namespace provided in Spring Data Commons. To populate the preceding data to your PersonRepository , declare a populator similar to the following: Declaring a Jackson repository populator <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd""> <repository:jackson2-populator locations=""classpath:data.json"" /> </beans> The preceding declaration causes the data.json file to be read and deserialized by a Jackson ObjectMapper . The type to which the JSON object is unmarshalled is determined by inspecting the _class attribute of the JSON document. The infrastructure eventually selects the appropriate repository to handle the object that was deserialized. To instead use XML to define the data the repositories should be populated with, you can use the unmarshaller-populator element. You configure it to use one of the XML marshaller options available in Spring OXM. See the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/data-access/oxm.html) for details. The following example shows how to unmarshall a repository populator with JAXB: Declaring an unmarshalling repository populator (using JAXB) <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xmlns:oxm=""http://www.springframework.org/schema/oxm"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd http://www.springframework.org/schema/oxm https://www.springframework.org/schema/oxm/spring-oxm.xsd""> <repository:unmarshaller-populator locations=""classpath:data.json"" unmarshaller-ref=""unmarshaller"" /> <oxm:jaxb2-marshaller contextPath=""com.acme"" /> </beans>"
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/null-handling.html","Null Handling of Repository Methods: As of Spring Data 2.0, repository CRUD methods that return an individual aggregate instance use Java 8’s Optional to indicate the potential absence of a value. Besides that, Spring Data supports returning the following wrapper types on query methods: com.google.common.base.Optional scala.Option io.vavr.control.Option Alternatively, query methods can choose not to use a wrapper type at all. The absence of a query result is then indicated by returning null . Repository methods returning collections, collection alternatives, wrappers, and streams are guaranteed never to return null but rather the corresponding empty representation. See “ Repository query return types(query-return-types-reference.html) ” for details. Nullability Annotations: You can express nullability constraints for repository methods by using Spring Framework’s nullability annotations(https://docs.spring.io/spring-framework/reference/6.1/core/null-safety.html) . They provide a tooling-friendly approach and opt-in null checks during runtime, as follows: @NonNullApi(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNullApi.html) : Used on the package level to declare that the default behavior for parameters and return values is, respectively, neither to accept nor to produce null values. @NonNull(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNull.html) : Used on a parameter or return value that must not be null (not needed on a parameter and return value where @NonNullApi applies). @Nullable(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/Nullable.html) : Used on a parameter or return value that can be null . Spring annotations are meta-annotated with JSR 305(https://jcp.org/en/jsr/detail?id=305) annotations (a dormant but widely used JSR). JSR 305 meta-annotations let tooling vendors (such as IDEA(https://www.jetbrains.com/help/idea/nullable-and-notnull-annotations.html) , Eclipse(https://help.eclipse.org/latest/index.jsp?topic=/org.eclipse.jdt.doc.user/tasks/task-using_external_null_annotations.htm) , and Kotlin(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) ) provide null-safety support in a generic way, without having to hard-code support for Spring annotations. To enable runtime checking of nullability constraints for query methods, you need to activate non-nullability on the package level by using Spring’s @NonNullApi in package-info.java , as shown in the following example: Declaring Non-nullability in package-info.java @org.springframework.lang.NonNullApi package com.acme; Once non-null defaulting is in place, repository query method invocations get validated at runtime for nullability constraints. If a query result violates the defined constraint, an exception is thrown. This happens when the method would return null but is declared as non-nullable (the default with the annotation defined on the package in which the repository resides). If you want to opt-in to nullable results again, selectively use @Nullable on individual methods. Using the result wrapper types mentioned at the start of this section continues to work as expected: an empty result is translated into the value that represents absence. The following example shows a number of the techniques just described: Using different nullability constraints package com.acme; (1) import org.springframework.lang.Nullable; interface UserRepository extends Repository<User, Long> { User getByEmailAddress(EmailAddress emailAddress); (2) @Nullable User findByEmailAddress(@Nullable EmailAddress emailAdress); (3) Optional<User> findOptionalByEmailAddress(EmailAddress emailAddress); (4) } 1 The repository resides in a package (or sub-package) for which we have defined non-null behavior. 2 Throws an EmptyResultDataAccessException when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . 3 Returns null when the query does not produce a result. Also accepts null as the value for emailAddress . 4 Returns Optional.empty() when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . Nullability in Kotlin-based Repositories: Kotlin has the definition of nullability constraints(https://kotlinlang.org/docs/reference/null-safety.html) baked into the language. Kotlin code compiles to bytecode, which does not express nullability constraints through method signatures but rather through compiled-in metadata. Make sure to include the kotlin-reflect JAR in your project to enable introspection of Kotlin’s nullability constraints. Spring Data repositories use the language mechanism to define those constraints to apply the same runtime checks, as follows: Using nullability constraints on Kotlin repositories interface UserRepository : Repository<User, String> { fun findByUsername(username: String): User (1) fun findByFirstname(firstname: String?): User? (2) } 1 The method defines both the parameter and the result as non-nullable (the Kotlin default). The Kotlin compiler rejects method invocations that pass null to the method. If the query yields an empty result, an EmptyResultDataAccessException is thrown. 2 This method accepts null for the firstname parameter and returns null if the query does not produce a result."
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/namespace-reference.html","Namespace reference: The <repositories /> Element: The <repositories /> element triggers the setup of the Spring Data repository infrastructure. The most important attribute is base-package , which defines the package to scan for Spring Data repository interfaces. See “ XML Configuration(create-instances.html#repositories.create-instances.xml) ”. The following table describes the attributes of the <repositories /> element: Table 1. Attributes Name Description base-package Defines the package to be scanned for repository interfaces that extend *Repository (the actual interface is determined by the specific Spring Data module) in auto-detection mode. All packages below the configured package are scanned, too. Wildcards are allowed. repository-impl-postfix Defines the postfix to autodetect custom repository implementations. Classes whose names end with the configured postfix are considered as candidates. Defaults to Impl . query-lookup-strategy Determines the strategy to be used to create finder queries. See “ Query Lookup Strategies(query-methods-details.html#repositories.query-methods.query-lookup-strategies) ” for details. Defaults to create-if-not-found . named-queries-location Defines the location to search for a Properties file containing externally defined queries. consider-nested-repositories Whether nested repository interface definitions should be considered. Defaults to false ."
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/ldap/reference/3.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/ldap/reference/3.3/ldap.html","LDAP Repositories: This chapter points out the specialties for repository support for LDAP. It builds on the core repository support explained in Working with Spring Data Repositories(repositories/introduction.html) . You should have a sound understanding of the basic concepts explained there. Section Summary: Configuration(ldap/configuration.html) Usage(ldap/usage.html) Query Methods(ldap/query-methods.html) Querydsl Support(ldap/querydsl.html) CDI Integration(ldap/cdi-integration.html)"
"https://docs.spring.io/spring-data/ldap/reference/3.3/ldap/configuration.html","Configuration: This section describes configuring Spring Data LDAP. Spring LDAP repositories can be enabled by using a <data-ldap:repositories> tag in your XML configuration or by using an @EnableLdapRepositories annotation on a configuration class: “ Spring Namespace(#ldap.namespace) ” (XML configuration) “ Annotation-based Configuration(#ldap.java-config) ” (Java configuration) To include support for LdapQuery parameters in automatically generated repositories, have your interface extend LdapRepository rather than CrudRepository . All Spring LDAP repositories must work with entities annotated with the ODM annotations, as described in Object-Directory Mapping(https://docs.spring.io/spring-ldap/reference/odm.html) . Since all ODM managed classes must have a Distinguished Name as the ID, all Spring LDAP repositories must have the ID type parameter set to javax.naming.Name . Indeed, the built-in LdapRepository(../api/java/org/springframework/data/ldap/repository/LdapRepository.html) only takes one type parameter: the managed entity class, which defaults the ID to javax.naming.Name . Due to specifics of the LDAP protocol, paging and sorting are not supported for Spring LDAP repositories. You must use ODM annotations, such as org.springframework.ldap.odm.annotations.Id . Using Spring Data’s annotation does not work, because Spring LDAP uses its own mapping layer. Annotation-based Configuration: The Spring Data LDAP repositories support can be activated through both JavaConfig as well as a custom XML namespace, as shown in the following example: Example 1. Spring Data LDAP repositories using JavaConfig @Configuration @EnableLdapRepositories(""com.acme.*.repositories"") class MyConfig { @Bean ContextSource contextSource() { LdapContextSource ldapContextSource = new LdapContextSource(); ldapContextSource.setUserDn(""cn=Admin""); ldapContextSource.setPassword(""secret""); ldapContextSource.setUrl(""ldap://127.0.0.1:389""); return ldapContextSource; } @Bean LdapTemplate ldapTemplate(ContextSource contextSource) { return new LdapTemplate(contextSource); } } This configuration causes the base packages to be scanned for interfaces that extend contain LDAP repositories and create Spring beans for each one found. If no base package is configured, the infrastructure scans the package of the annotated configuration class. Spring Namespace: The LDAP module of Spring Data contains a custom namespace that allows defining repository beans. It also contains certain features and element attributes that are special to LDAP. Generally, the LDAP repositories can be set up by using the repositories element, as shown in the following example: Example 2. Setting up LDAP repositories by using the namespace <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:ldap=""http://www.springframework.org/schema/ldap"" xmlns:data-ldap=""http://www.springframework.org/schema/data/ldap"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/ldap https://www.springframework.org/schema/ldap/spring-ldap.xsd http://www.springframework.org/schema/data/ldap https://www.springframework.org/schema/data/ldap/spring-ldap.xsd""> <ldap:context-source url=""ldap://127.0.0.1:389"" username=""cn=Admin"" password=""secret"" /> <ldap:ldap-template /> <data-ldap:repositories base-package=""com.acme.*.repositories"" /> </beans> This configuration causes the base packages to be scanned for interfaces that extend contain LDAP repositories and create Spring beans for each one found. By default, the repositories get an autowired LdapTemplate Spring bean that is called ldapTemplate , so you only need to configure ldap-template-ref explicitly if you deviate from this convention. Which is better, JavaConfig or XML? XML is how Spring was configured long ago. In today’s era of fast-growing Java, record types, annotations, and more, new projects typically use as much pure Java as possible. While there is no immediate plan to remove XML support, some of the newest features MAY not be available through XML. Using the repositories element looks up Spring Data repositories as described in Creating Repository Instances(../repositories/create-instances.html) . Custom Namespace Attributes: Beyond the default attributes of the repositories element(../repositories/namespace-reference.html) , the LDAP namespace offers additional attributes to let you gain more detailed control over the setup of the repositories: Table 1. Custom LDAP-specific attributes of the repositories element ldap-template-ref Explicitly wire the LdapTemplate to be used with the repositories being detected by the repositories element. Usually used if multiple LdapTemplate beans are used within the application. If not configured, Spring Data automatically looks up the LdapTemplate bean with the name ldapTemplate in the ApplicationContext . Spring Data LDAP requires a LdapMappingContext bean named ldapMappingContext to be present. If no such bean is defined, then Spring Data LDAP registers a default instance in the application context."
"https://docs.spring.io/spring-data/ldap/reference/3.3/ldap/usage.html","Usage: To access domain entities stored in a LDAP-compliant directory, you can use our sophisticated repository support that significantly eases implementation. To do so, create an interface for your repository, as the following example shows: Example 1. Sample Person entity @Entry(objectClasses = { ""person"", ""top"" }, base=""ou=someOu"") public class Person { @Id private Name dn; @Attribute(name=""cn"") @DnAttribute(value=""cn"", index=1) private String fullName; @Attribute(name=""firstName"") private String firstName; // No @Attribute annotation means this is bound to the LDAP attribute // with the same value private String firstName; @DnAttribute(value=""ou"", index=0) @Transient private String company; @Transient private String someUnmappedField; // ...more attributes below } We have a simple domain object here. Note that it has a property named dn of type Name . With that domain object, we can create a repository to persist objects of that type by defining an interface for it, as follows: Example 2. Basic repository interface to persist Person entities public interface PersonRepository extends CrudRepository<Person, Long> { // additional custom finder methods go here } Because our domain repository extends CrudRepository , it provides you with CRUD operations as well as methods for access to the entities. Working with the repository instance is a matter of dependency injecting it into a client. Example 3. Access to Person entities @ExtendWith(SpringExtension.class) @ContextConfiguration class PersonRepositoryTests { @Autowired PersonRepository repository; @Test void readAll() { List<Person> persons = repository.findAll(); assertThat(persons.isEmpty(), is(false)); } } The sample creates an application context with Spring’s unit test support, which will perform annotation-based dependency injection into test cases. Inside the test method, we use the repository to query the datastore."
"https://docs.spring.io/spring-data/ldap/reference/3.3/ldap/query-methods.html","Query Methods: Most of the data access operations you usually trigger on a repository result in a query being run against the LDAP directory. Defining such a query is a matter of declaring a method on the repository interface, as the following example shows: PersonRepository with query methods interface PersonRepository extends PagingAndSortingRepository<Person, String> { List<Person> findByLastname(String lastname); (1) List<Person> findByLastnameFirstname(String lastname, String firstname); (2) } 1 The method shows a query for all people with the given lastname . The query is derived by parsing the method name for constraints that can be concatenated with And and Or . Thus, the method name results in a query expression of (&(objectclass=person)(lastname=lastname)) . 2 The method shows a query for all people with the given lastname and firstname . The query is derived by parsing the method name. Thus, the method name results in a query expression of (&(objectclass=person)(lastname=lastname)(firstname=firstname)) . The following table provides samples of the keywords that you can use with query methods: Table 1. Supported keywords for query methods Keyword Sample Logical result LessThanEqual findByAgeLessThanEqual(int age) (attribute⇐age) GreaterThanEqual findByAgeGreaterThanEqual(int age) (attribute>=age) IsNotNull , NotNull findByFirstnameNotNull() (firstname=*) IsNull , Null findByFirstnameNull() (!(firstname=*)) Like findByFirstnameLike(String name) (firstname=name) NotLike , IsNotLike findByFirstnameNotLike(String name) (!(firstname=name*)) StartingWith findByStartingWith(String name) (firstname=name*) EndingWith findByFirstnameLike(String name) (firstname=*name) Containing findByFirstnameLike(String name) (firstname=*name*) (No keyword) findByFirstname(String name) (Firstname=name) Not findByFirstnameNot(String name) (!(Firstname=name))"
"https://docs.spring.io/spring-data/ldap/reference/3.3/ldap/querydsl.html","Querydsl Support: An Annotation Processor, LdapAnnotationProcessor , for generating Querydsl classes based on Spring LDAP ODM annotations. See Object-Directory Mapping(https://docs.spring.io/spring-ldap/docs/3.2.6/reference/#odm) for more information on the ODM annotations. A Query implementation, QueryDslLdapQuery , for building and running Querydsl queries in code. Spring Data repository support for Querydsl predicates. QueryDslPredicateExecutor includes a number of additional methods with appropriate parameters. You can extend this interface (along with LdapRepository ) to include this support in your repository."
"https://docs.spring.io/spring-data/ldap/reference/3.3/ldap/cdi-integration.html","CDI Integration: Instances of the repository interfaces are usually created by a container, for which Spring is the most natural choice when working with Spring Data. Spring Data LDAP includes a custom CDI extension that lets you use the repository abstraction in CDI environments. The extension is part of the JAR. To activate it, drop the Spring Data LDAP JAR into your classpath. You can now set up the infrastructure by implementing a CDI Producer for the LdapTemplate , as the following example shows: class LdapTemplateProducer { @Produces @ApplicationScoped public LdapOperations createLdapTemplate() { ContextSource contextSource = … return new LdapTemplate(contextSource); } } The Spring Data LDAP CDI extension picks up the LdapTemplate as a CDI bean and creates a proxy for a Spring Data repository whenever a bean of a repository type is requested by the container. Thus, obtaining an instance of a Spring Data repository is a matter of declaring an injected property, as the following example shows: class RepositoryClient { @Inject PersonRepository repository; public void businessMethod() { List<Person> people = repository.findAll(); } }"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/index.html","Spring Data MongoDB: Spring Data MongoDB provides support for the MongoDB database. It uses familiar Spring concepts such as a template classes for core API usage and lightweight repository style data access to ease development of applications with a consistent programming model. MongoDB(mongodb.html) MongoDB support and connectivity Repositories(repositories.html) Mongo Repositories Observability(observability/observability.html) Observability Integration Kotlin(kotlin.html) Kotlin support Wiki(https://github.com/spring-projects/spring-data-commons/wiki) What’s New, Upgrade Notes, Supported Versions, additional cross-version information. Mark Pollack; Thomas Risberg; Oliver Gierke; Costin Leau; Jon Brisbin; Thomas Darimont; Christoph Strobl; Mark Paluch; Jay Bryant © 2008-2024 VMware Inc. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/commons/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/migration-guides.html","Migration Guides: This section contains version-specific migration guides explaining how to upgrade between two versions. Section Summary: Migration Guide from 2.x to 3.x(migration-guide/migration-guide-2.x-to-3.x.html) Migration Guide from 3.x to 4.x(migration-guide/migration-guide-3.x-to-4.x.html)"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/migration-guide/migration-guide-2.x-to-3.x.html","Migration Guide from 2.x to 3.x: Spring Data MongoDB 3.x requires the MongoDB Java Driver 4.x To learn more about driver versions please visit the MongoDB Documentation(https://www.mongodb.com/docs/drivers/java/sync/current/upgrade/) . Dependency Changes: org.mongodb:mongo-java-driver (uber jar) got replaced with: bson-jar core-jar sync-jar The change in dependencies allows usage of the reactive support without having to pull the synchronous driver. NOTE: The new sync driver does no longer support com.mongodb.DBObject . Please use org.bson.Document instead. Signature Changes: MongoTemplate no longer supports com.mongodb.MongoClient and com.mongodb.MongoClientOptions . Please use com.mongodb.client.MongoClient and com.mongodb.MongoClientSettings instead. In case you’re using AbstractMongoConfiguration please switch to AbstractMongoClientConfiguration . Namespace Changes: The switch to com.mongodb.client.MongoClient requires an update of your configuration XML if you have one. The best way to provide required connection information is by using a connection string. Please see the MongoDB Documentation(https://docs.mongodb.com/manual/reference/connection-string/) for details. <mongo:mongo.mongo-client id=""with-defaults"" /> <context:property-placeholder location=""classpath:...""/> <mongo:mongo.mongo-client id=""client-just-host-port"" host=""${mongo.host}"" port=""${mongo.port}"" /> <mongo:mongo.mongo-client id=""client-using-connection-string"" connection-string=""mongodb://${mongo.host}:${mongo.port}/?replicaSet=rs0"" /> <mongo:mongo.mongo-client id=""client-with-settings"" replica-set=""rs0""> <mongo:client-settings cluster-connection-mode=""MULTIPLE"" cluster-type=""REPLICA_SET"" cluster-server-selection-timeout=""300"" cluster-local-threshold=""100"" cluster-hosts=""localhost:27018,localhost:27019,localhost:27020"" /> </mongo:mongo.mongo-client>"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/migration-guide/migration-guide-3.x-to-4.x.html","Migration Guide from 3.x to 4.x: Spring Data MongoDB 4.x requires the MongoDB Java Driver 4.8.x To learn more about driver versions please visit the MongoDB Documentation(https://www.mongodb.com/docs/drivers/java/sync/current/upgrade/) ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb.html","MongoDB Support: Spring Data support for MongoDB contains a wide range of features: Spring configuration support(mongodb/template-config.html) with Java-based @Configuration classes or an XML namespace for a Mongo driver instance and replica sets. MongoTemplate helper class(mongodb/template-api.html) that increases productivity when performing common Mongo operations. Includes integrated object mapping between documents and POJOs. Exception translation(mongodb/template-api.html#mongo-template.exception-translation) into Spring’s portable Data Access Exception hierarchy. Feature-rich Object Mapping(mongodb/mapping/mapping.html) integrated with Spring’s Conversion Service. Annotation-based mapping metadata(mongodb/mapping/mapping.html#mapping-usage-annotations) that is extensible to support other metadata formats. Persistence and mapping lifecycle events(mongodb/lifecycle-events.html) . Java-based Query, Criteria, and Update DSLs(mongodb/template-query-operations.html) . Automatic implementation of Repository interfaces(repositories.html) , including support for custom query methods. QueryDSL integration(mongodb/repositories/repositories.html#mongodb.repositories.queries.type-safe) to support type-safe queries. Multi-Document Transactions(mongodb/client-session-transactions.html) . GeoSpatial integration(mongodb/template-query-operations.html#mongo.geo-json) . For most tasks, you should use MongoTemplate or the Repository support, which both leverage the rich mapping functionality. MongoTemplate is the place to look for accessing functionality such as incrementing counters or ad-hoc CRUD operations. MongoTemplate also provides callback methods so that it is easy for you to get the low-level API artifacts, such as com.mongodb.client.MongoDatabase , to communicate directly with MongoDB. The goal with naming conventions on various API artifacts is to copy those in the base MongoDB Java driver so you can easily map your existing knowledge onto the Spring APIs. Section Summary: Requirements(preface.html) Getting Started(mongodb/getting-started.html) Connecting to MongoDB(mongodb/configuration.html) Template API(mongodb/template-api.html) GridFS Support(mongodb/template-gridfs.html) Object Mapping(mongodb/mapping/mapping.html) Value Expressions Fundamentals(mongodb/value-expressions.html) Lifecycle Events(mongodb/lifecycle-events.html) Auditing(mongodb/auditing.html) Sessions & Transactions(mongodb/client-session-transactions.html) Change Streams(mongodb/change-streams.html) Tailable Cursors(mongodb/tailable-cursors.html) Sharding(mongodb/sharding.html) Encryption (CSFLE)(mongodb/mongo-encryption.html)"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/preface.html","Requirements: The Spring Data MongoDB 4.x binaries require JDK level 17 and above and Spring Framework(https://spring.io/docs) 6.1.13 and above. In terms of database and driver, you need at least version 4.x of MongoDB(https://www.mongodb.org/) and a compatible MongoDB Java Driver (4.x or 5.x). Compatibility Matrix: The following compatibility matrix summarizes Spring Data versions to MongoDB driver/database versions. Database versions show the highest supported server version that pass the Spring Data test suite. You can use newer server versions unless your application uses functionality that is affected by changes in the MongoDB server(#compatibility.changes) . See also the official MongoDB driver compatibility matrix(https://www.mongodb.com/docs/drivers/java/sync/current/compatibility/) for driver- and server version compatibility. Spring Data Release Train Spring Data MongoDB Driver Version Server Version 2024.0 4.3.x 4.11.x & 5.x 6.x 2023.1 4.1.x 4.9.x 7.0.x 2023.0 4.1.x 4.9.x 6.0.x 2022.0 4.0.x 4.7.x 6.0.x 2021.2 3.4.x 4.6.x 5.0.x 2021.1 3.3.x 4.4.x 5.0.x 2021.0 3.2.x 4.1.x 4.4.x 2020.0 3.1.x 4.1.x 4.4.x Neumann 3.0.x 4.0.x 4.4.x Moore 2.2.x 3.11.x/Reactive Streams 1.12.x 4.2.x Lovelace 2.1.x 3.8.x/Reactive Streams 1.9.x 4.0.x Relevant Changes in MongoDB 4.4: Fields list must not contain text search score property when no $text criteria present. See also $text operator(https://docs.mongodb.com/manual/reference/operator/query/text/) Sort must not be an empty document when running map reduce. Relevant Changes in MongoDB 4.2: Removal of geoNear command. See also Removal of geoNear(https://docs.mongodb.com/manual/release-notes/4.2-compatibility/#remove-support-for-the-geonear-command) Removal of eval command. See also Removal of eval(https://docs.mongodb.com/manual/release-notes/4.2-compatibility/#remove-support-for-the-eval-command)"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/getting-started.html","Getting Started: An easy way to bootstrap setting up a working environment is to create a Spring-based project via start.spring.io(https://start.spring.io/#!type=maven-project&dependencies=data-mongodb) or create a Spring project in Spring Tools(https://spring.io/tools) . Examples Repository: The GitHub spring-data-examples repository(https://github.com/spring-projects/spring-data-examples) hosts several examples that you can download and play around with to get a feel for how the library works. Hello World: First, you need to set up a running MongoDB server. Refer to the MongoDB Quick Start guide(https://docs.mongodb.org/manual/core/introduction/) for an explanation on how to startup a MongoDB instance. Once installed, starting MongoDB is typically a matter of running the following command: /bin/mongod Then you can create a Person class to persist: package org.springframework.data.mongodb.example; public class Person { private String id; private String name; private int age; public Person(String name, int age) { this.name = name; this.age = age; } public String getId() { return id; } public String getName() { return name; } public int getAge() { return age; } @Override public String toString() { return ""Person [id="" + id + "", name="" + name + "", age="" + age + ""]""; } } You also need a main application to run: Imperative Reactive package org.springframework.data.mongodb.example; import static org.springframework.data.mongodb.core.query.Criteria.*; import org.springframework.data.mongodb.core.MongoOperations; import org.springframework.data.mongodb.core.MongoTemplate; import com.mongodb.client.MongoClients; public class MongoApplication { public static void main(String[] args) throws Exception { MongoOperations mongoOps = new MongoTemplate(MongoClients.create(), ""database""); mongoOps.insert(new Person(""Joe"", 34)); System.out.println(mongoOps.query(Person.class).matching(where(""name"").is(""Joe"")).firstValue()); mongoOps.dropCollection(""person""); } } package org.springframework.data.mongodb.example; import static org.springframework.data.mongodb.core.query.Criteria.*; import org.springframework.data.mongodb.core.ReactiveMongoOperations; import org.springframework.data.mongodb.core.ReactiveMongoTemplate; import com.mongodb.reactivestreams.client.MongoClients; public class ReactiveMongoApplication { public static void main(String[] args) throws Exception { ReactiveMongoOperations mongoOps = new ReactiveMongoTemplate(MongoClients.create(), ""database""); mongoOps.insert(new Person(""Joe"", 34)) .then(mongoOps.query(Person.class).matching(where(""name"").is(""Joe"")).first()) .doOnNext(System.out::println) .block(); mongoOps.dropCollection(""person"").block(); } } When you run the main program, the preceding examples produce the following output: 10:01:32,265 DEBUG o.s.data.mongodb.core.MongoTemplate - insert Document containing fields: [_class, age, name] in collection: Person 10:01:32,765 DEBUG o.s.data.mongodb.core.MongoTemplate - findOne using query: { ""name"" : ""Joe""} in db.collection: database.Person Person [id=4ddbba3c0be56b7e1b210166, name=Joe, age=34] 10:01:32,984 DEBUG o.s.data.mongodb.core.MongoTemplate - Dropped collection [database.person] Even in this simple example, there are few things to notice: You can instantiate the central helper class of Spring Mongo, MongoTemplate(template-api.html) , by using the standard or reactive MongoClient object and the name of the database to use. The mapper works against standard POJO objects without the need for any additional metadata (though you can optionally provide that information. See here(mapping/mapping.html) ). Conventions are used for handling the id field, converting it to be an ObjectId when stored in the database. Mapping conventions can use field access. Notice that the Person class has only getters. If the constructor argument names match the field names of the stored document, they are used to instantiate the object"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/configuration.html","Connecting to MongoDB: One of the first tasks when using MongoDB and Spring is to create a MongoClient object using the IoC container. There are two main ways to do this, either by using Java-based bean metadata or by using XML-based bean metadata. For those not familiar with how to configure the Spring container using Java-based bean metadata instead of XML-based metadata, see the high-level introduction in the reference docs here(https://docs.spring.io/spring/docs/3.2.x/spring-framework-reference/html/new-in-3.0.html#new-java-configuration) as well as the detailed documentation here(https://docs.spring.io/spring-framework/docs/6.1.13/reference/html/core.html#beans-java-instantiating-container) . Registering a Mongo Instance: The following example shows an example to register an instance of a MongoClient : Registering MongoClient Imperative Reactive XML @Configuration public class AppConfig { /* * Use the standard Mongo driver API to create a com.mongodb.client.MongoClient instance. */ public @Bean com.mongodb.client.MongoClient mongoClient() { return com.mongodb.client.MongoClients.create(""mongodb://localhost:27017""); } } @Configuration public class AppConfig { /* * Use the standard Mongo driver API to create a com.mongodb.client.MongoClient instance. */ public @Bean com.mongodb.reactivestreams.client.MongoClient mongoClient() { return com.mongodb.reactivestreams.client.MongoClients.create(""mongodb://localhost:27017""); } } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:mongo=""http://www.springframework.org/schema/data/mongo"" xsi:schemaLocation= "" http://www.springframework.org/schema/data/mongo https://www.springframework.org/schema/data/mongo/spring-mongo.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <!-- Default bean name is 'mongo' --> <mongo:mongo-client host=""localhost"" port=""27017""/> </beans> This approach lets you use the standard MongoClient instance, with the container using Spring’s MongoClientFactoryBean / ReactiveMongoClientFactoryBean . As compared to instantiating a MongoClient instance directly, the FactoryBean has the added advantage of also providing the container with an ExceptionTranslator implementation that translates MongoDB exceptions to exceptions in Spring’s portable DataAccessException hierarchy for data access classes annotated with the @Repository annotation. This hierarchy and the use of @Repository is described in Spring’s DAO support features(https://docs.spring.io/spring-framework/reference/6.1/data-access.html) . The following example shows an example of a Java-based bean metadata that supports exception translation on @Repository annotated classes: Registering a MongoClient via MongoClientFactoryBean / ReactiveMongoClientFactoryBean Imperative Reactive @Configuration public class AppConfig { /* * Factory bean that creates the com.mongodb.client.MongoClient instance */ public @Bean MongoClientFactoryBean mongo() { MongoClientFactoryBean mongo = new MongoClientFactoryBean(); mongo.setHost(""localhost""); return mongo; } } @Configuration public class AppConfig { /* * Factory bean that creates the com.mongodb.reactivestreams.client.MongoClient instance */ public @Bean ReactiveMongoClientFactoryBean mongo() { ReactiveMongoClientFactoryBean mongo = new ReactiveMongoClientFactoryBean(); mongo.setHost(""localhost""); return mongo; } } To access the MongoClient object created by the FactoryBean in other @Configuration classes or your own classes, use a private @Autowired MongoClient mongoClient; field. The MongoDatabaseFactory Interface: While MongoClient is the entry point to the MongoDB driver API, connecting to a specific MongoDB database instance requires additional information, such as the database name and an optional username and password. With that information, you can obtain a MongoDatabase object and access all the functionality of a specific MongoDB database instance. Spring provides the org.springframework.data.mongodb.core.MongoDatabaseFactory & org.springframework.data.mongodb.core.ReactiveMongoDatabaseFactory interfaces, shown in the following listing, to bootstrap connectivity to the database: Imperative Reactive public interface MongoDatabaseFactory { MongoDatabase getDatabase() throws DataAccessException; MongoDatabase getDatabase(String dbName) throws DataAccessException; } public interface ReactiveMongoDatabaseFactory { Mono<MongoDatabase> getDatabase() throws DataAccessException; Mono<MongoDatabase> getDatabase(String dbName) throws DataAccessException; } The following sections show how you can use the container with either Java-based or XML-based metadata to configure an instance of the MongoDatabaseFactory interface. In turn, you can use the MongoDatabaseFactory / ReactiveMongoDatabaseFactory instance to configure MongoTemplate / ReactiveMongoTemplate . Instead of using the IoC container to create an instance of the template, you can use them in standard Java code, as follows: Imperative Reactive public class MongoApplication { public static void main(String[] args) throws Exception { MongoOperations mongoOps = new MongoTemplate(new SimpleMongoClientDatabaseFactory(MongoClients.create(), ""database"")); // ... } } The code in bold highlights the use of SimpleMongoClientDbFactory and is the only difference between the listing shown in the getting started section(getting-started.html) . Use SimpleMongoClientDbFactory when choosing com.mongodb.client.MongoClient as the entrypoint of choice. public class ReactiveMongoApplication { public static void main(String[] args) throws Exception { ReactiveMongoOperations mongoOps = new MongoTemplate(new SimpleReactiveMongoDatabaseFactory(MongoClients.create(), ""database"")); // ... } } Registering a MongoDatabaseFactory / ReactiveMongoDatabaseFactory: To register a MongoDatabaseFactory / ReactiveMongoDatabaseFactory instance with the container, you write code much like what was highlighted in the previous section. The following listing shows a simple example: Imperative Reactive @Configuration public class MongoConfiguration { @Bean public MongoDatabaseFactory mongoDatabaseFactory() { return new SimpleMongoClientDatabaseFactory(MongoClients.create(), ""database""); } } @Configuration public class ReactiveMongoConfiguration { @Bean public ReactiveMongoDatabaseFactory mongoDatabaseFactory() { return new SimpleReactiveMongoDatabaseFactory(MongoClients.create(), ""database""); } } MongoDB Server generation 3 changed the authentication model when connecting to the DB. Therefore, some of the configuration options available for authentication are no longer valid. You should use the MongoClient -specific options for setting credentials through MongoCredential to provide authentication data, as shown in the following example: Java XML @Configuration public class MongoAppConfig extends AbstractMongoClientConfiguration { @Override public String getDatabaseName() { return ""database""; } @Override protected void configureClientSettings(Builder builder) { builder .credential(MongoCredential.createCredential(""name"", ""db"", ""pwd"".toCharArray())) .applyToClusterSettings(settings -> { settings.hosts(singletonList(new ServerAddress(""127.0.0.1"", 27017))); }); } } <mongo:db-factory dbname=""database"" /> Username and password credentials used in XML-based configuration must be URL-encoded when these contain reserved characters, such as : , % , @ , or , . The following example shows encoded credentials: m0ng0@dmin:mo_res:bw6},Qsdxx@admin@database → m0ng0%40dmin:mo_res%3Abw6%7D%2CQsdxx%40admin@database See section 2.2 of RFC 3986(https://tools.ietf.org/html/rfc3986#section-2.2) for further details. If you need to configure additional options on the com.mongodb.client.MongoClient instance that is used to create a SimpleMongoClientDbFactory , you can refer to an existing bean as shown in the following example. To show another common usage pattern, the following listing shows the use of a property placeholder, which lets you parametrize the configuration and the creation of a MongoTemplate : Java XML @Configuration @PropertySource(""classpath:/com/myapp/mongodb/config/mongo.properties"") public class MongoAppConfig extends AbstractMongoClientConfiguration { @Autowired Environment env; @Override public String getDatabaseName() { return ""database""; } @Override protected void configureClientSettings(Builder builder) { builder.applyToClusterSettings(settings -> { settings.hosts(singletonList( new ServerAddress(env.getProperty(""mongo.host""), env.getProperty(""mongo.port"", Integer.class)))); }); builder.applyToConnectionPoolSettings(settings -> { settings.maxConnectionLifeTime(env.getProperty(""mongo.pool-max-life-time"", Integer.class), TimeUnit.MILLISECONDS) .minSize(env.getProperty(""mongo.pool-min-size"", Integer.class)) .maxSize(env.getProperty(""mongo.pool-max-size"", Integer.class)) .maintenanceFrequency(10, TimeUnit.MILLISECONDS) .maintenanceInitialDelay(11, TimeUnit.MILLISECONDS) .maxConnectionIdleTime(30, TimeUnit.SECONDS) .maxWaitTime(15, TimeUnit.MILLISECONDS); }); } } <context:property-placeholder location=""classpath:/com/myapp/mongodb/config/mongo.properties""/> <mongo:mongo-client host=""${mongo.host}"" port=""${mongo.port}""> <mongo:client-settings connection-pool-max-connection-life-time=""${mongo.pool-max-life-time}"" connection-pool-min-size=""${mongo.pool-min-size}"" connection-pool-max-size=""${mongo.pool-max-size}"" connection-pool-maintenance-frequency=""10"" connection-pool-maintenance-initial-delay=""11"" connection-pool-max-connection-idle-time=""30"" connection-pool-max-wait-time=""15"" /> </mongo:mongo-client> <mongo:db-factory dbname=""database"" mongo-ref=""mongoClient""/> <bean id=""anotherMongoTemplate"" class=""org.springframework.data.mongodb.core.MongoTemplate""> <constructor-arg name=""mongoDbFactory"" ref=""mongoDbFactory""/> </bean>"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/template-api.html","Template API: The MongoTemplate(../api/java/org/springframework/data/mongodb/core/MongoTemplate.html) and its reactive(../api/java/org/springframework/data/mongodb/core/ReactiveMongoTemplate.html) counterpart class, located in the org.springframework.data.mongodb.core package, is the central class of Spring’s MongoDB support and provides a rich feature set for interacting with the database. The template offers convenience operations to create, update, delete, and query MongoDB documents and provides a mapping between your domain objects and MongoDB documents. Once configured, MongoTemplate is thread-safe and can be reused across multiple instances. Convenience Methods: The MongoTemplate(../api/java/org/springframework/data/mongodb/core/MongoTemplate.html) class implements the interface MongoOperations(../api/java/org/springframework/data/mongodb/core/MongoOperations.html) . In as much as possible, the methods on MongoOperations are named after methods available on the MongoDB driver Collection object, to make the API familiar to existing MongoDB developers who are used to the driver API. For example, you can find methods such as find , findAndModify , findAndReplace , findOne , insert , remove , save , update , and updateMulti . The design goal was to make it as easy as possible to transition between the use of the base MongoDB driver and MongoOperations . A major difference between the two APIs is that MongoOperations can be passed domain objects instead of Document . Also, MongoOperations has fluent APIs for Query , Criteria , and Update operations instead of populating a Document to specify the parameters for those operations. For more information please refer to the CRUD(template-crud-operations.html) and Query(template-query-operations.html) sections of the documentation. The preferred way to reference the operations on MongoTemplate instance is through its interface, MongoOperations . Execute Callbacks: MongoTemplate offers many convenience methods to help you easily perform common tasks. However, if you need to directly access the MongoDB driver API, you can use one of several Execute callback methods. The execute callbacks gives you a reference to either a MongoCollection or a MongoDatabase object. <T> T execute (Class<?> entityClass, CollectionCallback<T> action) : Runs the given CollectionCallback for the entity collection of the specified class. <T> T execute (String collectionName, CollectionCallback<T> action) : Runs the given CollectionCallback on the collection of the given name. <T> T execute (DbCallback<T> action) : Runs a DbCallback, translating any exceptions as necessary. Spring Data MongoDB provides support for the Aggregation Framework introduced to MongoDB in version 2.2. <T> T execute (String collectionName, DbCallback<T> action) : Runs a DbCallback on the collection of the given name translating any exceptions as necessary. <T> T executeInSession (DbCallback<T> action) : Runs the given DbCallback within the same connection to the database so as to ensure consistency in a write-heavy environment where you may read the data that you wrote. The following example uses the CollectionCallback(../api/java/org/springframework/data/mongodb/core/CollectionCallback.html) to return information about an index: Imperative Reactive boolean hasIndex = template.execute(""geolocation"", collection -> Streamable.of(collection.listIndexes(org.bson.Document.class)) .stream() .map(document -> document.get(""name"")) .anyMatch(""location_2d""::equals) ); Mono<Boolean> hasIndex = template.execute(""geolocation"", collection -> Flux.from(collection.listIndexes(org.bson.Document.class)) .map(document -> document.get(""name"")) .filterWhen(name -> Mono.just(""location_2d"".equals(name))) .map(it -> Boolean.TRUE) .single(Boolean.FALSE) ).next(); Fluent API: Being the central component when it comes to more low-level interaction with MongoDB MongoTemplate offers a wide range of methods covering needs from collection creation, index creation, and CRUD operations to more advanced functionality, such as Map-Reduce and aggregations. You can find multiple overloads for each method. Most of them cover optional or nullable parts of the API. FluentMongoOperations provides a more narrow interface for the common methods of MongoOperations and provides a more readable, fluent API. The entry points ( insert(…) , find(…) , update(…) , and others) follow a natural naming schema based on the operation to be run. Moving on from the entry point, the API is designed to offer only context-dependent methods that lead to a terminating method that invokes the actual MongoOperations counterpart — the all method in the case of the following example: Imperative Reactive List<Jedi> all = template.query(SWCharacter.class) (1) .inCollection(""star-wars"") (2) .as(Jedi.class) (3) .matching(query(where(""jedi"").is(true))) (4) .all(); 1 The type used to map fields used in the query to. 2 The collection name to use if not defined on the domain type. 3 Result type if not using the original domain type. 4 The lookup query. Flux<Jedi> all = template.query(SWCharacter.class) .inCollection(""star-wars"") .as(Jedi.class) .matching(query(where(""jedi"").is(true))) .all(); Using projections allows MongoTemplate to optimize result mapping by limiting the actual response to fields required by the projection target type. This applies as long as the Query(../api/java/org/springframework/data/mongodb/core/query/Query.html) itself does not contain any field restriction and the target type is a closed interface or DTO projection. Projections must not be applied to DBRefs(mapping/document-references.html) . You can switch between retrieving a single entity and retrieving multiple entities as a List or a Stream through the terminating methods: first() , one() , all() , or stream() . When writing a geo-spatial query with near(NearQuery) , the number of terminating methods is altered to include only the methods that are valid for running a geoNear command in MongoDB (fetching entities as a GeoResult within GeoResults ), as the following example shows: Imperative Reactive GeoResults<Jedi> results = template.query(SWCharacter.class) .as(Jedi.class) .near(alderaan) // NearQuery.near(-73.9667, 40.78).maxDis… .all(); Flux<GeoResult<Jedi>> results = template.query(SWCharacter.class) .as(Jedi.class) .near(alderaan) // NearQuery.near(-73.9667, 40.78).maxDis… .all(); Exception Translation: The Spring framework provides exception translation for a wide variety of database and mapping technologies. T his has traditionally been for JDBC and JPA. The Spring support for MongoDB extends this feature to the MongoDB Database by providing an implementation of the org.springframework.dao.support.PersistenceExceptionTranslator interface. The motivation behind mapping to Spring’s consistent data access exception hierarchy(https://docs.spring.io/spring-framework/reference/6.1/data-access.html#dao-exceptions) is that you are then able to write portable and descriptive exception handling code without resorting to coding against MongoDB error codes. All of Spring’s data access exceptions are inherited from the root DataAccessException class so that you can be sure to catch all database related exception within a single try-catch block. Note that not all exceptions thrown by the MongoDB driver inherit from the MongoException class. The inner exception and message are preserved so that no information is lost. Some of the mappings performed by the MongoExceptionTranslator are com.mongodb.Network to DataAccessResourceFailureException and MongoException error codes 1003, 12001, 12010, 12011, and 12012 to InvalidDataAccessApiUsageException . Look into the implementation for more details on the mapping. Domain Type Mapping: The mapping between MongoDB documents and domain classes is done by delegating to an implementation of the MongoConverter(../api/java/org/springframework/data/mongodb/core/convert/MongoConverter.html) interface. Spring provides MappingMongoConverter(../api/java/org/springframework/data/mongodb/core/convert/MappingMongoConverter.html) , but you can also write your own converter. While the MappingMongoConverter can use additional metadata to specify the mapping of objects to documents, it can also convert objects that contain no additional metadata by using some conventions for the mapping of IDs and collection names. These conventions, as well as the use of mapping annotations, are explained in the Mapping(mapping/mapping.html) chapter."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/template-config.html","Configuration: You can use the following configuration to create and register an instance of MongoTemplate , as the following example shows: Registering a MongoClient object and enabling Spring’s exception translation support Imperative Reactive XML @Configuration class ApplicationConfiguration { @Bean MongoClient mongoClient() { return MongoClients.create(""mongodb://localhost:27017""); } @Bean MongoOperations mongoTemplate(MongoClient mongoClient) { return new MongoTemplate(mongoClient, ""geospatial""); } } @Configuration class ReactiveApplicationConfiguration { @Bean MongoClient mongoClient() { return MongoClients.create(""mongodb://localhost:27017""); } @Bean ReactiveMongoOperations mongoTemplate(MongoClient mongoClient) { return new ReactiveMongoTemplate(mongoClient, ""geospatial""); } } <mongo:mongo-client host=""localhost"" port=""27017"" /> <bean id=""mongoTemplate"" class=""org.springframework.data.mongodb.core.MongoTemplate""> <constructor-arg ref=""mongoClient"" /> <constructor-arg name=""databaseName"" value=""geospatial"" /> </bean> There are several overloaded constructors of MongoTemplate(../api/java/org/springframework/data/mongodb/core/MongoTemplate.html) and ReactiveMongoTemplate(../api/java/org/springframework/data/mongodb/core/ReactiveMongoTemplate.html) : MongoTemplate(MongoClient mongo, String databaseName) : Takes the MongoClient object and the default database name to operate against. MongoTemplate(MongoDatabaseFactory mongoDbFactory) : Takes a MongoDbFactory object that encapsulated the MongoClient object, database name, and username and password. MongoTemplate(MongoDatabaseFactory mongoDbFactory, MongoConverter mongoConverter) : Adds a MongoConverter to use for mapping. Other optional properties that you might like to set when creating a MongoTemplate / ReactiveMongoTemplate are the default WriteResultCheckingPolicy , WriteConcern , ReadPreference and others listed below. Default Read Preference: The default read preference applied to read operations if no other preference was defined via the Query(template-query-operations.html#mongo.query.read-preference) . WriteResultChecking Policy: When in development, it is handy to either log or throw an exception if the com.mongodb.WriteResult returned from any MongoDB operation contains an error. It is quite common to forget to do this during development and then end up with an application that looks like it runs successfully when, in fact, the database was not modified according to your expectations. You can set the WriteResultChecking property of MongoTemplate to one of the following values: EXCEPTION or NONE , to either throw an Exception or do nothing, respectively. The default is to use a WriteResultChecking value of NONE . Default WriteConcern: If it has not yet been specified through the driver at a higher level (such as com.mongodb.client.MongoClient ), you can set the com.mongodb.WriteConcern property that the MongoTemplate uses for write operations. If the WriteConcern property is not set, it defaults to the one set in the MongoDB driver’s DB or Collection setting. WriteConcernResolver: For more advanced cases where you want to set different WriteConcern values on a per-operation basis (for remove, update, insert, and save operations), a strategy interface called WriteConcernResolver can be configured on MongoTemplate . Since MongoTemplate is used to persist POJOs, the WriteConcernResolver lets you create a policy that can map a specific POJO class to a WriteConcern value. The following listing shows the WriteConcernResolver interface: public interface WriteConcernResolver { WriteConcern resolve(MongoAction action); } You can use the MongoAction argument to determine the WriteConcern value or use the value of the Template itself as a default. MongoAction contains the collection name being written to, the java.lang.Class of the POJO, the converted Document , the operation ( REMOVE , UPDATE , INSERT , INSERT_LIST , or SAVE ), and a few other pieces of contextual information. The following example shows two sets of classes getting different WriteConcern settings: public class MyAppWriteConcernResolver implements WriteConcernResolver { @Override public WriteConcern resolve(MongoAction action) { if (action.getEntityType().getSimpleName().contains(""Audit"")) { return WriteConcern.ACKNOWLEDGED; } else if (action.getEntityType().getSimpleName().contains(""Metadata"")) { return WriteConcern.JOURNALED; } return action.getDefaultWriteConcern(); } } Publish entity lifecycle events: The template publishes lifecycle events(lifecycle-events.html#mongodb.mapping-usage.events) . In case there are no listeners present, this feature can be disabled. @Bean MongoOperations mongoTemplate(MongoClient mongoClient) { MongoTemplate template = new MongoTemplate(mongoClient, ""geospatial""); template.setEntityLifecycleEventsEnabled(false); // ... } Configure EntityCallbacks: Nest to lifecycle events the template invokes EntityCallbacks(lifecycle-events.html#mongo.entity-callbacks) which can be (if not auto configured) set via the template API. Imperative Reactive @Bean MongoOperations mongoTemplate(MongoClient mongoClient) { MongoTemplate template = new MongoTemplate(mongoClient, ""...""); template.setEntityCallbacks(EntityCallbacks.create(...)); // ... } @Bean ReactiveMongoOperations mongoTemplate(MongoClient mongoClient) { ReactiveMongoTemplate template = new ReactiveMongoTemplate(mongoClient, ""...""); template.setEntityCallbacks(ReactiveEntityCallbacks.create(...)); // ... } Document count configuration: By setting MongoTemplate#useEstimatedCount(…​) to true MongoTemplate#count(…​) operations, that use an empty filter query, will be delegated to estimatedCount , as long as there is no transaction active and the template is not bound to a session(client-session-transactions.html) . Please refer to the Counting Documents(template-document-count.html#mongo.query.count) section for more information."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/template-collection-management.html","Index and Collection Management: MongoTemplate and ReactiveMongoTemplate provide methods for managing indexes and collections. These methods are collected into a helper interface called IndexOperations respectively ReactiveIndexOperations . You can access these operations by calling the indexOps method and passing in either the collection name or the java.lang.Class of your entity (the collection name is derived from the .class , either by name or from annotation metadata). The following listing shows the IndexOperations interface: Imperative Reactive public interface IndexOperations { String ensureIndex(IndexDefinition indexDefinition); void alterIndex(String name, IndexOptions options); void dropIndex(String name); void dropAllIndexes(); List<IndexInfo> getIndexInfo(); } public interface ReactiveIndexOperations { Mono<String> ensureIndex(IndexDefinition indexDefinition); Mono<Void> alterIndex(String name, IndexOptions options); Mono<Void> dropIndex(String name); Mono<Void> dropAllIndexes(); Flux<IndexInfo> getIndexInfo(); Methods for Creating an Index: You can create an index on a collection to improve query performance by using the MongoTemplate class, as the following example shows: Imperative Reactive template.indexOps(Person.class) .ensureIndex(new Index().on(""name"",Order.ASCENDING)); Mono<String> createIndex = template.indexOps(Person.class) .ensureIndex(new Index().on(""name"",Order.ASCENDING)); ensureIndex makes sure that an index for the provided IndexDefinition exists for the collection. You can create standard, geospatial, and text indexes by using the IndexDefinition , GeoSpatialIndex and TextIndexDefinition classes. For example, given the Venue class defined in a previous section, you could declare a geospatial query, as the following example shows: template.indexOps(Venue.class) .ensureIndex(new GeospatialIndex(""location"")); Index and GeospatialIndex support configuration of collations(template-query-operations.html#mongo.query.collation) . Accessing Index Information: The IndexOperations interface has the getIndexInfo method that returns a list of IndexInfo objects. This list contains all the indexes defined on the collection. The following example defines an index on the Person class that has an age property: Imperative Reactive template.indexOps(Person.class) .ensureIndex(new Index().on(""age"", Order.DESCENDING).unique()); List<IndexInfo> indexInfoList = template.indexOps(Person.class) .getIndexInfo(); Mono<String> ageIndex = template.indexOps(Person.class) .ensureIndex(new Index().on(""age"", Order.DESCENDING).unique()); Flux<IndexInfo> indexInfo = ageIndex.then(template.indexOps(Person.class) .getIndexInfo()); Methods for Working with a Collection: The following example shows how to create a collection: Imperative Reactive MongoCollection<Document> collection = null; if (!template.getCollectionNames().contains(""MyNewCollection"")) { collection = mongoTemplate.createCollection(""MyNewCollection""); } MongoCollection<Document> collection = template.getCollectionNames().collectList() .flatMap(collectionNames -> { if(!collectionNames.contains(""MyNewCollection"")) { return template.createCollection(""MyNewCollection""); } return template.getMongoDatabase().map(db -> db.getCollection(""MyNewCollection"")); }); Collection creation allows customization with CollectionOptions and supports collations(collation.html) . Methods to interact with MongoCollections getCollectionNames : Returns a set of collection names. collectionExists : Checks to see if a collection with a given name exists. createCollection : Creates an uncapped collection. dropCollection : Drops the collection. getCollection : Gets a collection by name, creating it if it does not exist. Time Series: MongoDB 5.0 introduced Time Series(https://docs.mongodb.com/manual/core/timeseries-collections/) collections that are optimized to efficiently store documents over time such as measurements or events. Those collections need to be created as such before inserting any data. Collections can be created by either running the createCollection command, defining time series collection options or extracting options from a @TimeSeries annotation as shown in the examples below. Example 1. Create a Time Series Collection Create a Time Series via the MongoDB Driver template.execute(db -> { com.mongodb.client.model.CreateCollectionOptions options = new CreateCollectionOptions(); options.timeSeriesOptions(new TimeSeriesOptions(""timestamp"")); db.createCollection(""weather"", options); return ""OK""; }); Create a Time Series Collection with CollectionOptions template.createCollection(""weather"", CollectionOptions.timeSeries(""timestamp"")); Create a Time Series Collection derived from an Annotation @TimeSeries(collection=""weather"", timeField = ""timestamp"") public class Measurement { String id; Instant timestamp; // ... } template.createCollection(Measurement.class); The snippets above can easily be transferred to the reactive API offering the very same methods. Make sure to properly subscribe to the returned publishers."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/template-crud-operations.html","Saving, Updating, and Removing Documents: MongoTemplate / ReactiveMongoTemplatge let you save, update, and delete your domain objects and map those objects to documents stored in MongoDB. The API signatures of the imperative and reactive API are mainly the same only differing in their return types. While the synchronous API uses void , single Object and List the reactive counterpart consists of Mono<Void> , Mono<Object> and Flux . Consider the following class: public class Person { private String id; private String name; private int age; public Person(String name, int age) { this.name = name; this.age = age; } public String getId() { return id; } public String getName() { return name; } public int getAge() { return age; } @Override public String toString() { return ""Person [id="" + id + "", name="" + name + "", age="" + age + ""]""; } } Given the Person class in the preceding example, you can save, update and delete the object, as the following example shows: Imperative Reactive public class MongoApplication { private static final Log log = LogFactory.getLog(MongoApplication.class); public static void main(String[] args) { MongoOperations template = new MongoTemplate(new SimpleMongoClientDbFactory(MongoClients.create(), ""database"")); Person p = new Person(""Joe"", 34); // Insert is used to initially store the object into the database. template.insert(p); log.info(""Insert: "" + p); // Find p = template.findById(p.getId(), Person.class); log.info(""Found: "" + p); // Update template.updateFirst(query(where(""name"").is(""Joe"")), update(""age"", 35), Person.class); p = template.findOne(query(where(""name"").is(""Joe"")), Person.class); log.info(""Updated: "" + p); // Delete template.remove(p); // Check that deletion worked List<Person> people = template.findAll(Person.class); log.info(""Number of people = : "" + people.size()); template.dropCollection(Person.class); } } The preceding example would produce the following log output (including debug messages from MongoTemplate ): DEBUG apping.MongoPersistentEntityIndexCreator: 80 - Analyzing class class org.spring.example.Person for index information. DEBUG work.data.mongodb.core.MongoTemplate: 632 - insert Document containing fields: [_class, age, name] in collection: person INFO org.spring.example.MongoApp: 30 - Insert: Person [id=4ddc6e784ce5b1eba3ceaf5c, name=Joe, age=34] DEBUG work.data.mongodb.core.MongoTemplate:1246 - findOne using query: { ""_id"" : { ""$oid"" : ""4ddc6e784ce5b1eba3ceaf5c""}} in db.collection: database.person INFO org.spring.example.MongoApp: 34 - Found: Person [id=4ddc6e784ce5b1eba3ceaf5c, name=Joe, age=34] DEBUG work.data.mongodb.core.MongoTemplate: 778 - calling update using query: { ""name"" : ""Joe""} and update: { ""$set"" : { ""age"" : 35}} in collection: person DEBUG work.data.mongodb.core.MongoTemplate:1246 - findOne using query: { ""name"" : ""Joe""} in db.collection: database.person INFO org.spring.example.MongoApp: 39 - Updated: Person [id=4ddc6e784ce5b1eba3ceaf5c, name=Joe, age=35] DEBUG work.data.mongodb.core.MongoTemplate: 823 - remove using query: { ""id"" : ""4ddc6e784ce5b1eba3ceaf5c""} in collection: person INFO org.spring.example.MongoApp: 46 - Number of people = : 0 DEBUG work.data.mongodb.core.MongoTemplate: 376 - Dropped collection [database.person] public class ReactiveMongoApplication { private static final Logger log = LoggerFactory.getLogger(ReactiveMongoApplication.class); public static void main(String[] args) throws Exception { CountDownLatch latch = new CountDownLatch(1); ReactiveMongoTemplate template = new ReactiveMongoTemplate(MongoClients.create(), ""database""); template.insert(new Person(""Joe"", 34)).doOnNext(person -> log.info(""Insert: "" + person)) .flatMap(person -> template.findById(person.getId(), Person.class)) .doOnNext(person -> log.info(""Found: "" + person)) .zipWith(person -> template.updateFirst(query(where(""name"").is(""Joe"")), update(""age"", 35), Person.class)) .flatMap(tuple -> template.remove(tuple.getT1())).flatMap(deleteResult -> template.findAll(Person.class)) .count().doOnSuccess(count -> { log.info(""Number of people: "" + count); latch.countDown(); }) .subscribe(); latch.await(); } } MongoConverter caused implicit conversion between a String and an ObjectId stored in the database by recognizing (through convention) the Id property name. The preceding example is meant to show the use of save, update, and remove operations on MongoTemplate / ReactiveMongoTemplate and not to show complex mapping functionality. The query syntax used in the preceding example is explained in more detail in the section “ Querying Documents(template-query-operations.html) ”. MongoDB requires that you have an _id field for all documents. Please refer to the ID handling(#) section for details on the special treatment of this field. MongoDB collections can contain documents that represent instances of a variety of types. Please refer to the type mapping(converters-type-mapping.html) for details. Insert / Save: There are several convenient methods on MongoTemplate for saving and inserting your objects. To have more fine-grained control over the conversion process, you can register Spring converters with the MappingMongoConverter — for example Converter<Person, Document> and Converter<Document, Person> . The difference between insert and save operations is that a save operation performs an insert if the object is not already present. The simple case of using the save operation is to save a POJO. In this case, the collection name is determined by name (not fully qualified) of the class. You may also call the save operation with a specific collection name. You can use mapping metadata to override the collection in which to store the object. When inserting or saving, if the Id property is not set, the assumption is that its value will be auto-generated by the database. Consequently, for auto-generation of an ObjectId to succeed, the type of the Id property or field in your class must be a String , an ObjectId , or a BigInteger . The following example shows how to save a document and retrieving its contents: Inserting and retrieving documents using the MongoTemplate Imperative Reactive import static org.springframework.data.mongodb.core.query.Criteria.where; import static org.springframework.data.mongodb.core.query.Criteria.query; //... template.insert(new Person(""Bob"", 33)); Person person = template.query(Person.class) .matching(query(where(""age"").is(33))) .oneValue(); import static org.springframework.data.mongodb.core.query.Criteria.where; import static org.springframework.data.mongodb.core.query.Criteria.query; //... Mono<Person> person = mongoTemplate.insert(new Person(""Bob"", 33)) .then(mongoTemplate.query(Person.class) .matching(query(where(""age"").is(33))) .one()); The following insert and save operations are available: void save (Object objectToSave) : Save the object to the default collection. void save (Object objectToSave, String collectionName) : Save the object to the specified collection. A similar set of insert operations is also available: void insert (Object objectToSave) : Insert the object to the default collection. void insert (Object objectToSave, String collectionName) : Insert the object to the specified collection. How the _id Field is Handled in the Mapping Layer: MongoDB requires that you have an _id field for all documents. If you do not provide one, the driver assigns an ObjectId with a generated value without considering your domain model as the server isn’t aware of your identifier type. When you use the MappingMongoConverter , certain rules govern how properties from the Java class are mapped to this _id field: A property or field annotated with @Id ( org.springframework.data.annotation.Id ) maps to the _id field. A property or field without an annotation but named id maps to the _id field. The following outlines what type conversion, if any, is done on the property mapped to the _id document field when using the MappingMongoConverter (the default for MongoTemplate ). If possible, an id property or field declared as a String in the Java class is converted to and stored as an ObjectId by using a Spring Converter<String, ObjectId> . Valid conversion rules are delegated to the MongoDB Java driver. If it cannot be converted to an ObjectId , then the value is stored as a string in the database. An id property or field declared as BigInteger in the Java class is converted to and stored as an ObjectId by using a Spring Converter<BigInteger, ObjectId> . If no field or property specified in the previous sets of rules is present in the Java class, an implicit _id file is generated by the driver but not mapped to a property or field of the Java class. When querying and updating, MongoTemplate uses the converter that corresponds to the preceding rules for saving documents so that field names and types used in your queries can match what is in your domain classes. Some environments require a customized approach to map Id values such as data stored in MongoDB that did not run through the Spring Data mapping layer. Documents can contain _id values that can be represented either as ObjectId or as String . Reading documents from the store back to the domain type works just fine. Querying for documents via their id can be cumbersome due to the implicit ObjectId conversion. Therefore documents cannot be retrieved that way. For those cases @MongoId provides more control over the actual id mapping attempts. Example 1. @MongoId mapping public class PlainStringId { @MongoId String id; (1) } public class PlainObjectId { @MongoId ObjectId id; (2) } public class StringToObjectId { @MongoId(FieldType.OBJECT_ID) String id; (3) } 1 The id is treated as String without further conversion. 2 The id is treated as ObjectId . 3 The id is treated as ObjectId if the given String is a valid ObjectId hex, otherwise as String . Corresponds to @Id usage. Into Which Collection Are My Documents Saved?: There are two ways to manage the collection name that is used for the documents. The default collection name that is used is the class name changed to start with a lower-case letter. So a com.test.Person class is stored in the person collection. You can customize this by providing a different collection name with the @Document annotation. You can also override the collection name by providing your own collection name as the last parameter for the selected MongoTemplate method calls. Inserting or Saving Individual Objects: The MongoDB driver supports inserting a collection of documents in a single operation. The following methods in the MongoOperations interface support this functionality: insert : Inserts an object. If there is an existing document with the same id , an error is generated. insertAll : Takes a Collection of objects as the first parameter. This method inspects each object and inserts it into the appropriate collection, based on the rules specified earlier. save : Saves the object, overwriting any object that might have the same id . Inserting Several Objects in a Batch: The MongoDB driver supports inserting a collection of documents in one operation. The following methods in the MongoOperations interface support this functionality via insert or a dedicated BulkOperations interface. Batch Insert Imperative Reactive Collection<Person> inserted = template.insert(List.of(...), Person.class); Flux<Person> inserted = template.insert(List.of(...), Person.class); Bulk Insert Imperative Reactive BulkWriteResult result = template.bulkOps(BulkMode.ORDERED, Person.class) .insert(List.of(...)) .execute(); Mono<BulkWriteResult> result = template.bulkOps(BulkMode.ORDERED, Person.class) .insert(List.of(...)) .execute(); Server performance of batch and bulk is identical. However bulk operations do not publish lifecycle events(lifecycle-events.html) . Any @Version property that has not been set prior to calling insert will be auto initialized with 1 (in case of a simple type like int ) or 0 for wrapper types (eg. Integer ). Read more in the see Optimistic Locking(#mongo-template.optimistic-locking) section. Update: For updates, you can update the first document found by using MongoOperation.updateFirst or you can update all documents that were found to match the query by using the MongoOperation.updateMulti method or all on the fluent API. The following example shows an update of all SAVINGS accounts where we are adding a one-time $50.00 bonus to the balance by using the $inc operator: Updating documents by using the MongoTemplate / ReactiveMongoTemplate Imperative Reactive import static org.springframework.data.mongodb.core.query.Criteria.where; import org.springframework.data.mongodb.core.query.Update; // ... UpdateResult result = template.update(Account.class) .matching(where(""accounts.accountType"").is(Type.SAVINGS)) .apply(new Update().inc(""accounts.$.balance"", 50.00)) .all(); import static org.springframework.data.mongodb.core.query.Criteria.where; import org.springframework.data.mongodb.core.query.Update; // ... Mono<UpdateResult> result = template.update(Account.class) .matching(where(""accounts.accountType"").is(Type.SAVINGS)) .apply(new Update().inc(""accounts.$.balance"", 50.00)) .all(); In addition to the Query discussed earlier, we provide the update definition by using an Update object. The Update class has methods that match the update modifiers available for MongoDB. Most methods return the Update object to provide a fluent style for the API. @Version properties if not included in the Update will be automatically incremented. Read more in the see Optimistic Locking(#mongo-template.optimistic-locking) section. Methods for Running Updates for Documents: updateFirst : Updates the first document that matches the query document criteria with the updated document. updateMulti : Updates all objects that match the query document criteria with the updated document. updateFirst does not support ordering. Please use findAndModify(#mongo-template.find-and-upsert) to apply Sort . Index hints for the update operation can be provided via Query.withHint(…​) . Methods in the Update Class: You can use a little ""'syntax sugar'"" with the Update class, as its methods are meant to be chained together. Also, you can kick-start the creation of a new Update instance by using public static Update update(String key, Object value) and using static imports. The Update class contains the following methods: Update addToSet (String key, Object value) Update using the $addToSet update modifier Update currentDate (String key) Update using the $currentDate update modifier Update currentTimestamp (String key) Update using the $currentDate update modifier with $type timestamp Update inc (String key, Number inc) Update using the $inc update modifier Update max (String key, Object max) Update using the $max update modifier Update min (String key, Object min) Update using the $min update modifier Update multiply (String key, Number multiplier) Update using the $mul update modifier Update pop (String key, Update.Position pos) Update using the $pop update modifier Update pull (String key, Object value) Update using the $pull update modifier Update pullAll (String key, Object[] values) Update using the $pullAll update modifier Update push (String key, Object value) Update using the $push update modifier Update pushAll (String key, Object[] values) Update using the $pushAll update modifier Update rename (String oldName, String newName) Update using the $rename update modifier Update set (String key, Object value) Update using the $set update modifier Update setOnInsert (String key, Object value) Update using the $setOnInsert update modifier Update unset (String key) Update using the $unset update modifier Some update modifiers, such as $push and $addToSet , allow nesting of additional operators. // { $push : { ""category"" : { ""$each"" : [ ""spring"" , ""data"" ] } } } new Update().push(""category"").each(""spring"", ""data"") // { $push : { ""key"" : { ""$position"" : 0 , ""$each"" : [ ""Arya"" , ""Arry"" , ""Weasel"" ] } } } new Update().push(""key"").atPosition(Position.FIRST).each(Arrays.asList(""Arya"", ""Arry"", ""Weasel"")); // { $push : { ""key"" : { ""$slice"" : 5 , ""$each"" : [ ""Arya"" , ""Arry"" , ""Weasel"" ] } } } new Update().push(""key"").slice(5).each(Arrays.asList(""Arya"", ""Arry"", ""Weasel"")); // { $addToSet : { ""values"" : { ""$each"" : [ ""spring"" , ""data"" , ""mongodb"" ] } } } new Update().addToSet(""values"").each(""spring"", ""data"", ""mongodb""); Aggregation Pipeline Updates: Update methods exposed by MongoOperations and ReactiveMongoOperations also accept an Aggregation Pipeline(aggregation-framework.html) via AggregationUpdate . Using AggregationUpdate allows leveraging MongoDB 4.2 aggregations(https://docs.mongodb.com/manual/reference/method/db.collection.update/#update-with-aggregation-pipeline) in an update operation. Using aggregations in an update allows updating one or more fields by expressing multiple stages and multiple conditions with a single operation. The update can consist of the following stages: AggregationUpdate.set(…​).toValue(…​) → $set : { …​ } AggregationUpdate.unset(…​) → $unset : [ …​ ] AggregationUpdate.replaceWith(…​) → $replaceWith : { …​ } Example 2. Update Aggregation AggregationUpdate update = Aggregation.newUpdate() .set(""average"").toValue(ArithmeticOperators.valueOf(""tests"").avg()) (1) .set(""grade"").toValue(ConditionalOperators.switchCases( (2) when(valueOf(""average"").greaterThanEqualToValue(90)).then(""A""), when(valueOf(""average"").greaterThanEqualToValue(80)).then(""B""), when(valueOf(""average"").greaterThanEqualToValue(70)).then(""C""), when(valueOf(""average"").greaterThanEqualToValue(60)).then(""D"")) .defaultTo(""F"") ); template.update(Student.class) (3) .apply(update) .all(); (4) db.students.update( (3) { }, [ { $set: { average : { $avg: ""$tests"" } } }, (1) { $set: { grade: { $switch: { (2) branches: [ { case: { $gte: [ ""$average"", 90 ] }, then: ""A"" }, { case: { $gte: [ ""$average"", 80 ] }, then: ""B"" }, { case: { $gte: [ ""$average"", 70 ] }, then: ""C"" }, { case: { $gte: [ ""$average"", 60 ] }, then: ""D"" } ], default: ""F"" } } } } ], { multi: true } (4) ) 1 The 1st $set stage calculates a new field average based on the average of the tests field. 2 The 2nd $set stage calculates a new field grade based on the average field calculated by the first aggregation stage. 3 The pipeline is run on the students collection and uses Student for the aggregation field mapping. 4 Apply the update to all matching documents in the collection. Upsert: Related to performing an updateFirst operation, you can also perform an upsert operation, which will perform an insert if no document is found that matches the query. The document that is inserted is a combination of the query document and the update document. The following example shows how to use the upsert method: Imperative Reactive UpdateResult result = template.update(Person.class) .matching(query(where(""ssn"").is(1111).and(""firstName"").is(""Joe"").and(""Fraizer"").is(""Update"")) .apply(update(""address"", addr)) .upsert(); Mono<UpdateResult> result = template.update(Person.class) .matching(query(where(""ssn"").is(1111).and(""firstName"").is(""Joe"").and(""Fraizer"").is(""Update"")) .apply(update(""address"", addr)) .upsert(); upsert does not support ordering. Please use findAndModify(#mongo-template.find-and-upsert) to apply Sort . @Version properties if not included in the Update will be automatically initialized. Read more in the see Optimistic Locking(#mongo-template.optimistic-locking) section. Replacing Documents in a Collection: The various replace methods available via MongoTemplate allow to override the first matching Document. If no match is found a new one can be upserted (as outlined in the previous section) by providing ReplaceOptions with according configuration. Replace one Person tom = template.insert(new Person(""Motte"", 21)); (1) Query query = Query.query(Criteria.where(""firstName"").is(tom.getFirstName())); (2) tom.setFirstname(""Tom""); (3) template.replace(query, tom, ReplaceOptions.none()); (4) 1 Insert a new document. 2 The query used to identify the single document to replace. 3 Set up the replacement document which must hold either the same _id as the existing or no _id at all. 4 Run the replace operation. .Replace one with upsert Person tom = new Person(""id-123"", ""Tom"", 21) (1) Query query = Query.query(Criteria.where(""firstName"").is(tom.getFirstName())); template.replace(query, tom, ReplaceOptions.replaceOptions().upsert()); (2) 1 The _id value needs to be present for upsert, otherwise MongoDB will create a new potentially with the domain type incompatible ObjectId . As MongoDB is not aware of your domain type, any @Field(targetType) hints are not considered and the resulting ObjectId might be not compatible with your domain model. 2 Use upsert to insert a new document if no match is found It is not possible to change the _id of existing documents with a replace operation. On upsert MongoDB uses 2 ways of determining the new id for the entry: * The _id is used within the query as in {""_id"" : 1234 } * The _id is present in the replacement document. If no _id is provided in either way, MongoDB will create a new ObjectId for the document. This may lead to mapping and data lookup malfunctions if the used domain types id property has a different type like e.g. Long . Find and Modify: The findAndModify(…) method on MongoCollection can update a document and return either the old or newly updated document in a single operation. MongoTemplate provides four findAndModify overloaded methods that take Query and Update classes and converts from Document to your POJOs: <T> T findAndModify(Query query, Update update, Class<T> entityClass); <T> T findAndModify(Query query, Update update, Class<T> entityClass, String collectionName); <T> T findAndModify(Query query, Update update, FindAndModifyOptions options, Class<T> entityClass); <T> T findAndModify(Query query, Update update, FindAndModifyOptions options, Class<T> entityClass, String collectionName); The following example inserts a few Person objects into the container and performs a findAndUpdate operation: template.insert(new Person(""Tom"", 21)); template.insert(new Person(""Dick"", 22)); template.insert(new Person(""Harry"", 23)); Query query = new Query(Criteria.where(""firstName"").is(""Harry"")); Update update = new Update().inc(""age"", 1); Person oldValue = template.update(Person.class) .matching(query) .apply(update) .findAndModifyValue(); // oldValue.age == 23 Person newValue = template.query(Person.class) .matching(query) .findOneValue(); // newValye.age == 24 Person newestValue = template.update(Person.class) .matching(query) .apply(update) .withOptions(FindAndModifyOptions.options().returnNew(true)) // Now return the newly updated document when updating .findAndModifyValue(); // newestValue.age == 25 The FindAndModifyOptions method lets you set the options of returnNew , upsert , and remove . An example extending from the previous code snippet follows: Person upserted = template.update(Person.class) .matching(new Query(Criteria.where(""firstName"").is(""Mary""))) .apply(update) .withOptions(FindAndModifyOptions.options().upsert(true).returnNew(true)) .findAndModifyValue() @Version properties if not included in the Update will be automatically incremented. Read more in the see Optimistic Locking(#mongo-template.optimistic-locking) section. Find and Replace: The most straight forward method of replacing an entire Document is via its id using the save method. However this might not always be feasible. findAndReplace offers an alternative that allows to identify the document to replace via a simple query. Example 3. Find and Replace Documents Optional<User> result = template.update(Person.class) (1) .matching(query(where(""firstame"").is(""Tom""))) (2) .replaceWith(new Person(""Dick"")) .withOptions(FindAndReplaceOptions.options().upsert()) (3) .as(User.class) (4) .findAndReplace(); (5) 1 Use the fluent update API with the domain type given for mapping the query and deriving the collection name or just use MongoOperations#findAndReplace . 2 The actual match query mapped against the given domain type. Provide sort , fields and collation settings via the query. 3 Additional optional hook to provide options other than the defaults, like upsert . 4 An optional projection type used for mapping the operation result. If none given the initial domain type is used. 5 Trigger the actual processing. Use findAndReplaceValue to obtain the nullable result instead of an Optional . Please note that the replacement must not hold an id itself as the id of the existing Document will be carried over to the replacement by the store itself. Also keep in mind that findAndReplace will only replace the first document matching the query criteria depending on a potentially given sort order. Delete: You can use one of five overloaded methods to remove an object from the database: template.remove(tywin, ""GOT""); (1) template.remove(query(where(""lastname"").is(""lannister"")), ""GOT""); (2) template.remove(new Query().limit(3), ""GOT""); (3) template.findAllAndRemove(query(where(""lastname"").is(""lannister""), ""GOT""); (4) template.findAllAndRemove(new Query().limit(3), ""GOT""); (5) 1 Remove a single entity specified by its _id from the associated collection. 2 Remove all documents that match the criteria of the query from the GOT collection. 3 Remove the first three documents in the GOT collection. Unlike <2>, the documents to remove are identified by their _id , running the given query, applying sort , limit , and skip options first, and then removing all at once in a separate step. 4 Remove all documents matching the criteria of the query from the GOT collection. Unlike <3>, documents do not get deleted in a batch but one by one. 5 Remove the first three documents in the GOT collection. Unlike <3>, documents do not get deleted in a batch but one by one. Optimistic Locking: The @Version annotation provides syntax similar to that of JPA in the context of MongoDB and makes sure updates are only applied to documents with a matching version. Therefore, the actual value of the version property is added to the update query in such a way that the update does not have any effect if another operation altered the document in the meantime. In that case, an OptimisticLockingFailureException is thrown. The following example shows these features: @Document class Person { @Id String id; String firstname; String lastname; @Version Long version; } Person daenerys = template.insert(new Person(""Daenerys"")); (1) Person tmp = template.findOne(query(where(""id"").is(daenerys.getId())), Person.class); (2) daenerys.setLastname(""Targaryen""); template.save(daenerys); (3) template.save(tmp); // throws OptimisticLockingFailureException (4) 1 Intially insert document. version is set to 0 . 2 Load the just inserted document. version is still 0 . 3 Update the document with version = 0 . Set the lastname and bump version to 1 . 4 Try to update the previously loaded document that still has version = 0 . The operation fails with an OptimisticLockingFailureException , as the current version is 1 . Only certain CRUD operations on MongoTemplate do consider and alter version properties. Please consult MongoOperations java doc for detailed information. Optimistic Locking requires to set the WriteConcern to ACKNOWLEDGED . Otherwise OptimisticLockingFailureException can be silently swallowed. As of Version 2.2 MongoOperations also includes the @Version property when removing an entity from the database. To remove a Document without version check use MongoOperations#remove(Query,…​) instead of MongoOperations#remove(Object) . As of Version 2.2 repositories check for the outcome of acknowledged deletes when removing versioned entities. An OptimisticLockingFailureException is raised if a versioned entity cannot be deleted through CrudRepository.delete(Object) . In such case, the version was changed or the object was deleted in the meantime. Use CrudRepository.deleteById(ID) to bypass optimistic locking functionality and delete objects regardless of their version."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/template-query-operations.html","Querying Documents: You can use the Query and Criteria classes to express your queries. They have method names that mirror the native MongoDB operator names, such as lt , lte , is , and others. The Query and Criteria classes follow a fluent API style so that you can chain together multiple method criteria and queries while having easy-to-understand code. To improve readability, static imports let you avoid using the 'new' keyword for creating Query and Criteria instances. You can also use BasicQuery to create Query instances from plain JSON Strings, as shown in the following example: Example 1. Creating a Query instance from a plain JSON String BasicQuery query = new BasicQuery(""{ age : { $lt : 50 }, accounts.balance : { $gt : 1000.00 }}""); List<Person> result = mongoTemplate.find(query, Person.class); Querying Documents in a Collection: Earlier, we saw how to retrieve a single document by using the findOne and findById methods on MongoTemplate . These methods return a single domain object right way or using a reactive API a Mono emitting a single element. We can also query for a collection of documents to be returned as a list of domain objects. Assuming that we have a number of Person objects with name and age stored as documents in a collection and that each person has an embedded account document with a balance, we can now run a query using the following code: Querying for documents using the MongoTemplate Imperative Reactive import static org.springframework.data.mongodb.core.query.Criteria.where; import static org.springframework.data.mongodb.core.query.Query.query; // ... List<Person> result = template.query(Person.class) .matching(query(where(""age"").lt(50).and(""accounts.balance"").gt(1000.00d))) .all(); import static org.springframework.data.mongodb.core.query.Criteria.where; import static org.springframework.data.mongodb.core.query.Query.query; // ... Flux<Person> result = template.query(Person.class) .matching(query(where(""age"").lt(50).and(""accounts.balance"").gt(1000.00d))) .all(); All find methods take a Query object as a parameter. This object defines the criteria and options used to perform the query. The criteria are specified by using a Criteria object that has a static factory method named where to instantiate a new Criteria object. We recommend using static imports for org.springframework.data.mongodb.core.query.Criteria.where and Query.query to make the query more readable. The query should return a List or Flux of Person objects that meet the specified criteria. The rest of this section lists the methods of the Criteria and Query classes that correspond to the operators provided in MongoDB. Most methods return the Criteria object, to provide a fluent style for the API. Methods of the Criteria Class The Criteria class provides the following methods, all of which correspond to operators in MongoDB: Criteria all (Object o) Creates a criterion using the $all operator Criteria and (String key) Adds a chained Criteria with the specified key to the current Criteria and returns the newly created one Criteria andOperator (Criteria…​ criteria) Creates an and query using the $and operator for all of the provided criteria (requires MongoDB 2.0 or later) Criteria andOperator (Collection<Criteria> criteria) Creates an and query using the $and operator for all of the provided criteria (requires MongoDB 2.0 or later) Criteria elemMatch (Criteria c) Creates a criterion using the $elemMatch operator Criteria exists (boolean b) Creates a criterion using the $exists operator Criteria gt (Object o) Creates a criterion using the $gt operator Criteria gte (Object o) Creates a criterion using the $gte operator Criteria in (Object…​ o) Creates a criterion using the $in operator for a varargs argument. Criteria in (Collection<?> collection) Creates a criterion using the $in operator using a collection Criteria is (Object o) Creates a criterion using field matching ( { key:value } ). If the specified value is a document, the order of the fields and exact equality in the document matters. Criteria lt (Object o) Creates a criterion using the $lt operator Criteria lte (Object o) Creates a criterion using the $lte operator Criteria mod (Number value, Number remainder) Creates a criterion using the $mod operator Criteria ne (Object o) Creates a criterion using the $ne operator Criteria nin (Object…​ o) Creates a criterion using the $nin operator Criteria norOperator (Criteria…​ criteria) Creates an nor query using the $nor operator for all of the provided criteria Criteria norOperator (Collection<Criteria> criteria) Creates an nor query using the $nor operator for all of the provided criteria Criteria not () Creates a criterion using the $not meta operator which affects the clause directly following Criteria orOperator (Criteria…​ criteria) Creates an or query using the $or operator for all of the provided criteria Criteria orOperator (Collection<Criteria> criteria) Creates an or query using the $or operator for all of the provided criteria Criteria regex (String re) Creates a criterion using a $regex Criteria sampleRate (double sampleRate) Creates a criterion using the $sampleRate operator Criteria size (int s) Creates a criterion using the $size operator Criteria type (int t) Creates a criterion using the $type operator Criteria matchingDocumentStructure (MongoJsonSchema schema) Creates a criterion using the $jsonSchema operator for JSON schema criteria(mapping/mapping-schema.html) . $jsonSchema can only be applied on the top level of a query and not property specific. Use the properties attribute of the schema to match against nested fields. Criteria bits() is the gateway to MongoDB bitwise query operators(https://docs.mongodb.com/manual/reference/operator/query-bitwise/) like $bitsAllClear . The Criteria class also provides the following methods for geospatial queries. Criteria within (Circle circle) Creates a geospatial criterion using $geoWithin $center operators. Criteria within (Box box) Creates a geospatial criterion using a $geoWithin $box operation. Criteria withinSphere (Circle circle) Creates a geospatial criterion using $geoWithin $center operators. Criteria near (Point point) Creates a geospatial criterion using a $near operation Criteria nearSphere (Point point) Creates a geospatial criterion using $nearSphere$center operations. This is only available for MongoDB 1.7 and higher. Criteria minDistance (double minDistance) Creates a geospatial criterion using the $minDistance operation, for use with $near. Criteria maxDistance (double maxDistance) Creates a geospatial criterion using the $maxDistance operation, for use with $near. The Query class has some additional methods that allow to select certain fields as well as to limit and sort the result. Methods of the Query class Query addCriteria (Criteria criteria) used to add additional criteria to the query Field fields () used to define fields to be included in the query results Query limit (int limit) used to limit the size of the returned results to the provided limit (used for paging) Query skip (int skip) used to skip the provided number of documents in the results (used for paging) Query with (Sort sort) used to provide sort definition for the results Query with (ScrollPosition position) used to provide a scroll position (Offset- or Keyset-based pagination) to start or resume a Scroll The template API allows direct usage of result projections that enable you to map queries against a given domain type while projecting the operation result onto another one as outlined below. class template.query(SWCharacter.class) .as(Jedi.class) For more information on result projections please refer to the Projections(../repositories/projections.html) section of the documentation. Selecting fields: MongoDB supports projecting fields(https://docs.mongodb.com/manual/tutorial/project-fields-from-query-results/) returned by a query. A projection can include and exclude fields (the _id field is always included unless explicitly excluded) based on their name. Example 2. Selecting result fields public class Person { @Id String id; String firstname; @Field(""last_name"") String lastname; Address address; } query.fields().include(""lastname""); (1) query.fields().exclude(""id"").include(""lastname"") (2) query.fields().include(""address"") (3) query.fields().include(""address.city"") (4) 1 Result will contain both _id and last_name via { ""last_name"" : 1 } . 2 Result will only contain the last_name via { ""_id"" : 0, ""last_name"" : 1 } . 3 Result will contain the _id and entire address object via { ""address"" : 1 } . 4 Result will contain the _id and and address object that only contains the city field via { ""address.city"" : 1 } . Starting with MongoDB 4.4 you can use aggregation expressions for field projections as shown below: Example 3. Computing result fields using expressions query.fields() .project(MongoExpression.create(""'$toUpper' : '$last_name'"")) (1) .as(""last_name""); (2) query.fields() .project(StringOperators.valueOf(""lastname"").toUpper()) (3) .as(""last_name""); query.fields() .project(AggregationSpELExpression.expressionOf(""toUpper(lastname)"")) (4) .as(""last_name""); 1 Use a native expression. The used field name must refer to field names within the database document. 2 Assign the field name to which the expression result is projected. The resulting field name is not mapped against the domain model. 3 Use an AggregationExpression . Other than native MongoExpression , field names are mapped to the ones used in the domain model. 4 Use SpEL along with an AggregationExpression to invoke expression functions. Field names are mapped to the ones used in the domain model. @Query(fields=""…"") allows usage of expression field projections at Repository level as described in MongoDB JSON-based Query Methods and Field Restriction(repositories/repositories.html#mongodb.repositories.queries.json-based) . Additional Query Options: MongoDB offers various ways of applying meta information, like a comment or a batch size, to a query.Using the Query API directly there are several methods for those options. Hints: Index hints can be applied in two ways, using the index name or its field definition. template.query(Person.class) .matching(query(""..."").withHint(""index-to-use"")); template.query(Person.class) .matching(query(""..."").withHint(""{ firstname : 1 }"")); Cursor Batch Size: The cursor batch size defines the number of documents to return in each response batch. Query query = query(where(""firstname"").is(""luke"")) .cursorBatchSize(100) Collations: Using collations with collection operations is a matter of specifying a Collation instance in your query or operation options, as the following two examples show: Collation collation = Collation.of(""de""); Query query = new Query(Criteria.where(""firstName"").is(""Amél"")) .collation(collation); List<Person> results = template.find(query, Person.class); Read Preference: The ReadPreference to use can be set directly on the Query object to be run as outlined below. template.find(Person.class) .matching(query(where(...)).withReadPreference(ReadPreference.secondary())) .all(); The preference set on the Query instance will supersede the default ReadPreference of MongoTemplate . Comments: Queries can be equipped with comments which makes them easier to look up in server logs. template.find(Person.class) .matching(query(where(...)).comment(""Use the force luke!"")) .all(); Query Distinct Values: MongoDB provides an operation to obtain distinct values for a single field by using a query from the resulting documents. Resulting values are not required to have the same data type, nor is the feature limited to simple types. For retrieval, the actual result type does matter for the sake of conversion and typing. The following example shows how to query for distinct values: Example 4. Retrieving distinct values template.query(Person.class) (1) .distinct(""lastname"") (2) .all(); (3) 1 Query the Person collection. 2 Select distinct values of the lastname field. The field name is mapped according to the domain types property declaration, taking potential @Field annotations into account. 3 Retrieve all distinct values as a List of Object (due to no explicit result type being specified). Retrieving distinct values into a Collection of Object is the most flexible way, as it tries to determine the property value of the domain type and convert results to the desired type or mapping Document structures. Sometimes, when all values of the desired field are fixed to a certain type, it is more convenient to directly obtain a correctly typed Collection , as shown in the following example: Example 5. Retrieving strongly typed distinct values template.query(Person.class) (1) .distinct(""lastname"") (2) .as(String.class) (3) .all(); (4) 1 Query the collection of Person . 2 Select distinct values of the lastname field. The fieldname is mapped according to the domain types property declaration, taking potential @Field annotations into account. 3 Retrieved values are converted into the desired target type — in this case, String . It is also possible to map the values to a more complex type if the stored field contains a document. 4 Retrieve all distinct values as a List of String . If the type cannot be converted into the desired target type, this method throws a DataAccessException . += GeoSpatial Queries MongoDB supports GeoSpatial queries through the use of operators such as $near , $within , geoWithin , and $nearSphere . Methods specific to geospatial queries are available on the Criteria class. There are also a few shape classes ( Box , Circle , and Point ) that are used in conjunction with geospatial related Criteria methods. Using GeoSpatial queries requires attention when used within MongoDB transactions, see Special behavior inside transactions(client-session-transactions.html#mongo.transactions.behavior) . To understand how to perform GeoSpatial queries, consider the following Venue class (taken from the integration tests and relying on the rich MappingMongoConverter ): Venue.java @Document(collection=""newyork"") public class Venue { @Id private String id; private String name; private double[] location; @PersistenceConstructor Venue(String name, double[] location) { super(); this.name = name; this.location = location; } public Venue(String name, double x, double y) { super(); this.name = name; this.location = new double[] { x, y }; } public String getName() { return name; } public double[] getLocation() { return location; } @Override public String toString() { return ""Venue [id="" + id + "", name="" + name + "", location="" + Arrays.toString(location) + ""]""; } } To find locations within a Circle , you can use the following query: Circle circle = new Circle(-73.99171, 40.738868, 0.01); List<Venue> venues = template.find(new Query(Criteria.where(""location"").within(circle)), Venue.class); To find venues within a Circle using spherical coordinates, you can use the following query: Circle circle = new Circle(-73.99171, 40.738868, 0.003712240453784); List<Venue> venues = template.find(new Query(Criteria.where(""location"").withinSphere(circle)), Venue.class); To find venues within a Box , you can use the following query: //lower-left then upper-right Box box = new Box(new Point(-73.99756, 40.73083), new Point(-73.988135, 40.741404)); List<Venue> venues = template.find(new Query(Criteria.where(""location"").within(box)), Venue.class); To find venues near a Point , you can use the following queries: Point point = new Point(-73.99171, 40.738868); List<Venue> venues = template.find(new Query(Criteria.where(""location"").near(point).maxDistance(0.01)), Venue.class); Point point = new Point(-73.99171, 40.738868); List<Venue> venues = template.find(new Query(Criteria.where(""location"").near(point).minDistance(0.01).maxDistance(100)), Venue.class); To find venues near a Point using spherical coordinates, you can use the following query: Point point = new Point(-73.99171, 40.738868); List<Venue> venues = template.find(new Query( Criteria.where(""location"").nearSphere(point).maxDistance(0.003712240453784)), Venue.class); Geo-near Queries: Changed in 2.2! MongoDB 4.2(https://docs.mongodb.com/master/release-notes/4.2-compatibility/) removed support for the geoNear command which had been previously used to run the NearQuery . Spring Data MongoDB 2.2 MongoOperations#geoNear uses the $geoNear aggregation(https://docs.mongodb.com/manual/reference/operator/aggregation/geoNear/) instead of the geoNear command to run a NearQuery . The calculated distance (the dis when using a geoNear command) previously returned within a wrapper type now is embedded into the resulting document. If the given domain type already contains a property with that name, the calculated distance is named calculated-distance with a potentially random postfix. Target types may contain a property named after the returned distance to (additionally) read it back directly into the domain type as shown below. GeoResults<VenueWithDistanceField> = template.query(Venue.class) (1) .as(VenueWithDistanceField.class) (2) .near(NearQuery.near(new GeoJsonPoint(-73.99, 40.73), KILOMETERS)) .all(); 1 Domain type used to identify the target collection and potential query mapping. 2 Target type containing a dis field of type Number . MongoDB supports querying the database for geo locations and calculating the distance from a given origin at the same time. With geo-near queries, you can express queries such as ""find all restaurants in the surrounding 10 miles"". To let you do so, MongoOperations provides geoNear(…) methods that take a NearQuery as an argument (as well as the already familiar entity type and collection), as shown in the following example: Point location = new Point(-73.99171, 40.738868); NearQuery query = NearQuery.near(location).maxDistance(new Distance(10, Metrics.MILES)); GeoResults<Restaurant> = operations.geoNear(query, Restaurant.class); We use the NearQuery builder API to set up a query to return all Restaurant instances surrounding the given Point out to 10 miles. The Metrics enum used here actually implements an interface so that other metrics could be plugged into a distance as well. A Metric is backed by a multiplier to transform the distance value of the given metric into native distances. The sample shown here would consider the 10 to be miles. Using one of the built-in metrics (miles and kilometers) automatically triggers the spherical flag to be set on the query. If you want to avoid that, pass plain double values into maxDistance(…) . For more information, see the Javadoc of NearQuery(../api/java/org/springframework/data/mongodb/core/query/NearQuery.html) and Distance . The geo-near operations return a GeoResults wrapper object that encapsulates GeoResult instances. Wrapping GeoResults allows accessing the average distance of all results. A single GeoResult object carries the entity found plus its distance from the origin. GeoJSON Support: MongoDB supports GeoJSON(https://geojson.org/) and simple (legacy) coordinate pairs for geospatial data. Those formats can both be used for storing as well as querying data. See the MongoDB manual on GeoJSON support(https://docs.mongodb.org/manual/core/2dsphere/#geospatial-indexes-store-geojson/) to learn about requirements and restrictions. GeoJSON Types in Domain Classes: Usage of GeoJSON(https://geojson.org/) types in domain classes is straightforward. The org.springframework.data.mongodb.core.geo package contains types such as GeoJsonPoint , GeoJsonPolygon , and others. These types are extend the existing org.springframework.data.geo types. The following example uses a GeoJsonPoint(../api/java/org/springframework/data/mongodb/core/geo/GeoJsonPoint.html) : public class Store { String id; /** * { ""type"" : ""Point"", ""coordinates"" : [ x, y ] } */ GeoJsonPoint location; } If the coordinates of a GeoJSON object represent latitude and longitude pairs, the longitude goes first followed by latitude . GeoJsonPoint therefore treats getX() as longitude and getY() as latitude . GeoJSON Types in Repository Query Methods: Using GeoJSON types as repository query parameters forces usage of the $geometry operator when creating the query, as the following example shows: public interface StoreRepository extends CrudRepository<Store, String> { List<Store> findByLocationWithin(Polygon polygon); (1) } /* * { * ""location"": { * ""$geoWithin"": { * ""$geometry"": { * ""type"": ""Polygon"", * ""coordinates"": [ * [ * [-73.992514,40.758934], * [-73.961138,40.760348], * [-73.991658,40.730006], * [-73.992514,40.758934] * ] * ] * } * } * } * } */ repo.findByLocationWithin( (2) new GeoJsonPolygon( new Point(-73.992514, 40.758934), new Point(-73.961138, 40.760348), new Point(-73.991658, 40.730006), new Point(-73.992514, 40.758934))); (3) /* * { * ""location"" : { * ""$geoWithin"" : { * ""$polygon"" : [ [-73.992514,40.758934] , [-73.961138,40.760348] , [-73.991658,40.730006] ] * } * } * } */ repo.findByLocationWithin( (4) new Polygon( new Point(-73.992514, 40.758934), new Point(-73.961138, 40.760348), new Point(-73.991658, 40.730006))); 1 Repository method definition using the commons type allows calling it with both the GeoJSON and the legacy format. 2 Use GeoJSON type to make use of $geometry operator. 3 Note that GeoJSON polygons need to define a closed ring. 4 Use the legacy format $polygon operator. Metrics and Distance calculation: Then MongoDB $geoNear operator allows usage of a GeoJSON Point or legacy coordinate pairs. NearQuery.near(new Point(-73.99171, 40.738868)) { ""$geoNear"": { //... ""near"": [-73.99171, 40.738868] } } NearQuery.near(new GeoJsonPoint(-73.99171, 40.738868)) { ""$geoNear"": { //... ""near"": { ""type"": ""Point"", ""coordinates"": [-73.99171, 40.738868] } } } Though syntactically different the server is fine accepting both no matter what format the target Document within the collection is using. There is a huge difference in the distance calculation. Using the legacy format operates upon Radians on an Earth like sphere, whereas the GeoJSON format uses Meters . To avoid a serious headache make sure to set the Metric to the desired unit of measure which ensures the distance to be calculated correctly. In other words: Assume you’ve got 5 Documents like the ones below: { ""_id"" : ObjectId(""5c10f3735d38908db52796a5""), ""name"" : ""Penn Station"", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.99408, 40.75057 ] } } { ""_id"" : ObjectId(""5c10f3735d38908db52796a6""), ""name"" : ""10gen Office"", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.99171, 40.738868 ] } } { ""_id"" : ObjectId(""5c10f3735d38908db52796a9""), ""name"" : ""City Bakery "", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.992491, 40.738673 ] } } { ""_id"" : ObjectId(""5c10f3735d38908db52796aa""), ""name"" : ""Splash Bar"", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.992491, 40.738673 ] } } { ""_id"" : ObjectId(""5c10f3735d38908db52796ab""), ""name"" : ""Momofuku Milk Bar"", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.985839, 40.731698 ] } } Fetching all Documents within a 400 Meter radius from [-73.99171, 40.738868] would look like this using GeoJSON: Example 6. GeoNear with GeoJSON { ""$geoNear"": { ""maxDistance"": 400, (1) ""num"": 10, ""near"": { type: ""Point"", coordinates: [-73.99171, 40.738868] }, ""spherical"":true, (2) ""key"": ""location"", ""distanceField"": ""distance"" } } Returning the following 3 Documents: { ""_id"" : ObjectId(""5c10f3735d38908db52796a6""), ""name"" : ""10gen Office"", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.99171, 40.738868 ] } ""distance"" : 0.0 (3) } { ""_id"" : ObjectId(""5c10f3735d38908db52796a9""), ""name"" : ""City Bakery "", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.992491, 40.738673 ] } ""distance"" : 69.3582262492474 (3) } { ""_id"" : ObjectId(""5c10f3735d38908db52796aa""), ""name"" : ""Splash Bar"", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.992491, 40.738673 ] } ""distance"" : 69.3582262492474 (3) } 1 Maximum distance from center point in Meters . 2 GeoJSON always operates upon a sphere. 3 Distance from center point in Meters . Now, when using legacy coordinate pairs one operates upon Radians as discussed before. So we use Metrics#KILOMETERS when constructing the `$geoNear command. The Metric makes sure the distance multiplier is set correctly. Example 7. GeoNear with Legacy Coordinate Pairs { ""$geoNear"": { ""maxDistance"": 0.0000627142377, (1) ""distanceMultiplier"": 6378.137, (2) ""num"": 10, ""near"": [-73.99171, 40.738868], ""spherical"":true, (3) ""key"": ""location"", ""distanceField"": ""distance"" } } Returning the 3 Documents just like the GeoJSON variant: { ""_id"" : ObjectId(""5c10f3735d38908db52796a6""), ""name"" : ""10gen Office"", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.99171, 40.738868 ] } ""distance"" : 0.0 (4) } { ""_id"" : ObjectId(""5c10f3735d38908db52796a9""), ""name"" : ""City Bakery "", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.992491, 40.738673 ] } ""distance"" : 0.0693586286032982 (4) } { ""_id"" : ObjectId(""5c10f3735d38908db52796aa""), ""name"" : ""Splash Bar"", ""location"" : { ""type"" : ""Point"", ""coordinates"" : [ -73.992491, 40.738673 ] } ""distance"" : 0.0693586286032982 (4) } 1 Maximum distance from center point in Radians . 2 The distance multiplier so we get Kilometers as resulting distance. 3 Make sure we operate on a 2d_sphere index. 4 Distance from center point in Kilometers - take it times 1000 to match Meters of the GeoJSON variant. Full-text Search: Since version 2.6 of MongoDB, you can run full-text queries by using the $text operator. Methods and operations specific to full-text queries are available in TextQuery and TextCriteria . When doing full text search, see the MongoDB reference(https://docs.mongodb.org/manual/reference/operator/query/text/#behavior) for its behavior and limitations. Before you can actually use full-text search, you must set up the search index correctly. See Text Index(mapping/mapping.html#mapping-usage-indexes.text-index) for more detail on how to create index structures. The following example shows how to set up a full-text search: db.foo.createIndex( { title : ""text"", content : ""text"" }, { weights : { title : 3 } } ) A query searching for coffee cake can be defined and run as follows: Example 8. Full Text Query Query query = TextQuery .queryText(new TextCriteria().matchingAny(""coffee"", ""cake"")); List<Document> page = template.find(query, Document.class); To sort results by relevance according to the weights use TextQuery.sortByScore . Example 9. Full Text Query - Sort by Score Query query = TextQuery .queryText(new TextCriteria().matchingAny(""coffee"", ""cake"")) .sortByScore() (1) .includeScore(); (2) List<Document> page = template.find(query, Document.class); 1 Use the score property for sorting results by relevance which triggers .sort({'score': {'$meta': 'textScore'}}) . 2 Use TextQuery.includeScore() to include the calculated relevance in the resulting Document . You can exclude search terms by prefixing the term with - or by using notMatching , as shown in the following example (note that the two lines have the same effect and are thus redundant): // search for 'coffee' and not 'cake' TextQuery.queryText(new TextCriteria().matching(""coffee"").matching(""-cake"")); TextQuery.queryText(new TextCriteria().matching(""coffee"").notMatching(""cake"")); TextCriteria.matching takes the provided term as is. Therefore, you can define phrases by putting them between double quotation marks (for example, \""coffee cake\"") or using by TextCriteria.phrase. The following example shows both ways of defining a phrase: // search for phrase 'coffee cake' TextQuery.queryText(new TextCriteria().matching(""\""coffee cake\"""")); TextQuery.queryText(new TextCriteria().phrase(""coffee cake"")); You can set flags for $caseSensitive and $diacriticSensitive by using the corresponding methods on TextCriteria . Note that these two optional flags have been introduced in MongoDB 3.2 and are not included in the query unless explicitly set. Query by Example: Query by Example(repositories/query-methods.html#query-by-example) can be used on the Template API level run example queries. The following snipped shows how to query by example: Typed Example Query Person probe = new Person(); probe.lastname = ""stark""; Example example = Example.of(probe); Query query = new Query(new Criteria().alike(example)); List<Person> result = template.find(query, Person.class); By default Example is strictly typed. This means that the mapped query has an included type match, restricting it to probe assignable types. For example, when sticking with the default type key ( _class ), the query has restrictions such as ( _class : { $in : [ com.acme.Person] } ). By using the UntypedExampleMatcher , it is possible to bypass the default behavior and skip the type restriction. So, as long as field names match, nearly any domain type can be used as the probe for creating the reference, as the following example shows: Example 10. Untyped Example Query class JustAnArbitraryClassWithMatchingFieldName { @Field(""lastname"") String value; } JustAnArbitraryClassWithMatchingFieldNames probe = new JustAnArbitraryClassWithMatchingFieldNames(); probe.value = ""stark""; Example example = Example.of(probe, UntypedExampleMatcher.matching()); Query query = new Query(new Criteria().alike(example)); List<Person> result = template.find(query, Person.class); When including null values in the ExampleSpec , Spring Data Mongo uses embedded document matching instead of dot notation property matching. Doing so forces exact document matching for all property values and the property order in the embedded document. UntypedExampleMatcher is likely the right choice for you if you are storing different entities within a single collection or opted out of writing type hints. Also, keep in mind that using @TypeAlias requires eager initialization of the MappingContext . To do so, configure initialEntitySet to to ensure proper alias resolution for read operations. Spring Data MongoDB provides support for different matching options: StringMatcher options Matching Logical result DEFAULT (case-sensitive) {""firstname"" : firstname} DEFAULT (case-insensitive) {""firstname"" : { $regex: firstname, $options: 'i'}} EXACT (case-sensitive) {""firstname"" : { $regex: /^firstname$/}} EXACT (case-insensitive) {""firstname"" : { $regex: /^firstname$/, $options: 'i'}} STARTING (case-sensitive) {""firstname"" : { $regex: /^firstname/}} STARTING (case-insensitive) {""firstname"" : { $regex: /^firstname/, $options: 'i'}} ENDING (case-sensitive) {""firstname"" : { $regex: /firstname$/}} ENDING (case-insensitive) {""firstname"" : { $regex: /firstname$/, $options: 'i'}} CONTAINING (case-sensitive) {""firstname"" : { $regex: /.*firstname.*/}} CONTAINING (case-insensitive) {""firstname"" : { $regex: /.*firstname.*/, $options: 'i'}} REGEX (case-sensitive) {""firstname"" : { $regex: /firstname/}} REGEX (case-insensitive) {""firstname"" : { $regex: /firstname/, $options: 'i'}} Query a collection for matching JSON Schema: You can use a schema to query any collection for documents that match a given structure defined by a JSON schema, as the following example shows: Example 11. Query for Documents matching a $jsonSchema MongoJsonSchema schema = MongoJsonSchema.builder().required(""firstname"", ""lastname"").build(); template.find(query(matchingDocumentStructure(schema)), Person.class); Please refer to the JSON Schema(mapping/mapping-schema.html) section to learn more about the schema support in Spring Data MongoDB."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/template-document-count.html","Counting Documents: The template API offers various methods to count the number of documents matching a given criteria. One of them outlined below. template.query(Person.class) .matching(query(where(""firstname"").is(""luke""))) .count(); In pre-3.x versions of SpringData MongoDB the count operation used MongoDBs internal collection statistics. With the introduction of MongoDB Transactions(client-session-transactions.html#mongo.transactions) this was no longer possible because statistics would not correctly reflect potential changes during a transaction requiring an aggregation-based count approach. So in version 2.x MongoOperations.count() would use the collection statistics if no transaction was in progress, and the aggregation variant if so. As of Spring Data MongoDB 3.x any count operation uses regardless the existence of filter criteria the aggregation-based count approach via MongoDBs countDocuments . If the application is fine with the limitations of working upon collection statistics MongoOperations.estimatedCount() offers an alternative. By setting MongoTemplate#useEstimatedCount(…​) to true MongoTemplate#count(…​) operations, that use an empty filter query, will be delegated to estimatedCount , as long as there is no transaction active and the template is not bound to a session(client-session-transactions.html) . It will still be possible to obtain exact numbers via MongoTemplate#exactCount , but may speed up things. MongoDBs native countDocuments method and the $match aggregation, do not support $near and $nearSphere but require $geoWithin along with $center or $centerSphere which does not support $minDistance (see jira.mongodb.org/browse/SERVER-37043(https://jira.mongodb.org/browse/SERVER-37043) ). Therefore a given Query will be rewritten for count operations using Reactive -/ MongoTemplate to bypass the issue like shown below. { location : { $near : [-73.99171, 40.738868], $maxDistance : 1.1 } } (1) { location : { $geoWithin : { $center: [ [-73.99171, 40.738868], 1.1] } } } (2) { location : { $near : [-73.99171, 40.738868], $minDistance : 0.1, $maxDistance : 1.1 } } (3) {$and :[ { $nor :[ { location :{ $geoWithin :{ $center :[ [-73.99171, 40.738868 ], 0.01] } } } ]}, { location :{ $geoWithin :{ $center :[ [-73.99171, 40.738868 ], 1.1] } } } ] } (4) 1 Count source query using $near . 2 Rewritten query now using $geoWithin with $center . 3 Count source query using $near with $minDistance and $maxDistance . 4 Rewritten query now a combination of $nor $geowithin critierias to work around unsupported $minDistance ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/aggregation-framework.html","Aggregation Framework Support: Spring Data MongoDB provides support for the Aggregation Framework introduced to MongoDB in version 2.2. For further information, see the full reference documentation(https://docs.mongodb.org/manual/aggregation/) of the aggregation framework and other data aggregation tools for MongoDB. Basic Concepts: The Aggregation Framework support in Spring Data MongoDB is based on the following key abstractions: Aggregation(../api/java/org/springframework/data/mongodb/core/aggregation/Aggregation.html) and AggregationResults(../api/java/org/springframework/data/mongodb/core/aggregation/AggregationResults.html) . Aggregation An Aggregation represents a MongoDB aggregate operation and holds the description of the aggregation pipeline instructions. Aggregations are created by invoking the appropriate newAggregation(…) static factory method of the Aggregation class, which takes a list of AggregateOperation and an optional input class. The actual aggregate operation is run by the aggregate method of the MongoTemplate , which takes the desired output class as a parameter. TypedAggregation A TypedAggregation , just like an Aggregation , holds the instructions of the aggregation pipeline and a reference to the input type, that is used for mapping domain properties to actual document fields. At runtime, field references get checked against the given input type, considering potential @Field annotations. Changed in 3.2 referencing non-existent properties does no longer raise errors. To restore the previous behaviour use the strictMapping option of AggregationOptions . AggregationDefinition An AggregationDefinition represents a MongoDB aggregation pipeline operation and describes the processing that should be performed in this aggregation step. Although you could manually create an AggregationDefinition , we recommend using the static factory methods provided by the Aggregate class to construct an AggregateOperation . AggregationResults AggregationResults is the container for the result of an aggregate operation. It provides access to the raw aggregation result, in the form of a Document to the mapped objects and other information about the aggregation. The following listing shows the canonical example for using the Spring Data MongoDB support for the MongoDB Aggregation Framework: import static org.springframework.data.mongodb.core.aggregation.Aggregation.*; Aggregation agg = newAggregation( pipelineOP1(), pipelineOP2(), pipelineOPn() ); AggregationResults<OutputType> results = mongoTemplate.aggregate(agg, ""INPUT_COLLECTION_NAME"", OutputType.class); List<OutputType> mappedResult = results.getMappedResults(); Note that, if you provide an input class as the first parameter to the newAggregation method, the MongoTemplate derives the name of the input collection from this class. Otherwise, if you do not not specify an input class, you must provide the name of the input collection explicitly. If both an input class and an input collection are provided, the latter takes precedence. Supported Aggregation Operations & Stages The MongoDB Aggregation Framework provides the following types of aggregation stages and operations: addFields - AddFieldsOperation bucket / bucketAuto - BucketOperation / BucketAutoOperation count - CountOperation densify - DensifyOperation facet - FacetOperation geoNear - GeoNearOperation graphLookup - GraphLookupOperation group - GroupOperation limit - LimitOperation lookup - LookupOperation match - MatchOperation merge - MergeOperation project - ProjectionOperation redact - RedactOperation replaceRoot - ReplaceRootOperation sample - SampleOperation set - SetOperation setWindowFields - SetWindowFieldsOperation skip - SkipOperation sort / sortByCount - SortOperation / SortByCountOperation unionWith - UnionWithOperation unset - UnsetOperation unwind - UnwindOperation Unsupported aggregation stages (like $search(https://www.mongodb.com/docs/atlas/atlas-search/query-syntax/) for MongoDB Atlas) can be provided by implementing either AggregationOperation . Aggregation.stage is a shortcut for registering a pipeline stage by providing its JSON or Bson representation. Aggregation.stage("""""" { $search : { ""near"": { ""path"": ""released"", ""origin"": { ""$date"": { ""$numberLong"": ""..."" } } , ""pivot"": 7 } } } """"""); At the time of this writing, we provide support for the following Aggregation Operators in Spring Data MongoDB: Table 1. Aggregation Operators currently supported by Spring Data MongoDB Set Aggregation Operators setEquals , setIntersection , setUnion , setDifference , setIsSubset , anyElementTrue , allElementsTrue Group/Accumulator Aggregation Operators addToSet , bottom , bottomN , covariancePop , covarianceSamp , expMovingAvg , first , firstN , last , lastN max , maxN , min , minN , avg , push , sum , top , topN , count (*), median , percentile , stdDevPop , stdDevSamp Arithmetic Aggregation Operators abs , acos , acosh , add (* via plus ), asin , asin , atan , atan2 , atanh , ceil , cos , cosh , derivative , divide , exp , floor , integral , ln , log , log10 , mod , multiply , pow , round , sqrt , subtract (* via minus ), sin , sinh , tan , tanh , trunc String Aggregation Operators concat , substr , toLower , toUpper , strcasecmp , indexOfBytes , indexOfCP , regexFind , regexFindAll , regexMatch , replaceAll , replaceOne , split`, strLenBytes , strLenCP , substrCP , trim , ltrim , rtim Comparison Aggregation Operators eq (* via is ), gt , gte , lt , lte , ne Array Aggregation Operators arrayElementAt , arrayToObject , concatArrays , filter , first , in , indexOfArray , isArray , last , range`, reverseArray , reduce , size , sortArray , slice , zip Literal Operators literal Date Aggregation Operators dateSubstract , dateTrunc , dayOfYear , dayOfMonth , dayOfWeek , year , month , week , hour , minute , second , millisecond , dateAdd , dateDiff , dateToString , dateFromString , dateFromParts , dateToParts , isoDayOfWeek , isoWeek , isoWeekYear , tsIncrement , tsSecond Variable Operators map Conditional Aggregation Operators cond , ifNull , switch Type Aggregation Operators type Convert Aggregation Operators convert , degreesToRadians , toBool , toDate , toDecimal , toDouble , toInt , toLong , toObjectId , toString Object Aggregation Operators objectToArray , mergeObjects , getField , setField Script Aggregation Operators function , accumulator * The operation is mapped or added by Spring Data MongoDB. Note that the aggregation operations not listed here are currently not supported by Spring Data MongoDB. Comparison aggregation operators are expressed as Criteria expressions. Projection Expressions: Projection expressions are used to define the fields that are the outcome of a particular aggregation step. Projection expressions can be defined through the project method of the Aggregation class, either by passing a list of String objects or an aggregation framework Fields object. The projection can be extended with additional fields through a fluent API by using the and(String) method and aliased by using the as(String) method. Note that you can also define fields with aliases by using the Fields.field static factory method of the aggregation framework, which you can then use to construct a new Fields instance. References to projected fields in later aggregation stages are valid only for the field names of included fields or their aliases (including newly defined fields and their aliases). Fields not included in the projection cannot be referenced in later aggregation stages. The following listings show examples of projection expression: Example 1. Projection expression examples // generates {$project: {name: 1, netPrice: 1}} project(""name"", ""netPrice"") // generates {$project: {thing1: $thing2}} project().and(""thing1"").as(""thing2"") // generates {$project: {a: 1, b: 1, thing2: $thing1}} project(""a"",""b"").and(""thing1"").as(""thing2"") Example 2. Multi-Stage Aggregation using Projection and Sorting // generates {$project: {name: 1, netPrice: 1}}, {$sort: {name: 1}} project(""name"", ""netPrice""), sort(ASC, ""name"") // generates {$project: {name: $firstname}}, {$sort: {name: 1}} project().and(""firstname"").as(""name""), sort(ASC, ""name"") // does not work project().and(""firstname"").as(""name""), sort(ASC, ""firstname"") More examples for project operations can be found in the AggregationTests class. Note that further details regarding the projection expressions can be found in the corresponding section(https://docs.mongodb.org/manual/reference/operator/aggregation/project/#pipe._S_project) of the MongoDB Aggregation Framework reference documentation. Faceted Classification: As of Version 3.4, MongoDB supports faceted classification by using the Aggregation Framework. A faceted classification uses semantic categories (either general or subject-specific) that are combined to create the full classification entry. Documents flowing through the aggregation pipeline are classified into buckets. A multi-faceted classification enables various aggregations on the same set of input documents, without needing to retrieve the input documents multiple times. Buckets: Bucket operations categorize incoming documents into groups, called buckets, based on a specified expression and bucket boundaries. Bucket operations require a grouping field or a grouping expression. You can define them by using the bucket() and bucketAuto() methods of the Aggregate class. BucketOperation and BucketAutoOperation can expose accumulations based on aggregation expressions for input documents. You can extend the bucket operation with additional parameters through a fluent API by using the with…() methods and the andOutput(String) method. You can alias the operation by using the as(String) method. Each bucket is represented as a document in the output. BucketOperation takes a defined set of boundaries to group incoming documents into these categories. Boundaries are required to be sorted. The following listing shows some examples of bucket operations: Example 3. Bucket operation examples // generates {$bucket: {groupBy: $price, boundaries: [0, 100, 400]}} bucket(""price"").withBoundaries(0, 100, 400); // generates {$bucket: {groupBy: $price, default: ""Other"" boundaries: [0, 100]}} bucket(""price"").withBoundaries(0, 100).withDefault(""Other""); // generates {$bucket: {groupBy: $price, boundaries: [0, 100], output: { count: { $sum: 1}}}} bucket(""price"").withBoundaries(0, 100).andOutputCount().as(""count""); // generates {$bucket: {groupBy: $price, boundaries: [0, 100], 5, output: { titles: { $push: ""$title""}}} bucket(""price"").withBoundaries(0, 100).andOutput(""title"").push().as(""titles""); BucketAutoOperation determines boundaries in an attempt to evenly distribute documents into a specified number of buckets. BucketAutoOperation optionally takes a granularity value that specifies the preferred number(https://en.wikipedia.org/wiki/Preferred_number) series to use to ensure that the calculated boundary edges end on preferred round numbers or on powers of 10. The following listing shows examples of bucket operations: Example 4. Bucket operation examples // generates {$bucketAuto: {groupBy: $price, buckets: 5}} bucketAuto(""price"", 5) // generates {$bucketAuto: {groupBy: $price, buckets: 5, granularity: ""E24""}} bucketAuto(""price"", 5).withGranularity(Granularities.E24).withDefault(""Other""); // generates {$bucketAuto: {groupBy: $price, buckets: 5, output: { titles: { $push: ""$title""}}} bucketAuto(""price"", 5).andOutput(""title"").push().as(""titles""); To create output fields in buckets, bucket operations can use AggregationExpression through andOutput() and SpEL expressions(#mongo.aggregation.projection.expressions) through andOutputExpression() . Note that further details regarding bucket expressions can be found in the $bucket section(https://docs.mongodb.org/manual/reference/operator/aggregation/bucket/) and $bucketAuto section(https://docs.mongodb.org/manual/reference/operator/aggregation/bucketAuto/) of the MongoDB Aggregation Framework reference documentation. Multi-faceted Aggregation: Multiple aggregation pipelines can be used to create multi-faceted aggregations that characterize data across multiple dimensions (or facets) within a single aggregation stage. Multi-faceted aggregations provide multiple filters and categorizations to guide data browsing and analysis. A common implementation of faceting is how many online retailers provide ways to narrow down search results by applying filters on product price, manufacturer, size, and other factors. You can define a FacetOperation by using the facet() method of the Aggregation class. You can customize it with multiple aggregation pipelines by using the and() method. Each sub-pipeline has its own field in the output document where its results are stored as an array of documents. Sub-pipelines can project and filter input documents prior to grouping. Common use cases include extraction of date parts or calculations before categorization. The following listing shows facet operation examples: Example 5. Facet operation examples // generates {$facet: {categorizedByPrice: [ { $match: { price: {$exists : true}}}, { $bucketAuto: {groupBy: $price, buckets: 5}}]}} facet(match(Criteria.where(""price"").exists(true)), bucketAuto(""price"", 5)).as(""categorizedByPrice"")) // generates {$facet: {categorizedByCountry: [ { $match: { country: {$exists : true}}}, { $sortByCount: ""$country""}]}} facet(match(Criteria.where(""country"").exists(true)), sortByCount(""country"")).as(""categorizedByCountry"")) // generates {$facet: {categorizedByYear: [ // { $project: { title: 1, publicationYear: { $year: ""publicationDate""}}}, // { $bucketAuto: {groupBy: $price, buckets: 5, output: { titles: {$push:""$title""}}} // ]}} facet(project(""title"").and(""publicationDate"").extractYear().as(""publicationYear""), bucketAuto(""publicationYear"", 5).andOutput(""title"").push().as(""titles"")) .as(""categorizedByYear"")) Note that further details regarding facet operation can be found in the $facet section(https://docs.mongodb.org/manual/reference/operator/aggregation/facet/) of the MongoDB Aggregation Framework reference documentation. Sort By Count: Sort by count operations group incoming documents based on the value of a specified expression, compute the count of documents in each distinct group, and sort the results by count. It offers a handy shortcut to apply sorting when using Faceted Classification(#mongo.aggregation.facet) . Sort by count operations require a grouping field or grouping expression. The following listing shows a sort by count example: Example 6. Sort by count example // generates { $sortByCount: ""$country"" } sortByCount(""country""); A sort by count operation is equivalent to the following BSON (Binary JSON): { $group: { _id: <expression>, count: { $sum: 1 } } }, { $sort: { count: -1 } } Spring Expression Support in Projection Expressions: We support the use of SpEL expressions in projection expressions through the andExpression method of the ProjectionOperation and BucketOperation classes. This feature lets you define the desired expression as a SpEL expression. On running a query, the SpEL expression is translated into a corresponding MongoDB projection expression part. This arrangement makes it much easier to express complex calculations. Complex Calculations with SpEL expressions: Consider the following SpEL expression: 1 + (q + 1) / (q - 1) The preceding expression is translated into the following projection expression part: { ""$add"" : [ 1, { ""$divide"" : [ { ""$add"":[""$q"", 1]}, { ""$subtract"":[ ""$q"", 1]} ] }]} You can see examples in more context in Aggregation Framework Example 5(#mongo.aggregation.examples.example5) and Aggregation Framework Example 6(#mongo.aggregation.examples.example6) . You can find more usage examples for supported SpEL expression constructs in SpelExpressionTransformerUnitTests . Supported SpEL transformations SpEL Expression Mongo Expression Part a == b { $eq : [$a, $b] } a != b { $ne : [$a , $b] } a > b { $gt : [$a, $b] } a >= b { $gte : [$a, $b] } a < b { $lt : [$a, $b] } a ⇐ b { $lte : [$a, $b] } a + b { $add : [$a, $b] } a - b { $subtract : [$a, $b] } a * b { $multiply : [$a, $b] } a / b { $divide : [$a, $b] } a^b { $pow : [$a, $b] } a % b { $mod : [$a, $b] } a && b { $and : [$a, $b] } a || b { $or : [$a, $b] } !a { $not : [$a] } In addition to the transformations shown in the preceding table, you can use standard SpEL operations such as new to (for example) create arrays and reference expressions through their names (followed by the arguments to use in brackets). The following example shows how to create an array in this fashion: // { $setEquals : [$a, [5, 8, 13] ] } .andExpression(""setEquals(a, new int[]{5, 8, 13})""); Aggregation Framework Examples: The examples in this section demonstrate the usage patterns for the MongoDB Aggregation Framework with Spring Data MongoDB. Aggregation Framework Example 1: In this introductory example, we want to aggregate a list of tags to get the occurrence count of a particular tag from a MongoDB collection (called tags ) sorted by the occurrence count in descending order. This example demonstrates the usage of grouping, sorting, projections (selection), and unwinding (result splitting). class TagCount { String tag; int n; } import static org.springframework.data.mongodb.core.aggregation.Aggregation.*; Aggregation agg = newAggregation( project(""tags""), unwind(""tags""), group(""tags"").count().as(""n""), project(""n"").and(""tag"").previousOperation(), sort(DESC, ""n"") ); AggregationResults<TagCount> results = mongoTemplate.aggregate(agg, ""tags"", TagCount.class); List<TagCount> tagCount = results.getMappedResults(); The preceding listing uses the following algorithm: Create a new aggregation by using the newAggregation static factory method, to which we pass a list of aggregation operations. These aggregate operations define the aggregation pipeline of our Aggregation . Use the project operation to select the tags field (which is an array of strings) from the input collection. Use the unwind operation to generate a new document for each tag within the tags array. Use the group operation to define a group for each tags value for which we aggregate the occurrence count (by using the count aggregation operator and collecting the result in a new field called n ). Select the n field and create an alias for the ID field generated from the previous group operation (hence the call to previousOperation() ) with a name of tag . Use the sort operation to sort the resulting list of tags by their occurrence count in descending order. Call the aggregate method on MongoTemplate to let MongoDB perform the actual aggregation operation, with the created Aggregation as an argument. Note that the input collection is explicitly specified as the tags parameter to the aggregate Method. If the name of the input collection is not specified explicitly, it is derived from the input class passed as the first parameter to the newAggreation method. Aggregation Framework Example 2: This example is based on the Largest and Smallest Cities by State(https://docs.mongodb.org/manual/tutorial/aggregation-examples/#largest-and-smallest-cities-by-state) example from the MongoDB Aggregation Framework documentation. We added additional sorting to produce stable results with different MongoDB versions. Here we want to return the smallest and largest cities by population for each state by using the aggregation framework. This example demonstrates grouping, sorting, and projections (selection). class ZipInfo { String id; String city; String state; @Field(""pop"") int population; @Field(""loc"") double[] location; } class City { String name; int population; } class ZipInfoStats { String id; String state; City biggestCity; City smallestCity; } import static org.springframework.data.mongodb.core.aggregation.Aggregation.*; TypedAggregation<ZipInfo> aggregation = newAggregation(ZipInfo.class, group(""state"", ""city"") .sum(""population"").as(""pop""), sort(ASC, ""pop"", ""state"", ""city""), group(""state"") .last(""city"").as(""biggestCity"") .last(""pop"").as(""biggestPop"") .first(""city"").as(""smallestCity"") .first(""pop"").as(""smallestPop""), project() .and(""state"").previousOperation() .and(""biggestCity"") .nested(bind(""name"", ""biggestCity"").and(""population"", ""biggestPop"")) .and(""smallestCity"") .nested(bind(""name"", ""smallestCity"").and(""population"", ""smallestPop"")), sort(ASC, ""state"") ); AggregationResults<ZipInfoStats> result = mongoTemplate.aggregate(aggregation, ZipInfoStats.class); ZipInfoStats firstZipInfoStats = result.getMappedResults().get(0); Note that the ZipInfo class maps the structure of the given input-collection. The ZipInfoStats class defines the structure in the desired output format. The preceding listings use the following algorithm: Use the group operation to define a group from the input-collection. The grouping criteria is the combination of the state and city fields, which forms the ID structure of the group. We aggregate the value of the population property from the grouped elements by using the sum operator and save the result in the pop field. Use the sort operation to sort the intermediate-result by the pop , state and city fields, in ascending order, such that the smallest city is at the top and the biggest city is at the bottom of the result. Note that the sorting on state and city is implicitly performed against the group ID fields (which Spring Data MongoDB handled). Use a group operation again to group the intermediate result by state . Note that state again implicitly references a group ID field. We select the name and the population count of the biggest and smallest city with calls to the last(…) and first(…​) operators, respectively, in the project operation. Select the state field from the previous group operation. Note that state again implicitly references a group ID field. Because we do not want an implicitly generated ID to appear, we exclude the ID from the previous operation by using and(previousOperation()).exclude() . Because we want to populate the nested City structures in our output class, we have to emit appropriate sub-documents by using the nested method. Sort the resulting list of StateStats by their state name in ascending order in the sort operation. Note that we derive the name of the input collection from the ZipInfo class passed as the first parameter to the newAggregation method. Aggregation Framework Example 3: This example is based on the States with Populations Over 10 Million(https://docs.mongodb.org/manual/tutorial/aggregation-examples/#states-with-populations-over-10-million) example from the MongoDB Aggregation Framework documentation. We added additional sorting to produce stable results with different MongoDB versions. Here we want to return all states with a population greater than 10 million, using the aggregation framework. This example demonstrates grouping, sorting, and matching (filtering). class StateStats { @Id String id; String state; @Field(""totalPop"") int totalPopulation; } import static org.springframework.data.mongodb.core.aggregation.Aggregation.*; TypedAggregation<ZipInfo> agg = newAggregation(ZipInfo.class, group(""state"").sum(""population"").as(""totalPop""), sort(ASC, previousOperation(), ""totalPop""), match(where(""totalPop"").gte(10 * 1000 * 1000)) ); AggregationResults<StateStats> result = mongoTemplate.aggregate(agg, StateStats.class); List<StateStats> stateStatsList = result.getMappedResults(); The preceding listings use the following algorithm: Group the input collection by the state field and calculate the sum of the population field and store the result in the new field ""totalPop"" . Sort the intermediate result by the id-reference of the previous group operation in addition to the ""totalPop"" field in ascending order. Filter the intermediate result by using a match operation which accepts a Criteria query as an argument. Note that we derive the name of the input collection from the ZipInfo class passed as first parameter to the newAggregation method. Aggregation Framework Example 4: This example demonstrates the use of simple arithmetic operations in the projection operation. class Product { String id; String name; double netPrice; int spaceUnits; } import static org.springframework.data.mongodb.core.aggregation.Aggregation.*; TypedAggregation<Product> agg = newAggregation(Product.class, project(""name"", ""netPrice"") .and(""netPrice"").plus(1).as(""netPricePlus1"") .and(""netPrice"").minus(1).as(""netPriceMinus1"") .and(""netPrice"").multiply(1.19).as(""grossPrice"") .and(""netPrice"").divide(2).as(""netPriceDiv2"") .and(""spaceUnits"").mod(2).as(""spaceUnitsMod2"") ); AggregationResults<Document> result = mongoTemplate.aggregate(agg, Document.class); List<Document> resultList = result.getMappedResults(); Note that we derive the name of the input collection from the Product class passed as first parameter to the newAggregation method. Aggregation Framework Example 5: This example demonstrates the use of simple arithmetic operations derived from SpEL Expressions in the projection operation. class Product { String id; String name; double netPrice; int spaceUnits; } import static org.springframework.data.mongodb.core.aggregation.Aggregation.*; TypedAggregation<Product> agg = newAggregation(Product.class, project(""name"", ""netPrice"") .andExpression(""netPrice + 1"").as(""netPricePlus1"") .andExpression(""netPrice - 1"").as(""netPriceMinus1"") .andExpression(""netPrice / 2"").as(""netPriceDiv2"") .andExpression(""netPrice * 1.19"").as(""grossPrice"") .andExpression(""spaceUnits % 2"").as(""spaceUnitsMod2"") .andExpression(""(netPrice * 0.8 + 1.2) * 1.19"").as(""grossPriceIncludingDiscountAndCharge"") ); AggregationResults<Document> result = mongoTemplate.aggregate(agg, Document.class); List<Document> resultList = result.getMappedResults(); Aggregation Framework Example 6: This example demonstrates the use of complex arithmetic operations derived from SpEL Expressions in the projection operation. Note: The additional parameters passed to the addExpression method can be referenced with indexer expressions according to their position. In this example, we reference the first parameter of the parameters array with [0] . When the SpEL expression is transformed into a MongoDB aggregation framework expression, external parameter expressions are replaced with their respective values. class Product { String id; String name; double netPrice; int spaceUnits; } import static org.springframework.data.mongodb.core.aggregation.Aggregation.*; double shippingCosts = 1.2; TypedAggregation<Product> agg = newAggregation(Product.class, project(""name"", ""netPrice"") .andExpression(""(netPrice * (1-discountRate) + [0]) * (1+taxRate)"", shippingCosts).as(""salesPrice"") ); AggregationResults<Document> result = mongoTemplate.aggregate(agg, Document.class); List<Document> resultList = result.getMappedResults(); Note that we can also refer to other fields of the document within the SpEL expression. Aggregation Framework Example 7: This example uses conditional projection. It is derived from the $cond reference documentation(https://docs.mongodb.com/manual/reference/operator/aggregation/cond/) . public class InventoryItem { @Id int id; String item; String description; int qty; } public class InventoryItemProjection { @Id int id; String item; String description; int qty; int discount } import static org.springframework.data.mongodb.core.aggregation.Aggregation.*; TypedAggregation<InventoryItem> agg = newAggregation(InventoryItem.class, project(""item"").and(""discount"") .applyCondition(ConditionalOperator.newBuilder().when(Criteria.where(""qty"").gte(250)) .then(30) .otherwise(20)) .and(ifNull(""description"", ""Unspecified"")).as(""description"") ); AggregationResults<InventoryItemProjection> result = mongoTemplate.aggregate(agg, ""inventory"", InventoryItemProjection.class); List<InventoryItemProjection> stateStatsList = result.getMappedResults(); This one-step aggregation uses a projection operation with the inventory collection. We project the discount field by using a conditional operation for all inventory items that have a qty greater than or equal to 250 . A second conditional projection is performed for the description field. We apply the Unspecified description to all items that either do not have a description field or items that have a null description. As of MongoDB 3.6, it is possible to exclude fields from the projection by using a conditional expression. Example 7. Conditional aggregation projection TypedAggregation<Book> agg = Aggregation.newAggregation(Book.class, project(""title"") .and(ConditionalOperators.when(ComparisonOperators.valueOf(""author.middle"") (1) .equalToValue("""")) (2) .then(""$$REMOVE"") (3) .otherwiseValueOf(""author.middle"") (4) ) .as(""author.middle"")); 1 If the value of the field author.middle 2 does not contain a value, 3 then use $$REMOVE(https://docs.mongodb.com/manual/reference/aggregation-variables/#variable.REMOVE) to exclude the field. 4 Otherwise, add the field value of author.middle ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/template-gridfs.html","GridFS Support: MongoDB supports storing binary files inside its filesystem, GridFS. Spring Data MongoDB provides a GridFsOperations(../api/java/org/springframework/data/mongodb/gridfs/GridFsOperations.html) and ReactiveGridFsOperations(../api/java/org/springframework/data/mongodb/gridfs/ReactiveGridFsOperations.html) interface as well as the corresponding implementation, GridFsTemplate and ReactiveGridFsTemplate , to let you interact with the filesystem. You can set up a template instance by handing it a MongoDatabaseFactory / ReactiveMongoDatabaseFactory as well as a MongoConverter , as the following example shows: Imperative Reactive XML class GridFsConfiguration extends AbstractMongoClientConfiguration { // … further configuration omitted @Bean public GridFsTemplate gridFsTemplate() { return new GridFsTemplate(mongoDbFactory(), mappingMongoConverter()); } } class ReactiveGridFsConfiguration extends AbstractReactiveMongoConfiguration { // … further configuration omitted @Bean public ReactiveGridFsTemplate reactiveGridFsTemplate() { return new ReactiveGridFsTemplate(reactiveMongoDbFactory(), mappingMongoConverter()); } } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:mongo=""http://www.springframework.org/schema/data/mongo"" xsi:schemaLocation=""http://www.springframework.org/schema/data/mongo https://www.springframework.org/schema/data/mongo/spring-mongo.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <mongo:db-factory id=""mongoDbFactory"" dbname=""database"" /> <mongo:mapping-converter id=""converter"" /> <bean class=""org.springframework.data.mongodb.gridfs.GridFsTemplate""> <constructor-arg ref=""mongoDbFactory"" /> <constructor-arg ref=""converter"" /> </bean> </beans> The template can now be injected and used to perform storage and retrieval operations, as the following example shows: Using GridFS to store files Imperative Reactive class GridFsClient { @Autowired GridFsOperations operations; @Test public void storeFileToGridFs() { FileMetadata metadata = new FileMetadata(); // populate metadata Resource file = … // lookup File or Resource operations.store(file.getInputStream(), ""filename.txt"", metadata); } } The store(…) operations take an InputStream , a filename, and (optionally) metadata information about the file to store. The metadata can be an arbitrary object, which will be marshaled by the MongoConverter configured with the GridFsTemplate . Alternatively, you can also provide a Document . class ReactiveGridFsClient { @Autowired ReactiveGridFsTemplate operations; @Test public Mono<ObjectId> storeFileToGridFs() { FileMetadata metadata = new FileMetadata(); // populate metadata Publisher<DataBuffer> file = … // lookup File or Resource return operations.store(file, ""filename.txt"", metadata); } } The store(…) operations take an Publisher<DataBuffer> , a filename, and (optionally) metadata information about the file to store. The metadata can be an arbitrary object, which will be marshaled by the MongoConverter configured with the ReactiveGridFsTemplate . Alternatively, you can also provide a Document . The MongoDB’s driver uses AsyncInputStream and AsyncOutputStream interfaces to exchange binary streams. Spring Data MongoDB adapts these interfaces to Publisher<DataBuffer> . Read more about DataBuffer in Spring’s reference documentation(https://docs.spring.io/spring-framework/docs/6.1.13/reference/html/core.html#databuffers) . You can read files from the filesystem through either the find(…) or the getResources(…) methods. Let’s have a look at the find(…) methods first. You can either find a single file or multiple files that match a Query . You can use the GridFsCriteria helper class to define queries. It provides static factory methods to encapsulate default metadata fields (such as whereFilename() and whereContentType() ) or a custom one through whereMetaData() . The following example shows how to use the template to query for files: Using GridFsTemplate to query for files Imperative Reactive class GridFsClient { @Autowired GridFsOperations operations; @Test public void findFilesInGridFs() { GridFSFindIterable result = operations.find(query(whereFilename().is(""filename.txt""))); } } class ReactiveGridFsClient { @Autowired ReactiveGridFsTemplate operations; @Test public Flux<GridFSFile> findFilesInGridFs() { return operations.find(query(whereFilename().is(""filename.txt""))) } } Currently, MongoDB does not support defining sort criteria when retrieving files from GridFS. For this reason, any sort criteria defined on the Query instance handed into the find(…) method are disregarded. The other option to read files from the GridFs is to use the methods introduced by the ResourcePatternResolver interface. They allow handing an Ant path into the method and can thus retrieve files matching the given pattern. The following example shows how to use GridFsTemplate to read files: Using GridFsTemplate to read files Imperative Reactive class GridFsClient { @Autowired GridFsOperations operations; public GridFsResources[] readFilesFromGridFs() { return operations.getResources(""*.txt""); } } class ReactiveGridFsClient { @Autowired ReactiveGridFsOperations operations; public Flux<ReactiveGridFsResource> readFilesFromGridFs() { return operations.getResources(""*.txt""); } } GridFsOperations extends ResourcePatternResolver and lets the GridFsTemplate (for example) to be plugged into an ApplicationContext to read Spring Config files from MongoDB database. By default, GridFsTemplate obtains GridFSBucket once upon the first GridFS interaction. After that, the template instance reuses the cached bucket. To use different buckets, from the same Template instance use the constructor accepting Supplier<GridFSBucket> ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/mapping/mapping.html","Object Mapping: Rich mapping support is provided by the MappingMongoConverter . The converter holds a metadata model that provides a full feature set to map domain objects to MongoDB documents. The mapping metadata model is populated by using annotations on your domain objects. However, the infrastructure is not limited to using annotations as the only source of metadata information. The MappingMongoConverter also lets you map objects to documents without providing any additional metadata, by following a set of conventions. This section describes the features of the MappingMongoConverter , including fundamentals, how to use conventions for mapping objects to documents and how to override those conventions with annotation-based mapping metadata. Object Mapping Fundamentals: This section covers the fundamentals of Spring Data object mapping, object creation, field and property access, mutability and immutability. Note, that this section only applies to Spring Data modules that do not use the object mapping of the underlying data store (like JPA). Also be sure to consult the store-specific sections for store-specific object mapping, like indexes, customizing column or field names or the like. Core responsibility of the Spring Data object mapping is to create instances of domain objects and map the store-native data structures onto those. This means we need two fundamental steps: Instance creation by using one of the constructors exposed. Instance population to materialize all exposed properties. Object creation: Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. The value resolution assumes constructor/factory method argument names to match the property names of the entity, i.e. the resolution will be performed as if the property was to be populated, including all customizations in mapping (different datastore column or field name etc.). This also requires either parameter names information available in the class file or an @ConstructorProperties annotation being present on the constructor. The value resolution can be customized by using Spring Framework’s @Value value annotation using a store-specific SpEL expression. Please consult the section on store specific mappings for further details. Object creation internals To avoid the overhead of reflection, Spring Data object creation uses a factory class generated at runtime by default, which will call the domain classes constructor directly. I.e. for this example type: class Person { Person(String firstname, String lastname) { … } } we will create a factory class semantically equivalent to this one at runtime: class PersonObjectInstantiator implements ObjectInstantiator { Object newInstance(Object... args) { return new Person((String) args[0], (String) args[1]); } } This gives us a roundabout 10% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: it must not be a private class it must not be a non-static inner class it must not be a CGLib proxy class the constructor to be used by Spring Data must not be private If any of these criteria match, Spring Data will fall back to entity instantiation via reflection. Property population: Once an instance of the entity has been created, Spring Data populates all remaining persistent properties of that class. Unless already populated by the entity’s constructor (i.e. consumed through its constructor argument list), the identifier property will be populated first to allow the resolution of cyclic object references. After that, all non-transient properties that have not already been populated by the constructor are set on the entity instance. For that we use the following algorithm: If the property is immutable but exposes a with… method (see below), we use the with… method to create a new entity instance with the new property value. If property access (i.e. access through getters and setters) is defined, we’re invoking the setter method. If the property is mutable we set the field directly. If the property is immutable we’re using the constructor to be used by persistence operations (see Object creation(#mapping.object-creation) ) to create a copy of the instance. By default, we set the field value directly. Property population internals Similarly to our optimizations in object construction(#mapping.object-creation.details) we also use Spring Data runtime generated accessor classes to interact with the entity instance. class Person { private final Long id; private String firstname; private @AccessType(Type.PROPERTY) String lastname; Person() { this.id = null; } Person(Long id, String firstname, String lastname) { // Field assignments } Person withId(Long id) { return new Person(id, this.firstname, this.lastame); } void setLastname(String lastname) { this.lastname = lastname; } } A generated Property Accessor class PersonPropertyAccessor implements PersistentPropertyAccessor { private static final MethodHandle firstname; (2) private Person person; (1) public void setProperty(PersistentProperty property, Object value) { String name = property.getName(); if (""firstname"".equals(name)) { firstname.invoke(person, (String) value); (2) } else if (""id"".equals(name)) { this.person = person.withId((Long) value); (3) } else if (""lastname"".equals(name)) { this.person.setLastname((String) value); (4) } } } 1 PropertyAccessor’s hold a mutable instance of the underlying object. This is, to enable mutations of otherwise immutable properties. 2 By default, Spring Data uses field-access to read and write property values. As per visibility rules of private fields, MethodHandles are used to interact with fields. 3 The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. Calling withId(…) creates a new Person object. All subsequent mutations will take place in the new instance leaving the previous untouched. 4 Using property-access allows direct method invocations without using MethodHandles . This gives us a roundabout 25% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: Types must not reside in the default or under the java package. Types and their constructors must be public Types that are inner classes must be static . The used Java Runtime must allow for declaring classes in the originating ClassLoader . Java 9 and newer impose certain limitations. By default, Spring Data attempts to use generated property accessors and falls back to reflection-based ones if a limitation is detected. Let’s have a look at the following entity: A sample entity class Person { private final @Id Long id; (1) private final String firstname, lastname; (2) private final LocalDate birthday; private final int age; (3) private String comment; (4) private @AccessType(Type.PROPERTY) String remarks; (5) static Person of(String firstname, String lastname, LocalDate birthday) { (6) return new Person(null, firstname, lastname, birthday, Period.between(birthday, LocalDate.now()).getYears()); } Person(Long id, String firstname, String lastname, LocalDate birthday, int age) { (6) this.id = id; this.firstname = firstname; this.lastname = lastname; this.birthday = birthday; this.age = age; } Person withId(Long id) { (1) return new Person(id, this.firstname, this.lastname, this.birthday, this.age); } void setRemarks(String remarks) { (5) this.remarks = remarks; } } 1 The identifier property is final but set to null in the constructor. The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. The original Person instance stays unchanged as a new one is created. The same pattern is usually applied for other properties that are store managed but might have to be changed for persistence operations. The wither method is optional as the persistence constructor (see 6) is effectively a copy constructor and setting the property will be translated into creating a fresh instance with the new identifier value applied. 2 The firstname and lastname properties are ordinary immutable properties potentially exposed through getters. 3 The age property is an immutable but derived one from the birthday property. With the design shown, the database value will trump the defaulting as Spring Data uses the only declared constructor. Even if the intent is that the calculation should be preferred, it’s important that this constructor also takes age as parameter (to potentially ignore it) as otherwise the property population step will attempt to set the age field and fail due to it being immutable and no with… method being present. 4 The comment property is mutable and is populated by setting its field directly. 5 The remarks property is mutable and is populated by invoking the setter method. 6 The class exposes a factory method and a constructor for object creation. The core idea here is to use factory methods instead of additional constructors to avoid the need for constructor disambiguation through @PersistenceCreator . Instead, defaulting of properties is handled within the factory method. If you want Spring Data to use the factory method for object instantiation, annotate it with @PersistenceCreator . General recommendations: Try to stick to immutable objects — Immutable objects are straightforward to create as materializing an object is then a matter of calling its constructor only. Also, this avoids your domain objects to be littered with setter methods that allow client code to manipulate the objects state. If you need those, prefer to make them package protected so that they can only be invoked by a limited amount of co-located types. Constructor-only materialization is up to 30% faster than properties population. Provide an all-args constructor — Even if you cannot or don’t want to model your entities as immutable values, there’s still value in providing a constructor that takes all properties of the entity as arguments, including the mutable ones, as this allows the object mapping to skip the property population for optimal performance. Use factory methods instead of overloaded constructors to avoid @PersistenceCreator — With an all-argument constructor needed for optimal performance, we usually want to expose more application use case specific constructors that omit things like auto-generated identifiers etc. It’s an established pattern to rather use static factory methods to expose these variants of the all-args constructor. Make sure you adhere to the constraints that allow the generated instantiator and property accessor classes to be used — For identifiers to be generated, still use a final field in combination with an all-arguments persistence constructor (preferred) or a with… method — Use Lombok to avoid boilerplate code — As persistence operations usually require a constructor taking all arguments, their declaration becomes a tedious repetition of boilerplate parameter to field assignments that can best be avoided by using Lombok’s @AllArgsConstructor . Overriding Properties: Java’s allows a flexible design of domain classes where a subclass could define a property that is already declared with the same name in its superclass. Consider the following example: public class SuperType { private CharSequence field; public SuperType(CharSequence field) { this.field = field; } public CharSequence getField() { return this.field; } public void setField(CharSequence field) { this.field = field; } } public class SubType extends SuperType { private String field; public SubType(String field) { super(field); this.field = field; } @Override public String getField() { return this.field; } public void setField(String field) { this.field = field; // optional super.setField(field); } } Both classes define a field using assignable types. SubType however shadows SuperType.field . Depending on the class design, using the constructor could be the only default approach to set SuperType.field . Alternatively, calling super.setField(…) in the setter could set the field in SuperType . All these mechanisms create conflicts to some degree because the properties share the same name yet might represent two distinct values. Spring Data skips super-type properties if types are not assignable. That is, the type of the overridden property must be assignable to its super-type property type to be registered as override, otherwise the super-type property is considered transient. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be generally set without making any further assumptions of the setter implementation. Kotlin support: Spring Data adapts specifics of Kotlin to allow object creation and mutation. Kotlin object creation: Kotlin classes are supported to be instantiated, all classes are immutable by default and require explicit property declarations to define mutable properties. Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a constructor that is annotated with @PersistenceCreator , it is used. If the type is a Kotlin data class(#mapping.kotlin) the primary constructor is used. If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. Consider the following data class Person : data class Person(val id: String, val name: String) The class above compiles to a typical class with an explicit constructor. We can customize this class by adding another constructor and annotate it with @PersistenceCreator to indicate a constructor preference: data class Person(var id: String, val name: String) { @PersistenceCreator constructor(id: String) : this(id, ""unknown"") } Kotlin supports parameter optionality by allowing default values to be used if a parameter is not provided. When Spring Data detects a constructor with parameter defaulting, then it leaves these parameters absent if the data store does not provide a value (or simply returns null ) so Kotlin can apply parameter defaulting.Consider the following class that applies parameter defaulting for name data class Person(var id: String, val name: String = ""unknown"") Every time the name parameter is either not part of the result or its value is null , then the name defaults to unknown . Delegated properties are not supported with Spring Data. The mapping metadata filters delegated properties for Kotlin Data classes. In all other cases you can exclude synthetic fields for delegated properties by annotating the property with @delegate:org.springframework.data.annotation.Transient . Property population of Kotlin data classes: In Kotlin, all classes are immutable by default and require explicit property declarations to define mutable properties. Consider the following data class Person : data class Person(val id: String, val name: String) This class is effectively immutable. It allows creating new instances as Kotlin generates a copy(…) method that creates new object instances copying all property values from the existing object and applying property values provided as arguments to the method. Kotlin Overriding Properties: Kotlin allows declaring property overrides(https://kotlinlang.org/docs/inheritance.html#overriding-properties) to alter properties in subclasses. open class SuperType(open var field: Int) class SubType(override var field: Int = 1) : SuperType(field) { } Such an arrangement renders two properties with the name field . Kotlin generates property accessors (getters and setters) for each property in each class. Effectively, the code looks like as follows: public class SuperType { private int field; public SuperType(int field) { this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } public final class SubType extends SuperType { private int field; public SubType(int field) { super(field); this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } Getters and setters on SubType set only SubType.field and not SuperType.field . In such an arrangement, using the constructor is the only default approach to set SuperType.field . Adding a method to SubType to set SuperType.field via this.SuperType.field = … is possible but falls outside of supported conventions. Property overrides create conflicts to some degree because the properties share the same name yet might represent two distinct values. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be set. Kotlin Value Classes: Kotlin Value Classes are designed for a more expressive domain model to make underlying concepts explicit. Spring Data can read and write types that define properties using Value Classes. Consider the following domain model: @JvmInline value class EmailAddress(val theAddress: String) (1) data class Contact(val id: String, val name:String, val emailAddress: EmailAddress) (2) 1 A simple value class with a non-nullable value type. 2 Data class defining a property using the EmailAddress value class. Non-nullable properties using non-primitive value types are flattened in the compiled class to the value type. Nullable primitive value types or nullable value-in-value types are represented with their wrapper type and that affects how value types are represented in the database. Convention-based Mapping: MappingMongoConverter has a few conventions for mapping objects to documents when no additional mapping metadata is provided. The conventions are: The short Java class name is mapped to the collection name in the following manner. The class com.bigbank.SavingsAccount maps to the savingsAccount collection name. All nested objects are stored as nested objects in the document and not as DBRefs. The converter uses any Spring Converters registered with it to override the default mapping of object properties to document fields and values. The fields of an object are used to convert to and from fields in the document. Public JavaBean properties are not used. If you have a single non-zero-argument constructor whose constructor argument names match top-level field names of document, that constructor is used.Otherwise, the zero-argument constructor is used.If there is more than one non-zero-argument constructor, an exception will be thrown. How the _id field is handled in the mapping layer.: MongoDB requires that you have an _id field for all documents.If you don’t provide one the driver will assign a ObjectId with a generated value.The _id field can be of any type, other than arrays, so long as it is unique.The driver naturally supports all primitive types and Dates.When using the MappingMongoConverter there are certain rules that govern how properties from the Java class are mapped to the _id field. The following outlines what field will be mapped to the _id document field: A field annotated with @Id ( org.springframework.data.annotation.Id ) will be mapped to the _id field. Additionally, the name of the document field can be customized via the @Field annotation, in which case the document will not contain a field _id . A field without an annotation but named id will be mapped to the _id field. Table 1. Examples for the translation of _id field definitions Field definition Resulting Id-Fieldname in MongoDB String id _id @Field String id _id @Field(""x"") String id x @Id String x _id @Field(""x"") @Id String y _id ( @Field(name) is ignored, @Id takes precedence) The following outlines what type conversion, if any, will be done on the property mapped to the _id document field. If a field named id is declared as a String or BigInteger in the Java class it will be converted to and stored as an ObjectId if possible. ObjectId as a field type is also valid. If you specify a value for id in your application, the conversion to an ObjectId is done by the MongoDB driver. If the specified id value cannot be converted to an ObjectId, then the value will be stored as is in the document’s _id field. This also applies if the field is annotated with @Id . If a field is annotated with @MongoId in the Java class it will be converted to and stored as using its actual type. No further conversion happens unless @MongoId declares a desired field type. If no value is provided for the id field, a new ObjectId will be created and converted to the properties type. If a field is annotated with @MongoId(FieldType.…) in the Java class it will be attempted to convert the value to the declared FieldType . If no value is provided for the id field, a new ObjectId will be created and converted to the declared type. If a field named id is not declared as a String, BigInteger, or ObjectID in the Java class then you should assign it a value in your application so it can be stored 'as-is' in the document’s _id field. If no field named id is present in the Java class then an implicit _id file will be generated by the driver but not mapped to a property or field of the Java class. When querying and updating MongoTemplate will use the converter to handle conversions of the Query and Update objects that correspond to the above rules for saving documents so field names and types used in your queries will be able to match what is in your domain classes. Data Mapping and Type Conversion: Spring Data MongoDB supports all types that can be represented as BSON, MongoDB’s internal document format. In addition to these types, Spring Data MongoDB provides a set of built-in converters to map additional types. You can provide your own converters to adjust type conversion. See Custom Conversions - Overriding Default Mapping(custom-conversions.html) for further details. Built in Type conversions: Table 2. Type Type Type conversion Sample String native {""firstname"" : ""Dave""} double , Double , float , Float native {""weight"" : 42.5} int , Integer , short , Short native 32-bit integer {""height"" : 42} long , Long native 64-bit integer {""height"" : 42} Date , Timestamp native {""date"" : ISODate(""2019-11-12T23:00:00.809Z"")} byte[] native {""bin"" : { ""$binary"" : ""AQIDBA=="", ""$type"" : ""00"" }} java.util.UUID (Legacy UUID) native {""uuid"" : { ""$binary"" : ""MEaf1CFQ6lSphaa3b9AtlA=="", ""$type"" : ""03"" }} Date native {""date"" : ISODate(""2019-11-12T23:00:00.809Z"")} ObjectId native {""_id"" : ObjectId(""5707a2690364aba3136ab870"")} Array, List , BasicDBList native {""cookies"" : [ … ]} boolean , Boolean native {""active"" : true} null native {""value"" : null} Document native {""value"" : { … }} Decimal128 native {""value"" : NumberDecimal(…)} AtomicInteger calling get() before the actual conversion converter 32-bit integer {""value"" : ""741"" } AtomicLong calling get() before the actual conversion converter 64-bit integer {""value"" : ""741"" } BigInteger converter String {""value"" : ""741"" } BigDecimal converter String {""value"" : ""741.99"" } URL converter {""website"" : ""https://spring.io/projects/spring-data-mongodb/"" } Locale converter {""locale : ""en_US"" } char , Character converter {""char"" : ""a"" } NamedMongoScript converter Code {""_id"" : ""script name"", value: (some javascript code) } java.util.Currency converter {""currencyCode"" : ""EUR""} Instant (Java 8) native {""date"" : ISODate(""2019-11-12T23:00:00.809Z"")} Instant (Joda, JSR310-BackPort) converter {""date"" : ISODate(""2019-11-12T23:00:00.809Z"")} LocalDate (Joda, Java 8, JSR310-BackPort) converter / native (Java8) [ 1(#_footnotedef_1) ] {""date"" : ISODate(""2019-11-12T00:00:00.000Z"")} LocalDateTime , LocalTime (Joda, Java 8, JSR310-BackPort) converter / native (Java8) [ 2(#_footnotedef_2) ] {""date"" : ISODate(""2019-11-12T23:00:00.809Z"")} DateTime (Joda) converter {""date"" : ISODate(""2019-11-12T23:00:00.809Z"")} ZoneId (Java 8, JSR310-BackPort) converter {""zoneId"" : ""ECT - Europe/Paris""} Box converter {""box"" : { ""first"" : { ""x"" : 1.0 , ""y"" : 2.0} , ""second"" : { ""x"" : 3.0 , ""y"" : 4.0}} Polygon converter {""polygon"" : { ""points"" : [ { ""x"" : 1.0 , ""y"" : 2.0} , { ""x"" : 3.0 , ""y"" : 4.0} , { ""x"" : 4.0 , ""y"" : 5.0}]}} Circle converter {""circle"" : { ""center"" : { ""x"" : 1.0 , ""y"" : 2.0} , ""radius"" : 3.0 , ""metric"" : ""NEUTRAL""}} Point converter {""point"" : { ""x"" : 1.0 , ""y"" : 2.0}} GeoJsonPoint converter {""point"" : { ""type"" : ""Point"" , ""coordinates"" : [3.0 , 4.0] }} GeoJsonMultiPoint converter {""geoJsonLineString"" : {""type"":""MultiPoint"", ""coordinates"": [ [ 0 , 0 ], [ 0 , 1 ], [ 1 , 1 ] ] }} Sphere converter {""sphere"" : { ""center"" : { ""x"" : 1.0 , ""y"" : 2.0} , ""radius"" : 3.0 , ""metric"" : ""NEUTRAL""}} GeoJsonPolygon converter {""polygon"" : { ""type"" : ""Polygon"", ""coordinates"" : [[ [ 0 , 0 ], [ 3 , 6 ], [ 6 , 1 ], [ 0 , 0 ] ]] }} GeoJsonMultiPolygon converter {""geoJsonMultiPolygon"" : { ""type"" : ""MultiPolygon"", ""coordinates"" : [ [ [ [ -73.958 , 40.8003 ] , [ -73.9498 , 40.7968 ] ] ], [ [ [ -73.973 , 40.7648 ] , [ -73.9588 , 40.8003 ] ] ] ] }} GeoJsonLineString converter { ""geoJsonLineString"" : { ""type"" : ""LineString"", ""coordinates"" : [ [ 40 , 5 ], [ 41 , 6 ] ] }} GeoJsonMultiLineString converter {""geoJsonLineString"" : { ""type"" : ""MultiLineString"", coordinates: [ [ [ -73.97162 , 40.78205 ], [ -73.96374 , 40.77715 ] ], [ [ -73.97880 , 40.77247 ], [ -73.97036 , 40.76811 ] ] ] }} Collection Handling Collection handling depends on the actual values returned by MongoDB. If a document does not contain a field mapped to a collection, the mapping will not update the property. Which means the value will remain null , a java default or any value set during object creation. If a document contains a field to be mapped, but the field holds a null value (like: { 'list' : null } ), the property value is set to null . If a document contains a field to be mapped to a collection which is not null (like: { 'list' : [ …​ ] } ), the collection is populated with the mapped values. Generally, if you use constructor creation, then you can get hold of the value to be set. Property population can make use of default initialization values if a property value is not being provided by a query response. Mapping Configuration: Unless explicitly configured, an instance of MappingMongoConverter is created by default when you create a MongoTemplate . You can create your own instance of the MappingMongoConverter . Doing so lets you dictate where in the classpath your domain classes can be found, so that Spring Data MongoDB can extract metadata and construct indexes. Also, by creating your own instance, you can register Spring converters to map specific classes to and from the database. You can configure the MappingMongoConverter as well as com.mongodb.client.MongoClient and MongoTemplate by using either Java-based or XML-based metadata. The following example shows the configuration: Java XML @Configuration public class MongoConfig extends AbstractMongoClientConfiguration { @Override public String getDatabaseName() { return ""database""; } // the following are optional @Override public String getMappingBasePackage() { (1) return ""com.bigbank.domain""; } @Override void configureConverters(MongoConverterConfigurationAdapter adapter) { (2) adapter.registerConverter(new org.springframework.data.mongodb.test.PersonReadConverter()); adapter.registerConverter(new org.springframework.data.mongodb.test.PersonWriteConverter()); } @Bean public LoggingEventListener<MongoMappingEvent> mappingEventsListener() { return new LoggingEventListener<MongoMappingEvent>(); } } 1 The mapping base package defines the root path used to scan for entities used to pre initialize the MappingContext . By default the configuration classes package is used. 2 Configure additional custom converters for specific domain types that replace the default mapping procedure for those types with your custom implementation. <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:mongo=""http://www.springframework.org/schema/data/mongo"" xsi:schemaLocation="" http://www.springframework.org/schema/data/mongo https://www.springframework.org/schema/data/mongo/spring-mongo.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans-3.0.xsd""> <!-- Default bean name is 'mongo' --> <mongo:mongo-client host=""localhost"" port=""27017""/> <mongo:db-factory dbname=""database"" mongo-ref=""mongoClient""/> <!-- by default look for a Mongo object named 'mongo' - default name used for the converter is 'mappingConverter' --> <mongo:mapping-converter base-package=""com.bigbank.domain""> <mongo:custom-converters> <mongo:converter ref=""readConverter""/> <mongo:converter> <bean class=""org.springframework.data.mongodb.test.PersonWriteConverter""/> </mongo:converter> </mongo:custom-converters> </mongo:mapping-converter> <bean id=""readConverter"" class=""org.springframework.data.mongodb.test.PersonReadConverter""/> <!-- set the mapping converter to be used by the MongoTemplate --> <bean id=""mongoTemplate"" class=""org.springframework.data.mongodb.core.MongoTemplate""> <constructor-arg name=""mongoDbFactory"" ref=""mongoDbFactory""/> <constructor-arg name=""mongoConverter"" ref=""mappingConverter""/> </bean> <bean class=""org.springframework.data.mongodb.core.mapping.event.LoggingEventListener""/> </beans> AbstractMongoClientConfiguration requires you to implement methods that define a com.mongodb.client.MongoClient as well as provide a database name. AbstractMongoClientConfiguration also has a method named getMappingBasePackage(…) that you can override to tell the converter where to scan for classes annotated with the @Document annotation. You can add additional converters to the converter by overriding the customConversionsConfiguration method. MongoDB’s native JSR-310 support can be enabled through MongoConverterConfigurationAdapter.useNativeDriverJavaTimeCodecs() . Also shown in the preceding example is a LoggingEventListener , which logs MongoMappingEvent instances that are posted onto Spring’s ApplicationContextEvent infrastructure. Java Time Types We recommend using MongoDB’s native JSR-310 support via MongoConverterConfigurationAdapter.useNativeDriverJavaTimeCodecs() as described above as it is using an UTC based approach. The default JSR-310 support for java.time types inherited from Spring Data Commons uses the local machine timezone as reference and should only be used for backwards compatibility. AbstractMongoClientConfiguration creates a MongoTemplate instance and registers it with the container under the name mongoTemplate . The base-package property tells it where to scan for classes annotated with the @org.springframework.data.mongodb.core.mapping.Document annotation. If you want to rely on Spring Boot(https://spring.io/projects/spring-boot) to bootstrap Data MongoDB, but still want to override certain aspects of the configuration, you may want to expose beans of that type. For custom conversions you may eg. choose to register a bean of type MongoCustomConversions that will be picked up the by the Boot infrastructure. To learn more about this please make sure to read the Spring Boot Reference Documentation(https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#data.nosql.mongodb) . Metadata-based Mapping: To take full advantage of the object mapping functionality inside the Spring Data MongoDB support, you should annotate your mapped objects with the @Document annotation. Although it is not necessary for the mapping framework to have this annotation (your POJOs are mapped correctly, even without any annotations), it lets the classpath scanner find and pre-process your domain objects to extract the necessary metadata. If you do not use this annotation, your application takes a slight performance hit the first time you store a domain object, because the mapping framework needs to build up its internal metadata model so that it knows about the properties of your domain object and how to persist them. The following example shows a domain object: Example 1. Example domain object package com.mycompany.domain; @Document public class Person { @Id private ObjectId id; @Indexed private Integer ssn; private String firstName; @Indexed private String lastName; } The @Id annotation tells the mapper which property you want to use for the MongoDB _id property, and the @Indexed annotation tells the mapping framework to call createIndex(…) on that property of your document, making searches faster. Automatic index creation is only done for types annotated with @Document . Auto index creation is disabled by default and needs to be enabled through the configuration (see Index Creation(#mapping.index-creation) ). Mapping Annotation Overview: The MappingMongoConverter can use metadata to drive the mapping of objects to documents. The following annotations are available: @Id : Applied at the field level to mark the field used for identity purpose. @MongoId : Applied at the field level to mark the field used for identity purpose. Accepts an optional FieldType to customize id conversion. @Document : Applied at the class level to indicate this class is a candidate for mapping to the database. You can specify the name of the collection where the data will be stored. @DBRef : Applied at the field to indicate it is to be stored using a com.mongodb.DBRef. @DocumentReference : Applied at the field to indicate it is to be stored as a pointer to another document. This can be a single value (the id by default), or a Document provided via a converter. @Indexed : Applied at the field level to describe how to index the field. @CompoundIndex (repeatable): Applied at the type level to declare Compound Indexes. @GeoSpatialIndexed : Applied at the field level to describe how to geoindex the field. @TextIndexed : Applied at the field level to mark the field to be included in the text index. @HashIndexed : Applied at the field level for usage within a hashed index to partition data across a sharded cluster. @Language : Applied at the field level to set the language override property for text index. @Transient : By default, all fields are mapped to the document. This annotation excludes the field where it is applied from being stored in the database. Transient properties cannot be used within a persistence constructor as the converter cannot materialize a value for the constructor argument. @PersistenceConstructor : Marks a given constructor - even a package protected one - to use when instantiating the object from the database. Constructor arguments are mapped by name to the key values in the retrieved Document. @Value : This annotation is part of the Spring Framework . Within the mapping framework it can be applied to constructor arguments. This lets you use a Spring Expression Language statement to transform a key’s value retrieved in the database before it is used to construct a domain object. In order to reference a property of a given document one has to use expressions like: @Value(""#root.myProperty"") where root refers to the root of the given document. @Field : Applied at the field level it allows to describe the name and type of the field as it will be represented in the MongoDB BSON document thus allowing the name and type to be different than the fieldname of the class as well as the property type. @Version : Applied at field level is used for optimistic locking and checked for modification on save operations. The initial value is zero ( one for primitive types) which is bumped automatically on every update. The mapping metadata infrastructure is defined in a separate spring-data-commons project that is technology agnostic. Specific subclasses are using in the MongoDB support to support annotation based metadata. Other strategies are also possible to put in place if there is demand. Here is an example of a more complex mapping @Document @CompoundIndex(name = ""age_idx"", def = ""{'lastName': 1, 'age': -1}"") public class Person<T extends Address> { @Id private String id; @Indexed(unique = true) private Integer ssn; @Field(""fName"") private String firstName; @Indexed private String lastName; private Integer age; @Transient private Integer accountTotal; @DBRef private List<Account> accounts; private T address; public Person(Integer ssn) { this.ssn = ssn; } @PersistenceConstructor public Person(Integer ssn, String firstName, String lastName, Integer age, T address) { this.ssn = ssn; this.firstName = firstName; this.lastName = lastName; this.age = age; this.address = address; } public String getId() { return id; } // no setter for Id. (getter is only exposed for some unit testing) public Integer getSsn() { return ssn; } // other getters/setters omitted } @Field(targetType=…​) can come in handy when the native MongoDB type inferred by the mapping infrastructure does not match the expected one. Like for BigDecimal , which is represented as String instead of Decimal128 , just because earlier versions of MongoDB Server did not have support for it. public class Balance { @Field(targetType = DECIMAL128) private BigDecimal value; // ... } You may even consider your own, custom annotation. @Target(ElementType.FIELD) @Retention(RetentionPolicy.RUNTIME) @Field(targetType = FieldType.DECIMAL128) public @interface Decimal128 { } // ... public class Balance { @Decimal128 private BigDecimal value; // ... } Special Field Names: Generally speaking MongoDB uses the dot ( . ) character as a path separator for nested documents or arrays. This means that in a query (or update statement) a key like a.b.c targets an object structure as outlined below: { 'a' : { 'b' : { 'c' : … } } } Therefore, up until MongoDB 5.0 field names must not contain dots ( . ). Using a MappingMongoConverter#setMapKeyDotReplacement allowed circumvent some of the limitations when storing Map structures by substituting dots on write with another character. converter.setMapKeyDotReplacement(""-""); // ... source.map = Map.of(""key.with.dot"", ""value"") converter.write(source,...) // -> map : { 'key-with-dot', 'value' } With the release of MongoDB 5.0 this restriction on Document field names containing special characters was lifted. We highly recommend reading more about limitations on using dots in field names in the MongoDB Reference(https://www.mongodb.com/docs/manual/core/dot-dollar-considerations/) . To allow dots in Map structures please set preserveMapKeys on the MappingMongoConverter . Using @Field allows customizing the field name to consider dots in two ways. @Field(name = ""a.b"") : The name is considered to be a path. Operations expect a structure of nested objects such as { a : { b : … } } . @Field(name = ""a.b"", fieldNameType = KEY) : The names is considered a name as-is. Operations expect a field with the given value as { 'a.b' : ….. } Due to the special nature of the dot character in both MongoDB query and update statements field names containing dots cannot be targeted directly and therefore are excluded from being used in derived query methods. Consider the following Item having a categoryId property that is mapped to the field named cat.id . public class Item { @Field(name = ""cat.id"", fieldNameType = KEY) String categoryId; // ... } Its raw representation will look like { 'cat.id' : ""5b28b5e7-52c2"", ... } Since we cannot target the cat.id field directly (as this would be interpreted as a path) we need the help of the Aggregation Framework(../aggregation-framework.html#mongo.aggregation) . Query fields with a dot in its name template.query(Item.class) // $expr : { $eq : [ { $getField : { input : '$$CURRENT', 'cat.id' }, '5b28b5e7-52c2' ] } .matching(expr(ComparisonOperators.valueOf(ObjectOperators.getValueOf(""value"")).equalToValue(""5b28b5e7-52c2""))) (1) .all(); 1 The mapping layer takes care of translating the property name value into the actual field name. It is absolutely valid to use the target field name here as well. Update fields with a dot in its name template.update(Item.class) .matching(where(""id"").is(""r2d2"")) // $replaceWith: { $setField : { input: '$$CURRENT', field : 'cat.id', value : 'af29-f87f4e933f97' } } .apply(AggregationUpdate.newUpdate(ReplaceWithOperation.replaceWithValue(ObjectOperators.setValueTo(""value"", ""af29-f87f4e933f97"")))) (1) .first(); 1 The mapping layer takes care of translating the property name value into the actual field name. It is absolutely valid to use the target field name here as well. The above shows a simple example where the special field is present on the top document level. Increased levels of nesting increase the complexity of the aggregation expression required to interact with the field. Customized Object Construction: The mapping subsystem allows the customization of the object construction by annotating a constructor with the @PersistenceConstructor annotation. The values to be used for the constructor parameters are resolved in the following way: If a parameter is annotated with the @Value annotation, the given expression is evaluated and the result is used as the parameter value. If the Java type has a property whose name matches the given field of the input document, then it’s property information is used to select the appropriate constructor parameter to pass the input field value to. This works only if the parameter name information is present in the java .class files which can be achieved by compiling the source with debug information or using the new -parameters command-line switch for javac in Java 8. Otherwise, a MappingException will be thrown indicating that the given constructor parameter could not be bound. class OrderItem { private @Id String id; private int quantity; private double unitPrice; OrderItem(String id, @Value(""#root.qty ?: 0"") int quantity, double unitPrice) { this.id = id; this.quantity = quantity; this.unitPrice = unitPrice; } // getters/setters ommitted } Document input = new Document(""id"", ""4711""); input.put(""unitPrice"", 2.5); input.put(""qty"",5); OrderItem item = converter.read(OrderItem.class, input); The SpEL expression in the @Value annotation of the quantity parameter falls back to the value 0 if the given property path cannot be resolved. Additional examples for using the @PersistenceConstructor annotation can be found in the MappingMongoConverterUnitTests(https://github.com/spring-projects/spring-data-mongodb/blob/master/spring-data-mongodb/src/test/java/org/springframework/data/mongodb/core/convert/MappingMongoConverterUnitTests.java) test suite. Mapping Framework Events: Events are fired throughout the lifecycle of the mapping process. This is described in the Lifecycle Events(../lifecycle-events.html) section. Declaring these beans in your Spring ApplicationContext causes them to be invoked whenever the event is dispatched. 1(#_footnoteref_1) . Uses UTC zone offset. Configure via MongoConverterConfigurationAdapter(#mapping-configuration) 2(#_footnoteref_2) . Uses UTC zone offset. Configure via MongoConverterConfigurationAdapter(#mapping-configuration)"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/mapping/mapping-schema.html","JSON Schema: As of version 3.6, MongoDB supports collections that validate documents against a provided JSON Schema(https://docs.mongodb.com/manual/core/schema-validation/#json-schema) . The schema itself and both validation action and level can be defined when creating the collection, as the following example shows: Example 1. Sample JSON schema { ""type"": ""object"", (1) ""required"": [ ""firstname"", ""lastname"" ], (2) ""properties"": { (3) ""firstname"": { (4) ""type"": ""string"", ""enum"": [ ""luke"", ""han"" ] }, ""address"": { (5) ""type"": ""object"", ""properties"": { ""postCode"": { ""type"": ""string"", ""minLength"": 4, ""maxLength"": 5 } } } } } 1 JSON schema documents always describe a whole document from its root. A schema is a schema object itself that can contain embedded schema objects that describe properties and subdocuments. 2 required is a property that describes which properties are required in a document. It can be specified optionally, along with other schema constraints. See MongoDB’s documentation on available keywords(https://docs.mongodb.com/manual/reference/operator/query/jsonSchema/#available-keywords) . 3 properties is related to a schema object that describes an object type. It contains property-specific schema constraints. 4 firstname specifies constraints for the firstname field inside the document. Here, it is a string-based properties element declaring possible field values. 5 address is a subdocument defining a schema for values in its postCode field. You can provide a schema either by specifying a schema document (that is, by using the Document API to parse or build a document object) or by building it with Spring Data’s JSON schema utilities in org.springframework.data.mongodb.core.schema . MongoJsonSchema is the entry point for all JSON schema-related operations. The following example shows how use MongoJsonSchema.builder() to create a JSON schema: Example 2. Creating a JSON schema MongoJsonSchema.builder() (1) .required(""lastname"") (2) .properties( required(string(""firstname"").possibleValues(""luke"", ""han"")), (3) object(""address"") .properties(string(""postCode"").minLength(4).maxLength(5))) .build(); (4) 1 Obtain a schema builder to configure the schema with a fluent API. 2 Configure required properties either directly as shown here or with more details as in 3. 3 Configure the required String-typed firstname field, allowing only luke and han values. Properties can be typed or untyped. Use a static import of JsonSchemaProperty to make the syntax slightly more compact and to get entry points such as string(…) . 4 Build the schema object. There are already some predefined and strongly typed schema objects ( JsonSchemaObject and JsonSchemaProperty ) available through static methods on the gateway interfaces. However, you may need to build custom property validation rules, which can be created through the builder API, as the following example shows: // ""birthdate"" : { ""bsonType"": ""date"" } JsonSchemaProperty.named(""birthdate"").ofType(Type.dateType()); // ""birthdate"" : { ""bsonType"": ""date"", ""description"", ""Must be a date"" } JsonSchemaProperty.named(""birthdate"").with(JsonSchemaObject.of(Type.dateType()).description(""Must be a date"")); CollectionOptions provides the entry point to schema support for collections, as the following example shows: Example 3. Create collection with $jsonSchema MongoJsonSchema schema = MongoJsonSchema.builder().required(""firstname"", ""lastname"").build(); template.createCollection(Person.class, CollectionOptions.empty().schema(schema)); Generating a Schema: Setting up a schema can be a time consuming task and we encourage everyone who decides to do so, to really take the time it takes. It’s important, schema changes can be hard. However, there might be times when one does not want to balked with it, and that is where JsonSchemaCreator comes into play. JsonSchemaCreator and its default implementation generates a MongoJsonSchema out of domain types metadata provided by the mapping infrastructure. This means, that annotated properties(mapping.html#mapping-usage-annotations) as well as potential custom conversions(mapping.html#mapping-configuration) are considered. Example 4. Generate Json Schema from domain type public class Person { private final String firstname; (1) private final int age; (2) private Species species; (3) private Address address; (4) private @Field(fieldType=SCRIPT) String theForce; (5) private @Transient Boolean useTheForce; (6) public Person(String firstname, int age) { (1) (2) this.firstname = firstname; this.age = age; } // gettter / setter omitted } MongoJsonSchema schema = MongoJsonSchemaCreator.create(mongoOperations.getConverter()) .createSchemaFor(Person.class); template.createCollection(Person.class, CollectionOptions.empty().schema(schema)); { 'type' : 'object', 'required' : ['age'], (2) 'properties' : { 'firstname' : { 'type' : 'string' }, (1) 'age' : { 'bsonType' : 'int' } (2) 'species' : { (3) 'type' : 'string', 'enum' : ['HUMAN', 'WOOKIE', 'UNKNOWN'] } 'address' : { (4) 'type' : 'object' 'properties' : { 'postCode' : { 'type': 'string' } } }, 'theForce' : { 'type' : 'javascript'} (5) } } 1 Simple object properties are consideres regular properties. 2 Primitive types are considered required properties 3 Enums are restricted to possible values. 4 Object type properties are inspected and represented as nested documents. 5 String type property that is converted to Code by the converter. 6 @Transient properties are omitted when generating the schema. _id properties using types that can be converted into ObjectId like String are mapped to { type : 'object' } unless there is more specific information available via the @MongoId annotation. Table 1. Sepcial Schema Generation rules Java Schema Type Notes Object type : object with properties if metadata available. Collection type : array - Map type : object - Enum type : string with enum property holding the possible enumeration values. array type : array simple type array unless it’s a byte[] byte[] bsonType : binData - The above example demonstrated how to derive the schema from a very precise typed source. Using polymorphic elements within the domain model can lead to inaccurate schema representation for Object and generic <T> types, which are likely to represented as { type : 'object' } without further specification. MongoJsonSchemaCreator.property(…) allows defining additional details such as nested document types that should be considered when rendering the schema. Example 5. Specify additional types for properties class Root { Object value; } class A { String aValue; } class B { String bValue; } MongoJsonSchemaCreator.create() .property(""value"").withTypes(A.class, B.class) (1) { 'type' : 'object', 'properties' : { 'value' : { 'type' : 'object', 'properties' : { (1) 'aValue' : { 'type' : 'string' }, 'bValue' : { 'type' : 'string' } } } } } 1 Properties of the given types are merged into one element. MongoDBs schema-free approach allows storing documents of different structure in one collection. Those may be modeled having a common base class. Regardless of the chosen approach, MongoJsonSchemaCreator.merge(…) can help circumvent the need of merging multiple schema into one. Example 6. Merging multiple Schemas into a single Schema definition abstract class Root { String rootValue; } class A extends Root { String aValue; } class B extends Root { String bValue; } MongoJsonSchemaCreator.mergedSchemaFor(A.class, B.class) (1) { 'type' : 'object', 'properties' : { (1) 'rootValue' : { 'type' : 'string' }, 'aValue' : { 'type' : 'string' }, 'bValue' : { 'type' : 'string' } } } } 1 Properties (and their inherited ones) of the given types are combined into one schema. Properties with the same name need to refer to the same JSON schema in order to be combined. The following example shows a definition that cannot be merged automatically because of a data type mismatch. In this case a ConflictResolutionFunction must be provided to MongoJsonSchemaCreator . class A extends Root { String value; } class B extends Root { Integer value; } Encrypted Fields: MongoDB 4.2 Field Level Encryption(https://docs.mongodb.com/master/core/security-client-side-encryption/) allows to directly encrypt individual properties. Properties can be wrapped within an encrypted property when setting up the JSON Schema as shown in the example below. Example 7. Client-Side Field Level Encryption via Json Schema MongoJsonSchema schema = MongoJsonSchema.builder() .properties( encrypted(string(""ssn"")) .algorithm(""AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic"") .keyId(""*key0_id"") ).build(); Instead of defining encrypted fields manually it is possible leverage the @Encrypted annotation as shown in the snippet below. Example 8. Client-Side Field Level Encryption via Json Schema @Document @Encrypted(keyId = ""xKVup8B1Q+CkHaVRx+qa+g=="", algorithm = ""AEAD_AES_256_CBC_HMAC_SHA_512-Random"") (1) static class Patient { @Id String id; String name; @Encrypted (2) String bloodType; @Encrypted(algorithm = ""AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic"") (3) Integer ssn; } 1 Default encryption settings that will be set for encryptMetadata . 2 Encrypted field using default encryption settings. 3 Encrypted field overriding the default encryption algorithm. The @Encrypted Annotation supports resolving keyIds via SpEL Expressions. To do so additional environment metadata (via the MappingContext ) is required and must be provided. @Document @Encrypted(keyId = ""#{mongocrypt.keyId(#target)}"") static class Patient { @Id String id; String name; @Encrypted(algorithm = ""AEAD_AES_256_CBC_HMAC_SHA_512-Random"") String bloodType; @Encrypted(algorithm = ""AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic"") Integer ssn; } MongoJsonSchemaCreator schemaCreator = MongoJsonSchemaCreator.create(mappingContext); MongoJsonSchema patientSchema = schemaCreator .filter(MongoJsonSchemaCreator.encryptedOnly()) .createSchemaFor(Patient.class); The mongocrypt.keyId function is defined via an EvaluationContextExtension as shown in the snippet below. Providing a custom extension provides the most flexible way of computing keyIds. public class EncryptionExtension implements EvaluationContextExtension { @Override public String getExtensionId() { return ""mongocrypt""; } @Override public Map<String, Function> getFunctions() { return Collections.singletonMap(""keyId"", new Function(getMethod(""computeKeyId"", String.class), this)); } public String computeKeyId(String target) { // ... lookup via target element name } } JSON Schema Types: The following table shows the supported JSON schema types: Table 2. Supported JSON schema types Schema Type Java Type Schema Properties untyped - description , generated description , enum , allOf , anyOf , oneOf , not object Object required , additionalProperties , properties , minProperties , maxProperties , patternProperties array any array except byte[] uniqueItems , additionalItems , items , minItems , maxItems string String minLength , maxLentgth , pattern int int , Integer multipleOf , minimum , exclusiveMinimum , maximum , exclusiveMaximum long long , Long multipleOf , minimum , exclusiveMinimum , maximum , exclusiveMaximum double float , Float , double , Double multipleOf , minimum , exclusiveMinimum , maximum , exclusiveMaximum decimal BigDecimal multipleOf , minimum , exclusiveMinimum , maximum , exclusiveMaximum number Number multipleOf , minimum , exclusiveMinimum , maximum , exclusiveMaximum binData byte[] (none) boolean boolean , Boolean (none) null null (none) objectId ObjectId (none) date java.util.Date (none) timestamp BsonTimestamp (none) regex java.util.regex.Pattern (none) untyped is a generic type that is inherited by all typed schema types. It provides all untyped schema properties to typed schema types. For more information, see $jsonSchema(https://docs.mongodb.com/manual/reference/operator/query/jsonSchema/#op._S_jsonSchema) ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/mapping/custom-conversions.html","Custom Conversions: The following example of a Spring Converter implementation converts from a String to a custom Email value object: @ReadingConverter public class EmailReadConverter implements Converter<String, Email> { public Email convert(String source) { return Email.valueOf(source); } } If you write a Converter whose source and target type are native types, we cannot determine whether we should consider it as a reading or a writing converter. Registering the converter instance as both might lead to unwanted results. For example, a Converter<String, Long> is ambiguous, although it probably does not make sense to try to convert all String instances into Long instances when writing. To let you force the infrastructure to register a converter for only one way, we provide @ReadingConverter and @WritingConverter annotations to be used in the converter implementation. Converters are subject to explicit registration as instances are not picked up from a classpath or container scan to avoid unwanted registration with a conversion service and the side effects resulting from such a registration. Converters are registered with CustomConversions as the central facility that allows registration and querying for registered converters based on source- and target type. CustomConversions ships with a pre-defined set of converter registrations: JSR-310 Converters for conversion between java.time , java.util.Date and String types. Default converters for local temporal types (e.g. LocalDateTime to java.util.Date ) rely on system-default timezone settings to convert between those types. You can override the default converter, by registering your own converter. Converter Disambiguation: Generally, we inspect the Converter implementations for the source and target types they convert from and to. Depending on whether one of those is a type the underlying data access API can handle natively, we register the converter instance as a reading or a writing converter. The following examples show a writing- and a read converter (note the difference is in the order of the qualifiers on Converter ): // Write converter as only the target type is one that can be handled natively class MyConverter implements Converter<Person, String> { … } // Read converter as only the source type is one that can be handled natively class MyConverter implements Converter<String, Person> { … } Type based Converter: The most trivial way of influencing the mapping result is by specifying the desired native MongoDB target type via the @Field annotation. This allows to work with non MongoDB types like BigDecimal in the domain model while persisting values in native org.bson.types.Decimal128 format. Example 1. Explicit target type mapping public class Payment { @Id String id; (1) @Field(targetType = FieldType.DECIMAL128) (2) BigDecimal value; Date date; (3) } { ""_id"" : ObjectId(""5ca4a34fa264a01503b36af8""), (1) ""value"" : NumberDecimal(2.099), (2) ""date"" : ISODate(""2019-04-03T12:11:01.870Z"") (3) } 1 String id values that represent a valid ObjectId are converted automatically. See How the _id Field is Handled in the Mapping Layer(../template-crud-operations.html#mongo-template.id-handling) for details. 2 The desired target type is explicitly defined as Decimal128 which translates to NumberDecimal . Otherwise the BigDecimal value would have been truned into a String . 3 Date values are handled by the MongoDB driver itself an are stored as ISODate . The snippet above is handy for providing simple type hints. To gain more fine-grained control over the mapping process, you can register Spring converters with the MongoConverter implementations, such as the MappingMongoConverter . The MappingMongoConverter checks to see if any Spring converters can handle a specific class before attempting to map the object itself. To 'hijack' the normal mapping strategies of the MappingMongoConverter , perhaps for increased performance or other custom mapping needs, you first need to create an implementation of the Spring Converter interface and then register it with the MappingConverter . For more information on the Spring type conversion service, see the reference docs here(https://docs.spring.io/spring-framework/reference/6.1/core.html#validation) . Writing Converter: The following example shows an implementation of the Converter that converts from a Person object to a org.bson.Document : import org.springframework.core.convert.converter.Converter; import org.bson.Document; public class PersonWriteConverter implements Converter<Person, Document> { public Document convert(Person source) { Document document = new Document(); document.put(""_id"", source.getId()); document.put(""name"", source.getFirstName()); document.put(""age"", source.getAge()); return document; } } Reading Converter: The following example shows an implementation of a Converter that converts from a Document to a Person object: public class PersonReadConverter implements Converter<Document, Person> { public Person convert(Document source) { Person p = new Person((ObjectId) source.get(""_id""), (String) source.get(""name"")); p.setAge((Integer) source.get(""age"")); return p; } } Registering Converters: class MyMongoConfiguration extends AbstractMongoClientConfiguration { @Override public String getDatabaseName() { return ""database""; } @Override protected void configureConverters(MongoConverterConfigurationAdapter adapter) { adapter.registerConverter(new com.example.PersonReadConverter()); adapter.registerConverter(new com.example.PersonWriteConverter()); } }"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/mapping/property-converters.html","Property Converters: While type-based conversion(custom-conversions.html) already offers ways to influence the conversion and representation of certain types within the target store, it has limitations when only certain values or properties of a particular type should be considered for conversion. Property-based converters allow configuring conversion rules on a per-property basis, either declaratively (via @ValueConverter ) or programmatically (by registering a PropertyValueConverter for a specific property). A PropertyValueConverter can transform a given value into its store representation (write) and back (read) as the following listing shows. The additional ValueConversionContext provides additional information, such as mapping metadata and direct read and write methods. Example 1. A simple PropertyValueConverter class ReversingValueConverter implements PropertyValueConverter<String, String, ValueConversionContext> { @Override public String read(String value, ValueConversionContext context) { return reverse(value); } @Override public String write(String value, ValueConversionContext context) { return reverse(value); } } You can obtain PropertyValueConverter instances from CustomConversions#getPropertyValueConverter(…) by delegating to PropertyValueConversions , typically by using a PropertyValueConverterFactory to provide the actual converter. Depending on your application’s needs, you can chain or decorate multiple instances of PropertyValueConverterFactory — for example, to apply caching. By default, Spring Data MongoDB uses a caching implementation that can serve types with a default constructor or enum values. A set of predefined factories is available through the factory methods in PropertyValueConverterFactory . You can use PropertyValueConverterFactory.beanFactoryAware(…) to obtain a PropertyValueConverter instance from an ApplicationContext . You can change the default behavior through ConverterConfiguration . Declarative Value Converter: The most straight forward usage of a PropertyValueConverter is by annotating properties with the @ValueConverter annotation that defines the converter type: Example 2. Declarative PropertyValueConverter class Person { @ValueConverter(ReversingValueConverter.class) String ssn; } Programmatic Value Converter Registration: Programmatic registration registers PropertyValueConverter instances for properties within an entity model by using a PropertyValueConverterRegistrar , as the following example shows. The difference between declarative registration and programmatic registration is that programmatic registration happens entirely outside of the entity model. Such an approach is useful if you cannot or do not want to annotate the entity model. Example 3. Programmatic PropertyValueConverter registration PropertyValueConverterRegistrar registrar = new PropertyValueConverterRegistrar(); registrar.registerConverter(Address.class, ""street"", new PropertyValueConverter() { … }); (1) // type safe registration registrar.registerConverter(Person.class, Person::getSsn()) (2) .writing(value -> encrypt(value)) .reading(value -> decrypt(value)); 1 Register a converter for the field identified by its name. 2 Type safe variant that allows to register a converter and its conversion functions. This method uses class proxies to determine the property. Make sure that neither the class nor the accessors are final as otherwise this approach doesn’t work. Dot notation (such as registerConverter(Person.class, ""address.street"", …) ) for nagivating across properties into subdocuments is not supported when registering converters. MongoValueConverter offers a pre-typed PropertyValueConverter interface that uses MongoConversionContext . MongoCustomConversions configuration: By default, MongoCustomConversions can handle declarative value converters, depending on the configured PropertyValueConverterFactory . MongoConverterConfigurationAdapter helps to set up programmatic value conversions or define the PropertyValueConverterFactory to be used. Example 4. Configuration Sample MongoCustomConversions.create(configurationAdapter -> { SimplePropertyValueConversions valueConversions = new SimplePropertyValueConversions(); valueConversions.setConverterFactory(…); valueConversions.setValueConverterRegistry(new PropertyValueConverterRegistrar() .registerConverter(…) .buildRegistry()); configurationAdapter.setPropertyValueConversions(valueConversions); });"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/mapping/unwrapping-entities.html","Unwrapping Types: Unwrapped entities are used to design value objects in your Java domain model whose properties are flattened out into the parent’s MongoDB Document. Unwrapped Types Mapping: Consider the following domain model where User.name is annotated with @Unwrapped . The @Unwrapped annotation signals that all properties of UserName should be flattened out into the user document that owns the name property. Example 1. Sample Code of unwrapping objects class User { @Id String userId; @Unwrapped(onEmpty = USE_NULL) (1) UserName name; } class UserName { String firstname; String lastname; } { ""_id"" : ""1da2ba06-3ba7"", ""firstname"" : ""Emma"", ""lastname"" : ""Frost"" } 1 When loading the name property its value is set to null if both firstname and lastname are either null or not present. By using onEmpty=USE_EMPTY an empty UserName , with potential null value for its properties, will be created. For less verbose embeddable type declarations use @Unwrapped.Nullable and @Unwrapped.Empty instead @Unwrapped(onEmpty = USE_NULL) and @Unwrapped(onEmpty = USE_EMPTY) . Both annotations are meta-annotated with JSR-305 @javax.annotation.Nonnull to aid with nullability inspections. It is possible to use complex types within an unwrapped object. However, those must not be, nor contain unwrapped fields themselves. Unwrapped Types field names: A value object can be unwrapped multiple times by using the optional prefix attribute of the @Unwrapped annotation. By dosing so the chosen prefix is prepended to each property or @Field(""…"") name in the unwrapped object. Please note that values will overwrite each other if multiple properties render to the same field name. Example 2. Sample Code of unwrapped object with name prefix class User { @Id String userId; @Unwrapped.Nullable(prefix = ""u_"") (1) UserName name; @Unwrapped.Nullable(prefix = ""a_"") (2) UserName name; } class UserName { String firstname; String lastname; } { ""_id"" : ""a6a805bd-f95f"", ""u_firstname"" : ""Jean"", (1) ""u_lastname"" : ""Grey"", ""a_firstname"" : ""Something"", (2) ""a_lastname"" : ""Else"" } 1 All properties of UserName are prefixed with u_ . 2 All properties of UserName are prefixed with a_ . While combining the @Field annotation with @Unwrapped on the very same property does not make sense and therefore leads to an error. It is a totally valid approach to use @Field on any of the unwrapped types properties. Example 3. Sample Code unwrapping objects with @Field annotation public class User { @Id private String userId; @Unwrapped.Nullable(prefix = ""u-"") (1) UserName name; } public class UserName { @Field(""first-name"") (2) private String firstname; @Field(""last-name"") private String lastname; } { ""_id"" : ""2647f7b9-89da"", ""u-first-name"" : ""Barbara"", (2) ""u-last-name"" : ""Gordon"" } 1 All properties of UserName are prefixed with u- . 2 Final field names are a result of concatenating @Unwrapped(prefix) and @Field(name) . Query on Unwrapped Objects: Defining queries on unwrapped properties is possible on type- as well as field-level as the provided Criteria is matched against the domain type. Prefixes and potential custom field names will be considered when rendering the actual query. Use the property name of the unwrapped object to match against all contained fields as shown in the sample below. Example 4. Query on unwrapped object UserName userName = new UserName(""Carol"", ""Danvers"") Query findByUserName = query(where(""name"").is(userName)); User user = template.findOne(findByUserName, User.class); db.collection.find({ ""firstname"" : ""Carol"", ""lastname"" : ""Danvers"" }) It is also possible to address any field of the unwrapped object directly using its property name as shown in the snippet below. Example 5. Query on field of unwrapped object Query findByUserFirstName = query(where(""name.firstname"").is(""Shuri"")); List<User> users = template.findAll(findByUserFirstName, User.class); db.collection.find({ ""firstname"" : ""Shuri"" }) Sort by unwrapped field.: Fields of unwrapped objects can be used for sorting via their property path as shown in the sample below. Example 6. Sort on unwrapped field Query findByUserLastName = query(where(""name.lastname"").is(""Romanoff"")); List<User> user = template.findAll(findByUserName.withSort(Sort.by(""name.firstname"")), User.class); db.collection.find({ ""lastname"" : ""Romanoff"" }).sort({ ""firstname"" : 1 }) Though possible, using the unwrapped object itself as sort criteria includes all of its fields in unpredictable order and may result in inaccurate ordering. Field projection on unwrapped objects: Fields of unwrapped objects can be subject for projection either as a whole or via single fields as shown in the samples below. Example 7. Project on unwrapped object. Query findByUserLastName = query(where(""name.firstname"").is(""Gamora"")); findByUserLastName.fields().include(""name""); (1) List<User> user = template.findAll(findByUserName, User.class); db.collection.find({ ""lastname"" : ""Gamora"" }, { ""firstname"" : 1, ""lastname"" : 1 }) 1 A field projection on an unwrapped object includes all of its properties. Example 8. Project on a field of an unwrapped object. Query findByUserLastName = query(where(""name.lastname"").is(""Smoak"")); findByUserLastName.fields().include(""name.firstname""); (1) List<User> user = template.findAll(findByUserName, User.class); db.collection.find({ ""lastname"" : ""Smoak"" }, { ""firstname"" : 1 }) 1 A field projection on an unwrapped object includes all of its properties. Query By Example on unwrapped object.: Unwrapped objects can be used within an Example probe just as any other type. Please review the Query By Example(../template-query-operations.html#mongo.query-by-example) section, to learn more about this feature. Repository Queries on unwrapped objects.: The Repository abstraction allows deriving queries on fields of unwrapped objects as well as the entire object. Example 9. Repository queries on unwrapped objects. interface UserRepository extends CrudRepository<User, String> { List<User> findByName(UserName username); (1) List<User> findByNameFirstname(String firstname); (2) } 1 Matches against all fields of the unwrapped object. 2 Matches against the firstname . Index creation for unwrapped objects is suspended even if the repository create-query-indexes namespace attribute is set to true . Update on Unwrapped Objects: Unwrapped objects can be updated as any other object that is part of the domain model. The mapping layer takes care of flattening structures into their surroundings. It is possible to update single attributes of the unwrapped object as well as the entire value as shown in the examples below. Example 10. Update a single field of an unwrapped object. Update update = new Update().set(""name.firstname"", ""Janet""); template.update(User.class).matching(where(""id"").is(""Wasp"")) .apply(update).first() db.collection.update({ ""_id"" : ""Wasp"" }, { ""$set"" { ""firstname"" : ""Janet"" } }, { ... } ) Example 11. Update an unwrapped object. Update update = new Update().set(""name"", new Name(""Janet"", ""van Dyne"")); template.update(User.class).matching(where(""id"").is(""Wasp"")) .apply(update).first() db.collection.update({ ""_id"" : ""Wasp"" }, { ""$set"" { ""firstname"" : ""Janet"", ""lastname"" : ""van Dyne"", } }, { ... } ) Aggregations on Unwrapped Objects: The Aggregation Framework(../aggregation-framework.html) will attempt to map unwrapped values of typed aggregations. Please make sure to work with the property path including the wrapper object when referencing one of its values. Other than that no special action is required. Index on Unwrapped Objects: It is possible to attach the @Indexed annotation to properties of an unwrapped type just as it is done with regular objects. It is not possible to use @Indexed along with the @Unwrapped annotation on the owning property. public class User { @Id private String userId; @Unwrapped(onEmpty = USE_NULL) UserName name; (1) // Invalid -> InvalidDataAccessApiUsageException @Indexed (2) @Unwrapped(onEmpty = USE_Empty) Address address; } public class UserName { private String firstname; @Indexed private String lastname; (1) } 1 Index created for lastname in users collection. 2 Invalid @Indexed usage along with @Unwrapped"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/mapping/document-references.html","Using DBRefs: The mapping framework does not have to store child objects embedded within the document. You can also store them separately and use a DBRef to refer to that document. When the object is loaded from MongoDB, those references are eagerly resolved so that you get back a mapped object that looks the same as if it had been stored embedded within your top-level document. The following example uses a DBRef to refer to a specific document that exists independently of the object in which it is referenced (both classes are shown in-line for brevity’s sake): @Document public class Account { @Id private ObjectId id; private Float total; } @Document public class Person { @Id private ObjectId id; @Indexed private Integer ssn; @DBRef private List<Account> accounts; } You need not use @OneToMany or similar mechanisms because the List of objects tells the mapping framework that you want a one-to-many relationship. When the object is stored in MongoDB, there is a list of DBRefs rather than the Account objects themselves. When it comes to loading collections of DBRef s it is advisable to restrict references held in collection types to a specific MongoDB collection. This allows bulk loading of all references, whereas references pointing to different MongoDB collections need to be resolved one by one. The mapping framework does not handle cascading saves. If you change an Account object that is referenced by a Person object, you must save the Account object separately. Calling save on the Person object does not automatically save the Account objects in the accounts property. DBRef s can also be resolved lazily. In this case the actual Object or Collection of references is resolved on first access of the property. Use the lazy attribute of @DBRef to specify this. Required properties that are also defined as lazy loading DBRef and used as constructor arguments are also decorated with the lazy loading proxy making sure to put as little pressure on the database and network as possible. Lazily loaded DBRef s can be hard to debug. Make sure tooling does not accidentally trigger proxy resolution by e.g. calling toString() or some inline debug rendering invoking property getters. Please consider to enable trace logging for org.springframework.data.mongodb.core.convert.DefaultDbRefResolver to gain insight on DBRef resolution. Lazy loading may require class proxies, that in turn, might need access to jdk internals, that are not open, starting with Java 16+, due to JEP 396: Strongly Encapsulate JDK Internals by Default(https://openjdk.java.net/jeps/396) . For those cases please consider falling back to an interface type (eg. switch from ArrayList to List ) or provide the required --add-opens argument. Using Document References: Using @DocumentReference offers a flexible way of referencing entities in MongoDB. While the goal is the same as when using DBRefs(#) , the store representation is different. DBRef resolves to a document with a fixed structure as outlined in the MongoDB Reference documentation(https://docs.mongodb.com/manual/reference/database-references/) . Document references, do not follow a specific format. They can be literally anything, a single value, an entire document, basically everything that can be stored in MongoDB. By default, the mapping layer will use the referenced entities id value for storage and retrieval, like in the sample below. @Document class Account { @Id String id; Float total; } @Document class Person { @Id String id; @DocumentReference (1) List<Account> accounts; } Account account = … template.insert(account); (2) template.update(Person.class) .matching(where(""id"").is(…)) .apply(new Update().push(""accounts"").value(account)) (3) .first(); { ""_id"" : …, ""accounts"" : [ ""6509b9e"" … ] (4) } 1 Mark the collection of Account values to be referenced. 2 The mapping framework does not handle cascading saves, so make sure to persist the referenced entity individually. 3 Add the reference to the existing entity. 4 Referenced Account entities are represented as an array of their _id values. The sample above uses an _id -based fetch query ( { '_id' : ?#{#target} } ) for data retrieval and resolves linked entities eagerly. It is possible to alter resolution defaults (listed below) using the attributes of @DocumentReference Table 1. @DocumentReference defaults Attribute Description Default db The target database name for collection lookup. MongoDatabaseFactory.getMongoDatabase() collection The target collection name. The annotated property’s domain type, respectively the value type in case of Collection like or Map properties, collection name. lookup The single document lookup query evaluating placeholders via SpEL expressions using #target as the marker for a given source value. Collection like or Map properties combine individual lookups via an $or operator. An _id field based query ( { '_id' : ?#{#target} } ) using the loaded source value. sort Used for sorting result documents on server side. None by default. Result order of Collection like properties is restored based on the used lookup query on a best-effort basis. lazy If set to true value resolution is delayed upon first access of the property. Resolves properties eagerly by default. Lazy loading may require class proxies, that in turn, might need access to jdk internals, that are not open, starting with Java 16+, due to JEP 396: Strongly Encapsulate JDK Internals by Default(https://openjdk.java.net/jeps/396) . For those cases please consider falling back to an interface type (eg. switch from ArrayList to List ) or provide the required --add-opens argument. @DocumentReference(lookup) allows defining filter queries that can be different from the _id field and therefore offer a flexible way of defining references between entities as demonstrated in the sample below, where the Publisher of a book is referenced by its acronym instead of the internal id . @Document class Book { @Id ObjectId id; String title; List<String> author; @Field(""publisher_ac"") @DocumentReference(lookup = ""{ 'acronym' : ?#{#target} }"") (1) Publisher publisher; } @Document class Publisher { @Id ObjectId id; String acronym; (1) String name; @DocumentReference(lazy = true) (2) List<Book> books; } Book document { ""_id"" : 9a48e32, ""title"" : ""The Warded Man"", ""author"" : [""Peter V. Brett""], ""publisher_ac"" : ""DR"" } Publisher document { ""_id"" : 1a23e45, ""acronym"" : ""DR"", ""name"" : ""Del Rey"", … } 1 Use the acronym field to query for entities in the Publisher collection. 2 Lazy load back references to the Book collection. The above snippet shows the reading side of things when working with custom referenced objects. Writing requires a bit of additional setup as the mapping information do not express where #target stems from. The mapping layer requires registration of a Converter between the target document and DocumentPointer , like the one below: @WritingConverter class PublisherReferenceConverter implements Converter<Publisher, DocumentPointer<String>> { @Override public DocumentPointer<String> convert(Publisher source) { return () -> source.getAcronym(); } } If no DocumentPointer converter is provided the target reference document can be computed based on the given lookup query. In this case the association target properties are evaluated as shown in the following sample. @Document class Book { @Id ObjectId id; String title; List<String> author; @DocumentReference(lookup = ""{ 'acronym' : ?#{acc} }"") (1) (2) Publisher publisher; } @Document class Publisher { @Id ObjectId id; String acronym; (1) String name; // ... } { ""_id"" : 9a48e32, ""title"" : ""The Warded Man"", ""author"" : [""Peter V. Brett""], ""publisher"" : { ""acc"" : ""DOC"" } } 1 Use the acronym field to query for entities in the Publisher collection. 2 The field value placeholders of the lookup query (like acc ) is used to form the reference document. It is also possible to model relational style One-To-Many references using a combination of @ReadonlyProperty and @DocumentReference . This approach allows link types without storing the linking values within the owning document but rather on the referencing document as shown in the example below. @Document class Book { @Id ObjectId id; String title; List<String> author; ObjectId publisherId; (1) } @Document class Publisher { @Id ObjectId id; String acronym; String name; @ReadOnlyProperty (2) @DocumentReference(lookup=""{'publisherId':?#{#self._id} }"") (3) List<Book> books; } Book document { ""_id"" : 9a48e32, ""title"" : ""The Warded Man"", ""author"" : [""Peter V. Brett""], ""publisherId"" : 8cfb002 } Publisher document { ""_id"" : 8cfb002, ""acronym"" : ""DR"", ""name"" : ""Del Rey"" } 1 Set up the link from Book (reference) to Publisher (owner) by storing the Publisher.id within the Book document. 2 Mark the property holding the references to be readonly. This prevents storing references to individual Book s with the Publisher document. 3 Use the #self variable to access values within the Publisher document and in this retrieve Books with matching publisherId . With all the above in place it is possible to model all kind of associations between entities. Have a look at the non-exhaustive list of samples below to get feeling for what is possible. Example 1. Simple Document Reference using id field class Entity { @DocumentReference ReferencedObject ref; } // entity { ""_id"" : ""8cfb002"", ""ref"" : ""9a48e32"" (1) } // referenced object { ""_id"" : ""9a48e32"" (1) } 1 MongoDB simple type can be directly used without further configuration. Example 2. Simple Document Reference using id field with explicit lookup query class Entity { @DocumentReference(lookup = ""{ '_id' : '?#{#target}' }"") (1) ReferencedObject ref; } // entity { ""_id"" : ""8cfb002"", ""ref"" : ""9a48e32"" (1) } // referenced object { ""_id"" : ""9a48e32"" } 1 target defines the reference value itself. Example 3. Document Reference extracting the refKey field for the lookup query class Entity { @DocumentReference(lookup = ""{ '_id' : '?#{refKey}' }"") (1) (2) private ReferencedObject ref; } @WritingConverter class ToDocumentPointerConverter implements Converter<ReferencedObject, DocumentPointer<Document>> { public DocumentPointer<Document> convert(ReferencedObject source) { return () -> new Document(""refKey"", source.id); (1) } } // entity { ""_id"" : ""8cfb002"", ""ref"" : { ""refKey"" : ""9a48e32"" (1) } } // referenced object { ""_id"" : ""9a48e32"" } 1 The key used for obtaining the reference value must be the one used during write. 2 refKey is short for target.refKey . Example 4. Document Reference with multiple values forming the lookup query class Entity { @DocumentReference(lookup = ""{ 'firstname' : '?#{fn}', 'lastname' : '?#{ln}' }"") (1) (2) ReferencedObject ref; } // entity { ""_id"" : ""8cfb002"", ""ref"" : { ""fn"" : ""Josh"", (1) ""ln"" : ""Long"" (1) } } // referenced object { ""_id"" : ""9a48e32"", ""firstname"" : ""Josh"", (2) ""lastname"" : ""Long"", (2) } 1 Read/write the keys fn & ln from/to the linkage document based on the lookup query. 2 Use non id fields for the lookup of the target documents. Example 5. Document Reference reading from a target collection class Entity { @DocumentReference(lookup = ""{ '_id' : '?#{id}' }"", collection = ""?#{collection}"") (2) private ReferencedObject ref; } @WritingConverter class ToDocumentPointerConverter implements Converter<ReferencedObject, DocumentPointer<Document>> { public DocumentPointer<Document> convert(ReferencedObject source) { return () -> new Document(""id"", source.id) (1) .append(""collection"", … ); (2) } } // entity { ""_id"" : ""8cfb002"", ""ref"" : { ""id"" : ""9a48e32"", (1) ""collection"" : ""…"" (2) } } 1 Read/write the keys _id from/to the reference document to use them in the lookup query. 2 The collection name can be read from the reference document using its key. We know it is tempting to use all kinds of MongoDB query operators in the lookup query and this is fine. But there a few aspects to consider: Make sure to have indexes in place that support your lookup. Mind that resolution requires a server rountrip inducing latency, consider a lazy strategy. A collection of document references is bulk loaded using the $or operator. The original element order is restored in memory on a best-effort basis. Restoring the order is only possible when using equality expressions and cannot be done when using MongoDB query operators. In this case results will be ordered as they are received from the store or via the provided @DocumentReference(sort) attribute. A few more general remarks: Do you use cyclic references? Ask your self if you need them. Lazy document references are hard to debug. Make sure tooling does not accidentally trigger proxy resolution by e.g. calling toString() . There is no support for reading document references using reactive infrastructure."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/mapping/mapping-index-management.html","Index Creation: Spring Data MongoDB can automatically create indexes for entity types annotated with @Document . Index creation must be explicitly enabled since version 3.0 to prevent undesired effects with collection lifecyle and performance impact. Indexes are automatically created for the initial entity set on application startup and when accessing an entity type for the first time while the application runs. We generally recommend explicit index creation for application-based control of indexes as Spring Data cannot automatically create indexes for collections that were recreated while the application was running. IndexResolver provides an abstraction for programmatic index definition creation if you want to make use of @Indexed annotations such as @GeoSpatialIndexed , @TextIndexed , @CompoundIndex and @WildcardIndexed . You can use index definitions with IndexOperations to create indexes. A good point in time for index creation is on application startup, specifically after the application context was refreshed, triggered by observing ContextRefreshedEvent . This event guarantees that the context is fully initialized. Note that at this time other components, especially bean factories might have access to the MongoDB database. Map -like properties are skipped by the IndexResolver unless annotated with @WildcardIndexed because the map key must be part of the index definition. Since the purpose of maps is the usage of dynamic keys and values, the keys cannot be resolved from static mapping metadata. Example 1. Programmatic Index Creation for a single Domain Type class MyListener { @EventListener(ContextRefreshedEvent.class) public void initIndicesAfterStartup() { MappingContext<? extends MongoPersistentEntity<?>, MongoPersistentProperty> mappingContext = mongoTemplate .getConverter().getMappingContext(); IndexResolver resolver = new MongoPersistentEntityIndexResolver(mappingContext); IndexOperations indexOps = mongoTemplate.indexOps(DomainType.class); resolver.resolveIndexFor(DomainType.class).forEach(indexOps::ensureIndex); } } Example 2. Programmatic Index Creation for all Initial Entities class MyListener{ @EventListener(ContextRefreshedEvent.class) public void initIndicesAfterStartup() { MappingContext<? extends MongoPersistentEntity<?>, MongoPersistentProperty> mappingContext = mongoTemplate .getConverter().getMappingContext(); // consider only entities that are annotated with @Document mappingContext.getPersistentEntities() .stream() .filter(it -> it.isAnnotationPresent(Document.class)) .forEach(it -> { IndexOperations indexOps = mongoTemplate.indexOps(it.getType()); resolver.resolveIndexFor(it.getType()).forEach(indexOps::ensureIndex); }); } } Alternatively, if you want to ensure index and collection presence before any component is able to access your database from your application, declare a @Bean method for MongoTemplate and include the code from above before returning the MongoTemplate object. To turn automatic index creation ON please override autoIndexCreation() in your configuration. @Configuration public class Config extends AbstractMongoClientConfiguration { @Override public boolean autoIndexCreation() { return true; } // ... } Automatic index creation is turned OFF by default as of version 3.0. Compound Indexes: Compound indexes are also supported. They are defined at the class level, rather than on individual properties. Compound indexes are very important to improve the performance of queries that involve criteria on multiple fields Here’s an example that creates a compound index of lastName in ascending order and age in descending order: Example 3. Example Compound Index Usage package com.mycompany.domain; @Document @CompoundIndex(name = ""age_idx"", def = ""{'lastName': 1, 'age': -1}"") public class Person { @Id private ObjectId id; private Integer age; private String firstName; private String lastName; } @CompoundIndex is repeatable using @CompoundIndexes as its container. @Document @CompoundIndex(name = ""cmp-idx-one"", def = ""{'firstname': 1, 'lastname': -1}"") @CompoundIndex(name = ""cmp-idx-two"", def = ""{'address.city': -1, 'address.street': 1}"") public class Person { String firstname; String lastname; Address address; // ... } Hashed Indexes: Hashed indexes allow hash based sharding within a sharded cluster. Using hashed field values to shard collections results in a more random distribution. For details, refer to the MongoDB Documentation(https://docs.mongodb.com/manual/core/index-hashed/) . Here’s an example that creates a hashed index for _id : Example 4. Example Hashed Index Usage @Document public class DomainType { @HashIndexed @Id String id; // ... } Hashed indexes can be created next to other index definitions like shown below, in that case both indices are created: Example 5. Example Hashed Index Usage togehter with simple index @Document public class DomainType { @Indexed @HashIndexed String value; // ... } In case the example above is too verbose, a compound annotation allows to reduce the number of annotations that need to be declared on a property: Example 6. Example Composed Hashed Index Usage @Document public class DomainType { @IndexAndHash(name = ""idx..."") (1) String value; // ... } @Indexed @HashIndexed @Retention(RetentionPolicy.RUNTIME) public @interface IndexAndHash { @AliasFor(annotation = Indexed.class, attribute = ""name"") (1) String name() default """"; } 1 Potentially register an alias for certain attributes of the meta annotation. Although index creation via annotations comes in handy for many scenarios cosider taking over more control by setting up indices manually via IndexOperations . mongoOperations.indexOpsFor(Jedi.class) .ensureIndex(HashedIndex.hashed(""useTheForce"")); Wildcard Indexes: A WildcardIndex is an index that can be used to include all fields or specific ones based a given (wildcard) pattern. For details, refer to the MongoDB Documentation(https://docs.mongodb.com/manual/core/index-wildcard/) . The index can be set up programmatically using WildcardIndex via IndexOperations . Example 7. Programmatic WildcardIndex setup mongoOperations .indexOps(User.class) .ensureIndex(new WildcardIndex(""userMetadata"")); db.user.createIndex({ ""userMetadata.$**"" : 1 }, {}) The @WildcardIndex annotation allows a declarative index setup that can used either with a document type or property. If placed on a type that is a root level domain entity (one annotated with @Document ) , the index resolver will create a wildcard index for it. Example 8. Wildcard index on domain type @Document @WildcardIndexed public class Product { // … } db.product.createIndex({ ""$**"" : 1 },{}) The wildcardProjection can be used to specify keys to in-/exclude in the index. Example 9. Wildcard index with wildcardProjection @Document @WildcardIndexed(wildcardProjection = ""{ 'userMetadata.age' : 0 }"") public class User { private @Id String id; private UserMetadata userMetadata; } db.user.createIndex( { ""$**"" : 1 }, { ""wildcardProjection"" : { ""userMetadata.age"" : 0 } } ) Wildcard indexes can also be expressed by adding the annotation directly to the field. Please note that wildcardProjection is not allowed on nested paths such as properties. Projections on types annotated with @WildcardIndexed are omitted during index creation. Example 10. Wildcard index on property @Document public class User { private @Id String id; @WildcardIndexed private UserMetadata userMetadata; } db.user.createIndex({ ""userMetadata.$**"" : 1 }, {}) Text Indexes: The text index feature is disabled by default for MongoDB v.2.4. Creating a text index allows accumulating several fields into a searchable full-text index. It is only possible to have one text index per collection, so all fields marked with @TextIndexed are combined into this index. Properties can be weighted to influence the document score for ranking results. The default language for the text index is English.To change the default language, set the language attribute to whichever language you want (for example, @Document(language=""spanish"") ). Using a property called language or @Language lets you define a language override on a per-document base. The following example shows how to created a text index and set the language to Spanish: Example 11. Example Text Index Usage @Document(language = ""spanish"") class SomeEntity { @TextIndexed String foo; @Language String lang; Nested nested; } class Nested { @TextIndexed(weight=5) String bar; String roo; }"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/value-expressions.html","Value Expressions Fundamentals: Value Expressions are a combination of Spring Expression Language (SpEL)(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) and Property Placeholder Resolution(https://docs.spring.io/spring-framework/reference/6.1/core/beans/environment.html#beans-placeholder-resolution-in-statements) . They combine powerful evaluation of programmatic expressions with the simplicity to resort to property-placeholder resolution to obtain values from the Environment such as configuration properties. Expressions are expected to be defined by a trusted input such as an annotation value and not to be determined from user input. The following code demonstrates how to use expressions in the context of annotations. Example 1. Annotation Usage @Document(""orders-#{tenantService.getOrderCollection()}-${tenant-config.suffix}"") class Order { // … } Value Expressions can be defined from a sole SpEL Expression, a Property Placeholder or a composite expression mixing various expressions including literals. Example 2. Expression Examples #{tenantService.getOrderCollection()} (1) #{(1+1) + '-hello-world'} (2) ${tenant-config.suffix} (3) orders-${tenant-config.suffix} (4) #{tenantService.getOrderCollection()}-${tenant-config.suffix} (5) 1 Value Expression using a single SpEL Expression. 2 Value Expression using a static SpEL Expression evaluating to 2-hello-world . 3 Value Expression using a single Property Placeholder. 4 Composite expression comprised of the literal orders- and the Property Placeholder ${tenant-config.suffix} . 5 Composite expression using SpEL, Property Placeholders and literals. Using value expressions introduces a lot of flexibility to your code. Doing so requires evaluation of the expression on each usage and, therefore, value expression evaluation has an impact on the performance profile. Parsing and Evaluation: Value Expressions are parsed by the ValueExpressionParser API. Instances of ValueExpression are thread-safe and can be cached for later use to avoid repeated parsing. The following example shows the Value Expression API usage: Parsing and Evaluation Java Kotlin ValueParserConfiguration configuration = SpelExpressionParser::new; ValueEvaluationContext context = ValueEvaluationContext.of(environment, evaluationContext); ValueExpressionParser parser = ValueExpressionParser.create(configuration); ValueExpression expression = parser.parse(""Hello, World""); Object result = expression.evaluate(context); val configuration = ValueParserConfiguration { SpelExpressionParser() } val context = ValueEvaluationContext.of(environment, evaluationContext) val parser = ValueExpressionParser.create(configuration) val expression: ValueExpression = parser.parse(""Hello, World"") val result: Any = expression.evaluate(context) SpEL Expressions: SpEL Expressions(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) follow the Template style where the expression is expected to be enclosed within the #{…} format. Expressions are evaluated using an EvaluationContext that is provided by EvaluationContextProvider . The context itself is a powerful StandardEvaluationContext allowing a wide range of operations, access to static types and context extensions. Make sure to parse and evaluate only expressions from trusted sources such as annotations. Accepting user-provided expressions can create an entry path to exploit the application context and your system resulting in a potential security vulnerability. Extending the Evaluation Context: EvaluationContextProvider and its reactive variant ReactiveEvaluationContextProvider provide access to an EvaluationContext . ExtensionAwareEvaluationContextProvider and its reactive variant ReactiveExtensionAwareEvaluationContextProvider are default implementations that determine context extensions from an application context, specifically ListableBeanFactory . Extensions implement either EvaluationContextExtension or ReactiveEvaluationContextExtension to provide extension support to hydrate EvaluationContext . That are a root object, properties and functions (top-level methods). The following example shows a context extension that provides a root object, properties, functions and an aliased function. Implementing a EvaluationContextExtension Java Kotlin @Component public class MyExtension implements EvaluationContextExtension { @Override public String getExtensionId() { return ""my-extension""; } @Override public Object getRootObject() { return new CustomExtensionRootObject(); } @Override public Map<String, Object> getProperties() { Map<String, Object> properties = new HashMap<>(); properties.put(""key"", ""Hello""); return properties; } @Override public Map<String, Function> getFunctions() { Map<String, Function> functions = new HashMap<>(); try { functions.put(""aliasedMethod"", new Function(getClass().getMethod(""extensionMethod""))); return functions; } catch (Exception o_O) { throw new RuntimeException(o_O); } } public static String extensionMethod() { return ""Hello World""; } public static int add(int i1, int i2) { return i1 + i2; } } public class CustomExtensionRootObject { public boolean rootObjectInstanceMethod() { return true; } } @Component class MyExtension : EvaluationContextExtension { override fun getExtensionId(): String { return ""my-extension"" } override fun getRootObject(): Any? { return CustomExtensionRootObject() } override fun getProperties(): Map<String, Any> { val properties: MutableMap<String, Any> = HashMap() properties[""key""] = ""Hello"" return properties } override fun getFunctions(): Map<String, Function> { val functions: MutableMap<String, Function> = HashMap() try { functions[""aliasedMethod""] = Function(javaClass.getMethod(""extensionMethod"")) return functions } catch (o_O: Exception) { throw RuntimeException(o_O) } } companion object { fun extensionMethod(): String { return ""Hello World"" } fun add(i1: Int, i2: Int): Int { return i1 + i2 } } } class CustomExtensionRootObject { fun rootObjectInstanceMethod(): Boolean { return true } } Once the above shown extension is registered, you can use its exported methods, properties and root object to evaluate SpEL expressions: Example 3. Expression Evaluation Examples #{add(1, 2)} (1) #{extensionMethod()} (2) #{aliasedMethod()} (3) #{key} (4) #{rootObjectInstanceMethod()} (5) 1 Invoke the method add declared by MyExtension resulting in 3 as the method adds both numeric parameters and returns the sum. 2 Invoke the method extensionMethod declared by MyExtension resulting in Hello World . 3 Invoke the method aliasedMethod . The method is exposed as function and redirects into the method extensionMethod declared by MyExtension resulting in Hello World . 4 Evaluate the key property resulting in Hello . 5 Invoke the method rootObjectInstanceMethod on the root object instance CustomExtensionRootObject . You can find real-life context extensions at SecurityEvaluationContextExtension(https://github.com/spring-projects/spring-security/blob/main/data/src/main/java/org/springframework/security/data/repository/query/SecurityEvaluationContextExtension.java) . Property Placeholders: Property placeholders following the form ${…} refer to properties provided typically by a PropertySource through Environment . Properties are useful to resolve against system properties, application configuration files, environment configuration or property sources contributed by secret management systems. You can find more details on the property placeholders in Spring Framework’s documentation on @Value usage(https://docs.spring.io/spring-framework/reference/6.1/core/beans/annotation-config/value-annotations.html#page-title) ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/lifecycle-events.html","Lifecycle Events: The MongoDB mapping framework includes several org.springframework.context.ApplicationEvent events that your application can respond to by registering special beans in the ApplicationContext . Being based on Spring’s ApplicationContext event infrastructure enables other products, such as Spring Integration, to easily receive these events, as they are a well known eventing mechanism in Spring-based applications. Entity lifecycle events can be costly and you may notice a change in the performance profile when loading large result sets. You can disable lifecycle events on the Template API(../api/java/org/springframework/data/mongodb/core/MongoTemplate.html#setEntityLifecycleEventsEnabled(boolean)) . To intercept an object before it goes through the conversion process (which turns your domain object into a org.bson.Document ), you can register a subclass of AbstractMongoEventListener that overrides the onBeforeConvert method. When the event is dispatched, your listener is called and passed the domain object before it goes into the converter. The following example shows how to do so: public class BeforeConvertListener extends AbstractMongoEventListener<Person> { @Override public void onBeforeConvert(BeforeConvertEvent<Person> event) { ... does some auditing manipulation, set timestamps, whatever ... } } To intercept an object before it goes into the database, you can register a subclass of AbstractMongoEventListener(../api/java/org/springframework/data/mongodb/core/mapping/event/AbstractMongoEventListener.html) that overrides the onBeforeSave method. When the event is dispatched, your listener is called and passed the domain object and the converted com.mongodb.Document . The following example shows how to do so: public class BeforeSaveListener extends AbstractMongoEventListener<Person> { @Override public void onBeforeSave(BeforeSaveEvent<Person> event) { … change values, delete them, whatever … } } Declaring these beans in your Spring ApplicationContext causes them to be invoked whenever the event is dispatched. Callbacks on AbstractMappingEventListener : onBeforeConvert : Called in MongoTemplate insert , insertList , and save operations before the object is converted to a Document by a MongoConverter . onBeforeSave : Called in MongoTemplate insert , insertList , and save operations before inserting or saving the Document in the database. onAfterSave : Called in MongoTemplate insert , insertList , and save operations after inserting or saving the Document in the database. onAfterLoad : Called in MongoTemplate find , findAndRemove , findOne , and getCollection methods after the Document has been retrieved from the database. onAfterConvert : Called in MongoTemplate find , findAndRemove , findOne , and getCollection methods after the Document has been retrieved from the database was converted to a POJO. Lifecycle events are only emitted for root level types. Complex types used as properties within a document root are not subject to event publication unless they are document references annotated with @DBRef . Lifecycle events depend on an ApplicationEventMulticaster , which in case of the SimpleApplicationEventMulticaster can be configured with a TaskExecutor , and therefore gives no guarantees when an Event is processed. Entity Callbacks: The Spring Data infrastructure provides hooks for modifying an entity before and after certain methods are invoked. Those so called EntityCallback instances provide a convenient way to check and potentially modify an entity in a callback fashioned style. An EntityCallback looks pretty much like a specialized ApplicationListener . Some Spring Data modules publish store specific events (such as BeforeSaveEvent ) that allow modifying the given entity. In some cases, such as when working with immutable types, these events can cause trouble. Also, event publishing relies on ApplicationEventMulticaster . If configuring that with an asynchronous TaskExecutor it can lead to unpredictable outcomes, as event processing can be forked onto a Thread. Entity callbacks provide integration points with both synchronous and reactive APIs to guarantee in-order execution at well-defined checkpoints within the processing chain, returning a potentially modified entity or an reactive wrapper type. Entity callbacks are typically separated by API type. This separation means that a synchronous API considers only synchronous entity callbacks and a reactive implementation considers only reactive entity callbacks. The Entity Callback API has been introduced with Spring Data Commons 2.2. It is the recommended way of applying entity modifications. Existing store specific ApplicationEvents are still published before the invoking potentially registered EntityCallback instances. Implementing Entity Callbacks: An EntityCallback is directly associated with its domain type through its generic type argument. Each Spring Data module typically ships with a set of predefined EntityCallback interfaces covering the entity lifecycle. Anatomy of an EntityCallback @FunctionalInterface public interface BeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked before a domain object is saved. * Can return either the same or a modified instance. * * @return the domain object to be persisted. */ (1) T onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called before an entity is saved. Returns a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Anatomy of a reactive EntityCallback @FunctionalInterface public interface ReactiveBeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked on subscription, before a domain object is saved. * The returned Publisher can emit either the same or a modified instance. * * @return Publisher emitting the domain object to be persisted. */ (1) Publisher<T> onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called on subscription, before an entity is saved. Emits a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Optional entity callback parameters are defined by the implementing Spring Data module and inferred from call site of EntityCallback.callback() . Implement the interface suiting your application needs like shown in the example below: Example BeforeSaveCallback class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { (1) if(collection == ""user"") { return // ... } return // ... } @Override public int getOrder() { return 100; (2) } } 1 Callback implementation according to your requirements. 2 Potentially order the entity callback if multiple ones for the same domain type exist. Ordering follows lowest precedence. Registering Entity Callbacks: EntityCallback beans are picked up by the store specific implementations in case they are registered in the ApplicationContext . Most template APIs already implement ApplicationContextAware and therefore have access to the ApplicationContext The following example explains a collection of valid entity callback registrations: Example EntityCallback Bean registration @Order(1) (1) @Component class First implements BeforeSaveCallback<Person> { @Override public Person onBeforeSave(Person person) { return // ... } } @Component class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { // ... } @Override public int getOrder() { return 100; (2) } } @Configuration public class EntityCallbackConfiguration { @Bean BeforeSaveCallback<Person> unorderedLambdaReceiverCallback() { (3) return (BeforeSaveCallback<Person>) it -> // ... } } @Component class UserCallbacks implements BeforeConvertCallback<User>, BeforeSaveCallback<User> { (4) @Override public Person onBeforeConvert(User user) { return // ... } @Override public Person onBeforeSave(User user) { return // ... } } 1 BeforeSaveCallback receiving its order from the @Order annotation. 2 BeforeSaveCallback receiving its order via the Ordered interface implementation. 3 BeforeSaveCallback using a lambda expression. Unordered by default and invoked last. Note that callbacks implemented by a lambda expression do not expose typing information hence invoking these with a non-assignable entity affects the callback throughput. Use a class or enum to enable type filtering for the callback bean. 4 Combine multiple entity callback interfaces in a single implementation class. Store specific EntityCallbacks: Spring Data MongoDB uses the EntityCallback API for its auditing support and reacts on the following callbacks. Table 1. Supported Entity Callbacks Callback Method Description Order ReactiveBeforeConvertCallback BeforeConvertCallback onBeforeConvert(T entity, String collection) Invoked before a domain object is converted to org.bson.Document . Ordered.LOWEST_PRECEDENCE ReactiveAfterConvertCallback AfterConvertCallback onAfterConvert(T entity, org.bson.Document target, String collection) Invoked after a domain object is loaded. Can modify the domain object after reading it from a org.bson.Document . Ordered.LOWEST_PRECEDENCE ReactiveAuditingEntityCallback AuditingEntityCallback onBeforeConvert(Object entity, String collection) Marks an auditable entity created or modified 100 ReactiveBeforeSaveCallback BeforeSaveCallback onBeforeSave(T entity, org.bson.Document target, String collection) Invoked before a domain object is saved. Can modify the target, to be persisted, Document containing all mapped entity information. Ordered.LOWEST_PRECEDENCE ReactiveAfterSaveCallback AfterSaveCallback onAfterSave(T entity, org.bson.Document target, String collection) Invoked before a domain object is saved. Can modify the domain object, to be returned after save, Document containing all mapped entity information. Ordered.LOWEST_PRECEDENCE"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/auditing.html","Auditing: Since Spring Data MongoDB 1.4, auditing can be enabled by annotating a configuration class with the @EnableMongoAuditing annotation, as the following example shows: Imperative Reactive XML @Configuration @EnableMongoAuditing class Config { @Bean public AuditorAware<AuditableUser> myAuditorProvider() { return new AuditorAwareImpl(); } } @Configuration @EnableReactiveMongoAuditing class Config { @Bean public ReactiveAuditorAware<AuditableUser> myAuditorProvider() { return new ReactiveAuditorAwareImpl(); } } <mongo:auditing mapping-context-ref=""customMappingContext"" auditor-aware-ref=""yourAuditorAwareImpl""/> If you expose a bean of type AuditorAware / ReactiveAuditorAware to the ApplicationContext , the auditing infrastructure picks it up automatically and uses it to determine the current user to be set on domain types. If you have multiple implementations registered in the ApplicationContext , you can select the one to be used by explicitly setting the auditorAwareRef attribute of @EnableMongoAuditing ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/client-session-transactions.html","Sessions & Transactions: As of version 3.6, MongoDB supports the concept of sessions. The use of sessions enables MongoDB’s Causal Consistency(https://docs.mongodb.com/manual/core/read-isolation-consistency-recency/#causal-consistency) model, which guarantees running operations in an order that respects their causal relationships. Those are split into ServerSession instances and ClientSession instances. In this section, when we speak of a session, we refer to ClientSession . Operations within a client session are not isolated from operations outside the session. Both MongoOperations and ReactiveMongoOperations provide gateway methods for tying a ClientSession to the operations. MongoCollection and MongoDatabase use session proxy objects that implement MongoDB’s collection and database interfaces, so you need not add a session on each call. This means that a potential call to MongoCollection#find() is delegated to MongoCollection#find(ClientSession) . Methods such as (Reactive)MongoOperations#getCollection return native MongoDB Java Driver gateway objects (such as MongoCollection ) that themselves offer dedicated methods for ClientSession . These methods are NOT session-proxied. You should provide the ClientSession where needed when interacting directly with a MongoCollection or MongoDatabase and not through one of the #execute callbacks on MongoOperations . ClientSession support: The following example shows the usage of a session: Imperative Reactive ClientSessionOptions sessionOptions = ClientSessionOptions.builder() .causallyConsistent(true) .build(); ClientSession session = client.startSession(sessionOptions); (1) template.withSession(() -> session) .execute(action -> { Query query = query(where(""name"").is(""Durzo Blint"")); Person durzo = action.findOne(query, Person.class); (2) Person azoth = new Person(""Kylar Stern""); azoth.setMaster(durzo); action.insert(azoth); (3) return azoth; }); session.close() (4) 1 Obtain a new session from the server. 2 Use MongoOperation methods as before. The ClientSession gets applied automatically. 3 Make sure to close the ClientSession . 4 Close the session. When dealing with DBRef instances, especially lazily loaded ones, it is essential to not close the ClientSession before all data is loaded. Otherwise, lazy fetch fails. ClientSessionOptions sessionOptions = ClientSessionOptions.builder() .causallyConsistent(true) .build(); Publisher<ClientSession> session = client.startSession(sessionOptions); (1) template.withSession(session) .execute(action -> { Query query = query(where(""name"").is(""Durzo Blint"")); return action.findOne(query, Person.class) .flatMap(durzo -> { Person azoth = new Person(""Kylar Stern""); azoth.setMaster(durzo); return action.insert(azoth); (2) }); }, ClientSession::close) (3) .subscribe(); (4) 1 Obtain a Publisher for new session retrieval. 2 Use ReactiveMongoOperation methods as before. The ClientSession is obtained and applied automatically. 3 Make sure to close the ClientSession . 4 Nothing happens until you subscribe. See the Project Reactor Reference Guide(https://projectreactor.io/docs/core/release/reference/#reactive.subscribe) for details. By using a Publisher that provides the actual session, you can defer session acquisition to the point of actual subscription. Still, you need to close the session when done, so as to not pollute the server with stale sessions. Use the doFinally hook on execute to call ClientSession#close() when you no longer need the session. If you prefer having more control over the session itself, you can obtain the ClientSession through the driver and provide it through a Supplier . Reactive use of ClientSession is limited to Template API usage. There’s currently no session integration with reactive repositories. MongoDB Transactions: As of version 4, MongoDB supports Transactions(https://www.mongodb.com/transactions) . Transactions are built on top of Sessions(#) and, consequently, require an active ClientSession . Unless you specify a MongoTransactionManager within your application context, transaction support is DISABLED . You can use setSessionSynchronization(ALWAYS) to participate in ongoing non-native MongoDB transactions. To get full programmatic control over transactions, you may want to use the session callback on MongoOperations . The following example shows programmatic transaction control: Programmatic transactions Imperative Reactive ClientSession session = client.startSession(options); (1) template.withSession(session) .execute(action -> { session.startTransaction(); (2) try { Step step = // ...; action.insert(step); process(step); action.update(Step.class).apply(Update.set(""state"", // ... session.commitTransaction(); (3) } catch (RuntimeException e) { session.abortTransaction(); (4) } }, ClientSession::close) (5) 1 Obtain a new ClientSession . 2 Start the transaction. 3 If everything works out as expected, commit the changes. 4 Something broke, so roll back everything. 5 Do not forget to close the session when done. The preceding example lets you have full control over transactional behavior while using the session scoped MongoOperations instance within the callback to ensure the session is passed on to every server call. To avoid some of the overhead that comes with this approach, you can use a TransactionTemplate to take away some of the noise of manual transaction flow. Mono<DeleteResult> result = Mono .from(client.startSession()) (1) .flatMap(session -> { session.startTransaction(); (2) return Mono.from(collection.deleteMany(session, ...)) (3) .onErrorResume(e -> Mono.from(session.abortTransaction()).then(Mono.error(e))) (4) .flatMap(val -> Mono.from(session.commitTransaction()).then(Mono.just(val))) (5) .doFinally(signal -> session.close()); (6) }); 1 First we obviously need to initiate the session. 2 Once we have the ClientSession at hand, start the transaction. 3 Operate within the transaction by passing on the ClientSession to the operation. 4 If the operations completes exceptionally, we need to stop the transaction and preserve the error. 5 Or of course, commit the changes in case of success. Still preserving the operations result. 6 Lastly, we need to make sure to close the session. The culprit of the above operation is in keeping the main flows DeleteResult instead of the transaction outcome published via either commitTransaction() or abortTransaction() , which leads to a rather complicated setup. Unless you specify a ReactiveMongoTransactionManager within your application context, transaction support is DISABLED . You can use setSessionSynchronization(ALWAYS) to participate in ongoing non-native MongoDB transactions. Transactions with TransactionTemplate / TransactionalOperator: Spring Data MongoDB transactions support both TransactionTemplate and TransactionalOperator . Transactions with TransactionTemplate / TransactionalOperator Imperative Reactive template.setSessionSynchronization(ALWAYS); (1) // ... TransactionTemplate txTemplate = new TransactionTemplate(anyTxManager); (2) txTemplate.execute(new TransactionCallbackWithoutResult() { @Override protected void doInTransactionWithoutResult(TransactionStatus status) { (3) Step step = // ...; template.insert(step); process(step); template.update(Step.class).apply(Update.set(""state"", // ... } }); 1 Enable transaction synchronization during Template API configuration. 2 Create the TransactionTemplate using the provided PlatformTransactionManager . 3 Within the callback the ClientSession and transaction are already registered. Changing state of MongoTemplate during runtime (as you might think would be possible in item 1 of the preceding listing) can cause threading and visibility issues. template.setSessionSynchronization(ALWAYS); (1) // ... TransactionalOperator rxtx = TransactionalOperator.create(anyTxManager, new DefaultTransactionDefinition()); (2) Step step = // ...; template.insert(step); Mono<Void> process(step) .then(template.update(Step.class).apply(Update.set(""state"", …)) .as(rxtx::transactional) (3) .then(); 1 Enable transaction synchronization for Transactional participation. 2 Create the TransactionalOperator using the provided ReactiveTransactionManager . 3 TransactionalOperator.transactional(…) provides transaction management for all upstream operations. Transactions with MongoTransactionManager & ReactiveMongoTransactionManager: MongoTransactionManager / ReactiveMongoTransactionManager is the gateway to the well known Spring transaction support. It lets applications use the managed transaction features of Spring(https://docs.spring.io/spring-framework/reference/6.1/data-access.html#transaction) . The MongoTransactionManager binds a ClientSession to the thread whereas the ReactiveMongoTransactionManager is using the ReactorContext for this. MongoTemplate detects the session and operates on these resources which are associated with the transaction accordingly. MongoTemplate can also participate in other, ongoing transactions. The following example shows how to create and use transactions with a MongoTransactionManager : Transactions with MongoTransactionManager / ReactiveMongoTransactionManager Imperative Reactive @Configuration static class Config extends AbstractMongoClientConfiguration { @Bean MongoTransactionManager transactionManager(MongoDatabaseFactory dbFactory) { (1) return new MongoTransactionManager(dbFactory); } // ... } @Component public class StateService { @Transactional void someBusinessFunction(Step step) { (2) template.insert(step); process(step); template.update(Step.class).apply(Update.set(""state"", // ... }; }); 1 Register MongoTransactionManager in the application context. 2 Mark methods as transactional. @Transactional(readOnly = true) advises MongoTransactionManager to also start a transaction that adds the ClientSession to outgoing requests. @Configuration public class Config extends AbstractReactiveMongoConfiguration { @Bean ReactiveMongoTransactionManager transactionManager(ReactiveMongoDatabaseFactory factory) { (1) return new ReactiveMongoTransactionManager(factory); } // ... } @Service public class StateService { @Transactional Mono<UpdateResult> someBusinessFunction(Step step) { (2) return template.insert(step) .then(process(step)) .then(template.update(Step.class).apply(Update.set(""state"", …)); }; }); 1 Register ReactiveMongoTransactionManager in the application context. 2 Mark methods as transactional. @Transactional(readOnly = true) advises ReactiveMongoTransactionManager to also start a transaction that adds the ClientSession to outgoing requests. Controlling MongoDB-specific Transaction Options: Transactional service methods can require specific transaction options to run a transaction. Spring Data MongoDB’s transaction managers support evaluation of transaction labels such as @Transactional(label = { ""mongo:readConcern=available"" }) . By default, the label namespace using the mongo: prefix is evaluated by MongoTransactionOptionsResolver that is configured by default. Transaction labels are provided by TransactionAttribute and available to programmatic transaction control through TransactionTemplate and TransactionalOperator . Due to their declarative nature, @Transactional(label = …) provides a good starting point that also can serve as documentation. Currently, the following options are supported: Max Commit Time Controls the maximum execution time on the server for the commitTransaction operation. The format of the value corresponds with ISO-8601 duration format as used with Duration.parse(…) . Usage: mongo:maxCommitTime=PT1S Read Concern Sets the read concern for the transaction. Usage: mongo:readConcern=LOCAL|MAJORITY|LINEARIZABLE|SNAPSHOT|AVAILABLE Read Preference Sets the read preference for the transaction. Usage: mongo:readPreference=PRIMARY|SECONDARY|SECONDARY_PREFERRED|PRIMARY_PREFERRED|NEAREST Write Concern Sets the write concern for the transaction. Usage: mongo:writeConcern=ACKNOWLEDGED|W1|W2|W3|UNACKNOWLEDGED|JOURNALED|MAJORITY Nested transactions that join the outer transaction do not affect the initial transaction options as the transaction is already started. Transaction options are only applied when a new transaction is started. Special behavior inside transactions: Inside transactions, MongoDB server has a slightly different behavior. Connection Settings The MongoDB drivers offer a dedicated replica set name configuration option turing the driver into auto-detection mode. This option helps identify the primary replica set nodes and command routing during a transaction. Make sure to add replicaSet to the MongoDB URI. Please refer to connection string options(https://docs.mongodb.com/manual/reference/connection-string/#connections-connection-options) for further details. Collection Operations MongoDB does not support collection operations, such as collection creation, within a transaction. This also affects the on the fly collection creation that happens on first usage. Therefore make sure to have all required structures in place. Transient Errors MongoDB can add special labels to errors raised during transactional operations. Those may indicate transient failures that might vanish by merely retrying the operation. We highly recommend Spring Retry(https://github.com/spring-projects/spring-retry) for those purposes. Nevertheless one may override MongoTransactionManager#doCommit(MongoTransactionObject) to implement a Retry Commit Operation(https://docs.mongodb.com/manual/core/transactions/#retry-commit-operation) behavior as outlined in the MongoDB reference manual. Count MongoDB count operates upon collection statistics which may not reflect the actual situation within a transaction. The server responds with error 50851 when issuing a count command inside of a multi-document transaction. Once MongoTemplate detects an active transaction, all exposed count() methods are converted and delegated to the aggregation framework using $match and $count operators, preserving Query settings, such as collation . Restrictions apply when using geo commands inside of the aggregation count helper. The following operators cannot be used and must be replaced with a different operator: $where → $expr $near → $geoWithin with $center $nearSphere → $geoWithin with $centerSphere Queries using Criteria.near(…) and Criteria.nearSphere(…) must be rewritten to Criteria.within(…) respective Criteria.withinSphere(…) . Same applies for the near query keyword in repository query methods that must be changed to within . See also MongoDB JIRA ticket DRIVERS-518(https://jira.mongodb.org/browse/DRIVERS-518) for further reference. The following snippet shows count usage inside the session-bound closure: session.startTransaction(); template.withSession(session) .execute(action -> { action.count(query(where(""state"").is(""active"")), Step.class) ... The snippet above materializes in the following command: db.collection.aggregate( [ { $match: { state: ""active"" } }, { $count: ""totalEntityCount"" } ] ) instead of: db.collection.find( { state: ""active"" } ).count()"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/change-streams.html","Change Streams: As of MongoDB 3.6, Change Streams(https://docs.mongodb.com/manual/changeStreams/) let applications get notified about changes without having to tail the oplog. Change Stream support is only possible for replica sets or for a sharded cluster. Change Streams can be consumed with both, the imperative and the reactive MongoDB Java driver. It is highly recommended to use the reactive variant, as it is less resource-intensive. However, if you cannot use the reactive API, you can still obtain change events by using the messaging concept that is already prevalent in the Spring ecosystem. It is possible to watch both on a collection as well as database level, whereas the database level variant publishes changes from all collections within the database. When subscribing to a database change stream, make sure to use a suitable type for the event type as conversion might not apply correctly across different entity types. In doubt, use Document . Change Streams with MessageListener: Listening to a Change Stream by using a Sync Driver(https://docs.mongodb.com/manual/tutorial/change-streams-example/) creates a long running, blocking task that needs to be delegated to a separate component. In this case, we need to first create a MessageListenerContainer(../api/java/org/springframework/data/mongodb/core/messaging/MessageListenerContainer.html) which will be the main entry point for running the specific SubscriptionRequest tasks. Spring Data MongoDB already ships with a default implementation that operates on MongoTemplate and is capable of creating and running Task instances for a ChangeStreamRequest(../api/java/org/springframework/data/mongodb/core/messaging/ChangeStreamRequest.html) . The following example shows how to use Change Streams with MessageListener instances: Example 1. Change Streams with MessageListener instances MessageListenerContainer container = new DefaultMessageListenerContainer(template); container.start(); (1) MessageListener<ChangeStreamDocument<Document>, User> listener = System.out::println; (2) ChangeStreamRequestOptions options = new ChangeStreamRequestOptions(""db"", ""user"", ChangeStreamOptions.empty()); (3) Subscription subscription = container.register(new ChangeStreamRequest<>(listener, options), User.class); (4) // ... container.stop(); (5) 1 Starting the container initializes the resources and starts Task instances for already registered SubscriptionRequest instances. Requests added after startup are ran immediately. 2 Define the listener called when a Message is received. The Message#getBody() is converted to the requested domain type. Use Document to receive raw results without conversion. 3 Set the collection to listen to and provide additional options through ChangeStreamOptions . 4 Register the request. The returned Subscription can be used to check the current Task state and cancel it to free resources. 5 Do not forget to stop the container once you are sure you no longer need it. Doing so stops all running Task instances within the container. Errors while processing are passed on to an org.springframework.util.ErrorHandler . If not stated otherwise a log appending ErrorHandler gets applied by default. Please use register(request, body, errorHandler) to provide additional functionality. Reactive Change Streams: Subscribing to Change Streams with the reactive API is a more natural approach to work with streams. Still, the essential building blocks, such as ChangeStreamOptions , remain the same. The following example shows how to use Change Streams emitting ChangeStreamEvent s: Example 2. Change Streams emitting ChangeStreamEvent Flux<ChangeStreamEvent<User>> flux = reactiveTemplate.changeStream(User.class) (1) .watchCollection(""people"") .filter(where(""age"").gte(38)) (2) .listen(); (3) 1 The event target type the underlying document should be converted to. Leave this out to receive raw results without conversion. 2 Use an aggregation pipeline or just a query Criteria to filter events. 3 Obtain a Flux of change stream events. The ChangeStreamEvent#getBody() is converted to the requested domain type from (2). Resuming Change Streams: Change Streams can be resumed and resume emitting events where you left. To resume the stream, you need to supply either a resume token or the last known server time (in UTC). Use ChangeStreamOptions(../api/java/org/springframework/data/mongodb/core/ChangeStreamOptions.html) to set the value accordingly. The following example shows how to set the resume offset using server time: Example 3. Resume a Change Stream Flux<ChangeStreamEvent<User>> resumed = template.changeStream(User.class) .watchCollection(""people"") .resumeAt(Instant.now().minusSeconds(1)) (1) .listen(); 1 You may obtain the server time of an ChangeStreamEvent through the getTimestamp method or use the resumeToken exposed through getResumeToken . In some cases an Instant might not be a precise enough measure when resuming a Change Stream. Use a MongoDB native BsonTimestamp(https://docs.mongodb.com/manual/reference/bson-types/#timestamps) for that purpose."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/tailable-cursors.html","Tailable Cursors: By default, MongoDB automatically closes a cursor when the client exhausts all results supplied by the cursor. Closing a cursor on exhaustion turns a stream into a finite stream. For capped collections(https://docs.mongodb.com/manual/core/capped-collections/) , you can use a Tailable Cursor(https://docs.mongodb.com/manual/core/tailable-cursors/) that remains open after the client consumed all initially returned data. Capped collections can be created with MongoOperations.createCollection . To do so, provide the required CollectionOptions.empty().capped()…​ . Tailable cursors can be consumed with both, the imperative and the reactive MongoDB API. It is highly recommended to use the reactive variant, as it is less resource-intensive. However, if you cannot use the reactive API, you can still use a messaging concept that is already prevalent in the Spring ecosystem. Tailable Cursors with MessageListener: Listening to a capped collection using a Sync Driver creates a long running, blocking task that needs to be delegated to a separate component. In this case, we need to first create a MessageListenerContainer , which will be the main entry point for running the specific SubscriptionRequest . Spring Data MongoDB already ships with a default implementation that operates on MongoTemplate and is capable of creating and running Task instances for a TailableCursorRequest . The following example shows how to use tailable cursors with MessageListener instances: Example 1. Tailable Cursors with MessageListener instances MessageListenerContainer container = new DefaultMessageListenerContainer(template); container.start(); (1) MessageListener<Document, User> listener = System.out::println; (2) TailableCursorRequest request = TailableCursorRequest.builder() .collection(""orders"") (3) .filter(query(where(""value"").lt(100))) (4) .publishTo(listener) (5) .build(); container.register(request, User.class); (6) // ... container.stop(); (7) 1 Starting the container intializes the resources and starts Task instances for already registered SubscriptionRequest instances. Requests added after startup are ran immediately. 2 Define the listener called when a Message is received. The Message#getBody() is converted to the requested domain type. Use Document to receive raw results without conversion. 3 Set the collection to listen to. 4 Provide an optional filter for documents to receive. 5 Set the message listener to publish incoming Message s to. 6 Register the request. The returned Subscription can be used to check the current Task state and cancel it to free resources. 7 Do not forget to stop the container once you are sure you no longer need it. Doing so stops all running Task instances within the container. Reactive Tailable Cursors: Using tailable cursors with a reactive data types allows construction of infinite streams. A tailable cursor remains open until it is closed externally. It emits data as new documents arrive in a capped collection. Tailable cursors may become dead, or invalid, if either the query returns no match or the cursor returns the document at the “end” of the collection and the application then deletes that document. The following example shows how to create and use an infinite stream query: Example 2. Infinite Stream queries with ReactiveMongoOperations Flux<Person> stream = template.tail(query(where(""name"").is(""Joe"")), Person.class); Disposable subscription = stream.doOnNext(person -> System.out.println(person)).subscribe(); // … // Later: Dispose the subscription to close the stream subscription.dispose(); Spring Data MongoDB Reactive repositories support infinite streams by annotating a query method with @Tailable . This works for methods that return Flux and other reactive types capable of emitting multiple elements, as the following example shows: Example 3. Infinite Stream queries with ReactiveMongoRepository public interface PersonRepository extends ReactiveMongoRepository<Person, String> { @Tailable Flux<Person> findByFirstname(String firstname); } Flux<Person> stream = repository.findByFirstname(""Joe""); Disposable subscription = stream.doOnNext(System.out::println).subscribe(); // … // Later: Dispose the subscription to close the stream subscription.dispose();"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/sharding.html","Sharding: MongoDB supports large data sets via sharding, a method for distributing data across multiple database servers. Please refer to the MongoDB Documentation(https://docs.mongodb.com/manual/sharding/) to learn how to set up a sharded cluster, its requirements and limitations. Spring Data MongoDB uses the @Sharded annotation to identify entities stored in sharded collections as shown below. @Document(""users"") @Sharded(shardKey = { ""country"", ""userId"" }) (1) public class User { @Id Long id; @Field(""userid"") String userId; String country; } 1 The properties of the shard key get mapped to the actual field names. Sharded Collections: Spring Data MongoDB does not auto set up sharding for collections nor indexes required for it. The snippet below shows how to do so using the MongoDB client API. MongoDatabase adminDB = template.getMongoDbFactory() .getMongoDatabase(""admin""); (1) adminDB.runCommand(new Document(""enableSharding"", ""db"")); (2) Document shardCmd = new Document(""shardCollection"", ""db.users"") (3) .append(""key"", new Document(""country"", 1).append(""userid"", 1)); (4) adminDB.runCommand(shardCmd); 1 Sharding commands need to be run against the admin database. 2 Enable sharding for a specific database if necessary. 3 Shard a collection within the database having sharding enabled. 4 Specify the shard key. This example uses range based sharding. Shard Key Handling: The shard key consists of a single or multiple properties that must exist in every document in the target collection. It is used to distribute documents across shards. Adding the @Sharded annotation to an entity enables Spring Data MongoDB to apply best effort optimisations required for sharded scenarios. This means essentially adding required shard key information, if not already present, to replaceOne filter queries when upserting entities. This may require an additional server round trip to determine the actual value of the current shard key. By setting @Sharded(immutableKey = true) Spring Data does not attempt to check if an entity shard key was changed. Please see the MongoDB Documentation(https://docs.mongodb.com/manual/reference/method/db.collection.replaceOne/#upsert) for further details. The following list contains which operations are eligible for shard key auto-inclusion: (Reactive)CrudRepository.save(…) (Reactive)CrudRepository.saveAll(…) (Reactive)MongoTemplate.save(…)"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/mongo-encryption.html","Encryption (CSFLE): Client Side Encryption is a feature that encrypts data in your application before it is sent to MongoDB. We recommend you get familiar with the concepts, ideally from the MongoDB Documentation(https://www.mongodb.com/docs/manual/core/csfle/) to learn more about its capabilities and restrictions before you continue applying Encryption through Spring Data. Make sure to set the drivers com.mongodb.AutoEncryptionSettings to use client-side encryption. MongoDB does not support encryption for all field types. Specific data types require deterministic encryption to preserve equality comparison functionality. Automatic Encryption: MongoDB supports Client-Side Field Level Encryption(https://www.mongodb.com/docs/manual/core/csfle/) out of the box using the MongoDB driver with its Automatic Encryption feature. Automatic Encryption requires a JSON Schema(mapping/mapping-schema.html) that allows to perform encrypted read and write operations without the need to provide an explicit en-/decryption step. Please refer to the JSON Schema(mapping/mapping-schema.html#mongo.jsonSchema.encrypted-fields) section for more information on defining a JSON Schema that holds encryption information. To make use of a the MongoJsonSchema it needs to be combined with AutoEncryptionSettings which can be done eg. via a MongoClientSettingsBuilderCustomizer . @Bean MongoClientSettingsBuilderCustomizer customizer(MappingContext mappingContext) { return (builder) -> { // ... keyVaultCollection, kmsProvider, ... MongoJsonSchemaCreator schemaCreator = MongoJsonSchemaCreator.create(mappingContext); MongoJsonSchema patientSchema = schemaCreator .filter(MongoJsonSchemaCreator.encryptedOnly()) .createSchemaFor(Patient.class); AutoEncryptionSettings autoEncryptionSettings = AutoEncryptionSettings.builder() .keyVaultNamespace(keyVaultCollection) .kmsProviders(kmsProviders) .extraOptions(extraOpts) .schemaMap(Collections.singletonMap(""db.patient"", patientSchema.schemaDocument().toBsonDocument())) .build(); builder.autoEncryptionSettings(autoEncryptionSettings); }; } Explicit Encryption: Explicit encryption uses the MongoDB driver’s encryption library ( org.mongodb:mongodb-crypt ) to perform encryption and decryption tasks. The @ExplicitEncrypted annotation is a combination of the @Encrypted annotation used for JSON Schema creation(mapping/mapping-schema.html#mongo.jsonSchema.encrypted-fields) and a Property Converter(mapping/property-converters.html) . In other words, @ExplicitEncrypted uses existing building blocks to combine them for simplified explicit encryption support. Fields annotated with @ExplicitEncrypted are always encrypted as whole. Consider the following example: @ExplicitEncrypted(…) String simpleValue; (1) @ExplicitEncrypted(…) Address address; (2) @ExplicitEncrypted(…) List<...> list; (3) @ExplicitEncrypted(…) Map<..., ...> mapOfString; (4) 1 Encrypts the value of the simple type such as a String if not null . 2 Encrypts the entire Address object and all its nested fields as Document . To only encrypt parts of the Address , like Address#street the street field within Address needs to be annotated with @ExplicitEncrypted . 3 Collection -like fields are encrypted as single value and not per entry. 4 Map -like fields are encrypted as single value and not as a key/value entry. Client-Side Field Level Encryption allows you to choose between a deterministic and a randomized algorithm. Depending on the chosen algorithm(https://www.mongodb.com/docs/v5.0/reference/security-client-side-automatic-json-schema/#std-label-field-level-encryption-json-schema/) , different operations(https://www.mongodb.com/docs/manual/core/csfle/reference/supported-operations/) may be supported. To pick a certain algorithm use @ExplicitEncrypted(algorithm) , see EncryptionAlgorithms for algorithm constants. Please read the Encryption Types(https://www.mongodb.com/docs/manual/core/csfle/fundamentals/encryption-algorithms) manual for more information on algorithms and their usage. To perform the actual encryption we require a Data Encryption Key (DEK). Please refer to the MongoDB Documentation(https://www.mongodb.com/docs/manual/core/csfle/quick-start/#create-a-data-encryption-key) for more information on how to set up key management and create a Data Encryption Key. The DEK can be referenced directly via its id or a defined alternative name . The @EncryptedField annotation only allows referencing a DEK via an alternative name. It is possible to provide an EncryptionKeyResolver , which will be discussed later, to any DEK. Example 1. Reference the Data Encryption Key @EncryptedField(algorithm=…, altKeyName = ""secret-key"") (1) String ssn; @EncryptedField(algorithm=…, altKeyName = ""/name"") (2) String ssn; 1 Use the DEK stored with the alternative name secret-key . 2 Uses a field reference that will read the actual field value and use that for key lookup. Always requires the full document to be present for save operations. Fields cannot be used in queries/aggregations. By default, the @ExplicitEncrypted(value=…) attribute references a MongoEncryptionConverter . It is possible to change the default implementation and exchange it with any PropertyValueConverter implementation by providing the according type reference. To learn more about custom PropertyValueConverters and the required configuration, please refer to the Property Converters - Mapping specific fields(mapping/property-converters.html) section. MongoEncryptionConverter Setup: The converter setup for MongoEncryptionConverter requires a few steps as several components are involved. The bean setup consists of the following: The ClientEncryption engine A MongoEncryptionConverter instance configured with ClientEncryption and a EncryptionKeyResolver . A PropertyValueConverterFactory that uses the registered MongoEncryptionConverter bean. A side effect of using annotated key resolution is that the @ExplicitEncrypted annotation does not need to specify an alt key name. The EncryptionKeyResolver uses an EncryptionContext providing access to the property allowing for dynamic DEK resolution. Example 2. Sample MongoEncryptionConverter Configuration class Config extends AbstractMongoClientConfiguration { @Autowired ApplicationContext appContext; @Bean ClientEncryption clientEncryption() { (1) ClientEncryptionSettings encryptionSettings = ClientEncryptionSettings.builder(); // … return ClientEncryptions.create(encryptionSettings); } @Bean MongoEncryptionConverter encryptingConverter(ClientEncryption clientEncryption) { Encryption<BsonValue, BsonBinary> encryption = MongoClientEncryption.just(clientEncryption); EncryptionKeyResolver keyResolver = EncryptionKeyResolver.annotated((ctx) -> …); (2) return new MongoEncryptionConverter(encryption, keyResolver); (3) } @Override protected void configureConverters(MongoConverterConfigurationAdapter adapter) { adapter .registerPropertyValueConverterFactory(PropertyValueConverterFactory.beanFactoryAware(appContext)); (4) } } 1 Set up a Encryption engine using com.mongodb.client.vault.ClientEncryption . The instance is stateful and must be closed after usage. Spring takes care of this because ClientEncryption is Closeable . 2 Set up an annotation-based EncryptionKeyResolver to determine the EncryptionKey from annotations. 3 Create the MongoEncryptionConverter . 4 Enable for a PropertyValueConverter lookup from the BeanFactory ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories.html","Repositories: This chapter explains the basic foundations of Spring Data repositories and MongoDB specifics. Before continuing to the MongoDB specifics, make sure you have a sound understanding of the basic concepts. The goal of the Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); } Entity State Detection Strategies: The following table describes the strategies that Spring Data offers for detecting whether an entity is new: Table 1. Options for detection whether an entity is new in Spring Data @Id -Property inspection (the default) By default, Spring Data inspects the identifier property of the given entity. If the identifier property is null or 0 in case of primitive types, then the entity is assumed to be new. Otherwise, it is assumed to not be new. @Version -Property inspection If a property annotated with @Version is present and null , or in case of a version property of primitive type 0 the entity is considered new. If the version property is present but has a different value, the entity is considered to not be new. If no version property is present Spring Data falls back to inspection of the identifier property. Implementing Persistable If an entity implements Persistable , Spring Data delegates the new detection to the isNew(…) method of the entity. See the Javadoc(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//index.html?org/springframework/data/domain/Persistable.html) for details. Note: Properties of Persistable will get detected and persisted if you use AccessType.PROPERTY . To avoid that, use @Transient . Providing a custom EntityInformation implementation You can customize the EntityInformation abstraction used in the repository base implementation by creating a subclass of the module specific repository factory and overriding the getEntityInformation(…) method. You then have to register the custom implementation of module specific repository factory as a Spring bean. Note that this should rarely be necessary. Cassandra provides no means to generate identifiers upon inserting data. As consequence, entities must be associated with identifier values. Spring Data defaults to identifier inspection to determine whether an entity is new. If you want to use auditing(../mongodb/auditing.html) make sure to either use Optimistic Locking(../mongodb/template-crud-operations.html#mongo-template.optimistic-locking) or implement Persistable for proper entity state detection."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/repositories/repositories.html","MongoDB Repositories: This chapter points out the specialties for repository support for MongoDB. This chapter builds on the core repository support explained in core concepts(../../repositories/core-concepts.html) . You should have a sound understanding of the basic concepts explained there. Usage: To access domain entities stored in a MongoDB, you can use our sophisticated repository support that eases implementation quite significantly. To do so, create an interface for your repository, as the following example shows: Example 1. Sample Person entity public class Person { @Id private String id; private String firstname; private String lastname; private Address address; // … getters and setters omitted } Note that the domain type shown in the preceding example has a property named id of type String .The default serialization mechanism used in MongoTemplate (which backs the repository support) regards properties named id as the document ID. Currently, we support String , ObjectId , and BigInteger as ID types. Please see ID mapping(../template-crud-operations.html#mongo-template.id-handling) for more information about on how the id field is handled in the mapping layer. Now that we have a domain object, we can define an interface that uses it, as follows: Basic repository interface to persist Person entities Imperative Reactive public interface PersonRepository extends PagingAndSortingRepository<Person, String> { // additional custom query methods go here } public interface PersonRepository extends ReactiveSortingRepository<Person, String> { // additional custom query methods go here } To start using the repository, use the @EnableMongoRepositories annotation. That annotation carries the same attributes as the namespace element. If no base package is configured, the infrastructure scans the package of the annotated configuration class. The following example shows how to configuration your application to use MongoDB repositories: Imperative Reactive XML @Configuration @EnableMongoRepositories(""com.acme. .repositories"") class ApplicationConfig extends AbstractMongoClientConfiguration { @Override protected String getDatabaseName() { return ""e-store""; } @Override protected String getMappingBasePackage() { return ""com.acme. .repositories""; } } @Configuration @EnableReactiveMongoRepositories(""com.acme. .repositories"") class ApplicationConfig extends AbstractReactiveMongoConfiguration { @Override protected String getDatabaseName() { return ""e-store""; } @Override protected String getMappingBasePackage() { return ""com.acme. .repositories""; } } MongoDB uses two different drivers for imperative (synchronous/blocking) and reactive (non-blocking) data access. You must create a connection by using the Reactive Streams driver to provide the required infrastructure for Spring Data’s Reactive MongoDB support. Consequently, you must provide a separate configuration for MongoDB’s Reactive Streams driver. Note that your application operates on two different connections if you use reactive and blocking Spring Data MongoDB templates and repositories. <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:mongo=""http://www.springframework.org/schema/data/mongo"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/data/mongo https://www.springframework.org/schema/data/mongo/spring-mongo-1.0.xsd""> <mongo:mongo-client id=""mongoClient"" /> <bean id=""mongoTemplate"" class=""org.springframework.data.mongodb.core.MongoTemplate""> <constructor-arg ref=""mongoClient"" /> <constructor-arg value=""databaseName"" /> </bean> <mongo:repositories base-package=""com.acme.*.repositories"" /> </beans> This namespace element causes the base packages to be scanned for interfaces that extend MongoRepository and create Spring beans for each one found. By default, the repositories get a MongoTemplate Spring bean wired that is called mongoTemplate , so you only need to configure mongo-template-ref explicitly if you deviate from this convention. Because our domain repository extends PagingAndSortingRepository , it provides you with methods for paginated and sorted access to the entities. In the case of reactive repositories only ReactiveSortingRepository is available since the notion of a Page is not applicable. However finder methods still accept a Sort and Limit parameter. The reactive space offers various reactive composition libraries. The most common libraries are RxJava(https://github.com/ReactiveX/RxJava) and Project Reactor(https://projectreactor.io/) . Spring Data MongoDB is built on top of the MongoDB Reactive Streams(https://mongodb.github.io/mongo-java-driver-reactivestreams/) driver, to provide maximal interoperability by relying on the Reactive Streams(https://www.reactive-streams.org/) initiative. Static APIs, such as ReactiveMongoOperations , are provided by using Project Reactor’s Flux and Mono types. Project Reactor offers various adapters to convert reactive wrapper types ( Flux to Observable and vice versa), but conversion can easily clutter your code. Spring Data’s Reactive Repository abstraction is a dynamic API, mostly defined by you and your requirements as you declare query methods. Reactive MongoDB repositories can be implemented by using either RxJava or Project Reactor wrapper types by extending from one of the following library-specific repository interfaces: ReactiveCrudRepository ReactiveSortingRepository RxJava3CrudRepository RxJava3SortingRepository Spring Data converts reactive wrapper types behind the scenes so that you can stick to your favorite composition library. In case you want to obtain methods for basic CRUD operations also add the CrudRepository interface. Working with the repository instance is just a matter of dependency injecting it into a client . Consequently, accessing the second page of Person objects at a page size of 10 would resemble the following code: Paging access to Person entities Imperative Reactive @ExtendWith(SpringExtension.class) @ContextConfiguration class PersonRepositoryTests { @Autowired PersonRepository repository; @Test void readsFirstPageCorrectly() { Page<Person> persons = repository.findAll(PageRequest.of(0, 10)); assertThat(persons.isFirstPage()).isTrue(); } } @ExtendWith(SpringExtension.class) @ContextConfiguration class PersonRepositoryTests { @Autowired PersonRepository repository; @Test void readsFirstPageCorrectly() { Flux<Person> persons = repository.findAll(Sort.unsorted(), Limit.of(10)); persons.as(StepVerifer::create) .expectNextCount(10) .verifyComplete(); } } The preceding example creates an application context with Spring’s unit test support, which performs annotation-based dependency injection into test cases. Inside the test method, we use the repository to query the datastore. We hand the repository a PageRequest instance that requests the first page of Person objects at a page size of 10. Type-safe Query Methods: MongoDB repository and its reactive counterpart integrates with the Querydsl(http://www.querydsl.com/) project, which provides a way to perform type-safe queries. Instead of writing queries as inline strings or externalizing them into XML files they are constructed via a fluent API. — Querydsl Team It provides the following features: Code completion in the IDE (all properties, methods, and operations can be expanded in your favorite Java IDE). Almost no syntactically invalid queries allowed (type-safe on all levels). Domain types and properties can be referenced safely — no strings involved! Adapts better to refactoring changes in domain types. Incremental query definition is easier. See the QueryDSL documentation(http://www.querydsl.com/static/querydsl/latest/reference/html/) for how to bootstrap your environment for APT-based code generation using Maven or Ant. QueryDSL lets you write queries such as the following: Imperative Reactive QPerson person = new QPerson(""person""); List<Person> result = repository.findAll(person.address.zipCode.eq(""C0123"")); Page<Person> page = repository.findAll(person.lastname.contains(""a""), PageRequest.of(0, 2, Direction.ASC, ""lastname"")); QPerson person = QPerson.person; Flux<Person> result = repository.findAll(person.address.zipCode.eq(""C0123"")); QPerson is a class that is generated by the Java annotation post-processing tool. It is a Predicate that lets you write type-safe queries. Notice that there are no strings in the query other than the C0123 value. You can use the generated Predicate class by using the QuerydslPredicateExecutor / ReactiveQuerydslPredicateExecutor interface, which the following listing shows: Imperative Reactive public interface QuerydslPredicateExecutor<T> { T findOne(Predicate predicate); List<T> findAll(Predicate predicate); List<T> findAll(Predicate predicate, Sort sort); List<T> findAll(Predicate predicate, OrderSpecifier<?>... orders); Page<T> findAll(Predicate predicate, Pageable pageable); List<T> findAll(OrderSpecifier<?>... orders); Long count(Predicate predicate); Boolean exists(Predicate predicate); } interface ReactiveQuerydslPredicateExecutor<T> { Mono<T> findOne(Predicate predicate); Flux<T> findAll(Predicate predicate); Flux<T> findAll(Predicate predicate, Sort sort); Flux<T> findAll(Predicate predicate, OrderSpecifier<?>... orders); Flux<T> findAll(OrderSpecifier<?>... orders); Mono<Long> count(Predicate predicate); Mono<Boolean> exists(Predicate predicate); } To use this in your repository implementation, add it to the list of repository interfaces from which your interface inherits, as the following example shows: Imperative Reactive interface PersonRepository extends MongoRepository<Person, String>, QuerydslPredicateExecutor<Person> { // additional query methods go here } interface PersonRepository extends ReactiveMongoRepository<Person, String>, ReactiveQuerydslPredicateExecutor<Person> { // additional query methods go here } Please note that joins (DBRef’s) are not supported with Reactive MongoDB support."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/create-instances.html","Creating Repository Instances: This section covers how to create instances and bean definitions for the defined repository interfaces. Java Configuration: Use the store-specific @EnableMongoRepositories annotation on a Java configuration class to define a configuration for repository activation. For an introduction to Java-based configuration of the Spring container, see JavaConfig in the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/java.html) . A sample configuration to enable Spring Data repositories resembles the following: Sample annotation-based repository configuration @Configuration @EnableJpaRepositories(""com.acme.repositories"") class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } The preceding example uses the JPA-specific annotation, which you would change according to the store module you actually use. The same applies to the definition of the EntityManagerFactory bean. See the sections covering the store-specific configuration. XML Configuration: Each Spring Data module includes a repositories element that lets you define a base package that Spring scans for you, as shown in the following example: Enabling Spring Data repositories via XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans:beans xmlns:beans=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans:beans> In the preceding example, Spring is instructed to scan com.acme.repositories and all its sub-packages for interfaces extending Repository or one of its sub-interfaces. For each interface found, the infrastructure registers the persistence technology-specific FactoryBean to create the appropriate proxies that handle invocations of the query methods. Each bean is registered under a bean name that is derived from the interface name, so an interface of UserRepository would be registered under userRepository . Bean names for nested repository interfaces are prefixed with their enclosing type name. The base package attribute allows wildcards so that you can define a pattern of scanned packages. Using Filters: By default, the infrastructure picks up every interface that extends the persistence technology-specific Repository sub-interface located under the configured base package and creates a bean instance for it. However, you might want more fine-grained control over which interfaces have bean instances created for them. To do so, use filter elements inside the repository declaration. The semantics are exactly equivalent to the elements in Spring’s component filters. For details, see the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/classpath-scanning.html) for these elements. For example, to exclude certain interfaces from instantiation as repository beans, you could use the following configuration: Using filters Java XML @Configuration @EnableMongoRepositories(basePackages = ""com.acme.repositories"", includeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeRepository"") }, excludeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeOtherRepository"") }) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } <repositories base-package=""com.acme.repositories""> <context:include-filter type=""regex"" expression="".*SomeRepository"" /> <context:exclude-filter type=""regex"" expression="".*SomeOtherRepository"" /> </repositories> The preceding example includes all interfaces ending with SomeRepository and excludes those ending with SomeOtherRepository from being instantiated. Standalone Usage: You can also use the repository infrastructure outside of a Spring container — for example, in CDI environments.You still need some Spring libraries in your classpath, but, generally, you can set up repositories programmatically as well.The Spring Data modules that provide repository support ship with a persistence technology-specific RepositoryFactory that you can use, as follows: Standalone usage of the repository factory RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class);"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/query-methods-details.html","Defining Query Methods: The repository proxy has two ways to derive a store-specific query from the method name: By deriving the query from the method name directly. By using a manually defined query. Available options depend on the actual store. However, there must be a strategy that decides what actual query is created. The next section describes the available options. Query Lookup Strategies: The following strategies are available for the repository infrastructure to resolve the query. With XML configuration, you can configure the strategy at the namespace through the query-lookup-strategy attribute. For Java configuration, you can use the queryLookupStrategy attribute of the EnableMongoRepositories annotation. Some strategies may not be supported for particular datastores. CREATE attempts to construct a store-specific query from the query method name. The general approach is to remove a given set of well known prefixes from the method name and parse the rest of the method. You can read more about query construction in “ Query Creation(#repositories.query-methods.query-creation) ”. USE_DECLARED_QUERY tries to find a declared query and throws an exception if it cannot find one. The query can be defined by an annotation somewhere or declared by other means. See the documentation of the specific store to find available options for that store. If the repository infrastructure does not find a declared query for the method at bootstrap time, it fails. CREATE_IF_NOT_FOUND (the default) combines CREATE and USE_DECLARED_QUERY . It looks up a declared query first, and, if no declared query is found, it creates a custom method name-based query. This is the default lookup strategy and, thus, is used if you do not configure anything explicitly. It allows quick query definition by method names but also custom-tuning of these queries by introducing declared queries as needed. Query Creation: The query builder mechanism built into the Spring Data repository infrastructure is useful for building constraining queries over entities of the repository. The following example shows how to create a number of queries: Query creation from method names interface PersonRepository extends Repository<Person, Long> { List<Person> findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // Enables the distinct flag for the query List<Person> findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List<Person> findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); // Enabling ignoring case for an individual property List<Person> findByLastnameIgnoreCase(String lastname); // Enabling ignoring case for all suitable properties List<Person> findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // Enabling static ORDER BY for a query List<Person> findByLastnameOrderByFirstnameAsc(String lastname); List<Person> findByLastnameOrderByFirstnameDesc(String lastname); } Parsing query method names is divided into subject and predicate. The first part ( find…By , exists…By ) defines the subject of the query, the second part forms the predicate. The introducing clause (subject) can contain further expressions. Any text between find (or other introducing keywords) and By is considered to be descriptive unless using one of the result-limiting keywords such as a Distinct to set a distinct flag on the query to be created or Top/First to limit query results(#repositories.limit-query-result) . The appendix contains the full list of query method subject keywords(query-keywords-reference.html#appendix.query.method.subject) and query method predicate keywords including sorting and letter-casing modifiers(query-keywords-reference.html#appendix.query.method.predicate) . However, the first By acts as a delimiter to indicate the start of the actual criteria predicate. At a very basic level, you can define conditions on entity properties and concatenate them with And and Or . The actual result of parsing the method depends on the persistence store for which you create the query. However, there are some general things to notice: The expressions are usually property traversals combined with operators that can be concatenated. You can combine property expressions with AND and OR . You also get support for operators such as Between , LessThan , GreaterThan , and Like for the property expressions. The supported operators can vary by datastore, so consult the appropriate part of your reference documentation. The method parser supports setting an IgnoreCase flag for individual properties (for example, findByLastnameIgnoreCase(…) ) or for all properties of a type that supports ignoring case (usually String instances — for example, findByLastnameAndFirstnameAllIgnoreCase(…) ). Whether ignoring cases is supported may vary by store, so consult the relevant sections in the reference documentation for the store-specific query method. You can apply static ordering by appending an OrderBy clause to the query method that references a property and by providing a sorting direction ( Asc or Desc ). To create a query method that supports dynamic sorting, see “ Paging, Iterating Large Results, Sorting & Limiting(#repositories.special-parameters) ”. Property Expressions: Property expressions can refer only to a direct property of the managed entity, as shown in the preceding example. At query creation time, you already make sure that the parsed property is a property of the managed domain class. However, you can also define constraints by traversing nested properties. Consider the following method signature: List<Person> findByAddressZipCode(ZipCode zipCode); Assume a Person has an Address with a ZipCode . In that case, the method creates the x.address.zipCode property traversal. The resolution algorithm starts by interpreting the entire part ( AddressZipCode ) as the property and checks the domain class for a property with that name (uncapitalized). If the algorithm succeeds, it uses that property. If not, the algorithm splits up the source at the camel-case parts from the right side into a head and a tail and tries to find the corresponding property — in our example, AddressZip and Code . If the algorithm finds a property with that head, it takes the tail and continues building the tree down from there, splitting the tail up in the way just described. If the first split does not match, the algorithm moves the split point to the left ( Address , ZipCode ) and continues. Although this should work for most cases, it is possible for the algorithm to select the wrong property. Suppose the Person class has an addressZip property as well. The algorithm would match in the first split round already, choose the wrong property, and fail (as the type of addressZip probably has no code property). To resolve this ambiguity you can use _ inside your method name to manually define traversal points. So our method name would be as follows: List<Person> findByAddress_ZipCode(ZipCode zipCode); Because we treat underscores ( _ ) as a reserved character, we strongly advise to follow standard Java naming conventions (that is, not using underscores in property names but applying camel case instead). Field Names starting with underscore: Field names may start with underscores like String _name . Make sure to preserve the _ as in _name and use double _ to split nested paths like user__name . Upper Case Field Names: Field names that are all uppercase can be used as such. Nested paths if applicable require splitting via _ as in USER_name . Field Names with 2nd uppercase letter: Field names that consist of a starting lower case letter followed by an uppercase one like String qCode can be resolved by starting with two upper case letters as in QCode . Please be aware of potential path ambiguities. Path Ambiguities: In the following sample the arrangement of properties qCode and q , with q containing a property called code , creates an ambiguity for the path QCode . record Container(String qCode, Code q) {} record Code(String code) {} Since a direct match on a property is considered first, any potential nested paths will not be considered and the algorithm picks the qCode field. In order to select the code field in q the underscore notation Q_Code is required. Repository Methods Returning Collections or Iterables: Query methods that return multiple results can use standard Java Iterable , List , and Set . Beyond that, we support returning Spring Data’s Streamable , a custom extension of Iterable , as well as collection types provided by Vavr(https://www.vavr.io/) . Refer to the appendix explaining all possible query method return types(query-return-types-reference.html#appendix.query.return.types) . Using Streamable as Query Method Return Type: You can use Streamable as alternative to Iterable or any collection type. It provides convenience methods to access a non-parallel Stream (missing from Iterable ) and the ability to directly ….filter(…) and ….map(…) over the elements and concatenate the Streamable to others: Using Streamable to combine query method results interface PersonRepository extends Repository<Person, Long> { Streamable<Person> findByFirstnameContaining(String firstname); Streamable<Person> findByLastnameContaining(String lastname); } Streamable<Person> result = repository.findByFirstnameContaining(""av"") .and(repository.findByLastnameContaining(""ea"")); Returning Custom Streamable Wrapper Types: Providing dedicated wrapper types for collections is a commonly used pattern to provide an API for a query result that returns multiple elements. Usually, these types are used by invoking a repository method returning a collection-like type and creating an instance of the wrapper type manually. You can avoid that additional step as Spring Data lets you use these wrapper types as query method return types if they meet the following criteria: The type implements Streamable . The type exposes either a constructor or a static factory method named of(…) or valueOf(…) that takes Streamable as an argument. The following listing shows an example: class Product { (1) MonetaryAmount getPrice() { … } } @RequiredArgsConstructor(staticName = ""of"") class Products implements Streamable<Product> { (2) private final Streamable<Product> streamable; public MonetaryAmount getTotal() { (3) return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); } @Override public Iterator<Product> iterator() { (4) return streamable.iterator(); } } interface ProductRepository implements Repository<Product, Long> { Products findAllByDescriptionContaining(String text); (5) } 1 A Product entity that exposes API to access the product’s price. 2 A wrapper type for a Streamable<Product> that can be constructed by using Products.of(…) (factory method created with the Lombok annotation). A standard constructor taking the Streamable<Product> will do as well. 3 The wrapper type exposes an additional API, calculating new values on the Streamable<Product> . 4 Implement the Streamable interface and delegate to the actual result. 5 That wrapper type Products can be used directly as a query method return type. You do not need to return Streamable<Product> and manually wrap it after the query in the repository client. Support for Vavr Collections: Vavr(https://www.vavr.io/) is a library that embraces functional programming concepts in Java. It ships with a custom set of collection types that you can use as query method return types, as the following table shows: Vavr collection type Used Vavr implementation type Valid Java source types io.vavr.collection.Seq io.vavr.collection.List java.util.Iterable io.vavr.collection.Set io.vavr.collection.LinkedHashSet java.util.Iterable io.vavr.collection.Map io.vavr.collection.LinkedHashMap java.util.Map You can use the types in the first column (or subtypes thereof) as query method return types and get the types in the second column used as implementation type, depending on the Java type of the actual query result (third column). Alternatively, you can declare Traversable (the Vavr Iterable equivalent), and we then derive the implementation class from the actual return value. That is, a java.util.List is turned into a Vavr List or Seq , a java.util.Set becomes a Vavr LinkedHashSet Set , and so on. Streaming Query Results: You can process the results of query methods incrementally by using a Java 8 Stream<T> as the return type. Instead of wrapping the query results in a Stream , data store-specific methods are used to perform the streaming, as shown in the following example: Stream the result of a query with Java 8 Stream<T> @Query(""select u from User u"") Stream<User> findAllByCustomQueryAndStream(); Stream<User> readAllByFirstnameNotNull(); @Query(""select u from User u"") Stream<User> streamAllPaged(Pageable pageable); A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example: Working with a Stream<T> result in a try-with-resources block try (Stream<User> stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } Not all Spring Data modules currently support Stream<T> as a return type. Asynchronous Query Results: You can run repository queries asynchronously by using Spring’s asynchronous method running capability(https://docs.spring.io/spring-framework/reference/6.1/integration/scheduling.html) . This means the method returns immediately upon invocation while the actual query occurs in a task that has been submitted to a Spring TaskExecutor . Asynchronous queries differ from reactive queries and should not be mixed. See the store-specific documentation for more details on reactive support. The following example shows a number of asynchronous queries: @Async Future<User> findByFirstname(String firstname); (1) @Async CompletableFuture<User> findOneByFirstname(String firstname); (2) 1 Use java.util.concurrent.Future as the return type. 2 Use a Java 8 java.util.concurrent.CompletableFuture as the return type. Paging, Iterating Large Results, Sorting & Limiting: To handle parameters in your query, define method parameters as already seen in the preceding examples. Besides that, the infrastructure recognizes certain specific types like Pageable , Sort and Limit , to apply pagination, sorting and limiting to your queries dynamically. The following example demonstrates these features: Using Pageable , Slice , Sort and Limit in query methods Page<User> findByLastname(String lastname, Pageable pageable); Slice<User> findByLastname(String lastname, Pageable pageable); List<User> findByLastname(String lastname, Sort sort); List<User> findByLastname(String lastname, Sort sort, Limit limit); List<User> findByLastname(String lastname, Pageable pageable); APIs taking Sort , Pageable and Limit expect non- null values to be handed into methods. If you do not want to apply any sorting or pagination, use Sort.unsorted() , Pageable.unpaged() and Limit.unlimited() . The first method lets you pass an org.springframework.data.domain.Pageable instance to the query method to dynamically add paging to your statically defined query. A Page knows about the total number of elements and pages available. It does so by the infrastructure triggering a count query to calculate the overall number. As this might be expensive (depending on the store used), you can instead return a Slice . A Slice knows only about whether a next Slice is available, which might be sufficient when walking through a larger result set. Sorting options are handled through the Pageable instance, too. If you need only sorting, add an org.springframework.data.domain.Sort parameter to your method. As you can see, returning a List is also possible. In this case, the additional metadata required to build the actual Page instance is not created (which, in turn, means that the additional count query that would have been necessary is not issued). Rather, it restricts the query to look up only the given range of entities. To find out how many pages you get for an entire query, you have to trigger an additional count query. By default, this query is derived from the query you actually trigger. Special parameters may only be used once within a query method. Some special parameters described above are mutually exclusive. Please consider the following list of invalid parameter combinations. Parameters Example Reason Pageable and Sort findBy…​(Pageable page, Sort sort) Pageable already defines Sort Pageable and Limit findBy…​(Pageable page, Limit limit) Pageable already defines a limit. The Top keyword used to limit results can be used to along with Pageable whereas Top defines the total maximum of results, whereas the Pageable parameter may reduce this number. Which Method is Appropriate?: The value provided by the Spring Data abstractions is perhaps best shown by the possible query method return types outlined in the following table below. The table shows which types you can return from a query method Table 1. Consuming Large Query Results Method Amount of Data Fetched Query Structure Constraints List<T>(#repositories.collections-and-iterables) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Streamable<T>(#repositories.collections-and-iterables.streamable) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Stream<T>(#repositories.query-streaming) Chunked (one-by-one or in batches) depending on Stream consumption. Single query using typically cursors. Streams must be closed after usage to avoid resource leaks. Flux<T> Chunked (one-by-one or in batches) depending on Flux consumption. Single query using typically cursors. Store module must provide reactive infrastructure. Slice<T> Pageable.getPageSize() + 1 at Pageable.getOffset() One to many queries fetching data starting at Pageable.getOffset() applying limiting. A Slice can only navigate to the next Slice . Slice provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Window provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Page<T> Pageable.getPageSize() at Pageable.getOffset() One to many queries starting at Pageable.getOffset() applying limiting. Additionally, COUNT(…) query to determine the total number of elements can be required. Often times, COUNT(…) queries are required that are costly. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Paging and Sorting: You can define simple sorting expressions by using property names. You can concatenate expressions to collect multiple criteria into one expression. Defining sort expressions Sort sort = Sort.by(""firstname"").ascending() .and(Sort.by(""lastname"").descending()); For a more type-safe way to define sort expressions, start with the type for which to define the sort expression and use method references to define the properties on which to sort. Defining sort expressions by using the type-safe API TypedSort<Person> person = Sort.sort(Person.class); Sort sort = person.by(Person::getFirstname).ascending() .and(person.by(Person::getLastname).descending()); TypedSort.by(…) makes use of runtime proxies by (typically) using CGlib, which may interfere with native image compilation when using tools such as Graal VM Native. If your store implementation supports Querydsl, you can also use the generated metamodel types to define sort expressions: Defining sort expressions by using the Querydsl API QSort sort = QSort.by(QPerson.firstname.asc()) .and(QSort.by(QPerson.lastname.desc())); Limiting Query Results: In addition to paging it is possible to limit the result size using a dedicated Limit parameter. You can also limit the results of query methods by using the First or Top keywords, which you can use interchangeably but may not be mixed with a Limit parameter. You can append an optional numeric value to Top or First to specify the maximum result size to be returned. If the number is left out, a result size of 1 is assumed. The following example shows how to limit the query size: Limiting the result size of a query with Top and First List<User> findByLastname(Limit limit); User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page<User> queryFirst10ByLastname(String lastname, Pageable pageable); Slice<User> findTop3ByLastname(String lastname, Pageable pageable); List<User> findFirst10ByLastname(String lastname, Sort sort); List<User> findTop10ByLastname(String lastname, Pageable pageable); The limiting expressions also support the Distinct keyword for datastores that support distinct queries. Also, for the queries that limit the result set to one instance, wrapping the result into with the Optional keyword is supported. If pagination or slicing is applied to a limiting query pagination (and the calculation of the number of available pages), it is applied within the limited result. Limiting the results in combination with dynamic sorting by using a Sort parameter lets you express query methods for the 'K' smallest as well as for the 'K' biggest elements."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/repositories/query-methods.html","MongoDB-specific Query Methods: Most of the data access operations you usually trigger on a repository result in a query being executed against the MongoDB databases. Defining such a query is a matter of declaring a method on the repository interface, as the following example shows: PersonRepository with query methods Imperative Reactive public interface PersonRepository extends PagingAndSortingRepository<Person, String> { List<Person> findByLastname(String lastname); (1) Page<Person> findByFirstname(String firstname, Pageable pageable); (2) Person findByShippingAddresses(Address address); (3) Person findFirstByLastname(String lastname); (4) Stream<Person> findAllBy(); (5) } 1 The findByLastname method shows a query for all people with the given last name. The query is derived by parsing the method name for constraints that can be concatenated with And and Or . Thus, the method name results in a query expression of {""lastname"" : lastname} . 2 Applies pagination to a query. You can equip your method signature with a Pageable parameter and let the method return a Page instance and Spring Data automatically pages the query accordingly. 3 Shows that you can query based on properties that are not primitive types. Throws IncorrectResultSizeDataAccessException if more than one match is found. 4 Uses the First keyword to restrict the query to only the first result. Unlike <3>, this method does not throw an exception if more than one match is found. 5 Uses a Java 8 Stream that reads and converts individual elements while iterating the stream. public interface ReactivePersonRepository extends ReactiveSortingRepository<Person, String> { Flux<Person> findByFirstname(String firstname); (1) Flux<Person> findByFirstname(Publisher<String> firstname); (2) Flux<Person> findByFirstnameOrderByLastname(String firstname, Pageable pageable); (3) Mono<Person> findByFirstnameAndLastname(String firstname, String lastname); (4) Mono<Person> findFirstByLastname(String lastname); (5) } 1 The method shows a query for all people with the given lastname . The query is derived by parsing the method name for constraints that can be concatenated with And and Or . Thus, the method name results in a query expression of {""lastname"" : lastname} . 2 The method shows a query for all people with the given firstname once the firstname is emitted by the given Publisher . 3 Use Pageable to pass offset and sorting parameters to the database. 4 Find a single entity for the given criteria. It completes with IncorrectResultSizeDataAccessException on non-unique results. 5 Unless <4>, the first entity is always emitted even if the query yields more result documents. The Page return type (as in Mono<Page> ) is not supported by reactive repositories. It is possible to use Pageable in derived finder methods, to pass on sort , limit and offset parameters to the query to reduce load and network traffic. The returned Flux will only emit data within the declared range. Pageable page = PageRequest.of(1, 10, Sort.by(""lastname"")); Flux<Person> persons = repository.findByFirstnameOrderByLastname(""luke"", page); We do not support referring to parameters that are mapped as DBRef in the domain class. Supported keywords for query methods Keyword Sample Logical result After findByBirthdateAfter(Date date) {""birthdate"" : {""$gt"" : date}} GreaterThan findByAgeGreaterThan(int age) {""age"" : {""$gt"" : age}} GreaterThanEqual findByAgeGreaterThanEqual(int age) {""age"" : {""$gte"" : age}} Before findByBirthdateBefore(Date date) {""birthdate"" : {""$lt"" : date}} LessThan findByAgeLessThan(int age) {""age"" : {""$lt"" : age}} LessThanEqual findByAgeLessThanEqual(int age) {""age"" : {""$lte"" : age}} Between findByAgeBetween(int from, int to) findByAgeBetween(Range<Integer> range) {""age"" : {""$gt"" : from, ""$lt"" : to}} lower / upper bounds ( $gt / $gte & $lt / $lte ) according to Range In findByAgeIn(Collection ages) {""age"" : {""$in"" : [ages…​]}} NotIn findByAgeNotIn(Collection ages) {""age"" : {""$nin"" : [ages…​]}} IsNotNull , NotNull findByFirstnameNotNull() {""firstname"" : {""$ne"" : null}} IsNull , Null findByFirstnameNull() {""firstname"" : null} Like , StartingWith , EndingWith findByFirstnameLike(String name) {""firstname"" : name} (name as regex) NotLike , IsNotLike findByFirstnameNotLike(String name) {""firstname"" : { ""$not"" : name }} (name as regex) Containing on String findByFirstnameContaining(String name) {""firstname"" : name} (name as regex) NotContaining on String findByFirstnameNotContaining(String name) {""firstname"" : { ""$not"" : name}} (name as regex) Containing on Collection findByAddressesContaining(Address address) {""addresses"" : { ""$in"" : address}} NotContaining on Collection findByAddressesNotContaining(Address address) {""addresses"" : { ""$not"" : { ""$in"" : address}}} Regex findByFirstnameRegex(String firstname) {""firstname"" : {""$regex"" : firstname }} (No keyword) findByFirstname(String name) {""firstname"" : name} Not findByFirstnameNot(String name) {""firstname"" : {""$ne"" : name}} Near findByLocationNear(Point point) {""location"" : {""$near"" : [x,y]}} Near findByLocationNear(Point point, Distance max) {""location"" : {""$near"" : [x,y], ""$maxDistance"" : max}} Near findByLocationNear(Point point, Distance min, Distance max) {""location"" : {""$near"" : [x,y], ""$minDistance"" : min, ""$maxDistance"" : max}} Within findByLocationWithin(Circle circle) {""location"" : {""$geoWithin"" : {""$center"" : [ [x, y], distance]}}} Within findByLocationWithin(Box box) {""location"" : {""$geoWithin"" : {""$box"" : [ [x1, y1], x2, y2]}}} IsTrue , True findByActiveIsTrue() {""active"" : true} IsFalse , False findByActiveIsFalse() {""active"" : false} Exists findByLocationExists(boolean exists) {""location"" : {""$exists"" : exists }} IgnoreCase findByUsernameIgnoreCase(String username) {""username"" : {""$regex"" : ""^username$"", ""$options"" : ""i"" }} If the property criterion compares a document, the order of the fields and exact equality in the document matters. Geo-spatial Queries: As you saw in the preceding table of keywords, a few keywords trigger geo-spatial operations within a MongoDB query. The Near keyword allows some further modification, as the next few examples show. The following example shows how to define a near query that finds all persons with a given distance of a given point: Advanced Near queries Imperative Reactive public interface PersonRepository extends MongoRepository<Person, String> { // { 'location' : { '$near' : [point.x, point.y], '$maxDistance' : distance}} List<Person> findByLocationNear(Point location, Distance distance); } interface PersonRepository extends ReactiveMongoRepository<Person, String> { // { 'location' : { '$near' : [point.x, point.y], '$maxDistance' : distance}} Flux<Person> findByLocationNear(Point location, Distance distance); } Adding a Distance parameter to the query method allows restricting results to those within the given distance. If the Distance was set up containing a Metric , we transparently use $nearSphere instead of $code , as the following example shows: Example 1. Using Distance with Metrics Point point = new Point(43.7, 48.8); Distance distance = new Distance(200, Metrics.KILOMETERS); … = repository.findByLocationNear(point, distance); // {'location' : {'$nearSphere' : [43.7, 48.8], '$maxDistance' : 0.03135711885774796}} Reactive Geo-spatial repository queries support the domain type and GeoResult<T> results within a reactive wrapper type. GeoPage and GeoResults are not supported as they contradict the deferred result approach with pre-calculating the average distance. However, you can still pass in a Pageable argument to page results yourself. Using a Distance with a Metric causes a $nearSphere (instead of a plain $near ) clause to be added. Beyond that, the actual distance gets calculated according to the Metrics used. (Note that Metric does not refer to metric units of measure. It could be miles rather than kilometers. Rather, metric refers to the concept of a system of measurement, regardless of which system you use.) Using @GeoSpatialIndexed(type = GeoSpatialIndexType.GEO_2DSPHERE) on the target property forces usage of the $nearSphere operator. Imperative Reactive public interface PersonRepository extends MongoRepository<Person, String> { // {'geoNear' : 'location', 'near' : [x, y] } GeoResults<Person> findByLocationNear(Point location); // No metric: {'geoNear' : 'person', 'near' : [x, y], maxDistance : distance } // Metric: {'geoNear' : 'person', 'near' : [x, y], 'maxDistance' : distance, // 'distanceMultiplier' : metric.multiplier, 'spherical' : true } GeoResults<Person> findByLocationNear(Point location, Distance distance); // Metric: {'geoNear' : 'person', 'near' : [x, y], 'minDistance' : min, // 'maxDistance' : max, 'distanceMultiplier' : metric.multiplier, // 'spherical' : true } GeoResults<Person> findByLocationNear(Point location, Distance min, Distance max); // {'geoNear' : 'location', 'near' : [x, y] } GeoResults<Person> findByLocationNear(Point location); } interface PersonRepository extends ReactiveMongoRepository<Person, String> { // {'geoNear' : 'location', 'near' : [x, y] } Flux<GeoResult<Person>> findByLocationNear(Point location); // No metric: {'geoNear' : 'person', 'near' : [x, y], maxDistance : distance } // Metric: {'geoNear' : 'person', 'near' : [x, y], 'maxDistance' : distance, // 'distanceMultiplier' : metric.multiplier, 'spherical' : true } Flux<GeoResult<Person>> findByLocationNear(Point location, Distance distance); // Metric: {'geoNear' : 'person', 'near' : [x, y], 'minDistance' : min, // 'maxDistance' : max, 'distanceMultiplier' : metric.multiplier, // 'spherical' : true } Flux<GeoResult<Person>> findByLocationNear(Point location, Distance min, Distance max); // {'geoNear' : 'location', 'near' : [x, y] } Flux<GeoResult<Person>> findByLocationNear(Point location); } JSON-based Query Methods and Field Restriction: By adding the org.springframework.data.mongodb.repository.Query annotation to your repository query methods, you can specify a MongoDB JSON query string to use instead of having the query be derived from the method name, as the following example shows: Imperative Reactive public interface PersonRepository extends MongoRepository<Person, String> { @Query(""{ 'firstname' : ?0 }"") List<Person> findByThePersonsFirstname(String firstname); } public interface PersonRepository extends ReactiveMongoRepository<Person, String> { @Query(""{ 'firstname' : ?0 }"") Flux<Person> findByThePersonsFirstname(String firstname); } The ?0 placeholder lets you substitute the value from the method arguments into the JSON query string. String parameter values are escaped during the binding process, which means that it is not possible to add MongoDB specific operators through the argument. You can also use the filter property to restrict the set of properties that is mapped into the Java object, as the following example shows: Imperative Reactive public interface PersonRepository extends MongoRepository<Person, String> { @Query(value=""{ 'firstname' : ?0 }"", fields=""{ 'firstname' : 1, 'lastname' : 1}"") List<Person> findByThePersonsFirstname(String firstname); } public interface PersonRepository extends ReactiveMongoRepository<Person, String> { @Query(value=""{ 'firstname' : ?0 }"", fields=""{ 'firstname' : 1, 'lastname' : 1}"") Flux<Person> findByThePersonsFirstname(String firstname); } The query in the preceding example returns only the firstname , lastname and Id properties of the Person objects. The age property, a java.lang.Integer , is not set and its value is therefore null. JSON-based Queries with SpEL Expressions: Query strings and field definitions can be used together with SpEL expressions to create dynamic queries at runtime. SpEL expressions can provide predicate values and can be used to extend predicates with subdocuments. Expressions expose method arguments through an array that contains all the arguments. The following query uses [0] to declare the predicate value for lastname (which is equivalent to the ?0 parameter binding): Imperative Reactive public interface PersonRepository extends MongoRepository<Person, String> { @Query(""{'lastname': ?#{[0]} }"") List<Person> findByQueryWithExpression(String param0); } public interface PersonRepository extends ReactiveMongoRepository<Person, String> { @Query(""{'lastname': ?#{[0]} }"") Flux<Person> findByQueryWithExpression(String param0); } Expressions can be used to invoke functions, evaluate conditionals, and construct values. SpEL expressions used in conjunction with JSON reveal a side-effect, because Map-like declarations inside of SpEL read like JSON, as the following example shows: Imperative Reactive public interface PersonRepository extends MongoRepository<Person, String> { @Query(""{'id': ?#{ [0] ? {$exists :true} : [1] }}"") List<Person> findByQueryWithExpressionAndNestedObject(boolean param0, String param1); } public interface PersonRepository extends ReactiveMongoRepository<Person, String> { @Query(""{'id': ?#{ [0] ? {$exists :true} : [1] }}"") Flux<Person> findByQueryWithExpressionAndNestedObject(boolean param0, String param1); } SpEL in query strings can be a powerful way to enhance queries. However, they can also accept a broad range of unwanted arguments. Make sure to sanitize strings before passing them to the query to avoid creation of vulnerabilities or unwanted changes to your query. Expression support is extensible through the Query SPI: EvaluationContextExtension & ReactiveEvaluationContextExtension The Query SPI can contribute properties and functions and can customize the root object. Extensions are retrieved from the application context at the time of SpEL evaluation when the query is built. The following example shows how to use an evaluation context extension: Imperative Reactive public class SampleEvaluationContextExtension extends EvaluationContextExtensionSupport { @Override public String getExtensionId() { return ""security""; } @Override public Map<String, Object> getProperties() { return Collections.singletonMap(""principal"", SecurityContextHolder.getCurrent().getPrincipal()); } } public class SampleEvaluationContextExtension implements ReactiveEvaluationContextExtension { @Override public String getExtensionId() { return ""security""; } @Override public Mono<? extends EvaluationContextExtension> getExtension() { return Mono.just(new EvaluationContextExtensionSupport() { ... }); } } Bootstrapping MongoRepositoryFactory yourself is not application context-aware and requires further configuration to pick up Query SPI extensions. Reactive query methods can make use of org.springframework.data.spel.spi.ReactiveEvaluationContextExtension . Full-text Search Queries: MongoDB’s full-text search feature is store-specific and, therefore, can be found on MongoRepository rather than on the more general CrudRepository . We need a document with a full-text index (see “ Text Indexes(../mapping/mapping.html#mapping-usage-indexes.text-index) ” to learn how to create a full-text index). Additional methods on MongoRepository take TextCriteria as an input parameter. In addition to those explicit methods, it is also possible to add a TextCriteria -derived repository method. The criteria are added as an additional AND criteria. Once the entity contains a @TextScore -annotated property, the document’s full-text score can be retrieved. Furthermore, the @TextScore annotated also makes it possible to sort by the document’s score, as the following example shows: @Document class FullTextDocument { @Id String id; @TextIndexed String title; @TextIndexed String content; @TextScore Float score; } interface FullTextRepository extends Repository<FullTextDocument, String> { // Execute a full-text search and define sorting dynamically List<FullTextDocument> findAllBy(TextCriteria criteria, Sort sort); // Paginate over a full-text search result Page<FullTextDocument> findAllBy(TextCriteria criteria, Pageable pageable); // Combine a derived query with a full-text search List<FullTextDocument> findByTitleOrderByScoreDesc(String title, TextCriteria criteria); } Sort sort = Sort.by(""score""); TextCriteria criteria = TextCriteria.forDefaultLanguage().matchingAny(""spring"", ""data""); List<FullTextDocument> result = repository.findAllBy(criteria, sort); criteria = TextCriteria.forDefaultLanguage().matching(""film""); Page<FullTextDocument> page = repository.findAllBy(criteria, PageRequest.of(1, 1, sort)); List<FullTextDocument> result = repository.findByTitleOrderByScoreDesc(""mongodb"", criteria); Aggregation Methods: The repository layer offers means to interact with the aggregation framework(../aggregation-framework.html) via annotated repository query methods. Similar to the JSON based queries(repositories.html#mongodb.repositories.queries.json-based) , you can define a pipeline using the org.springframework.data.mongodb.repository.Aggregation annotation. The definition may contain simple placeholders like ?0 as well as SpEL expressions(https://docs.spring.io/spring-framework/reference/6.1/core.html#expressions) ?#{ … } . Example 2. Aggregating Repository Method public interface PersonRepository extends CrudRepository<Person, String> { @Aggregation(""{ $group: { _id : $lastname, names : { $addToSet : $firstname } } }"") List<PersonAggregate> groupByLastnameAndFirstnames(); (1) @Aggregation(""{ $group: { _id : $lastname, names : { $addToSet : $firstname } } }"") List<PersonAggregate> groupByLastnameAndFirstnames(Sort sort); (2) @Aggregation(""{ $group: { _id : $lastname, names : { $addToSet : ?0 } } }"") List<PersonAggregate> groupByLastnameAnd(String property); (3) @Aggregation(""{ $group: { _id : $lastname, names : { $addToSet : ?0 } } }"") Slice<PersonAggregate> groupByLastnameAnd(String property, Pageable page); (4) @Aggregation(""{ $group: { _id : $lastname, names : { $addToSet : $firstname } } }"") Stream<PersonAggregate> groupByLastnameAndFirstnamesAsStream(); (5) @Aggregation(""{ $group : { _id : null, total : { $sum : $age } } }"") SumValue sumAgeUsingValueWrapper(); (6) @Aggregation(""{ $group : { _id : null, total : { $sum : $age } } }"") Long sumAge(); (7) @Aggregation(""{ $group : { _id : null, total : { $sum : $age } } }"") AggregationResults<SumValue> sumAgeRaw(); (8) @Aggregation(""{ '$project': { '_id' : '$lastname' } }"") List<String> findAllLastnames(); (9) @Aggregation(pipeline = { ""{ $group : { _id : '$author', books: { $push: '$title' } } }"", ""{ $out : 'authors' }"" }) void groupAndOutSkippingOutput(); (10) } public class PersonAggregate { private @Id String lastname; (2) private List<String> names; public PersonAggregate(String lastname, List<String> names) { // ... } // Getter / Setter omitted } public class SumValue { private final Long total; (6) (8) public SumValue(Long total) { // ... } // Getter omitted } 1 Aggregation pipeline to group first names by lastname in the Person collection returning these as PersonAggregate . 2 If Sort argument is present, $sort is appended after the declared pipeline stages so that it only affects the order of the final results after having passed all other aggregation stages. Therefore, the Sort properties are mapped against the methods return type PersonAggregate which turns Sort.by(""lastname"") into { $sort : { '_id', 1 } } because PersonAggregate.lastname is annotated with @Id . 3 Replaces ?0 with the given value for property for a dynamic aggregation pipeline. 4 $skip , $limit and $sort can be passed on via a Pageable argument. Same as in <2>, the operators are appended to the pipeline definition. Methods accepting Pageable can return Slice for easier pagination. 5 Aggregation methods can return Stream to consume results directly from an underlying cursor. Make sure to close the stream after consuming it to release the server-side cursor by either calling close() or through try-with-resources . 6 Map the result of an aggregation returning a single Document to an instance of a desired SumValue target type. 7 Aggregations resulting in single document holding just an accumulation result like e.g. $sum can be extracted directly from the result Document . To gain more control, you might consider AggregationResult as method return type as shown in <7>. 8 Obtain the raw AggregationResults mapped to the generic target wrapper type SumValue or org.bson.Document . 9 Like in <6>, a single value can be directly obtained from multiple result Document s. 10 Skips the output of the $out stage when return type is void . In some scenarios, aggregations might require additional options, such as a maximum run time, additional log comments, or the permission to temporarily write data to disk. Use the @Meta annotation to set those options via maxExecutionTimeMs , comment or allowDiskUse . interface PersonRepository extends CrudRepository<Person, String> { @Meta(allowDiskUse = true) @Aggregation(""{ $group: { _id : $lastname, names : { $addToSet : $firstname } } }"") List<PersonAggregate> groupByLastnameAndFirstnames(); } Or use @Meta to create your own annotation as shown in the sample below. @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.METHOD }) @Meta(allowDiskUse = true) @interface AllowDiskUse { } interface PersonRepository extends CrudRepository<Person, String> { @AllowDiskUse @Aggregation(""{ $group: { _id : $lastname, names : { $addToSet : $firstname } } }"") List<PersonAggregate> groupByLastnameAndFirstnames(); } Simple-type single-result inspects the returned Document and checks for the following: Only one entry in the document, return it. Two entries, one is the _id value. Return the other. Return for the first value assignable to the return type. Throw an exception if none of the above is applicable. The Page return type is not supported for repository methods using @Aggregation . However, you can use a Pageable argument to add $skip , $limit and $sort to the pipeline and let the method return Slice . Query by Example: Introduction: This chapter provides an introduction to Query by Example and explains how to use it. Query by Example (QBE) is a user-friendly querying technique with a simple interface. It allows dynamic query creation and does not require you to write queries that contain field names. In fact, Query by Example does not require you to write queries by using store-specific query languages at all. This chapter explains the core concepts of Query by Example. The information is pulled from the Spring Data Commons module. Depending on your database, String matching support can be limited. Usage: The Query by Example API consists of four parts: Probe: The actual example of a domain object with populated fields. ExampleMatcher : The ExampleMatcher carries details on how to match particular fields. It can be reused across multiple Examples. Example : An Example consists of the probe and the ExampleMatcher . It is used to create the query. FetchableFluentQuery : A FetchableFluentQuery offers a fluent API, that allows further customization of a query derived from an Example . Using the fluent API lets you specify ordering projection and result processing for your query. Query by Example is well suited for several use cases: Querying your data store with a set of static or dynamic constraints. Frequent refactoring of the domain objects without worrying about breaking existing queries. Working independently of the underlying data store API. Query by Example also has several limitations: No support for nested or grouped property constraints, such as firstname = ?0 or (firstname = ?1 and lastname = ?2) . Store-specific support on string matching. Depending on your databases, String matching can support starts/contains/ends/regex for strings. Exact matching for other property types. Before getting started with Query by Example, you need to have a domain object. To get started, create an interface for your repository, as shown in the following example: Sample Person object public class Person { @Id private String id; private String firstname; private String lastname; private Address address; // … getters and setters omitted } The preceding example shows a simple domain object. You can use it to create an Example . By default, fields having null values are ignored, and strings are matched by using the store specific defaults. Inclusion of properties into a Query by Example criteria is based on nullability. Properties using primitive types ( int , double , …) are always included unless the ExampleMatcher ignores the property path(#query-by-example.matchers) . Examples can be built by either using the of factory method or by using ExampleMatcher(#query-by-example.matchers) . Example is immutable. The following listing shows a simple Example: Example 3. Simple Example Person person = new Person(); (1) person.setFirstname(""Dave""); (2) Example<Person> example = Example.of(person); (3) 1 Create a new instance of the domain object. 2 Set the properties to query. 3 Create the Example . You can run the example queries by using repositories. To do so, let your repository interface extend QueryByExampleExecutor<T> . The following listing shows an excerpt from the QueryByExampleExecutor interface: The QueryByExampleExecutor public interface QueryByExampleExecutor<T> { <S extends T> S findOne(Example<S> example); <S extends T> Iterable<S> findAll(Example<S> example); // … more functionality omitted. } Example Matchers: Examples are not limited to default settings. You can specify your own defaults for string matching, null handling, and property-specific settings by using the ExampleMatcher , as shown in the following example: Example 4. Example matcher with customized matching Person person = new Person(); (1) person.setFirstname(""Dave""); (2) ExampleMatcher matcher = ExampleMatcher.matching() (3) .withIgnorePaths(""lastname"") (4) .withIncludeNullValues() (5) .withStringMatcher(StringMatcher.ENDING); (6) Example<Person> example = Example.of(person, matcher); (7) 1 Create a new instance of the domain object. 2 Set properties. 3 Create an ExampleMatcher to expect all values to match. It is usable at this stage even without further configuration. 4 Construct a new ExampleMatcher to ignore the lastname property path. 5 Construct a new ExampleMatcher to ignore the lastname property path and to include null values. 6 Construct a new ExampleMatcher to ignore the lastname property path, to include null values, and to perform suffix string matching. 7 Create a new Example based on the domain object and the configured ExampleMatcher . By default, the ExampleMatcher expects all values set on the probe to match. If you want to get results matching any of the predicates defined implicitly, use ExampleMatcher.matchingAny() . You can specify behavior for individual properties (such as ""firstname"" and ""lastname"" or, for nested properties, ""address.city""). You can tune it with matching options and case sensitivity, as shown in the following example: Configuring matcher options ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", endsWith()) .withMatcher(""lastname"", startsWith().ignoreCase()); } Another way to configure matcher options is to use lambdas (introduced in Java 8). This approach creates a callback that asks the implementor to modify the matcher. You need not return the matcher, because configuration options are held within the matcher instance. The following example shows a matcher that uses lambdas: Configuring matcher options with lambdas ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", match -> match.endsWith()) .withMatcher(""firstname"", match -> match.startsWith()); } Queries created by Example use a merged view of the configuration. Default matching settings can be set at the ExampleMatcher level, while individual settings can be applied to particular property paths. Settings that are set on ExampleMatcher are inherited by property path settings unless they are defined explicitly. Settings on a property patch have higher precedence than default settings. The following table describes the scope of the various ExampleMatcher settings: Table 1. Scope of ExampleMatcher settings Setting Scope Null-handling ExampleMatcher String matching ExampleMatcher and property path Ignoring properties Property path Case sensitivity ExampleMatcher and property path Value transformation Property path Fluent API: QueryByExampleExecutor offers one more method, which we did not mention so far: <S extends T, R> R findBy(Example<S> example, Function<FluentQuery.FetchableFluentQuery<S>, R> queryFunction) . As with other methods, it executes a query derived from an Example . However, with the second argument, you can control aspects of that execution that you cannot dynamically control otherwise. You do so by invoking the various methods of the FetchableFluentQuery in the second argument. sortBy lets you specify an ordering for your result. as lets you specify the type to which you want the result to be transformed. project limits the queried attributes. first , firstValue , one , oneValue , all , page , stream , count , and exists define what kind of result you get and how the query behaves when more than the expected number of results are available. Use the fluent API to get the last of potentially many results, ordered by lastname. Optional<Person> match = repository.findBy(example, q -> q .sortBy(Sort.by(""lastname"").descending()) .first() ); Running an Example: The following example shows how to query by example when using a repository (of Person objects, in this case): Example 5. Query by Example using a repository public interface PersonRepository extends QueryByExampleExecutor<Person> { } public class PersonService { @Autowired PersonRepository personRepository; public List<Person> findPeople(Person probe) { return personRepository.findAll(Example.of(probe)); } } Scrolling: Scrolling is a more fine-grained approach to iterate through larger results set chunks. Scrolling consists of a stable sort, a scroll type (Offset- or Keyset-based scrolling) and result limiting. You can define simple sorting expressions by using property names and define static result limiting using the Top or First keyword(../../repositories/query-methods-details.html#repositories.limit-query-result) through query derivation. You can concatenate expressions to collect multiple criteria into one expression. Scroll queries return a Window<T> that allows obtaining the element’s scroll position to fetch the next Window<T> until your application has consumed the entire query result. Similar to consuming a Java Iterator<List<…>> by obtaining the next batch of results, query result scrolling lets you access the a ScrollPosition through Window.positionAt(…​) . Window<User> users = repository.findFirst10ByLastnameOrderByFirstname(""Doe"", ScrollPosition.offset()); do { for (User u : users) { // consume the user } // obtain the next Scroll users = repository.findFirst10ByLastnameOrderByFirstname(""Doe"", users.positionAt(users.size() - 1)); } while (!users.isEmpty() && users.hasNext()); The ScrollPosition identifies the exact position of an element with the entire query result. Query execution treats the position parameter exclusive , results will start after the given position. ScrollPosition#offset() and ScrollPosition#keyset() as special incarnations of a ScrollPosition indicating the start of a scroll operation. WindowIterator provides a utility to simplify scrolling across Window s by removing the need to check for the presence of a next Window and applying the ScrollPosition . WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(ScrollPosition.offset()); while (users.hasNext()) { User u = users.next(); // consume the user } Scrolling using Offset: Offset scrolling uses similar to pagination, an Offset counter to skip a number of results and let the data source only return results beginning at the given Offset. This simple mechanism avoids large results being sent to the client application. However, most databases require materializing the full query result before your server can return the results. Example 6. Using OffsetScrollPosition with Repository Query Methods interface UserRepository extends Repository<User, Long> { Window<User> findFirst10ByLastnameOrderByFirstname(String lastname, OffsetScrollPosition position); } WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(OffsetScrollPosition.initial()); (1) 1 Start with no offset to include the element at position 0 . There is a difference between ScollPosition.offset() and ScollPosition.offset(0L) . The former indicates the start of scroll operation, pointing to no specific offset whereas the latter identifies the first element (at position 0 ) of the result. Given the exclusive nature of scrolling, using ScollPosition.offset(0) skips the first element and translate to an offset of 1 . Scrolling using Keyset-Filtering: Offset-based requires most databases require materializing the entire result before your server can return the results. So while the client only sees the portion of the requested results, your server needs to build the full result, which causes additional load. Keyset-Filtering approaches result subset retrieval by leveraging built-in capabilities of your database aiming to reduce the computation and I/O requirements for individual queries. This approach maintains a set of keys to resume scrolling by passing keys into the query, effectively amending your filter criteria. The core idea of Keyset-Filtering is to start retrieving results using a stable sorting order. Once you want to scroll to the next chunk, you obtain a ScrollPosition that is used to reconstruct the position within the sorted result. The ScrollPosition captures the keyset of the last entity within the current Window . To run the query, reconstruction rewrites the criteria clause to include all sort fields and the primary key so that the database can leverage potential indexes to run the query. The database needs only constructing a much smaller result from the given keyset position without the need to fully materialize a large result and then skipping results until reaching a particular offset. Keyset-Filtering requires the keyset properties (those used for sorting) to be non-nullable. This limitation applies due to the store specific null value handling of comparison operators as well as the need to run queries against an indexed source. Keyset-Filtering on nullable properties will lead to unexpected results. Using KeysetScrollPosition with Repository Query Methods interface UserRepository extends Repository<User, Long> { Window<User> findFirst10ByLastnameOrderByFirstname(String lastname, KeysetScrollPosition position); } WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(ScrollPosition.keyset()); (1) 1 Start at the very beginning and do not apply additional filtering. Keyset-Filtering works best when your database contains an index that matches the sort fields, hence a static sort works well. Scroll queries applying Keyset-Filtering require to the properties used in the sort order to be returned by the query, and these must be mapped in the returned entity. You can use interface and DTO projections, however make sure to include all properties that you’ve sorted by to avoid keyset extraction failures. When specifying your Sort order, it is sufficient to include sort properties relevant to your query; You do not need to ensure unique query results if you do not want to. The keyset query mechanism amends your sort order by including the primary key (or any remainder of composite primary keys) to ensure each query result is unique. Sorting Results: MongoDB repositories allow various approaches to define sorting order. Let’s take a look at the following example: Sorting Query Results Imperative Reactive public interface PersonRepository extends MongoRepository<Person, String> { List<Person> findByFirstnameSortByAgeDesc(String firstname); (1) List<Person> findByFirstname(String firstname, Sort sort); (2) @Query(sort = ""{ age : -1 }"") List<Person> findByFirstname(String firstname); (3) @Query(sort = ""{ age : -1 }"") List<Person> findByLastname(String lastname, Sort sort); (4) } 1 Static sorting derived from method name. SortByAgeDesc results in { age : -1 } for the sort parameter. 2 Dynamic sorting using a method argument. Sort.by(DESC, ""age"") creates { age : -1 } for the sort parameter. 3 Static sorting via Query annotation. Sort parameter applied as stated in the sort attribute. 4 Default sorting via Query annotation combined with dynamic one via a method argument. Sort.unsorted() results in { age : -1 } . Using Sort.by(ASC, ""age"") overrides the defaults and creates { age : 1 } . Sort.by (ASC, ""firstname"") alters the default and results in { age : -1, firstname : 1 } . public interface PersonRepository extends ReactiveMongoRepository<Person, String> { Flux<Person> findByFirstnameSortByAgeDesc(String firstname); Flux<Person> findByFirstname(String firstname, Sort sort); @Query(sort = ""{ age : -1 }"") Flux<Person> findByFirstname(String firstname); @Query(sort = ""{ age : -1 }"") Flux<Person> findByLastname(String lastname, Sort sort); } Index Hints: The @Hint annotation allows to override MongoDB’s default index selection and forces the database to use the specified index instead. Example 7. Example of index hints @Hint(""lastname-idx"") (1) List<Person> findByLastname(String lastname); @Query(value = ""{ 'firstname' : ?0 }"", hint = ""firstname-idx"") (2) List<Person> findByFirstname(String firstname); 1 Use the index with name lastname-idx . 2 The @Query annotation defines the hint alias which is equivalent to adding the @Hint annotation. For more information about index creation please refer to the Collection Management(../template-collection-management.html) section. Collation Support: Next to the general Collation Support(../collation.html) repositories allow to define the collation for various operations. public interface PersonRepository extends MongoRepository<Person, String> { @Query(collation = ""en_US"") (1) List<Person> findByFirstname(String firstname); @Query(collation = ""{ 'locale' : 'en_US' }"") (2) List<Person> findPersonByFirstname(String firstname); @Query(collation = ""?1"") (3) List<Person> findByFirstname(String firstname, Object collation); @Query(collation = ""{ 'locale' : '?1' }"") (4) List<Person> findByFirstname(String firstname, String collation); List<Person> findByFirstname(String firstname, Collation collation); (5) @Query(collation = ""{ 'locale' : 'en_US' }"") List<Person> findByFirstname(String firstname, @Nullable Collation collation); (6) } 1 Static collation definition resulting in { 'locale' : 'en_US' } . 2 Static collation definition resulting in { 'locale' : 'en_US' } . 3 Dynamic collation depending on 2nd method argument. Allowed types include String (eg. 'en_US'), Locacle (eg. Locacle.US) and Document (eg. new Document(""locale"", ""en_US"")) 4 Dynamic collation depending on 2nd method argument. 5 Apply the Collation method parameter to the query. 6 The Collation method parameter overrides the default collation from @Query if not null. In case you enabled the automatic index creation for repository finder methods a potential static collation definition, as shown in (1) and (2), will be included when creating the index. The most specifc Collation outrules potentially defined others. Which means Method argument over query method annotation over domain type annotation. To streamline usage of collation attributes throughout the codebase it is also possible to use the @Collation annotation, which serves as a meta annotation for the ones mentioned above. The same rules and locations apply, plus, direct usage of @Collation supersedes any collation values defined on @Query and other annotations. Which means, if a collation is declared via @Query and additionally via @Collation , then the one from @Collation is picked. Example 8. Using @Collation @Collation(""en_US"") (1) class Game { // ... } interface GameRepository extends Repository<Game, String> { @Collation(""en_GB"") (2) List<Game> findByTitle(String title); @Collation(""de_AT"") (3) @Query(collation=""en_GB"") List<Game> findByDescriptionContaining(String keyword); } 1 Instead of @Document(collation=…​) . 2 Instead of @Query(collation=…​) . 3 Favors @Collation over meta usage. Read Preferences: The @ReadPreference annotation allows you to configure MongoDB’s ReadPreferences. Example 9. Example of read preferences @ReadPreference(""primaryPreferred"") (1) public interface PersonRepository extends CrudRepository<Person, String> { @ReadPreference(""secondaryPreferred"") (2) List<Person> findWithReadPreferenceAnnotationByLastname(String lastname); @Query(readPreference = ""nearest"") (3) List<Person> findWithReadPreferenceAtTagByFirstname(String firstname); List<Person> findWithReadPreferenceAtTagByFirstname(String firstname); (4) 1 Configure read preference for all repository operations (including inherited, non custom implementation ones) that do not have a query-level definition. Therefore, in this case the read preference mode will be primaryPreferred 2 Use the read preference mode defined in annotation ReadPreference , in this case secondaryPreferred 3 The @Query annotation defines the read preference mode alias which is equivalent to adding the @ReadPreference annotation. 4 This query will use the read preference mode defined in the repository. The MongoOperations and Query API offer more fine grained control for ReadPreference ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/repositories/modifying-methods.html","MongoDB-specific Data Manipulation Methods: Next to the query methods(query-methods.html) it is possible to update data with specialized methods. Update Methods: You can also use the keywords in the preceding table to create queries that identify matching documents for running updates on them. The actual update action is defined by the @Update annotation on the method itself, as the following listing shows. Note that the naming schema for derived queries starts with find . Using update (as in updateAllByLastname(…​) ) is allowed only in combination with @Query . The update is applied to all matching documents and it is not possible to limit the scope by passing in a Page or by using any of the limiting keywords(#repositories.limit-query-result) . The return type can be either void or a numeric type, such as long , to hold the number of modified documents. Example 1. Update Methods public interface PersonRepository extends CrudRepository<Person, String> { @Update(""{ '$inc' : { 'visits' : 1 } }"") long findAndIncrementVisitsByLastname(String lastname); (1) @Update(""{ '$inc' : { 'visits' : ?1 } }"") void findAndIncrementVisitsByLastname(String lastname, int increment); (2) @Update(""{ '$inc' : { 'visits' : ?#{[1]} } }"") long findAndIncrementVisitsUsingSpELByLastname(String lastname, int increment); (3) @Update(pipeline = {""{ '$set' : { 'visits' : { '$add' : [ '$visits', ?1 ] } } }""}) void findAndIncrementVisitsViaPipelineByLastname(String lastname, int increment); (4) @Update(""{ '$push' : { 'shippingAddresses' : ?1 } }"") long findAndPushShippingAddressByEmail(String email, Address address); (5) @Query(""{ 'lastname' : ?0 }"") @Update(""{ '$inc' : { 'visits' : ?1 } }"") void updateAllByLastname(String lastname, int increment); (6) } 1 The filter query for the update is derived from the method name. The update is “as is” and does not bind any parameters. 2 The actual increment value is defined by the increment method argument that is bound to the ?1 placeholder. 3 Use the Spring Expression Language (SpEL) for parameter binding. 4 Use the pipeline attribute to issue aggregation pipeline updates(../template-crud-operations.html#mongo-template.aggregation-update) . 5 The update may contain complex objects. 6 Combine a string based query(repositories.html#mongodb.repositories.queries.json-based) with an update. Repository updates do not emit persistence nor mapping lifecycle events. Delete Methods: The keywords in the preceding table can be used in conjunction with delete…By or remove…By to create queries that delete matching documents. Delete…By Query Imperative Reactive public interface PersonRepository extends MongoRepository<Person, String> { List <Person> deleteByLastname(String lastname); (1) Long deletePersonByLastname(String lastname); (2) @Nullable Person deleteSingleByLastname(String lastname); (3) Optional<Person> deleteByBirthdate(Date birthdate); (4) } 1 Using a return type of List retrieves and returns all matching documents before actually deleting them. 2 A numeric return type directly removes the matching documents, returning the total number of documents removed. 3 A single domain type result retrieves and removes the first matching document. 4 Same as in 3 but wrapped in an Optional type. public interface PersonRepository extends ReactiveMongoRepository<Person, String> { Flux<Person> deleteByLastname(String lastname); (1) Mono<Long> deletePersonByLastname(String lastname); (2) Mono<Person> deleteSingleByLastname(String lastname); (3) } 1 Using a return type of Flux retrieves and returns all matching documents before actually deleting them. 2 A numeric return type directly removes the matching documents, returning the total number of documents removed. 3 A single domain type result retrieves and removes the first matching document."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/projections.html","Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableMongoRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableMongoRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" />"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/null-handling.html","Null Handling of Repository Methods: As of Spring Data 2.0, repository CRUD methods that return an individual aggregate instance use Java 8’s Optional to indicate the potential absence of a value. Besides that, Spring Data supports returning the following wrapper types on query methods: com.google.common.base.Optional scala.Option io.vavr.control.Option Alternatively, query methods can choose not to use a wrapper type at all. The absence of a query result is then indicated by returning null . Repository methods returning collections, collection alternatives, wrappers, and streams are guaranteed never to return null but rather the corresponding empty representation. See “ Repository query return types(query-return-types-reference.html) ” for details. Nullability Annotations: You can express nullability constraints for repository methods by using Spring Framework’s nullability annotations(https://docs.spring.io/spring-framework/reference/6.1/core/null-safety.html) . They provide a tooling-friendly approach and opt-in null checks during runtime, as follows: @NonNullApi(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNullApi.html) : Used on the package level to declare that the default behavior for parameters and return values is, respectively, neither to accept nor to produce null values. @NonNull(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNull.html) : Used on a parameter or return value that must not be null (not needed on a parameter and return value where @NonNullApi applies). @Nullable(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/Nullable.html) : Used on a parameter or return value that can be null . Spring annotations are meta-annotated with JSR 305(https://jcp.org/en/jsr/detail?id=305) annotations (a dormant but widely used JSR). JSR 305 meta-annotations let tooling vendors (such as IDEA(https://www.jetbrains.com/help/idea/nullable-and-notnull-annotations.html) , Eclipse(https://help.eclipse.org/latest/index.jsp?topic=/org.eclipse.jdt.doc.user/tasks/task-using_external_null_annotations.htm) , and Kotlin(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) ) provide null-safety support in a generic way, without having to hard-code support for Spring annotations. To enable runtime checking of nullability constraints for query methods, you need to activate non-nullability on the package level by using Spring’s @NonNullApi in package-info.java , as shown in the following example: Declaring Non-nullability in package-info.java @org.springframework.lang.NonNullApi package com.acme; Once non-null defaulting is in place, repository query method invocations get validated at runtime for nullability constraints. If a query result violates the defined constraint, an exception is thrown. This happens when the method would return null but is declared as non-nullable (the default with the annotation defined on the package in which the repository resides). If you want to opt-in to nullable results again, selectively use @Nullable on individual methods. Using the result wrapper types mentioned at the start of this section continues to work as expected: an empty result is translated into the value that represents absence. The following example shows a number of the techniques just described: Using different nullability constraints package com.acme; (1) import org.springframework.lang.Nullable; interface UserRepository extends Repository<User, Long> { User getByEmailAddress(EmailAddress emailAddress); (2) @Nullable User findByEmailAddress(@Nullable EmailAddress emailAdress); (3) Optional<User> findOptionalByEmailAddress(EmailAddress emailAddress); (4) } 1 The repository resides in a package (or sub-package) for which we have defined non-null behavior. 2 Throws an EmptyResultDataAccessException when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . 3 Returns null when the query does not produce a result. Also accepts null as the value for emailAddress . 4 Returns Optional.empty() when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . Nullability in Kotlin-based Repositories: Kotlin has the definition of nullability constraints(https://kotlinlang.org/docs/reference/null-safety.html) baked into the language. Kotlin code compiles to bytecode, which does not express nullability constraints through method signatures but rather through compiled-in metadata. Make sure to include the kotlin-reflect JAR in your project to enable introspection of Kotlin’s nullability constraints. Spring Data repositories use the language mechanism to define those constraints to apply the same runtime checks, as follows: Using nullability constraints on Kotlin repositories interface UserRepository : Repository<User, String> { fun findByUsername(username: String): User (1) fun findByFirstname(firstname: String?): User? (2) } 1 The method defines both the parameter and the result as non-nullable (the Kotlin default). The Kotlin compiler rejects method invocations that pass null to the method. If the query yields an empty result, an EmptyResultDataAccessException is thrown. 2 This method accepts null for the firstname parameter and returns null if the query does not produce a result."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/mongodb/repositories/cdi-integration.html","CDI Integration: Instances of the repository interfaces are usually created by a container, and Spring is the most natural choice when working with Spring Data. As of version 1.3.0, Spring Data MongoDB ships with a custom CDI extension that lets you use the repository abstraction in CDI environments. The extension is part of the JAR. To activate it, drop the Spring Data MongoDB JAR into your classpath. You can now set up the infrastructure by implementing a CDI Producer for the MongoTemplate , as the following example shows: class MongoTemplateProducer { @Produces @ApplicationScoped public MongoOperations createMongoTemplate() { MongoDatabaseFactory factory = new SimpleMongoClientDatabaseFactory(MongoClients.create(), ""database""); return new MongoTemplate(factory); } } The Spring Data MongoDB CDI extension picks up the MongoTemplate available as a CDI bean and creates a proxy for a Spring Data repository whenever a bean of a repository type is requested by the container. Thus, obtaining an instance of a Spring Data repository is a matter of declaring an @Inject -ed property, as the following example shows: class RepositoryClient { @Inject PersonRepository repository; public void businessMethod() { List<Person> people = repository.findAll(); } }"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/observability/observability.html","Observability: Spring Data MongoDB currently has the most up-to-date code to support Observability in your MongoDB application. These changes, however, haven’t been picked up by Spring Boot (yet). Until those changes are applied, if you wish to use Spring Data MongoDB’s flavor of Observability, you must carry out the following steps. First of all, you must opt into Spring Data MongoDB’s configuration settings by customizing MongoClientSettings through either your @SpringBootApplication class or one of your configuration classes. Example 1. Registering MongoDB Micrometer customizer setup @Bean MongoClientSettingsBuilderCustomizer mongoMetricsSynchronousContextProvider(ObservationRegistry registry) { return (clientSettingsBuilder) -> { clientSettingsBuilder.contextProvider(ContextProviderFactory.create(registry)) .addCommandListener(new MongoObservationCommandListener(registry)); }; } Your project must include Spring Boot Actuator . Disable Spring Boot’s autoconfigured MongoDB command listener and enable tracing manually by adding the following properties to your application.properties Example 2. Custom settings to apply # Disable Spring Boot's autoconfigured tracing management.metrics.mongo.command.enabled=false # Enable it manually management.tracing.enabled=true Be sure to add any other relevant settings needed to configure the tracer you are using based upon Micrometer’s reference documentation. This should do it! You are now running with Spring Data MongoDB’s usage of Spring Observability’s Observation API. See also OpenTelemetry Semantic Conventions(https://opentelemetry.io/docs/reference/specification/trace/semantic_conventions/database/#mongodb) for further reference."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/observability/conventions.html","Conventions: Below you can find a list of all GlobalObservationConvention and ObservationConvention declared by this project. Table 1. ObservationConvention implementations ObservationConvention Class Name Applicable ObservationContext Class Name org.springframework.data.mongodb.observability.DefaultMongoHandlerObservationConvention MongoHandlerContext org.springframework.data.mongodb.observability.MongoHandlerObservationConvention MongoHandlerContext"
"https://docs.spring.io/spring-data/mongodb/reference/4.3/observability/metrics.html","Metrics: Below you can find a list of all metrics declared by this project. Mongodb Command Observation: Timer created around a MongoDB command execution. Metric name spring.data.mongodb.command . Type timer . Metric name spring.data.mongodb.command.active . Type long task timer . KeyValues that are added after starting the Observation might be missing from the *.active metrics. Micrometer internally uses nanoseconds for the baseunit. However, each backend determines the actual baseunit. (i.e. Prometheus uses seconds) Fully qualified name of the enclosing class org.springframework.data.mongodb.observability.MongoObservation . Table 1. Low cardinality Keys Name Description db.connection_string (required) MongoDB connection string. db.mongodb.collection (required) MongoDB collection name. db.name (required) MongoDB database name. db.operation (required) MongoDB command value. db.system (required) MongoDB database system. db.user (required) MongoDB user. net.peer.name (required) Name of the database host. net.peer.port (required) Logical remote port number. net.sock.peer.addr (required) Mongo peer address. net.sock.peer.port (required) Mongo peer port. net.transport (required) Network transport. spring.data.mongodb.cluster_id (required) MongoDB cluster identifier."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/observability/spans.html","Spans: Below you can find a list of all spans declared by this project. Mongodb Command Observation Span: Timer created around a MongoDB command execution. Span name spring.data.mongodb.command . Fully qualified name of the enclosing class org.springframework.data.mongodb.observability.MongoObservation . Table 1. Tag Keys Name Description db.connection_string (required) MongoDB connection string. db.mongodb.collection (required) MongoDB collection name. db.name (required) MongoDB database name. db.operation (required) MongoDB command value. db.system (required) MongoDB database system. db.user (required) MongoDB user. net.peer.name (required) Name of the database host. net.peer.port (required) Logical remote port number. net.sock.peer.addr (required) Mongo peer address. net.sock.peer.port (required) Mongo peer port. net.transport (required) Network transport. spring.data.mongodb.cluster_id (required) MongoDB cluster identifier."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/kotlin.html","Kotlin Support: Kotlin(https://kotlinlang.org) is a statically typed language that targets the JVM (and other platforms) which allows writing concise and elegant code while providing excellent interoperability(https://kotlinlang.org/docs/reference/java-interop.html) with existing libraries written in Java. Spring Data provides first-class support for Kotlin and lets developers write Kotlin applications almost as if Spring Data was a Kotlin native framework. The easiest way to build a Spring application with Kotlin is to leverage Spring Boot and its dedicated Kotlin support(https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-kotlin.html) . This comprehensive tutorial(https://spring.io/guides/tutorials/spring-boot-kotlin/) will teach you how to build Spring Boot applications with Kotlin using start.spring.io(https://start.spring.io/#!language=kotlin&type=gradle-project) ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/kotlin/requirements.html","Requirements: Spring Data supports Kotlin 1.3 and requires kotlin-stdlib (or one of its variants, such as kotlin-stdlib-jdk8 ) and kotlin-reflect to be present on the classpath. Those are provided by default if you bootstrap a Kotlin project via start.spring.io(https://start.spring.io/#!language=kotlin&type=gradle-project) ."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/kotlin/null-safety.html","Null Safety: One of Kotlin’s key features is null safety(https://kotlinlang.org/docs/null-safety.html) , which cleanly deals with null values at compile time. This makes applications safer through nullability declarations and the expression of “value or no value” semantics without paying the cost of wrappers, such as Optional . (Kotlin allows using functional constructs with nullable values. See this comprehensive guide to Kotlin null safety(https://www.baeldung.com/kotlin/null-safety) .) Although Java does not let you express null safety in its type system, Spring Data API is annotated with JSR-305(https://jcp.org/en/jsr/detail?id=305) tooling friendly annotations declared in the org.springframework.lang package. By default, types from Java APIs used in Kotlin are recognized as platform types(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) , for which null checks are relaxed. Kotlin support for JSR-305 annotations(https://kotlinlang.org/docs/reference/java-interop.html#jsr-305-support) and Spring nullability annotations provide null safety for the whole Spring Data API to Kotlin developers, with the advantage of dealing with null related issues at compile time. See Null Handling of Repository Methods(../repositories/null-handling.html) how null safety applies to Spring Data Repositories. You can configure JSR-305 checks by adding the -Xjsr305 compiler flag with the following options: -Xjsr305={strict|warn|ignore} . For Kotlin versions 1.1+, the default behavior is the same as -Xjsr305=warn . The strict value is required take Spring Data API null-safety into account. Kotlin types inferred from Spring API but should be used with the knowledge that Spring API nullability declaration could evolve, even between minor releases and that more checks may be added in the future. Generic type arguments, varargs, and array elements nullability are not supported yet, but should be in an upcoming release."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/kotlin/extensions.html","Extensions: Kotlin extensions(https://kotlinlang.org/docs/reference/extensions.html) provide the ability to extend existing classes with additional functionality. Spring Data Kotlin APIs use these extensions to add new Kotlin-specific conveniences to existing Spring APIs. Keep in mind that Kotlin extensions need to be imported to be used. Similar to static imports, an IDE should automatically suggest the import in most cases. For example, Kotlin reified type parameters(https://kotlinlang.org/docs/reference/inline-functions.html#reified-type-parameters) provide a workaround for JVM generics type erasure(https://docs.oracle.com/javase/tutorial/java/generics/erasure.html) , and Spring Data provides some extensions to take advantage of this feature. This allows for a better Kotlin API. To retrieve a list of SWCharacter objects in Java, you would normally write the following: Flux<SWCharacter> characters = template.query(SWCharacter.class).inTable(""star-wars"").all() With Kotlin and the Spring Data extensions, you can instead write the following: val characters = template.query<SWCharacter>().inTable(""star-wars"").all() // or (both are equivalent) val characters : Flux<SWCharacter> = template.query().inTable(""star-wars"").all() As in Java, characters in Kotlin is strongly typed, but Kotlin’s clever type inference allows for shorter syntax. Type-safe Queries for Kotlin: Kotlin embraces domain-specific language creation through its language syntax and its extension system. Spring Data MongoDB ships with a Kotlin Extension for Criteria using Kotlin property references(https://kotlinlang.org/docs/reference/reflection.html#property-references) to build type-safe queries. Queries using this extension are typically benefit from improved readability. Most keywords on Criteria have a matching Kotlin extension, such as inValues and regex . Consider the following example explaining Type-safe Queries: import org.springframework.data.mongodb.core.query.* mongoOperations.find<Book>( Query(Book::title isEqualTo ""Moby-Dick"") (1) ) mongoOperations.find<Book>( Query(titlePredicate = Book::title exists true) ) mongoOperations.find<Book>( Query( Criteria().andOperator( Book::price gt 5, Book::price lt 10 )) ) // Binary operators mongoOperations.find<BinaryMessage>( Query(BinaryMessage::payload bits { allClear(0b101) }) (2) ) // Nested Properties (i.e. refer to ""book.author"") mongoOperations.find<Book>( Query(Book::author / Author::name regex ""^H"") (3) ) 1 isEqualTo() is an infix extension function with receiver type KProperty<T> that returns Criteria . 2 For bitwise operators, pass a lambda argument where you call one of the methods of Criteria.BitwiseCriteriaOperators . 3 To construct nested properties, use the / character (overloaded operator div )."
"https://docs.spring.io/spring-data/mongodb/reference/4.3/kotlin/coroutines.html","Coroutines: Kotlin Coroutines(https://kotlinlang.org/docs/reference/coroutines-overview.html) are instances of suspendable computations allowing to write non-blocking code imperatively. On language side, suspend functions provides an abstraction for asynchronous operations while on library side kotlinx.coroutines(https://github.com/Kotlin/kotlinx.coroutines) provides functions like async { }(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/async.html) and types like Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) . Spring Data modules provide support for Coroutines on the following scope: Deferred(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/-deferred/index.html) and Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) return values support in Kotlin extensions Dependencies: Coroutines support is enabled when kotlinx-coroutines-core , kotlinx-coroutines-reactive and kotlinx-coroutines-reactor dependencies are in the classpath: Dependencies to add in Maven pom.xml <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-core</artifactId> </dependency> <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-reactive</artifactId> </dependency> <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-reactor</artifactId> </dependency> Supported versions 1.3.0 and above. How Reactive translates to Coroutines?: For return values, the translation from Reactive to Coroutines APIs is the following: fun handler(): Mono<Void> becomes suspend fun handler() fun handler(): Mono<T> becomes suspend fun handler(): T or suspend fun handler(): T? depending on if the Mono can be empty or not (with the advantage of being more statically typed) fun handler(): Flux<T> becomes fun handler(): Flow<T> Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) is Flux equivalent in Coroutines world, suitable for hot or cold stream, finite or infinite streams, with the following main differences: Flow is push-based while Flux is push-pull hybrid Backpressure is implemented via suspending functions Flow has only a single suspending collect method(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/collect.html) and operators are implemented as extensions(https://kotlinlang.org/docs/reference/extensions.html) Operators are easy to implement(https://github.com/Kotlin/kotlinx.coroutines/tree/master/kotlinx-coroutines-core/common/src/flow/operators) thanks to Coroutines Extensions allow adding custom operators to Flow Collect operations are suspending functions map operator(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/map.html) supports asynchronous operation (no need for flatMap ) since it takes a suspending function parameter Read this blog post about Going Reactive with Spring, Coroutines and Kotlin Flow(https://spring.io/blog/2019/04/12/going-reactive-with-spring-coroutines-and-kotlin-flow) for more details, including how to run code concurrently with Coroutines. Repositories: Here is an example of a Coroutines repository: interface CoroutineRepository : CoroutineCrudRepository<User, String> { suspend fun findOne(id: String): User fun findByFirstname(firstname: String): Flow<User> suspend fun findAllByFirstname(id: String): List<User> } Coroutines repositories are built on reactive repositories to expose the non-blocking nature of data access through Kotlin’s Coroutines. Methods on a Coroutines repository can be backed either by a query method or a custom implementation. Invoking a custom implementation method propagates the Coroutines invocation to the actual implementation method if the custom method is suspend -able without requiring the implementation method to return a reactive type such as Mono or Flux . Note that depending on the method declaration the coroutine context may or may not be available. To retain access to the context, either declare your method using suspend or return a type that enables context propagation such as Flow . suspend fun findOne(id: String): User : Retrieve the data once and synchronously by suspending. fun findByFirstname(firstname: String): Flow<User> : Retrieve a stream of data. The Flow is created eagerly while data is fetched upon Flow interaction ( Flow.collect(…) ). fun getUser(): User : Retrieve data once blocking the thread and without context propagation. This should be avoided. Coroutines repositories are only discovered when the repository extends the CoroutineCrudRepository interface."
"https://docs.spring.io/spring-data/redis/reference/3.3/index.html","Spring Data Redis: Spring Data Redis provides Redis connectivity and repository support for the Redis database. It eases development of applications with a consistent programming model that need to access Redis data sources. Redis(redis.html) Redis support and connectivity Repositories(repositories.html) Redis Repositories Observability(observability.html) Observability Integration Wiki(https://github.com/spring-projects/spring-data-commons/wiki) What’s New, Upgrade Notes, Supported Versions, additional cross-version information. Costin Leau, Jennifer Hickey, Christoph Strobl, Thomas Darimont, Mark Paluch, Jay Bryant © 2008-2024 VMware, Inc. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically."
"https://docs.spring.io/spring-data/redis/reference/3.3/commons/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped. Once you’ve decided to upgrade your application, you can find detailed information regarding specific features in the rest of the document. You can find migration guides(../upgrading.html#redis.upgrading) specific to major version migrations at the end of this document. Spring Data’s documentation is specific to that version, so any information that you find in here will contain the most up-to-date changes that are in that version."
"https://docs.spring.io/spring-data/redis/reference/3.3/upgrading.html","Migration Guides: This section contains details about migration steps, deprecations, and removals. Upgrading from 2.x to 3.x: Re-/moved Types: Type Replacement o.s.d.redis.Version o.s.d.util.Version o.s.d.redis.VersionParser - o.s.d.redis.connection.RedisZSetCommands.Aggregate o.s.d.redis.connection.zset.Aggregate o.s.d.redis.connection.RedisZSetCommands.Tuple o.s.d.redis.connection.zset.Tuple o.s.d.redis.connection.RedisZSetCommands.Weights o.s.d.redis.connection.zset.Weights o.s.d.redis.connection.RedisZSetCommands.Range o.s.d.domain.Range o.s.d.redis.connection.RedisZSetCommands.Limit o.s.d.redis.connection.Limit.java o.s.d.redis.connection.jedis.JedisUtils - o.s.d.redis.connection.jedis.JedisVersionUtil - o.s.d.redis.core.convert.CustomConversions o.s.d.convert.CustomConversions Changed Methods and Types: Table 1. Core Type Method Replacement o.s.d.redis.core.Cursor open - o.s.d.redis.core.RedisTemplate execute doWithKeys o.s.d.redis.stream.StreamMessageListenerContainer isAutoAck isAutoAcknowledge o.s.d.redis.stream.StreamMessageListenerContainer autoAck autoAcknowledge Table 2. Redis Connection Type Method Replacement o.s.d.redis.connection.ClusterCommandExecutionFailureException getCauses getSuppressed o.s.d.redis.connection.RedisConnection bgWriteAof bgReWriteAof o.s.d.redis.connection.RedisConnection slaveOf replicaOf o.s.d.redis.connection.RedisConnection slaveOfNoOne replicaOfNoOne o.s.d.redis.connection.ReactiveClusterCommands clusterGetSlaves clusterGetReplicas o.s.d.redis.connection.ReactiveClusterCommands clusterGetMasterSlaveMap clusterGetMasterReplicaMap o.s.d.redis.connection.ReactiveKeyCommands getNewName getNewKey o.s.d.redis.connection.RedisClusterNode.Flag SLAVE REPLICA o.s.d.redis.connection.RedisClusterNode.Builder slaveOf replicaOf o.s.d.redis.connection.RedisNode isSlave isReplica o.s.d.redis.connection.RedisSentinelCommands slaves replicas o.s.d.redis.connection.RedisServer getNumberSlaves getNumberReplicas o.s.d.redis.connection.RedisServerCommands slaveOf replicaOf o.s.d.redis.core.ClusterOperations getSlaves getReplicas o.s.d.redis.core.RedisOperations slaveOf replicaOf Table 3. Redis Operations Type Method Replacement o.s.d.redis.core.GeoOperations & BoundGeoOperations geoAdd add o.s.d.redis.core.GeoOperations & BoundGeoOperations geoDist distance o.s.d.redis.core.GeoOperations & BoundGeoOperations geoHash hash o.s.d.redis.core.GeoOperations & BoundGeoOperations geoPos position o.s.d.redis.core.GeoOperations & BoundGeoOperations geoRadius radius o.s.d.redis.core.GeoOperations & BoundGeoOperations geoRadiusByMember radius o.s.d.redis.core.GeoOperations & BoundGeoOperations geoRemove remove Table 4. Redis Cache Type Method Replacement o.s.d.redis.cache.RedisCacheConfiguration prefixKeysWith prefixCacheNameWith o.s.d.redis.cache.RedisCacheConfiguration getKeyPrefix getKeyPrefixFor Jedis: Please read the Jedis upgrading guide(https://github.com/redis/jedis/blob/v4.0.0/docs/3to4.md) which covers important driver changes. Table 5. Jedis Redis Connection Type Method Replacement o.s.d.redis.connection.jedis.JedisConnectionFactory getShardInfo can be obtained via JedisClientConfiguration o.s.d.redis.connection.jedis.JedisConnectionFactory setShardInfo can be set via JedisClientConfiguration o.s.d.redis.connection.jedis.JedisConnectionFactory createCluster now requires a Connection instead of Jedis instance o.s.d.redis.connection.jedis.JedisConverters has package visibility now o.s.d.redis.connection.jedis.JedisConverters tuplesToTuples - o.s.d.redis.connection.jedis.JedisConverters tuplesToTuples - o.s.d.redis.connection.jedis.JedisConverters stringListToByteList - o.s.d.redis.connection.jedis.JedisConverters stringSetToByteSet - o.s.d.redis.connection.jedis.JedisConverters stringMapToByteMap - o.s.d.redis.connection.jedis.JedisConverters tupleSetToTupleSet - o.s.d.redis.connection.jedis.JedisConverters toTupleSet - o.s.d.redis.connection.jedis.JedisConverters toDataAccessException o.s.d.redis.connection.jedis.JedisExceptionConverter#convert Transactions / Pipelining: Pipelining and Transactions are now mutually exclusive. The usage of server or connection commands in pipeline/transactions mode is no longer possible. Lettuce: Lettuce Pool: LettucePool and its implementation DefaultLettucePool have been removed without replacement. Please refer to the driver documentation(https://lettuce.io/core/release/reference/index.html#_connection_pooling) for driver native pooling capabilities. Methods accepting pooling parameters have been updated. This effects methods on LettuceConnectionFactory and LettuceConnection . Lettuce Authentication: AuthenticatingRedisClient has been removed without replacement. Please refer to the driver documentation(https://lettuce.io/core/release/reference/index.html#basic.redisuri) for RedisURI to set authentication data."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis.html","Redis: One of the key-value stores supported by Spring Data is Redis(https://redis.io) . To quote the Redis project home page: Redis is an advanced key-value store. It is similar to memcached but the dataset is not volatile, and values can be strings, exactly like in memcached, but also lists, sets, and ordered sets. All this data types can be manipulated with atomic operations to push/pop elements, add/remove elements, perform server side union, intersection, difference between sets, and so forth. Redis supports different kind of sorting abilities. Spring Data Redis provides easy configuration and access to Redis from Spring applications. It offers both low-level and high-level abstractions for interacting with the store, freeing the user from infrastructural concerns. Spring Data support for Redis contains a wide range of features: RedisTemplate and ReactiveRedisTemplate helper class(redis/template.html) that increases productivity when performing common Redis operations. Includes integrated serialization between objects and values. Exception translation into Spring’s portable Data Access Exception hierarchy. Automatic implementation of Repository interfaces(repositories.html) , including support for custom query methods. Feature-rich Object Mapping(redis/redis-repositories/mapping.html) integrated with Spring’s Conversion Service. Annotation-based mapping metadata that is extensible to support other metadata formats. Transactions(redis/transactions.html) and Pipelining(redis/pipelining.html) . Redis Cache(redis/redis-cache.html) integration through Spring’s Cache abstraction. Redis Pub/Sub Messaging(redis/pubsub.html) and Redis Stream(redis/redis-streams.html) Listeners. Redis Collection Implementations(redis/support-classes.html) for Java such as RedisList or RedisSet . Why Spring Data Redis?: The Spring Framework is the leading full-stack Java/JEE application framework. It provides a lightweight container and a non-invasive programming model enabled by the use of dependency injection, AOP, and portable service abstractions. NoSQL(https://en.wikipedia.org/wiki/NoSQL) storage systems provide an alternative to classical RDBMS for horizontal scalability and speed. In terms of implementation, key-value stores represent one of the largest (and oldest) members in the NoSQL space. The Spring Data Redis (SDR) framework makes it easy to write Spring applications that use the Redis key-value store by eliminating the redundant tasks and boilerplate code required for interacting with the store through Spring’s excellent infrastructure support. Redis Support High-level View: The Redis support provides several components.For most tasks, the high-level abstractions and support services are the best choice.Note that, at any point, you can move between layers.For example, you can get a low-level connection (or even the native library) to communicate directly with Redis. Section Summary: Getting Started(redis/getting-started.html) Drivers(redis/drivers.html) Connection Modes(redis/connection-modes.html) RedisTemplate(redis/template.html) Redis Cache(redis/redis-cache.html) Redis Cluster(redis/cluster.html) Hash Mapping(redis/hash-mappers.html) Pub/Sub Messaging(redis/pubsub.html) Redis Streams(redis/redis-streams.html) Scripting(redis/scripting.html) Redis Transactions(redis/transactions.html) Pipelining(redis/pipelining.html) Support Classes(redis/support-classes.html)"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/getting-started.html","Getting Started: An easy way to bootstrap setting up a working environment is to create a Spring-based project via start.spring.io(https://start.spring.io/#!type=maven-project&dependencies=data-redis) or create a Spring project in Spring Tools(https://spring.io/tools) . Examples Repository: The GitHub spring-data-examples repository(https://github.com/spring-projects/spring-data-examples) hosts several examples that you can download and play around with to get a feel for how the library works. Hello World: First, you need to set up a running Redis server. Spring Data Redis requires Redis 2.6 or above and Spring Data Redis integrates with Lettuce(https://github.com/lettuce-io/lettuce-core) and Jedis(https://github.com/redis/jedis) , two popular open-source Java libraries for Redis. Now you can create a simple Java application that stores and reads a value to and from Redis. Create the main application to run, as the following example shows: Imperative Reactive import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.StringRedisSerializer; public class RedisApplication { private static final Log LOG = LogFactory.getLog(RedisApplication.class); public static void main(String[] args) { LettuceConnectionFactory connectionFactory = new LettuceConnectionFactory(); connectionFactory.afterPropertiesSet(); RedisTemplate<String, String> template = new RedisTemplate<>(); template.setConnectionFactory(connectionFactory); template.setDefaultSerializer(StringRedisSerializer.UTF_8); template.afterPropertiesSet(); template.opsForValue().set(""foo"", ""bar""); LOG.info(""Value at foo:"" + template.opsForValue().get(""foo"")); connectionFactory.destroy(); } } import reactor.core.publisher.Mono; import java.time.Duration; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory; import org.springframework.data.redis.core.ReactiveRedisTemplate; import org.springframework.data.redis.serializer.RedisSerializationContext; public class ReactiveRedisApplication { private static final Log LOG = LogFactory.getLog(ReactiveRedisApplication.class); public static void main(String[] args) { LettuceConnectionFactory connectionFactory = new LettuceConnectionFactory(); connectionFactory.afterPropertiesSet(); ReactiveRedisTemplate<String, String> template = new ReactiveRedisTemplate<>(connectionFactory, RedisSerializationContext.string()); Mono<Boolean> set = template.opsForValue().set(""foo"", ""bar""); set.block(Duration.ofSeconds(10)); LOG.info(""Value at foo:"" + template.opsForValue().get(""foo"").block(Duration.ofSeconds(10))); connectionFactory.destroy(); } } Even in this simple example, there are a few notable things to point out: You can create an instance of RedisTemplate(../api/java/org/springframework/data/redis/core/RedisTemplate.html) (or ReactiveRedisTemplate(../api/java/org/springframework/data/redis/core/ReactiveRedisTemplate.html) for reactive usage) with a RedisConnectionFactory(../api/java/org/springframework/data/redis/connection/RedisConnectionFactory.html) . Connection factories are an abstraction on top of the supported drivers. There’s no single way to use Redis as it comes with support for a wide range of data structures such as plain keys (""strings""), lists, sets, sorted sets, streams, hashes and so on."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/drivers.html","Drivers: One of the first tasks when using Redis and Spring is to connect to the store through the IoC container. To do that, a Java connector (or binding) is required. No matter the library you choose, you need to use only one set of Spring Data Redis APIs (which behaves consistently across all connectors). The org.springframework.data.redis.connection package and its RedisConnection and RedisConnectionFactory interfaces for working with and retrieving active connections to Redis. RedisConnection and RedisConnectionFactory: RedisConnection provides the core building block for Redis communication, as it handles the communication with the Redis backend. It also automatically translates underlying connecting library exceptions to Spring’s consistent DAO exception hierarchy(https://docs.spring.io/spring-framework/reference/6.1/data-access.html#dao-exceptions) so that you can switch connectors without any code changes, as the operation semantics remain the same. For the corner cases where the native library API is required, RedisConnection provides a dedicated method ( getNativeConnection ) that returns the raw, underlying object used for communication. Active RedisConnection objects are created through RedisConnectionFactory . In addition, the factory acts as PersistenceExceptionTranslator objects, meaning that, once declared, they let you do transparent exception translation. For example, you can do exception translation through the use of the @Repository annotation and AOP. For more information, see the dedicated section(https://docs.spring.io/spring-framework/reference/6.1/data-access.html#orm-exception-translation) in the Spring Framework documentation. RedisConnection classes are not Thread-safe. While the underlying native connection, such as Lettuce’s StatefulRedisConnection , may be Thread-safe, Spring Data Redis’s LettuceConnection class itself is not. Therefore, you should not share instances of a RedisConnection across multiple Threads. This is especially true for transactional, or blocking Redis operations and commands, such as BLPOP . In transactional and pipelining operations, for instance, RedisConnection holds onto unguarded mutable state to complete the operation correctly, thereby making it unsafe to use with multiple Threads. This is by design. If you need to share (stateful) Redis resources, like connections, across multiple Threads, for performance reasons or otherwise, then you should acquire the native connection and use the Redis client library (driver) API directly. Alternatively, you can use the RedisTemplate , which acquires and manages connections for operations (and Redis commands) in a Thread-safe manner. See documentation(template.html) on RedisTemplate for more details. Depending on the underlying configuration, the factory can return a new connection or an existing connection (when a pool or shared native connection is used). The easiest way to work with a RedisConnectionFactory is to configure the appropriate connector through the IoC container and inject it into the using class. Unfortunately, currently, not all connectors support all Redis features. When invoking a method on the Connection API that is unsupported by the underlying library, an UnsupportedOperationException is thrown. The following overview explains features that are supported by the individual Redis connectors: Table 1. Feature Availability across Redis Connectors Supported Feature Lettuce Jedis Standalone Connections X X Master/Replica Connections(../redis.html#redis:write-to-master-read-from-replica) X Redis Sentinel(../redis.html#redis:sentinel) Master Lookup, Sentinel Authentication, Replica Reads Master Lookup Redis Cluster(cluster.html) Cluster Connections, Cluster Node Connections, Replica Reads Cluster Connections, Cluster Node Connections Transport Channels TCP, OS-native TCP (epoll, kqueue), Unix Domain Sockets TCP Connection Pooling X (using commons-pool2 ) X (using commons-pool2 ) Other Connection Features Singleton-connection sharing for non-blocking commands Pipelining and Transactions mutually exclusive. Cannot use server/connection commands in pipeline/transactions. SSL Support X X Pub/Sub(pubsub.html) X X Pipelining(pipelining.html) X X (Pipelining and Transactions mutually exclusive) Transactions(transactions.html) X X (Pipelining and Transactions mutually exclusive) Datatype support Key, String, List, Set, Sorted Set, Hash, Server, Stream, Scripting, Geo, HyperLogLog Key, String, List, Set, Sorted Set, Hash, Server, Stream, Scripting, Geo, HyperLogLog Reactive (non-blocking) API X Configuring the Lettuce Connector: Lettuce(https://github.com/lettuce-io/lettuce-core) is a Netty(https://netty.io/) -based open-source connector supported by Spring Data Redis through the org.springframework.data.redis.connection.lettuce package. Add the following to the pom.xml files dependencies element: <dependencies> <!-- other dependency elements omitted --> <dependency> <groupId>io.lettuce</groupId> <artifactId>lettuce-core</artifactId> <version>6.3.2.RELEASE</version> </dependency> </dependencies> The following example shows how to create a new Lettuce connection factory: @Configuration class AppConfig { @Bean public LettuceConnectionFactory redisConnectionFactory() { return new LettuceConnectionFactory(new RedisStandaloneConfiguration(""server"", 6379)); } } There are also a few Lettuce-specific connection parameters that can be tweaked. By default, all LettuceConnection instances created by the LettuceConnectionFactory share the same thread-safe native connection for all non-blocking and non-transactional operations. To use a dedicated connection each time, set shareNativeConnection to false . LettuceConnectionFactory can also be configured to use a LettucePool for pooling blocking and transactional connections or all connections if shareNativeConnection is set to false . The following example shows a more sophisticated configuration, including SSL and timeouts, that uses LettuceClientConfigurationBuilder : @Bean public LettuceConnectionFactory lettuceConnectionFactory() { LettuceClientConfiguration clientConfig = LettuceClientConfiguration.builder() .useSsl().and() .commandTimeout(Duration.ofSeconds(2)) .shutdownTimeout(Duration.ZERO) .build(); return new LettuceConnectionFactory(new RedisStandaloneConfiguration(""localhost"", 6379), clientConfig); } For more detailed client configuration tweaks, see LettuceClientConfiguration(../api/java/org/springframework/data/redis/connection/lettuce/LettuceClientConfiguration.html) . Lettuce integrates with Netty’s native transports(https://netty.io/wiki/native-transports.html) , letting you use Unix domain sockets to communicate with Redis. Make sure to include the appropriate native transport dependencies that match your runtime environment. The following example shows how to create a Lettuce Connection factory for a Unix domain socket at /var/run/redis.sock : @Configuration class AppConfig { @Bean public LettuceConnectionFactory redisConnectionFactory() { return new LettuceConnectionFactory(new RedisSocketConfiguration(""/var/run/redis.sock"")); } } Netty currently supports the epoll (Linux) and kqueue (BSD/macOS) interfaces for OS-native transport. Configuring the Jedis Connector: Jedis(https://github.com/redis/jedis) is a community-driven connector supported by the Spring Data Redis module through the org.springframework.data.redis.connection.jedis package. Add the following to the pom.xml files dependencies element: <dependencies> <!-- other dependency elements omitted --> <dependency> <groupId>redis.clients</groupId> <artifactId>jedis</artifactId> <version>5.0.2</version> </dependency> </dependencies> In its simplest form, the Jedis configuration looks as follow: @Configuration class AppConfig { @Bean public JedisConnectionFactory redisConnectionFactory() { return new JedisConnectionFactory(); } } For production use, however, you might want to tweak settings such as the host or password, as shown in the following example: @Configuration class RedisConfiguration { @Bean public JedisConnectionFactory redisConnectionFactory() { RedisStandaloneConfiguration config = new RedisStandaloneConfiguration(""server"", 6379); return new JedisConnectionFactory(config); } }"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/connection-modes.html","Connection Modes: Redis can be operated in various setups. Each mode of operation requires specific configuration that is explained in the following sections. Redis Standalone: The easiest way to get started is by using Redis Standalone with a single Redis server, Configure LettuceConnectionFactory(../api/java/org/springframework/data/redis/connection/lettuce/LettuceConnectionFactory.html) or JedisConnectionFactory(../api/java/org/springframework/data/redis/connection/jedis/JedisConnectionFactory.html) , as shown in the following example: @Configuration class RedisStandaloneConfiguration { /** * Lettuce */ @Bean public RedisConnectionFactory lettuceConnectionFactory() { return new LettuceConnectionFactory(new RedisStandaloneConfiguration(""server"", 6379)); } /** * Jedis */ @Bean public RedisConnectionFactory jedisConnectionFactory() { return new JedisConnectionFactory(new RedisStandaloneConfiguration(""server"", 6379)); } } Write to Master, Read from Replica: The Redis Master/Replica setup — without automatic failover (for automatic failover see: Sentinel(#redis:sentinel) ) — not only allows data to be safely stored at more nodes. It also allows, by using Lettuce(drivers.html#redis:connectors:lettuce) , reading data from replicas while pushing writes to the master. You can set the read/write strategy to be used by using LettuceClientConfiguration , as shown in the following example: @Configuration class WriteToMasterReadFromReplicaConfiguration { @Bean public LettuceConnectionFactory redisConnectionFactory() { LettuceClientConfiguration clientConfig = LettuceClientConfiguration.builder() .readFrom(REPLICA_PREFERRED) .build(); RedisStandaloneConfiguration serverConfig = new RedisStandaloneConfiguration(""server"", 6379); return new LettuceConnectionFactory(serverConfig, clientConfig); } } For environments reporting non-public addresses through the INFO command (for example, when using AWS), use RedisStaticMasterReplicaConfiguration(../api/java/org/springframework/data/redis/connection/RedisStaticMasterReplicaConfiguration.html) instead of RedisStandaloneConfiguration(../api/java/org/springframework/data/redis/connection/RedisStandaloneConfiguration.html) . Please note that RedisStaticMasterReplicaConfiguration does not support Pub/Sub because of missing Pub/Sub message propagation across individual servers. Redis Sentinel: For dealing with high-availability Redis, Spring Data Redis has support for Redis Sentinel(https://redis.io/topics/sentinel) , using RedisSentinelConfiguration(../api/java/org/springframework/data/redis/connection/RedisSentinelConfiguration.html) , as shown in the following example: /** * Lettuce */ @Bean public RedisConnectionFactory lettuceConnectionFactory() { RedisSentinelConfiguration sentinelConfig = new RedisSentinelConfiguration() .master(""mymaster"") .sentinel(""127.0.0.1"", 26379) .sentinel(""127.0.0.1"", 26380); return new LettuceConnectionFactory(sentinelConfig); } /** * Jedis */ @Bean public RedisConnectionFactory jedisConnectionFactory() { RedisSentinelConfiguration sentinelConfig = new RedisSentinelConfiguration() .master(""mymaster"") .sentinel(""127.0.0.1"", 26379) .sentinel(""127.0.0.1"", 26380); return new JedisConnectionFactory(sentinelConfig); } RedisSentinelConfiguration can also be defined through RedisSentinelConfiguration.of(PropertySource) , which lets you pick up the following properties: Configuration Properties spring.redis.sentinel.master : name of the master node. spring.redis.sentinel.nodes : Comma delimited list of host:port pairs. spring.redis.sentinel.username : The username to apply when authenticating with Redis Sentinel (requires Redis 6) spring.redis.sentinel.password : The password to apply when authenticating with Redis Sentinel spring.redis.sentinel.dataNode.username : The username to apply when authenticating with Redis Data Node spring.redis.sentinel.dataNode.password : The password to apply when authenticating with Redis Data Node spring.redis.sentinel.dataNode.database : The database index to apply when authenticating with Redis Data Node Sometimes, direct interaction with one of the Sentinels is required. Using RedisConnectionFactory.getSentinelConnection() or RedisConnection.getSentinelCommands() gives you access to the first active Sentinel configured. Redis Cluster: Cluster support(cluster.html) is based on the same building blocks as non-clustered communication. RedisClusterConnection(../api/java/org/springframework/data/redis/connection/RedisClusterConnection.html) , an extension to RedisConnection , handles the communication with the Redis Cluster and translates errors into the Spring DAO exception hierarchy. RedisClusterConnection instances are created with the RedisConnectionFactory , which has to be set up with the associated RedisClusterConfiguration(../api/java/org/springframework/data/redis/connection/RedisClusterConfiguration.html) , as shown in the following example: Example 1. Sample RedisConnectionFactory Configuration for Redis Cluster @Component @ConfigurationProperties(prefix = ""spring.redis.cluster"") public class ClusterConfigurationProperties { /* * spring.redis.cluster.nodes[0] = 127.0.0.1:7379 * spring.redis.cluster.nodes[1] = 127.0.0.1:7380 * ... */ List<String> nodes; /** * Get initial collection of known cluster nodes in format {@code host:port}. * * @return */ public List<String> getNodes() { return nodes; } public void setNodes(List<String> nodes) { this.nodes = nodes; } } @Configuration public class AppConfig { /** * Type safe representation of application.properties */ @Autowired ClusterConfigurationProperties clusterProperties; public @Bean RedisConnectionFactory connectionFactory() { return new LettuceConnectionFactory( new RedisClusterConfiguration(clusterProperties.getNodes())); } } RedisClusterConfiguration can also be defined through RedisClusterConfiguration.of(PropertySource) , which lets you pick up the following properties: Configuration Properties spring.redis.cluster.nodes : Comma-delimited list of host:port pairs. spring.redis.cluster.max-redirects : Number of allowed cluster redirections. The initial configuration points driver libraries to an initial set of cluster nodes. Changes resulting from live cluster reconfiguration are kept only in the native driver and are not written back to the configuration."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/template.html","Working with Objects through RedisTemplate: Most users are likely to use RedisTemplate(../api/java/org/springframework/data/redis/core/RedisTemplate.html) and its corresponding package, org.springframework.data.redis.core or its reactive variant ReactiveRedisTemplate(../api/java/org/springframework/data/redis/core/ReactiveRedisTemplate.html) . The template is, in fact, the central class of the Redis module, due to its rich feature set. The template offers a high-level abstraction for Redis interactions. While [Reactive]RedisConnection offers low-level methods that accept and return binary values ( byte arrays), the template takes care of serialization and connection management, freeing the user from dealing with such details. The RedisTemplate(../api/java/org/springframework/data/redis/core/RedisTemplate.html) class implements the RedisOperations(../api/java/org/springframework/data/redis/core/RedisOperations.html) interface and its reactive variant ReactiveRedisTemplate(../api/java/org/springframework/data/redis/core/ReactiveRedisTemplate.html) implements ReactiveRedisOperations(../api/java/org/springframework/data/redis/core/ReactiveRedisOperations.html) . The preferred way to reference operations on a [Reactive]RedisTemplate instance is through the [Reactive]RedisOperations interface. Moreover, the template provides operations views (following the grouping from the Redis command reference(https://redis.io/commands) ) that offer rich, generified interfaces for working against a certain type or certain key (through the KeyBound interfaces) as described in the following table: Operational views Imperative Reactive Interface Description Key Type Operations GeoOperations(../api/java/org/springframework/data/redis/core/GeoOperations.html) Redis geospatial operations, such as GEOADD , GEORADIUS ,…​ HashOperations(../api/java/org/springframework/data/redis/core/HashOperations.html) Redis hash operations HyperLogLogOperations(../api/java/org/springframework/data/redis/core/HyperLogLogOperations.html) Redis HyperLogLog operations, such as PFADD , PFCOUNT ,…​ ListOperations(../api/java/org/springframework/data/redis/core/ListOperations.html) Redis list operations SetOperations(../api/java/org/springframework/data/redis/core/SetOperations.html) Redis set operations ValueOperations(../api/java/org/springframework/data/redis/core/ValueOperations.html) Redis string (or value) operations ZSetOperations(../api/java/org/springframework/data/redis/core/ZSetOperations.html) Redis zset (or sorted set) operations Key Bound Operations BoundGeoOperations(../api/java/org/springframework/data/redis/core/BoundGeoOperations.html) Redis key bound geospatial operations BoundHashOperations(../api/java/org/springframework/data/redis/core/BoundHashOperations.html) Redis hash key bound operations BoundKeyOperations(../api/java/org/springframework/data/redis/core/BoundKeyOperations.html) Redis key bound operations BoundListOperations(../api/java/org/springframework/data/redis/core/BoundListOperations.html) Redis list key bound operations BoundSetOperations(../api/java/org/springframework/data/redis/core/BoundSetOperations.html) Redis set key bound operations BoundValueOperations(../api/java/org/springframework/data/redis/core/BoundValueOperations.html) Redis string (or value) key bound operations BoundZSetOperations(../api/java/org/springframework/data/redis/core/BoundZSetOperations.html) Redis zset (or sorted set) key bound operations Interface Description Key Type Operations ReactiveGeoOperations(../api/java/org/springframework/data/redis/core/ReactiveGeoOperations.html) Redis geospatial operations such as GEOADD , GEORADIUS , and others) ReactiveHashOperations(../api/java/org/springframework/data/redis/core/ReactiveHashOperations.html) Redis hash operations ReactiveHyperLogLogOperations(../api/java/org/springframework/data/redis/core/ReactiveHyperLogLogOperations.html) Redis HyperLogLog operations such as ( PFADD , PFCOUNT , and others) ReactiveListOperations(../api/java/org/springframework/data/redis/core/ReactiveListOperations.html) Redis list operations ReactiveSetOperations(../api/java/org/springframework/data/redis/core/ReactiveSetOperations.html) Redis set operations ReactiveValueOperations(../api/java/org/springframework/data/redis/core/ReactiveValueOperations.html) Redis string (or value) operations ReactiveZSetOperations(../api/java/org/springframework/data/redis/core/ReactiveZSetOperations.html) Redis zset (or sorted set) operations Once configured, the template is thread-safe and can be reused across multiple instances. RedisTemplate uses a Java-based serializer for most of its operations. This means that any object written or read by the template is serialized and deserialized through Java. You can change the serialization mechanism on the template, and the Redis module offers several implementations, which are available in the org.springframework.data.redis.serializer package. See Serializers(#redis:serializer) for more information. You can also set any of the serializers to null and use RedisTemplate with raw byte arrays by setting the enableDefaultSerializer property to false . Note that the template requires all keys to be non-null. However, values can be null as long as the underlying serializer accepts them. Read the Javadoc of each serializer for more information. For cases where you need a certain template view, declare the view as a dependency and inject the template. The container automatically performs the conversion, eliminating the opsFor[X] calls, as shown in the following example: Configuring Template API Java Imperative Java Reactive XML @Configuration class MyConfig { @Bean LettuceConnectionFactory connectionFactory() { return new LettuceConnectionFactory(); } @Bean RedisTemplate<String, String> redisTemplate(RedisConnectionFactory connectionFactory) { RedisTemplate<String, String> template = new RedisTemplate<>(); template.setConnectionFactory(connectionFactory); return template; } } @Configuration class MyConfig { @Bean LettuceConnectionFactory connectionFactory() { return new LettuceConnectionFactory(); } @Bean ReactiveRedisTemplate<String, String> ReactiveRedisTemplate(ReactiveRedisConnectionFactory connectionFactory) { return new ReactiveRedisTemplate<>(connectionFactory, RedisSerializationContext.string()); } } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:p=""http://www.springframework.org/schema/p"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <bean id=""redisConnectionFactory"" class=""org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory""/> <!-- redis template definition --> <bean id=""redisTemplate"" class=""org.springframework.data.redis.core.RedisTemplate"" p:connection-factory-ref=""redisConnectionFactory""/> ... </beans> Pushing an item to a List using [Reactive]RedisTemplate Imperative Reactive public class Example { // inject the actual operations @Autowired private RedisOperations<String, String> operations; // inject the template as ListOperations @Resource(name=""redisTemplate"") private ListOperations<String, String> listOps; public void addLink(String userId, URL url) { listOps.leftPush(userId, url.toExternalForm()); } } public class Example { // inject the actual template @Autowired private ReactiveRedisOperations<String, String> operations; public Mono<Long> addLink(String userId, URL url) { return operations.opsForList().leftPush(userId, url.toExternalForm()); } } String-focused Convenience Classes: Since it is quite common for the keys and values stored in Redis to be java.lang.String , the Redis modules provides two extensions to RedisConnection and RedisTemplate , respectively the StringRedisConnection (and its DefaultStringRedisConnection implementation) and StringRedisTemplate as a convenient one-stop solution for intensive String operations. In addition to being bound to String keys, the template and the connection use the StringRedisSerializer underneath, which means the stored keys and values are human-readable (assuming the same encoding is used both in Redis and your code). The following listings show an example: Java Imperative Java Reactive XML @Configuration class RedisConfiguration { @Bean LettuceConnectionFactory redisConnectionFactory() { return new LettuceConnectionFactory(); } @Bean StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; } } @Configuration class RedisConfiguration { @Bean LettuceConnectionFactory redisConnectionFactory() { return new LettuceConnectionFactory(); } @Bean ReactiveStringRedisTemplate reactiveRedisTemplate(ReactiveRedisConnectionFactory factory) { return new ReactiveStringRedisTemplate<>(factory); } } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:p=""http://www.springframework.org/schema/p"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <bean id=""redisConnectionFactory"" class=""org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory""/> <bean id=""stringRedisTemplate"" class=""org.springframework.data.redis.core.StringRedisTemplate"" p:connection-factory-ref=""redisConnectionFactory""/> </beans> Imperative Reactive public class Example { @Autowired private StringRedisTemplate redisTemplate; public void addLink(String userId, URL url) { redisTemplate.opsForList().leftPush(userId, url.toExternalForm()); } } public class Example { @Autowired private ReactiveStringRedisTemplate redisTemplate; public Mono<Long> addLink(String userId, URL url) { return redisTemplate.opsForList().leftPush(userId, url.toExternalForm()); } } As with the other Spring templates, RedisTemplate and StringRedisTemplate let you talk directly to Redis through the RedisCallback interface. This feature gives complete control to you, as it talks directly to the RedisConnection . Note that the callback receives an instance of StringRedisConnection when a StringRedisTemplate is used. The following example shows how to use the RedisCallback interface: public void useCallback() { redisOperations.execute(new RedisCallback<Object>() { public Object doInRedis(RedisConnection connection) throws DataAccessException { Long size = connection.dbSize(); // Can cast to StringRedisConnection if using a StringRedisTemplate ((StringRedisConnection)connection).set(""key"", ""value""); } }); } Serializers: From the framework perspective, the data stored in Redis is only bytes. While Redis itself supports various types, for the most part, these refer to the way the data is stored rather than what it represents. It is up to the user to decide whether the information gets translated into strings or any other objects. In Spring Data, the conversion between the user (custom) types and raw data (and vice-versa) is handled by Spring Data Redis in the org.springframework.data.redis.serializer package. This package contains two types of serializers that, as the name implies, take care of the serialization process: Two-way serializers based on RedisSerializer(../api/java/org/springframework/data/redis/serializer/RedisSerializer.html) . Element readers and writers that use RedisElementReader and RedisElementWriter . The main difference between these variants is that RedisSerializer primarily serializes to byte[] while readers and writers use ByteBuffer . Multiple implementations are available (including two that have been already mentioned in this documentation): JdkSerializationRedisSerializer(../api/java/org/springframework/data/redis/serializer/JdkSerializationRedisSerializer.html) , which is used by default for RedisCache(../api/java/org/springframework/data/redis/cache/RedisCache.html) and RedisTemplate(../api/java/org/springframework/data/redis/core/RedisTemplate.html) . the StringRedisSerializer . However, one can use OxmSerializer for Object/XML mapping through Spring OXM(https://docs.spring.io/spring-framework/reference/6.1/data-access.html#oxm) support or Jackson2JsonRedisSerializer(../api/java/org/springframework/data/redis/serializer/Jackson2JsonRedisSerializer.html) or GenericJackson2JsonRedisSerializer(../api/java/org/springframework/data/redis/serializer/GenericJackson2JsonRedisSerializer.html) for storing data in JSON(https://en.wikipedia.org/wiki/JSON) format. Do note that the storage format is not limited only to values. It can be used for keys, values, or hashes without any restrictions. By default, RedisCache(../api/java/org/springframework/data/redis/cache/RedisCache.html) and RedisTemplate(../api/java/org/springframework/data/redis/core/RedisTemplate.html) are configured to use Java native serialization. Java native serialization is known for allowing the running of remote code caused by payloads that exploit vulnerable libraries and classes injecting unverified bytecode. Manipulated input could lead to unwanted code being run in the application during the deserialization step. As a consequence, do not use serialization in untrusted environments. In general, we strongly recommend any other message format (such as JSON) instead. If you are concerned about security vulnerabilities due to Java serialization, consider the general-purpose serialization filter mechanism at the core JVM level: Filter Incoming Serialization Data(https://docs.oracle.com/en/java/javase/17/core/serialization-filtering1.html) . JEP 290(https://openjdk.org/jeps/290) . OWASP: Deserialization of untrusted data(https://owasp.org/www-community/vulnerabilities/Deserialization_of_untrusted_data) ."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-cache.html","Redis Cache: Spring Data Redis provides an implementation of Spring Framework’s Cache Abstraction(https://docs.spring.io/spring-framework/reference/6.1/integration.html#cache) in the org.springframework.data.redis.cache package. To use Redis as a backing implementation, add RedisCacheManager(../api/java/org/springframework/data/redis/cache/RedisCacheManager.html) to your configuration, as follows: @Bean public RedisCacheManager cacheManager(RedisConnectionFactory connectionFactory) { return RedisCacheManager.create(connectionFactory); } RedisCacheManager behavior can be configured with RedisCacheManager.RedisCacheManagerBuilder(../api/java/org/springframework/data/redis/cache/RedisCacheManager.RedisCacheManagerBuilder.html) , letting you set the default RedisCacheManager(../api/java/org/springframework/data/redis/cache/RedisCacheManager.html) , transaction behavior, and predefined caches. RedisCacheManager cacheManager = RedisCacheManager.builder(connectionFactory) .cacheDefaults(RedisCacheConfiguration.defaultCacheConfig()) .transactionAware() .withInitialCacheConfigurations(Collections.singletonMap(""predefined"", RedisCacheConfiguration.defaultCacheConfig().disableCachingNullValues())) .build(); As shown in the preceding example, RedisCacheManager allows custom configuration on a per-cache basis. The behavior of RedisCache(../api/java/org/springframework/data/redis/cache/RedisCache.html) created by RedisCacheManager(../api/java/org/springframework/data/redis/cache/RedisCacheManager.html) is defined with RedisCacheConfiguration . The configuration lets you set key expiration times, prefixes, and RedisSerializer implementations for converting to and from the binary storage format, as shown in the following example: RedisCacheConfiguration cacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofSeconds(1)) .disableCachingNullValues(); RedisCacheManager(../api/java/org/springframework/data/redis/cache/RedisCacheManager.html) defaults to a lock-free RedisCacheWriter(../api/java/org/springframework/data/redis/cache/RedisCacheWriter.html) for reading and writing binary values. Lock-free caching improves throughput. The lack of entry locking can lead to overlapping, non-atomic commands for the Cache putIfAbsent and clean operations, as those require multiple commands to be sent to Redis. The locking counterpart prevents command overlap by setting an explicit lock key and checking against presence of this key, which leads to additional requests and potential command wait times. Locking applies on the cache level , not per cache entry . It is possible to opt in to the locking behavior as follows: RedisCacheManager cacheMangager = RedisCacheManager .build(RedisCacheWriter.lockingRedisCacheWriter(connectionFactory)) .cacheDefaults(RedisCacheConfiguration.defaultCacheConfig()) ... By default, any key for a cache entry gets prefixed with the actual cache name followed by two colons ( :: ). This behavior can be changed to a static as well as a computed prefix. The following example shows how to set a static prefix: // static key prefix RedisCacheConfiguration.defaultCacheConfig().prefixCacheNameWith(""(͡° ᴥ ͡°)""); The following example shows how to set a computed prefix: // computed key prefix RedisCacheConfiguration.defaultCacheConfig() .computePrefixWith(cacheName -> ""¯\_(ツ)_/¯"" + cacheName); The cache implementation defaults to use KEYS and DEL to clear the cache. KEYS can cause performance issues with large keyspaces. Therefore, the default RedisCacheWriter can be created with a BatchStrategy to switch to a SCAN -based batch strategy. The SCAN strategy requires a batch size to avoid excessive Redis command round trips: RedisCacheManager cacheManager = RedisCacheManager .build(RedisCacheWriter.nonLockingRedisCacheWriter(connectionFactory, BatchStrategies.scan(1000))) .cacheDefaults(RedisCacheConfiguration.defaultCacheConfig()) ... The KEYS batch strategy is fully supported using any driver and Redis operation mode (Standalone, Clustered). SCAN is fully supported when using the Lettuce driver. Jedis supports SCAN only in non-clustered modes. The following table lists the default settings for RedisCacheManager : Table 1. RedisCacheManager defaults Setting Value Cache Writer Non-locking, KEYS batch strategy Cache Configuration RedisCacheConfiguration#defaultConfiguration Initial Caches None Transaction Aware No The following table lists the default settings for RedisCacheConfiguration : Table 2. RedisCacheConfiguration defaults Key Expiration None Cache null Yes Prefix Keys Yes Default Prefix The actual cache name Key Serializer StringRedisSerializer Value Serializer JdkSerializationRedisSerializer Conversion Service DefaultFormattingConversionService with default cache key converters By default RedisCache , statistics are disabled. Use RedisCacheManagerBuilder.enableStatistics() to collect local hits and misses through RedisCache#getStatistics() , returning a snapshot of the collected data. Redis Cache Expiration: The implementation of time-to-idle (TTI) as well as time-to-live (TTL) varies in definition and behavior even across different data stores. In general: time-to-live (TTL) expiration - TTL is only set and reset by a create or update data access operation. As long as the entry is written before the TTL expiration timeout, including on creation, an entry’s timeout will reset to the configured duration of the TTL expiration timeout. For example, if the TTL expiration timeout is set to 5 minutes, then the timeout will be set to 5 minutes on entry creation and reset to 5 minutes anytime the entry is updated thereafter and before the 5-minute interval expires. If no update occurs within 5 minutes, even if the entry was read several times, or even just read once during the 5-minute interval, the entry will still expire. The entry must be written to prevent the entry from expiring when declaring a TTL expiration policy. time-to-idle (TTI) expiration - TTI is reset anytime the entry is also read as well as for entry updates, and is effectively and extension to the TTL expiration policy. Some data stores expire an entry when TTL is configured no matter what type of data access operation occurs on the entry (reads, writes, or otherwise). After the set, configured TTL expiration timeout, the entry is evicted from the data store regardless. Eviction actions (for example: destroy, invalidate, overflow-to-disk (for persistent stores), etc.) are data store specific. Time-To-Live (TTL) Expiration: Spring Data Redis’s Cache implementation supports time-to-live (TTL) expiration on cache entries. Users can either configure the TTL expiration timeout with a fixed Duration or a dynamically computed Duration per cache entry by supplying an implementation of the new RedisCacheWriter.TtlFunction interface. The RedisCacheWriter.TtlFunction interface was introduced in Spring Data Redis 3.2.0 . If all cache entries should expire after a set duration of time, then simply configure a TTL expiration timeout with a fixed Duration , as follows: RedisCacheConfiguration fiveMinuteTtlExpirationDefaults = RedisCacheConfiguration.defaultCacheConfig().enableTtl(Duration.ofMinutes(5)); However, if the TTL expiration timeout should vary by cache entry, then you must provide a custom implementation of the RedisCacheWriter.TtlFunction interface: enum MyCustomTtlFunction implements TtlFunction { INSTANCE; @Override public Duration getTimeToLive(Object key, @Nullable Object value) { // compute a TTL expiration timeout (Duration) based on the cache entry key and/or value } } Under-the-hood, a fixed Duration TTL expiration is wrapped in a TtlFunction implementation returning the provided Duration . Then, you can either configure the fixed Duration or the dynamic, per-cache entry Duration TTL expiration on a global basis using: Global fixed Duration TTL expiration timeout RedisCacheManager cacheManager = RedisCacheManager.builder(redisConnectionFactory) .cacheDefaults(fiveMinuteTtlExpirationDefaults) .build(); Or, alternatively: Global, dynamically computed per-cache entry Duration TTL expiration timeout RedisCacheConfiguration defaults = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(MyCustomTtlFunction.INSTANCE); RedisCacheManager cacheManager = RedisCacheManager.builder(redisConnectionFactory) .cacheDefaults(defaults) .build(); Of course, you can combine both global and per-cache configuration using: Global fixed Duration TTL expiration timeout RedisCacheConfiguration predefined = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(MyCustomTtlFunction.INSTANCE); Map<String, RedisCacheConfiguration> initialCaches = Collections.singletonMap(""predefined"", predefined); RedisCacheManager cacheManager = RedisCacheManager.builder(redisConnectionFactory) .cacheDefaults(fiveMinuteTtlExpirationDefaults) .withInitialCacheConfigurations(initialCaches) .build(); Time-To-Idle (TTI) Expiration: Redis itself does not support the concept of true, time-to-idle (TTI) expiration. Still, using Spring Data Redis’s Cache implementation, it is possible to achieve time-to-idle (TTI) expiration-like behavior. The configuration of TTI in Spring Data Redis’s Cache implementation must be explicitly enabled, that is, is opt-in. Additionally, you must also provide TTL configuration using either a fixed Duration or a custom implementation of the TtlFunction interface as described above in Redis Cache Expiration(#redis:support:cache-abstraction:expiration) . For example: @Configuration @EnableCaching class RedisConfiguration { @Bean RedisConnectionFactory redisConnectionFactory() { // ... } @Bean RedisCacheManager cacheManager(RedisConnectionFactory connectionFactory) { RedisCacheConfiguration defaults = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofMinutes(5)) .enableTimeToIdle(); return RedisCacheManager.builder(connectionFactory) .cacheDefaults(defaults) .build(); } } Because Redis servers do not implement a proper notion of TTI, then TTI can only be achieved with Redis commands accepting expiration options. In Redis, the ""expiration"" is technically a time-to-live (TTL) policy. However, TTL expiration can be passed when reading the value of a key thereby effectively resetting the TTL expiration timeout, as is now the case in Spring Data Redis’s Cache.get(key) operation. RedisCache.get(key) is implemented by calling the Redis GETEX command. The Redis GETEX(https://redis.io/commands/getex) command is only available in Redis version 6.2.0 and later. Therefore, if you are not using Redis 6.2.0 or later, then it is not possible to use Spring Data Redis’s TTI expiration. A command execution exception will be thrown if you enable TTI against an incompatible Redis (server) version. No attempt is made to determine if the Redis server version is correct and supports the GETEX command. In order to achieve true time-to-idle (TTI) expiration-like behavior in your Spring Data Redis application, then an entry must be consistently accessed with (TTL) expiration on every read or write operation. There are no exceptions to this rule. If you are mixing and matching different data access patterns across your Spring Data Redis application (for example: caching, invoking operations using RedisTemplate and possibly, or especially when using Spring Data Repository CRUD operations), then accessing an entry may not necessarily prevent the entry from expiring if TTL expiration was set. For example, an entry maybe ""put"" in (written to) the cache during a @Cacheable service method invocation with a TTL expiration (i.e. SET <expiration options> ) and later read using a Spring Data Redis Repository before the expiration timeout (using GET without expiration options). A simple GET without specifying expiration options will not reset the TTL expiration timeout on an entry. Therefore, the entry may expire before the next data access operation, even though it was just read. Since this cannot be enforced in the Redis server, then it is the responsibility of your application to consistently access an entry when time-to-idle expiration is configured, in and outside of caching, where appropriate."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/cluster.html","Redis Cluster: Working with Redis Cluster(https://redis.io/topics/cluster-spec) requires Redis Server version 3.0+. See the Cluster Tutorial(https://redis.io/topics/cluster-tutorial) for more information. When using Redis Repositories(../repositories.html) with Redis Cluster, make yourself familiar with how to run Redis Repositories on a Cluster(redis-repositories/cluster.html) . Do not rely on keyspace events when using Redis Cluster as keyspace events are not replicated across shards. Pub/Sub subscribes to a random cluster node(https://github.com/spring-projects/spring-data-redis/issues/1111) which only receives keyspace events from a single shard. Use single-node Redis to avoid keyspace event loss. Working With Redis Cluster Connection: Redis Cluster behaves differently from single-node Redis or even a Sentinel-monitored master-replica environment. This is because the automatic sharding maps a key to one of 16384 slots, which are distributed across the nodes. Therefore, commands that involve more than one key must assert all keys map to the exact same slot to avoid cross-slot errors. A single cluster node serves only a dedicated set of keys. Commands issued against one particular server return results only for those keys served by that server. As a simple example, consider the KEYS command. When issued to a server in a cluster environment, it returns only the keys served by the node the request is sent to and not necessarily all keys within the cluster. So, to get all keys in a cluster environment, you must read the keys from all the known master nodes. While redirects for specific keys to the corresponding slot-serving node are handled by the driver libraries, higher-level functions, such as collecting information across nodes or sending commands to all nodes in the cluster, are covered by RedisClusterConnection . Picking up the keys example from earlier, this means that the keys(pattern) method picks up every master node in the cluster and simultaneously runs the KEYS command on every master node while picking up the results and returning the cumulated set of keys. To just request the keys of a single node RedisClusterConnection provides overloads for those methods (for example, keys(node, pattern) ). A RedisClusterNode can be obtained from RedisClusterConnection.clusterGetNodes or it can be constructed by using either the host and the port or the node Id. The following example shows a set of commands being run across the cluster: Example 1. Sample of Running Commands Across the Cluster [email protected](/cdn-cgi/l/email-protection) :7379 > cluster nodes 6b38bb... 127.0.0.1:7379 master - 0 0 25 connected 0-5460 (1) 7bb78c... 127.0.0.1:7380 master - 0 1449730618304 2 connected 5461-20242 (2) 164888... 127.0.0.1:7381 master - 0 1449730618304 3 connected 10923-20243 (3) b8b5ee... 127.0.0.1:7382 slave 6b38bb... 0 1449730618304 25 connected (4) RedisClusterConnection connection = connectionFactory.getClusterConnnection(); connection.set(""thing1"", value); (5) connection.set(""thing2"", value); (6) connection.keys(""*""); (7) connection.keys(NODE_7379, ""*""); (8) connection.keys(NODE_7380, ""*""); (9) connection.keys(NODE_7381, ""*""); (10) connection.keys(NODE_7382, ""*""); (11) 1 Master node serving slots 0 to 5460 replicated to replica at 7382 2 Master node serving slots 5461 to 10922 3 Master node serving slots 10923 to 16383 4 Replica node holding replicants of the master at 7379 5 Request routed to node at 7381 serving slot 12182 6 Request routed to node at 7379 serving slot 5061 7 Request routed to nodes at 7379, 7380, 7381 → [thing1, thing2] 8 Request routed to node at 7379 → [thing2] 9 Request routed to node at 7380 → [] 10 Request routed to node at 7381 → [thing1] 11 Request routed to node at 7382 → [thing2] When all keys map to the same slot, the native driver library automatically serves cross-slot requests, such as MGET . However, once this is not the case, RedisClusterConnection runs multiple parallel GET commands against the slot-serving nodes and again returns an accumulated result. This is less performant than the single-slot approach and, therefore, should be used with care. If in doubt, consider pinning keys to the same slot by providing a prefix in curly brackets, such as {my-prefix}.thing1 and {my-prefix}.thing2 , which will both map to the same slot number. The following example shows cross-slot request handling: Example 2. Sample of Cross-Slot Request Handling [email protected](/cdn-cgi/l/email-protection) :7379 > cluster nodes 6b38bb... 127.0.0.1:7379 master - 0 0 25 connected 0-5460 (1) 7bb... RedisClusterConnection connection = connectionFactory.getClusterConnnection(); connection.set(""thing1"", value); // slot: 12182 connection.set(""{thing1}.thing2"", value); // slot: 12182 connection.set(""thing2"", value); // slot: 5461 connection.mGet(""thing1"", ""{thing1}.thing2""); (2) connection.mGet(""thing1"", ""thing2""); (3) 1 Same Configuration as in the sample before. 2 Keys map to same slot → 127.0.0.1:7381 MGET thing1 {thing1}.thing2 3 Keys map to different slots and get split up into single slot ones routed to the according nodes → 127.0.0.1:7379 GET thing2 → 127.0.0.1:7381 GET thing1 The preceding examples demonstrate the general strategy followed by Spring Data Redis. Be aware that some operations might require loading huge amounts of data into memory to compute the desired command. Additionally, not all cross-slot requests can safely be ported to multiple single slot requests and error if misused (for example, PFCOUNT ). Working with RedisTemplate and ClusterOperations: See the Working with Objects through RedisTemplate(template.html) section for information about the general purpose, configuration, and usage of RedisTemplate . Be careful when setting up RedisTemplate#keySerializer using any of the JSON RedisSerializers , as changing JSON structure has immediate influence on hash slot calculation. RedisTemplate provides access to cluster-specific operations through the ClusterOperations interface, which can be obtained from RedisTemplate.opsForCluster() . This lets you explicitly run commands on a single node within the cluster while retaining the serialization and deserialization features configured for the template. It also provides administrative commands (such as CLUSTER MEET ) or more high-level operations (for example, resharding). The following example shows how to access RedisClusterConnection with RedisTemplate : Example 3. Accessing RedisClusterConnection with RedisTemplate ClusterOperations clusterOps = redisTemplate.opsForCluster(); clusterOps.shutdown(NODE_7379); (1) 1 Shut down node at 7379 and cross fingers there is a replica in place that can take over. Redis Cluster pipelining is currently only supported throug the Lettuce driver except for the following commands when using cross-slot keys: rename , renameNX , sort , bLPop , bRPop , rPopLPush , bRPopLPush , info , sMove , sInter , sInterStore , sUnion , sUnionStore , sDiff , sDiffStore . Same-slot keys are fully supported."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/hash-mappers.html","Hash Mapping: Data can be stored by using various data structures within Redis. Jackson2JsonRedisSerializer(../api/java/org/springframework/data/redis/serializer/Jackson2JsonRedisSerializer.html) can convert objects in JSON(https://en.wikipedia.org/wiki/JSON) format. Ideally, JSON can be stored as a value by using plain keys. You can achieve a more sophisticated mapping of structured objects by using Redis hashes. Spring Data Redis offers various strategies for mapping data to hashes (depending on the use case): Direct mapping, by using HashOperations(../api/java/org/springframework/data/redis/core/HashOperations.html) and a serializer(../redis.html#redis:serializer) Using Redis Repositories(../repositories.html) Using HashMapper(../api/java/org/springframework/data/redis/hash/HashMapper.html) and HashOperations(../api/java/org/springframework/data/redis/core/HashOperations.html) Hash Mappers: Hash mappers are converters of map objects to a Map<K, V> and back. HashMapper(../api/java/org/springframework/data/redis/hash/HashMapper.html) is intended for using with Redis Hashes. Multiple implementations are available: BeanUtilsHashMapper(../api/java/org/springframework/data/redis/hash/BeanUtilsHashMapper.html) using Spring’s BeanUtils(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/beans/BeanUtils.html) . ObjectHashMapper(../api/java/org/springframework/data/redis/hash/ObjectHashMapper.html) using Object-to-Hash Mapping(redis-repositories/mapping.html) . Jackson2HashMapper(#redis.hashmappers.jackson2) using FasterXML Jackson(https://github.com/FasterXML/jackson) . The following example shows one way to implement hash mapping: public class Person { String firstname; String lastname; // … } public class HashMapping { @Resource(name = ""redisTemplate"") HashOperations<String, byte[], byte[]> hashOperations; HashMapper<Object, byte[], byte[]> mapper = new ObjectHashMapper(); public void writeHash(String key, Person person) { Map<byte[], byte[]> mappedHash = mapper.toHash(person); hashOperations.putAll(key, mappedHash); } public Person loadHash(String key) { Map<byte[], byte[]> loadedHash = hashOperations.entries(key); return (Person) mapper.fromHash(loadedHash); } } Jackson2HashMapper: Jackson2HashMapper(../api/java/org/springframework/data/redis/hash/Jackson2HashMapper.html) provides Redis Hash mapping for domain objects by using FasterXML Jackson(https://github.com/FasterXML/jackson) . Jackson2HashMapper can map top-level properties as Hash field names and, optionally, flatten the structure. Simple types map to simple values. Complex types (nested objects, collections, maps, and so on) are represented as nested JSON. Flattening creates individual hash entries for all nested properties and resolves complex types into simple types, as far as possible. Consider the following class and the data structure it contains: public class Person { String firstname; String lastname; Address address; Date date; LocalDateTime localDateTime; } public class Address { String city; String country; } The following table shows how the data in the preceding class would appear in normal mapping: Table 1. Normal Mapping Hash Field Value firstname Jon lastname Snow address { ""city"" : ""Castle Black"", ""country"" : ""The North"" } date 1561543964015 localDateTime 2018-01-02T12:13:14 The following table shows how the data in the preceding class would appear in flat mapping: Table 2. Flat Mapping Hash Field Value firstname Jon lastname Snow address.city Castle Black address.country The North date 1561543964015 localDateTime 2018-01-02T12:13:14 Flattening requires all property names to not interfere with the JSON path. Using dots or brackets in map keys or as property names is not supported when you use flattening. The resulting hash cannot be mapped back into an Object. java.util.Date and java.util.Calendar are represented with milliseconds. JSR-310 Date/Time types are serialized to their toString form if jackson-datatype-jsr310 is on the class path."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/pubsub.html","Pub/Sub Messaging: Spring Data provides dedicated messaging integration for Redis, similar in functionality and naming to the JMS integration in Spring Framework. Redis messaging can be roughly divided into two areas of functionality: Publication or production of messages Subscription or consumption of messages This is an example of the pattern often called Publish/Subscribe (Pub/Sub for short). The RedisTemplate class is used for message production. For asynchronous reception similar to Java EE’s message-driven bean style, Spring Data provides a dedicated message listener container that is used to create Message-Driven POJOs (MDPs) and, for synchronous reception, the RedisConnection contract. The org.springframework.data.redis.connection and org.springframework.data.redis.listener packages provide the core functionality for Redis messaging. Publishing (Sending Messages): To publish a message, you can use, as with the other operations, either the low-level [Reactive]RedisConnection or the high-level [Reactive]RedisOperations . Both entities offer the publish method, which accepts the message and the destination channel as arguments. While RedisConnection requires raw data (array of bytes), the [Reactive]RedisOperations lets arbitrary objects be passed in as messages, as shown in the following example: Imperative Reactive // send message through connection RedisConnection con = … byte[] msg = … byte[] channel = … con.pubSubCommands().publish(msg, channel); // send message through RedisOperations RedisOperations operations = … Long numberOfClients = operations.convertAndSend(""hello!"", ""world""); // send message through connection ReactiveRedisConnection con = … ByteBuffer[] msg = … ByteBuffer[] channel = … con.pubSubCommands().publish(msg, channel); // send message through ReactiveRedisOperations ReactiveRedisOperations operations = … Mono<Long> numberOfClients = operations.convertAndSend(""hello!"", ""world""); Subscribing (Receiving Messages): On the receiving side, one can subscribe to one or multiple channels either by naming them directly or by using pattern matching. The latter approach is quite useful, as it not only lets multiple subscriptions be created with one command but can also listen on channels not yet created at subscription time (as long as they match the pattern). At the low-level, RedisConnection offers the subscribe and pSubscribe methods that map the Redis commands for subscribing by channel or by pattern, respectively. Note that multiple channels or patterns can be used as arguments. To change the subscription of a connection or query whether it is listening, RedisConnection provides the getSubscription and isSubscribed methods. Subscription commands in Spring Data Redis are blocking. That is, calling subscribe on a connection causes the current thread to block as it starts waiting for messages. The thread is released only if the subscription is canceled, which happens when another thread invokes unsubscribe or pUnsubscribe on the same connection. See “ Message Listener Containers(#redis:pubsub:subscribe:containers) ” (later in this document) for a solution to this problem. As mentioned earlier, once subscribed, a connection starts waiting for messages. Only commands that add new subscriptions, modify existing subscriptions, and cancel existing subscriptions are allowed. Invoking anything other than subscribe , pSubscribe , unsubscribe , or pUnsubscribe throws an exception. In order to subscribe to messages, one needs to implement the MessageListener callback. Each time a new message arrives, the callback gets invoked and the user code gets run by the onMessage method. The interface gives access not only to the actual message but also to the channel it has been received through and the pattern (if any) used by the subscription to match the channel. This information lets the callee differentiate between various messages not just by content but also examining additional details. Message Listener Containers: Due to its blocking nature, low-level subscription is not attractive, as it requires connection and thread management for every single listener. To alleviate this problem, Spring Data offers RedisMessageListenerContainer(../api/java/org/springframework/data/redis/listener/RedisMessageListenerContainer.html) , which does all the heavy lifting. If you are familiar with EJB and JMS, you should find the concepts familiar, as it is designed to be as close as possible to the support in Spring Framework and its message-driven POJOs (MDPs). RedisMessageListenerContainer(../api/java/org/springframework/data/redis/listener/RedisMessageListenerContainer.html) acts as a message listener container. It is used to receive messages from a Redis channel and drive the MessageListener(../api/java/org/springframework/data/redis/connection/MessageListener.html) instances that are injected into it. The listener container is responsible for all threading of message reception and dispatches into the listener for processing. A message listener container is the intermediary between an MDP and a messaging provider and takes care of registering to receive messages, resource acquisition and release, exception conversion, and the like. This lets you as an application developer write the (possibly complex) business logic associated with receiving a message (and reacting to it) and delegates boilerplate Redis infrastructure concerns to the framework. A MessageListener(../api/java/org/springframework/data/redis/connection/MessageListener.html) can additionally implement SubscriptionListener(../api/java/org/springframework/data/redis/connection/SubscriptionListener.html) to receive notifications upon subscription/unsubscribe confirmation. Listening to subscription notifications can be useful when synchronizing invocations. Furthermore, to minimize the application footprint, RedisMessageListenerContainer(../api/java/org/springframework/data/redis/listener/RedisMessageListenerContainer.html) lets one connection and one thread be shared by multiple listeners even though they do not share a subscription. Thus, no matter how many listeners or channels an application tracks, the runtime cost remains the same throughout its lifetime. Moreover, the container allows runtime configuration changes so that you can add or remove listeners while an application is running without the need for a restart. Additionally, the container uses a lazy subscription approach, using a RedisConnection only when needed. If all the listeners are unsubscribed, cleanup is automatically performed, and the thread is released. To help with the asynchronous nature of messages, the container requires a java.util.concurrent.Executor (or Spring’s TaskExecutor ) for dispatching the messages. Depending on the load, the number of listeners, or the runtime environment, you should change or tweak the executor to better serve your needs. In particular, in managed environments (such as app servers), it is highly recommended to pick a proper TaskExecutor to take advantage of its runtime. The MessageListenerAdapter: The MessageListenerAdapter(../api/java/org/springframework/data/redis/listener/adapter/MessageListenerAdapter.html) class is the final component in Spring’s asynchronous messaging support. In a nutshell, it lets you expose almost any class as a MDP (though there are some constraints). Consider the following interface definition: public interface MessageDelegate { void handleMessage(String message); void handleMessage(Map message); void handleMessage(byte[] message); void handleMessage(Serializable message); // pass the channel/pattern as well void handleMessage(Serializable message, String channel); } Notice that, although the interface does not extend the MessageListener interface, it can still be used as a MDP by using the MessageListenerAdapter(../api/java/org/springframework/data/redis/listener/adapter/MessageListenerAdapter.html) class. Notice also how the various message handling methods are strongly typed according to the contents of the various Message types that they can receive and handle. In addition, the channel or pattern to which a message is sent can be passed in to the method as the second argument of type String : public class DefaultMessageDelegate implements MessageDelegate { // implementation elided for clarity... } Notice how the above implementation of the `MessageDelegate` interface (the above `DefaultMessageDelegate` class) has *no* Redis dependencies at all. It truly is a POJO that we make into an MDP with the following configuration: Java XML @Configuration class MyConfig { // … @Bean DefaultMessageDelegate listener() { return new DefaultMessageDelegate(); } @Bean MessageListenerAdapter messageListenerAdapter(DefaultMessageDelegate listener) { return new MessageListenerAdapter(listener, ""handleMessage""); } @Bean RedisMessageListenerContainer redisMessageListenerContainer(RedisConnectionFactory connectionFactory, MessageListenerAdapter listener) { RedisMessageListenerContainer container = new RedisMessageListenerContainer(); container.setConnectionFactory(connectionFactory); container.addMessageListener(listener, ChannelTopic.of(""chatroom"")); return container; } } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:redis=""http://www.springframework.org/schema/redis"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/redis https://www.springframework.org/schema/redis/spring-redis.xsd""> <!-- the default ConnectionFactory --> <redis:listener-container> <!-- the method attribute can be skipped as the default method name is ""handleMessage"" --> <redis:listener ref=""listener"" method=""handleMessage"" topic=""chatroom"" /> </redis:listener-container> <bean id=""listener"" class=""redisexample.DefaultMessageDelegate""/> ... </beans> The listener topic can be either a channel (for example, topic=""chatroom"" ) or a pattern (for example, topic=""*room"" ) The preceding example uses the Redis namespace to declare the message listener container and automatically register the POJOs as listeners. The full-blown beans definition follows: <bean id=""messageListener"" class=""org.springframework.data.redis.listener.adapter.MessageListenerAdapter""> <constructor-arg> <bean class=""redisexample.DefaultMessageDelegate""/> </constructor-arg> </bean> <bean id=""redisContainer"" class=""org.springframework.data.redis.listener.RedisMessageListenerContainer""> <property name=""connectionFactory"" ref=""connectionFactory""/> <property name=""messageListeners""> <map> <entry key-ref=""messageListener""> <bean class=""org.springframework.data.redis.listener.ChannelTopic""> <constructor-arg value=""chatroom""/> </bean> </entry> </map> </property> </bean> Each time a message is received, the adapter automatically and transparently performs translation (using the configured RedisSerializer ) between the low-level format and the required object type. Any exception caused by the method invocation is caught and handled by the container (by default, exceptions get logged). Reactive Message Listener Container: Spring Data offers ReactiveRedisMessageListenerContainer(../api/java/org/springframework/data/redis/listener/ReactiveRedisMessageListenerContainer.html) which does all the heavy lifting of conversion and subscription state management on behalf of the user. The message listener container itself does not require external threading resources. It uses the driver threads to publish messages. ReactiveRedisConnectionFactory factory = … ReactiveRedisMessageListenerContainer container = new ReactiveRedisMessageListenerContainer(factory); Flux<ChannelMessage<String, String>> stream = container.receive(ChannelTopic.of(""my-channel"")); To await and ensure proper subscription, you can use the receiveLater method that returns a Mono<Flux<ChannelMessage>> . The resulting Mono completes with an inner publisher as a result of completing the subscription to the given topics. By intercepting onNext signals, you can synchronize server-side subscriptions. ReactiveRedisConnectionFactory factory = … ReactiveRedisMessageListenerContainer container = new ReactiveRedisMessageListenerContainer(factory); Mono<Flux<ChannelMessage<String, String>>> stream = container.receiveLater(ChannelTopic.of(""my-channel"")); stream.doOnNext(inner -> // notification hook when Redis subscriptions are synchronized with the server) .flatMapMany(Function.identity()) .…; Subscribing via template API: As mentioned above you can directly use ReactiveRedisTemplate(../api/java/org/springframework/data/redis/core/ReactiveRedisTemplate.html) to subscribe to channels / patterns. This approach offers a straight forward, though limited solution as you lose the option to add subscriptions after the initial ones. Nevertheless you still can control the message stream via the returned Flux using eg. take(Duration) . When done reading, on error or cancellation all bound resources are freed again. redisTemplate.listenToChannel(""channel1"", ""channel2"").doOnNext(msg -> { // message processing ... }).subscribe();"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-streams.html","Redis Streams: Redis Streams model a log data structure in an abstract approach. Typically, logs are append-only data structures and are consumed from the beginning on, at a random position, or by streaming new messages. Learn more about Redis Streams in the Redis reference documentation(https://redis.io/topics/streams-intro) . Redis Streams can be roughly divided into two areas of functionality: Appending records Consuming records Although this pattern has similarities to Pub/Sub(pubsub.html) , the main difference lies in the persistence of messages and how they are consumed. While Pub/Sub relies on the broadcasting of transient messages (i.e. if you don’t listen, you miss a message), Redis Stream use a persistent, append-only data type that retains messages until the stream is trimmed. Another difference in consumption is that Pub/Sub registers a server-side subscription. Redis pushes arriving messages to the client while Redis Streams require active polling. The org.springframework.data.redis.connection and org.springframework.data.redis.stream packages provide the core functionality for Redis Streams. Appending: To send a record, you can use, as with the other operations, either the low-level RedisConnection or the high-level StreamOperations . Both entities offer the add ( xAdd ) method, which accepts the record and the destination stream as arguments. While RedisConnection requires raw data (array of bytes), the StreamOperations lets arbitrary objects be passed in as records, as shown in the following example: // append message through connection RedisConnection con = … byte[] stream = … ByteRecord record = StreamRecords.rawBytes(…).withStreamKey(stream); con.xAdd(record); // append message through RedisTemplate RedisTemplate template = … StringRecord record = StreamRecords.string(…).withStreamKey(""my-stream""); template.opsForStream().add(record); Stream records carry a Map , key-value tuples, as their payload. Appending a record to a stream returns the RecordId that can be used as further reference. Consuming: On the consuming side, one can consume one or multiple streams. Redis Streams provide read commands that allow consumption of the stream from an arbitrary position (random access) within the known stream content and beyond the stream end to consume new stream record. At the low-level, RedisConnection offers the xRead and xReadGroup methods that map the Redis commands for reading and reading within a consumer group, respectively. Note that multiple streams can be used as arguments. Subscription commands in Redis can be blocking. That is, calling xRead on a connection causes the current thread to block as it starts waiting for messages. The thread is released only if the read command times out or receives a message. To consume stream messages, one can either poll for messages in application code, or use one of the two Asynchronous reception through Message Listener Containers(#redis.streams.receive.containers) , the imperative or the reactive one. Each time a new records arrives, the container notifies the application code. Synchronous reception: While stream consumption is typically associated with asynchronous processing, it is possible to consume messages synchronously. The overloaded StreamOperations.read(…) methods provide this functionality. During a synchronous receive, the calling thread potentially blocks until a message becomes available. The property StreamReadOptions.block specifies how long the receiver should wait before giving up waiting for a message. // Read message through RedisTemplate RedisTemplate template = … List<MapRecord<K, HK, HV>> messages = template.opsForStream().read(StreamReadOptions.empty().count(2), StreamOffset.latest(""my-stream"")); List<MapRecord<K, HK, HV>> messages = template.opsForStream().read(Consumer.from(""my-group"", ""my-consumer""), StreamReadOptions.empty().count(2), StreamOffset.create(""my-stream"", ReadOffset.lastConsumed())) Asynchronous reception through Message Listener Containers: Due to its blocking nature, low-level polling is not attractive, as it requires connection and thread management for every single consumer. To alleviate this problem, Spring Data offers message listeners, which do all the heavy lifting. If you are familiar with EJB and JMS, you should find the concepts familiar, as it is designed to be as close as possible to the support in Spring Framework and its message-driven POJOs (MDPs). Spring Data ships with two implementations tailored to the used programming model: StreamMessageListenerContainer(../api/java/org/springframework/data/redis/stream/StreamMessageListenerContainer.html) acts as message listener container for imperative programming models. It is used to consume records from a Redis Stream and drive the StreamListener(../api/java/org/springframework/data/redis/stream/StreamListener.html) instances that are injected into it. StreamReceiver(../api/java/org/springframework/data/redis/stream/StreamReceiver.html) provides a reactive variant of a message listener. It is used to consume messages from a Redis Stream as potentially infinite stream and emit stream messages through a Flux . StreamMessageListenerContainer and StreamReceiver are responsible for all threading of message reception and dispatch into the listener for processing. A message listener container/receiver is the intermediary between an MDP and a messaging provider and takes care of registering to receive messages, resource acquisition and release, exception conversion, and the like. This lets you as an application developer write the (possibly complex) business logic associated with receiving a message (and reacting to it) and delegates boilerplate Redis infrastructure concerns to the framework. Both containers allow runtime configuration changes so that you can add or remove subscriptions while an application is running without the need for a restart. Additionally, the container uses a lazy subscription approach, using a RedisConnection only when needed. If all the listeners are unsubscribed, it automatically performs a cleanup, and the thread is released. Imperative StreamMessageListenerContainer: In a fashion similar to a Message-Driven Bean (MDB) in the EJB world, the Stream-Driven POJO (SDP) acts as a receiver for Stream messages. The one restriction on an SDP is that it must implement the StreamListener(../api/java/org/springframework/data/redis/stream/StreamListener.html) interface. Please also be aware that in the case where your POJO receives messages on multiple threads, it is important to ensure that your implementation is thread-safe. class ExampleStreamListener implements StreamListener<String, MapRecord<String, String, String>> { @Override public void onMessage(MapRecord<String, String, String> message) { System.out.println(""MessageId: "" + message.getId()); System.out.println(""Stream: "" + message.getStream()); System.out.println(""Body: "" + message.getValue()); } } StreamListener represents a functional interface so implementations can be rewritten using their Lambda form: message -> { System.out.println(""MessageId: "" + message.getId()); System.out.println(""Stream: "" + message.getStream()); System.out.println(""Body: "" + message.getValue()); }; Once you’ve implemented your StreamListener , it’s time to create a message listener container and register a subscription: RedisConnectionFactory connectionFactory = … StreamListener<String, MapRecord<String, String, String>> streamListener = … StreamMessageListenerContainerOptions<String, MapRecord<String, String, String>> containerOptions = StreamMessageListenerContainerOptions .builder().pollTimeout(Duration.ofMillis(100)).build(); StreamMessageListenerContainer<String, MapRecord<String, String, String>> container = StreamMessageListenerContainer.create(connectionFactory, containerOptions); Subscription subscription = container.receive(StreamOffset.fromStart(""my-stream""), streamListener); Please refer to the Javadoc of the various message listener containers for a full description of the features supported by each implementation. Reactive StreamReceiver: Reactive consumption of streaming data sources typically happens through a Flux of events or messages. The reactive receiver implementation is provided with StreamReceiver and its overloaded receive(…) messages. The reactive approach requires fewer infrastructure resources such as threads in comparison to StreamMessageListenerContainer as it is leveraging threading resources provided by the driver. The receiving stream is a demand-driven publisher of StreamMessage : Flux<MapRecord<String, String, String>> messages = … return messages.doOnNext(it -> { System.out.println(""MessageId: "" + message.getId()); System.out.println(""Stream: "" + message.getStream()); System.out.println(""Body: "" + message.getValue()); }); Now we need to create the StreamReceiver and register a subscription to consume stream messages: ReactiveRedisConnectionFactory connectionFactory = … StreamReceiverOptions<String, MapRecord<String, String, String>> options = StreamReceiverOptions.builder().pollTimeout(Duration.ofMillis(100)) .build(); StreamReceiver<String, MapRecord<String, String, String>> receiver = StreamReceiver.create(connectionFactory, options); Flux<MapRecord<String, String, String>> messages = receiver.receive(StreamOffset.fromStart(""my-stream"")); Please refer to the Javadoc of the various message listener containers for a full description of the features supported by each implementation. Demand-driven consumption uses backpressure signals to activate and deactivate polling. StreamReceiver subscriptions pause polling if the demand is satisfied until subscribers signal further demand. Depending on the ReadOffset strategy, this can cause messages to be skipped. Acknowledge strategies: When you read with messages via a Consumer Group , the server will remember that a given message was delivered and add it to the Pending Entries List (PEL). A list of messages delivered but not yet acknowledged. Messages have to be acknowledged via StreamOperations.acknowledge in order to be removed from the Pending Entries List as shown in the snippet below. StreamMessageListenerContainer<String, MapRecord<String, String, String>> container = ... container.receive(Consumer.from(""my-group"", ""my-consumer""), (1) StreamOffset.create(""my-stream"", ReadOffset.lastConsumed()), msg -> { // ... redisTemplate.opsForStream().acknowledge(""my-group"", msg); (2) }); 1 Read as my-consumer from group my-group . Received messages are not acknowledged. 2 Acknowledged the message after processing. To auto acknowledge messages on receive use receiveAutoAck instead of receive . ReadOffset strategies: Stream read operations accept a read offset specification to consume messages from the given offset on. ReadOffset represents the read offset specification. Redis supports 3 variants of offsets, depending on whether you consume the stream standalone or within a consumer group: ReadOffset.latest() – Read the latest message. ReadOffset.from(…) – Read after a specific message Id. ReadOffset.lastConsumed() – Read after the last consumed message Id (consumer-group only). In the context of a message container-based consumption, we need to advance (or increment) the read offset when consuming a message. Advancing depends on the requested ReadOffset and consumption mode (with/without consumer groups). The following matrix explains how containers advance ReadOffset : Table 1. ReadOffset Advancing Read offset Standalone Consumer Group Latest Read latest message Read latest message Specific Message Id Use last seen message as the next MessageId Use last seen message as the next MessageId Last Consumed Use last seen message as the next MessageId Last consumed message as per consumer group Reading from a specific message id and the last consumed message can be considered safe operations that ensure consumption of all messages that were appended to the stream. Using the latest message for read can skip messages that were added to the stream while the poll operation was in the state of dead time. Polling introduces a dead time in which messages can arrive between individual polling commands. Stream consumption is not a linear contiguous read but split into repeating XREAD calls. Serialization: Any Record sent to the stream needs to be serialized to its binary format. Due to the streams closeness to the hash data structure the stream key, field names and values use the according serializers configured on the RedisTemplate . Table 2. Stream Serialization Stream Property Serializer Description key keySerializer used for Record#getStream() field hashKeySerializer used for each map key in the payload value hashValueSerializer used for each map value in the payload Please make sure to review RedisSerializer s in use and note that if you decide to not use any serializer you need to make sure those values are binary already. Object Mapping: Simple Values: StreamOperations allows to append simple values, via ObjectRecord , directly to the stream without having to put those values into a Map structure. The value will then be assigned to an payload field and can be extracted when reading back the value. ObjectRecord<String, String> record = StreamRecords.newRecord() .in(""my-stream"") .ofObject(""my-value""); redisTemplate() .opsForStream() .add(record); (1) List<ObjectRecord<String, String>> records = redisTemplate() .opsForStream() .read(String.class, StreamOffset.fromStart(""my-stream"")); 1 XADD my-stream * ""_class"" ""java.lang.String"" ""_raw"" ""my-value"" ObjectRecord s pass through the very same serialization process as the all other records, thus the Record can also obtained using the untyped read operation returning a MapRecord . Complex Values: Adding a complex value to the stream can be done in 3 ways: Convert to simple value using e. g. a String JSON representation. Serialize the value with a suitable RedisSerializer . Convert the value into a Map suitable for serialization using a HashMapper(hash-mappers.html) . The first variant is the most straight forward one but neglects the field value capabilities offered by the stream structure, still the values in the stream will be readable for other consumers. The 2nd option holds the same benefits as the first one, but may lead to a very specific consumer limitations as the all consumers must implement the very same serialization mechanism. The HashMapper approach is the a bit more complex one making use of the steams hash structure, but flattening the source. Still other consumers remain able to read the records as long as suitable serializer combinations are chosen. HashMappers(hash-mappers.html) convert the payload to a Map with specific types. Make sure to use Hash-Key and Hash-Value serializers that are capable of (de-)serializing the hash. ObjectRecord<String, User> record = StreamRecords.newRecord() .in(""user-logon"") .ofObject(new User(""night"", ""angel"")); redisTemplate() .opsForStream() .add(record); (1) List<ObjectRecord<String, User>> records = redisTemplate() .opsForStream() .read(User.class, StreamOffset.fromStart(""user-logon"")); 1 XADD user-logon * ""_class"" ""com.example.User"" ""firstname"" ""night"" ""lastname"" ""angel"" StreamOperations use by default ObjectHashMapper(redis-repositories/mapping.html) . You may provide a HashMapper suitable for your requirements when obtaining StreamOperations . redisTemplate() .opsForStream(new Jackson2HashMapper(true)) .add(record); (1) 1 XADD user-logon * ""firstname"" ""night"" ""@class"" ""com.example.User"" ""lastname"" ""angel"" A StreamMessageListenerContainer may not be aware of any @TypeAlias used on domain types as those need to be resolved through a MappingContext . Make sure to initialize RedisMappingContext with a initialEntitySet . @Bean RedisMappingContext redisMappingContext() { RedisMappingContext ctx = new RedisMappingContext(); ctx.setInitialEntitySet(Collections.singleton(Person.class)); return ctx; } @Bean RedisConverter redisConverter(RedisMappingContext mappingContext) { return new MappingRedisConverter(mappingContext); } @Bean ObjectHashMapper hashMapper(RedisConverter converter) { return new ObjectHashMapper(converter); } @Bean StreamMessageListenerContainer streamMessageListenerContainer(RedisConnectionFactory connectionFactory, ObjectHashMapper hashMapper) { StreamMessageListenerContainerOptions<String, ObjectRecord<String, Object>> options = StreamMessageListenerContainerOptions.builder() .objectMapper(hashMapper) .build(); return StreamMessageListenerContainer.create(connectionFactory, options); }"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/scripting.html","Scripting: Redis versions 2.6 and higher provide support for running Lua scripts through the eval(https://redis.io/commands/eval) and evalsha(https://redis.io/commands/evalsha) commands. Spring Data Redis provides a high-level abstraction for running scripts that handles serialization and automatically uses the Redis script cache. Scripts can be run by calling the execute methods of RedisTemplate and ReactiveRedisTemplate . Both use a configurable ScriptExecutor(../api/java/org/springframework/data/redis/core/script/ScriptExecutor.html) (or ReactiveScriptExecutor(../api/java/org/springframework/data/redis/core/script/ReactiveScriptExecutor.html) ) to run the provided script. By default, the ScriptExecutor(../api/java/org/springframework/data/redis/core/script/ScriptExecutor.html) (or ReactiveScriptExecutor(../api/java/org/springframework/data/redis/core/script/ReactiveScriptExecutor.html) ) takes care of serializing the provided keys and arguments and deserializing the script result. This is done through the key and value serializers of the template. There is an additional overload that lets you pass custom serializers for the script arguments and the result. The default ScriptExecutor(../api/java/org/springframework/data/redis/core/script/ScriptExecutor.html) optimizes performance by retrieving the SHA1 of the script and attempting first to run evalsha , falling back to eval if the script is not yet present in the Redis script cache. The following example runs a common “check-and-set” scenario by using a Lua script. This is an ideal use case for a Redis script, as it requires that running a set of commands atomically, and the behavior of one command is influenced by the result of another. @Bean public RedisScript<Boolean> script() { ScriptSource scriptSource = new ResourceScriptSource(new ClassPathResource(""META-INF/scripts/checkandset.lua"")); return RedisScript.of(scriptSource, Boolean.class); } Imperative Reactive public class Example { @Autowired RedisOperations<String, String> redisOperations; @Autowired RedisScript<Boolean> script; public boolean checkAndSet(String expectedValue, String newValue) { return redisOperations.execute(script, singletonList(""key""), asList(expectedValue, newValue)); } } public class Example { @Autowired ReactiveRedisOperations<String, String> redisOperations; @Autowired RedisScript<Boolean> script; public Flux<Boolean> checkAndSet(String expectedValue, String newValue) { return redisOperations.execute(script, singletonList(""key""), asList(expectedValue, newValue)); } } -- checkandset.lua local current = redis.call('GET', KEYS[1]) if current == ARGV[1] then redis.call('SET', KEYS[1], ARGV[2]) return true end return false The preceding code configures a RedisScript(../api/java/org/springframework/data/redis/core/script/RedisScript.html) pointing to a file called checkandset.lua , which is expected to return a boolean value. The script resultType should be one of Long , Boolean , List , or a deserialized value type. It can also be null if the script returns a throw-away status (specifically, OK ). It is ideal to configure a single instance of DefaultRedisScript in your application context to avoid re-calculation of the script’s SHA1 on every script run. The checkAndSet method above then runs the scripts. Scripts can be run within a SessionCallback(../api/java/org/springframework/data/redis/core/SessionCallback.html) as part of a transaction or pipeline. See “ Redis Transactions(transactions.html) ” and “ Pipelining(pipelining.html) ” for more information. The scripting support provided by Spring Data Redis also lets you schedule Redis scripts for periodic running by using the Spring Task and Scheduler abstractions. See the Spring Framework(https://spring.io/projects/spring-framework/) documentation for more details."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/transactions.html","Redis Transactions: Redis provides support for transactions(https://redis.io/topics/transactions) through the multi , exec , and discard commands. These operations are available on RedisTemplate(../api/java/org/springframework/data/redis/core/RedisTemplate.html) . However, RedisTemplate is not guaranteed to run all the operations in the transaction with the same connection. Spring Data Redis provides the SessionCallback(../api/java/org/springframework/data/redis/core/SessionCallback.html) interface for use when multiple operations need to be performed with the same connection , such as when using Redis transactions.The following example uses the multi method: //execute a transaction List<Object> txResults = redisOperations.execute(new SessionCallback<List<Object>>() { public List<Object> execute(RedisOperations operations) throws DataAccessException { operations.multi(); operations.opsForSet().add(""key"", ""value1""); // This will contain the results of all operations in the transaction return operations.exec(); } }); System.out.println(""Number of items added to set: "" + txResults.get(0)); RedisTemplate uses its value, hash key, and hash value serializers to deserialize all results of exec before returning. There is an additional exec method that lets you pass a custom serializer for transaction results. It is worth mentioning that in case between multi() and exec() an exception happens (e.g. a timeout exception in case Redis does not respond within the timeout) then the connection may get stuck in a transactional state. To prevent such a situation need have to discard the transactional state to clear the connection: List<Object> txResults = redisOperations.execute(new SessionCallback<List<Object>>() { public List<Object> execute(RedisOperations operations) throws DataAccessException { boolean transactionStateIsActive = true; try { operations.multi(); operations.opsForSet().add(""key"", ""value1""); // This will contain the results of all operations in the transaction return operations.exec(); } catch (RuntimeException e) { operations.discard(); throw e; } } }); @Transactional Support: By default, RedisTemplate does not participate in managed Spring transactions. If you want RedisTemplate to make use of Redis transaction when using @Transactional or TransactionTemplate , you need to be explicitly enable transaction support for each RedisTemplate by setting setEnableTransactionSupport(true) . Enabling transaction support binds RedisConnection to the current transaction backed by a ThreadLocal . If the transaction finishes without errors, the Redis transaction gets commited with EXEC , otherwise rolled back with DISCARD . Redis transactions are batch-oriented. Commands issued during an ongoing transaction are queued and only applied when committing the transaction. Spring Data Redis distinguishes between read-only and write commands in an ongoing transaction. Read-only commands, such as KEYS , are piped to a fresh (non-thread-bound) RedisConnection to allow reads. Write commands are queued by RedisTemplate and applied upon commit. The following example shows how to configure transaction management: Example 1. Configuration enabling Transaction Management @Configuration @EnableTransactionManagement (1) public class RedisTxContextConfiguration { @Bean public StringRedisTemplate redisTemplate() { StringRedisTemplate template = new StringRedisTemplate(redisConnectionFactory()); // explicitly enable transaction support template.setEnableTransactionSupport(true); (2) return template; } @Bean public RedisConnectionFactory redisConnectionFactory() { // jedis || Lettuce } @Bean public PlatformTransactionManager transactionManager() throws SQLException { return new DataSourceTransactionManager(dataSource()); (3) } @Bean public DataSource dataSource() throws SQLException { // ... } } 1 Configures a Spring Context to enable declarative transaction management(https://docs.spring.io/spring-framework/reference/6.1/data-access.html#transaction-declarative) . 2 Configures RedisTemplate to participate in transactions by binding connections to the current thread. 3 Transaction management requires a PlatformTransactionManager . Spring Data Redis does not ship with a PlatformTransactionManager implementation. Assuming your application uses JDBC, Spring Data Redis can participate in transactions by using existing transaction managers. The following examples each demonstrate a usage constraint: Example 2. Usage Constraints // must be performed on thread-bound connection template.opsForValue().set(""thing1"", ""thing2""); // read operation must be run on a free (not transaction-aware) connection template.keys(""*""); // returns null as values set within a transaction are not visible template.opsForValue().get(""thing1"");"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/pipelining.html","Pipelining: Redis provides support for pipelining(https://redis.io/topics/pipelining) , which involves sending multiple commands to the server without waiting for the replies and then reading the replies in a single step. Pipelining can improve performance when you need to send several commands in a row, such as adding many elements to the same List. Spring Data Redis provides several RedisTemplate methods for running commands in a pipeline. If you do not care about the results of the pipelined operations, you can use the standard execute method, passing true for the pipeline argument. The executePipelined methods run the provided RedisCallback or SessionCallback in a pipeline and return the results, as shown in the following example: //pop a specified number of items from a queue List<Object> results = stringRedisTemplate.executePipelined( new RedisCallback<Object>() { public Object doInRedis(RedisConnection connection) throws DataAccessException { StringRedisConnection stringRedisConn = (StringRedisConnection)connection; for(int i=0; i< batchSize; i++) { stringRedisConn.rPop(""myqueue""); } return null; } }); The preceding example runs a bulk right pop of items from a queue in a pipeline. The results List contains all the popped items. RedisTemplate uses its value, hash key, and hash value serializers to deserialize all results before returning, so the returned items in the preceding example are Strings. There are additional executePipelined methods that let you pass a custom serializer for pipelined results. Note that the value returned from the RedisCallback is required to be null , as this value is discarded in favor of returning the results of the pipelined commands. The Lettuce driver supports fine-grained flush control that allows to either flush commands as they appear, buffer or send them at connection close. LettuceConnectionFactory factory = // ... factory.setPipeliningFlushPolicy(PipeliningFlushPolicy.buffered(3)); (1) 1 Buffer locally and flush after every 3rd command. Pipelining is limited to Redis Standalone. Redis Cluster is currently only supported through the Lettuce driver except for the following commands when using cross-slot keys: rename , renameNX , sort , bLPop , bRPop , rPopLPush , bRPopLPush , info , sMove , sInter , sInterStore , sUnion , sUnionStore , sDiff , sDiffStore . Same-slot keys are fully supported."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/support-classes.html","Support Classes: Package org.springframework.data.redis.support offers various reusable components that rely on Redis as a backing store. Currently, the package contains various JDK-based interface implementations on top of Redis, such as atomic(https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/concurrent/atomic/package-summary.html) counters and JDK Collections(https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Collection.html) . RedisList(../api/java/org/springframework/data/redis/support/collections/RedisList.html) is forward-compatible with Java 21 SequencedCollection . The atomic counters make it easy to wrap Redis key incrementation while the collections allow easy management of Redis keys with minimal storage exposure or API leakage. In particular, the RedisSet(../api/java/org/springframework/data/redis/support/collections/RedisSet.html) and RedisZSet(../api/java/org/springframework/data/redis/support/collections/RedisZSet.html) interfaces offer easy access to the set operations supported by Redis, such as intersection and union . RedisList(../api/java/org/springframework/data/redis/support/collections/RedisList.html) implements the List , Queue , and Deque contracts (and their equivalent blocking siblings) on top of Redis, exposing the storage as a FIFO (First-In-First-Out), LIFO (Last-In-First-Out) or capped collection with minimal configuration. The following example shows the configuration for a bean that uses a RedisList(../api/java/org/springframework/data/redis/support/collections/RedisList.html) : Java XML @Configuration class MyConfig { // … @Bean RedisList<String> stringRedisTemplate(RedisTemplate<String, String> redisTemplate) { return new DefaultRedisList<>(template, ""queue-key""); } } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:p=""http://www.springframework.org/schema/p"" xsi:schemaLocation="" http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <bean id=""queue"" class=""org.springframework.data.redis.support.collections.DefaultRedisList""> <constructor-arg ref=""redisTemplate""/> <constructor-arg value=""queue-key""/> </bean> </beans> The following example shows a Java configuration example for a Deque : public class AnotherExample { // injected private Deque<String> queue; public void addTag(String tag) { queue.push(tag); } } As shown in the preceding example, the consuming code is decoupled from the actual storage implementation. In fact, there is no indication that Redis is used underneath. This makes moving from development to production environments transparent and highly increases testability (the Redis implementation can be replaced with an in-memory one)."
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories.html","Redis Repositories: This chapter explains the basic foundations of Spring Data repositories and Redis specifics. Before continuing to the Redis specifics, make sure you have a sound understanding of the basic concepts. The goal of the Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. Working with Redis Repositories lets you seamlessly convert and store domain objects in Redis Hashes, apply custom mapping strategies, and use secondary indexes. Redis Repositories require at least Redis Server version 2.8.0 and do not work with transactions. Make sure to use a RedisTemplate with disabled transaction support(redis/transactions.html#tx.spring) . Section Summary: Core concepts(repositories/core-concepts.html) Defining Repository Interfaces(repositories/definition.html) Creating Repository Instances(repositories/create-instances.html) Usage(redis/redis-repositories/usage.html) Object Mapping Fundamentals(repositories/object-mapping.html) Object-to-Hash Mapping(redis/redis-repositories/mapping.html) Keyspaces(redis/redis-repositories/keyspaces.html) Secondary Indexes(redis/redis-repositories/indexes.html) Time To Live(redis/redis-repositories/expirations.html) Redis-specific Query Methods(redis/redis-repositories/queries.html) Query by Example(redis/redis-repositories/query-by-example.html) Redis Repositories Running on a Cluster(redis/redis-repositories/cluster.html) Redis Repositories Anatomy(redis/redis-repositories/anatomy.html) Projections(repositories/projections.html) Custom Repository Implementations(repositories/custom-implementations.html) Publishing Events from Aggregate Roots(repositories/core-domain-events.html) Null Handling of Repository Methods(repositories/null-handling.html) CDI Integration(redis/redis-repositories/cdi-integration.html) Repository query keywords(repositories/query-keywords-reference.html) Repository query return types(repositories/query-return-types-reference.html)"
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); } Entity State Detection Strategies: The following table describes the strategies that Spring Data offers for detecting whether an entity is new: Table 1. Options for detection whether an entity is new in Spring Data @Id -Property inspection (the default) By default, Spring Data inspects the identifier property of the given entity. If the identifier property is null or 0 in case of primitive types, then the entity is assumed to be new. Otherwise, it is assumed to not be new. @Version -Property inspection If a property annotated with @Version is present and null , or in case of a version property of primitive type 0 the entity is considered new. If the version property is present but has a different value, the entity is considered to not be new. If no version property is present Spring Data falls back to inspection of the identifier property. Implementing Persistable If an entity implements Persistable , Spring Data delegates the new detection to the isNew(…) method of the entity. See the Javadoc(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//index.html?org/springframework/data/domain/Persistable.html) for details. Note: Properties of Persistable will get detected and persisted if you use AccessType.PROPERTY . To avoid that, use @Transient . Providing a custom EntityInformation implementation You can customize the EntityInformation abstraction used in the repository base implementation by creating a subclass of the module specific repository factory and overriding the getEntityInformation(…) method. You then have to register the custom implementation of module specific repository factory as a Spring bean. Note that this should rarely be necessary."
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/create-instances.html","Creating Repository Instances: This section covers how to create instances and bean definitions for the defined repository interfaces. Java Configuration: Use the store-specific @EnableRedisRepositories annotation on a Java configuration class to define a configuration for repository activation. For an introduction to Java-based configuration of the Spring container, see JavaConfig in the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/java.html) . A sample configuration to enable Spring Data repositories resembles the following: Sample annotation-based repository configuration @Configuration @EnableJpaRepositories(""com.acme.repositories"") class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } The preceding example uses the JPA-specific annotation, which you would change according to the store module you actually use. The same applies to the definition of the EntityManagerFactory bean. See the sections covering the store-specific configuration. XML Configuration: Each Spring Data module includes a repositories element that lets you define a base package that Spring scans for you, as shown in the following example: Enabling Spring Data repositories via XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans:beans xmlns:beans=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans:beans> In the preceding example, Spring is instructed to scan com.acme.repositories and all its sub-packages for interfaces extending Repository or one of its sub-interfaces. For each interface found, the infrastructure registers the persistence technology-specific FactoryBean to create the appropriate proxies that handle invocations of the query methods. Each bean is registered under a bean name that is derived from the interface name, so an interface of UserRepository would be registered under userRepository . Bean names for nested repository interfaces are prefixed with their enclosing type name. The base package attribute allows wildcards so that you can define a pattern of scanned packages. Using Filters: By default, the infrastructure picks up every interface that extends the persistence technology-specific Repository sub-interface located under the configured base package and creates a bean instance for it. However, you might want more fine-grained control over which interfaces have bean instances created for them. To do so, use filter elements inside the repository declaration. The semantics are exactly equivalent to the elements in Spring’s component filters. For details, see the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/classpath-scanning.html) for these elements. For example, to exclude certain interfaces from instantiation as repository beans, you could use the following configuration: Using filters Java XML @Configuration @EnableRedisRepositories(basePackages = ""com.acme.repositories"", includeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeRepository"") }, excludeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeOtherRepository"") }) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } <repositories base-package=""com.acme.repositories""> <context:include-filter type=""regex"" expression="".*SomeRepository"" /> <context:exclude-filter type=""regex"" expression="".*SomeOtherRepository"" /> </repositories> The preceding example includes all interfaces ending with SomeRepository and excludes those ending with SomeOtherRepository from being instantiated. Standalone Usage: You can also use the repository infrastructure outside of a Spring container — for example, in CDI environments.You still need some Spring libraries in your classpath, but, generally, you can set up repositories programmatically as well.The Spring Data modules that provide repository support ship with a persistence technology-specific RepositoryFactory that you can use, as follows: Standalone usage of the repository factory RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class);"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/usage.html","Usage: Spring Data Redis lets you easily implement domain entities, as shown in the following example: Example 1. Sample Person Entity @RedisHash(""people"") public class Person { @Id String id; String firstname; String lastname; Address address; } We have a pretty simple domain object here. Note that it has a @RedisHash annotation on its type and a property named id that is annotated with org.springframework.data.annotation.Id . Those two items are responsible for creating the actual key used to persist the hash. Properties annotated with @Id as well as those named id are considered as the identifier properties. Those with the annotation are favored over others. To now actually have a component responsible for storage and retrieval, we need to define a repository interface, as shown in the following example: Example 2. Basic Repository Interface To Persist Person Entities public interface PersonRepository extends CrudRepository<Person, String> { } As our repository extends CrudRepository , it provides basic CRUD and finder operations. The thing we need in between to glue things together is the corresponding Spring configuration, shown in the following example: Example 3. JavaConfig for Redis Repositories @Configuration @EnableRedisRepositories public class ApplicationConfig { @Bean public RedisConnectionFactory connectionFactory() { return new LettuceConnectionFactory(); } @Bean public RedisTemplate<?, ?> redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate<byte[], byte[]> template = new RedisTemplate<byte[], byte[]>(); template.setConnectionFactory(redisConnectionFactory); return template; } } Given the preceding setup, we can inject PersonRepository into our components, as shown in the following example: Example 4. Access to Person Entities @Autowired PersonRepository repo; public void basicCrudOperations() { Person rand = new Person(""rand"", ""al'thor""); rand.setAddress(new Address(""emond's field"", ""andor"")); repo.save(rand); (1) repo.findOne(rand.getId()); (2) repo.count(); (3) repo.delete(rand); (4) } 1 Generates a new id if the current value is null or reuses an already set id value and stores properties of type Person inside the Redis Hash with a key that has a pattern of keyspace:id — in this case, it might be people:5d67b7e1-8640-2024-beeb-c666fab4c0e5 . 2 Uses the provided id to retrieve the object stored at keyspace:id . 3 Counts the total number of entities available within the keyspace, people , defined by @RedisHash on Person . 4 Removes the key for the given object from Redis. Persisting References: Marking properties with @Reference allows storing a simple key reference instead of copying values into the hash itself. On loading from Redis, references are resolved automatically and mapped back into the object, as shown in the following example: Example 5. Sample Property Reference _class = org.example.Person id = e2c7dcee-b8cd-4424-883e-736ce564363e firstname = rand lastname = al’thor mother = people:a9d4b3a0-50d3-4538-a2fc-f7fc2581ee56 (1) 1 Reference stores the whole key ( keyspace:id ) of the referenced object. Referenced Objects are not persisted when the referencing object is saved. You must persist changes on referenced objects separately, since only the reference is stored. Indexes set on properties of referenced types are not resolved. Persisting Partial Updates: In some cases, you need not load and rewrite the entire entity just to set a new value within it. A session timestamp for the last active time might be such a scenario where you want to alter one property. PartialUpdate lets you define set and delete actions on existing objects while taking care of updating potential expiration times of both the entity itself and index structures. The following example shows a partial update: Example 6. Sample Partial Update PartialUpdate<Person> update = new PartialUpdate<Person>(""e2c7dcee"", Person.class) .set(""firstname"", ""mat"") (1) .set(""address.city"", ""emond's field"") (2) .del(""age""); (3) template.update(update); update = new PartialUpdate<Person>(""e2c7dcee"", Person.class) .set(""address"", new Address(""caemlyn"", ""andor"")) (4) .set(""attributes"", singletonMap(""eye-color"", ""grey"")); (5) template.update(update); update = new PartialUpdate<Person>(""e2c7dcee"", Person.class) .refreshTtl(true); (6) .set(""expiration"", 1000); template.update(update); 1 Set the simple firstname property to mat . 2 Set the simple 'address.city' property to 'emond’s field' without having to pass in the entire object. This does not work when a custom conversion is registered. 3 Remove the age property. 4 Set complex address property. 5 Set a map of values, which removes the previously existing map and replaces the values with the given ones. 6 Automatically update the server expiration time when altering Time To Live(expirations.html) . Updating complex objects as well as map (or other collection) structures requires further interaction with Redis to determine existing values, which means that rewriting the entire entity might be faster."
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/object-mapping.html","Object Mapping Fundamentals: This section covers the fundamentals of Spring Data object mapping, object creation, field and property access, mutability and immutability. Note, that this section only applies to Spring Data modules that do not use the object mapping of the underlying data store (like JPA). Also be sure to consult the store-specific sections for store-specific object mapping, like indexes, customizing column or field names or the like. Core responsibility of the Spring Data object mapping is to create instances of domain objects and map the store-native data structures onto those. This means we need two fundamental steps: Instance creation by using one of the constructors exposed. Instance population to materialize all exposed properties. Object creation: Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. The value resolution assumes constructor/factory method argument names to match the property names of the entity, i.e. the resolution will be performed as if the property was to be populated, including all customizations in mapping (different datastore column or field name etc.). This also requires either parameter names information available in the class file or an @ConstructorProperties annotation being present on the constructor. The value resolution can be customized by using Spring Framework’s @Value value annotation using a store-specific SpEL expression. Please consult the section on store specific mappings for further details. Object creation internals To avoid the overhead of reflection, Spring Data object creation uses a factory class generated at runtime by default, which will call the domain classes constructor directly. I.e. for this example type: class Person { Person(String firstname, String lastname) { … } } we will create a factory class semantically equivalent to this one at runtime: class PersonObjectInstantiator implements ObjectInstantiator { Object newInstance(Object... args) { return new Person((String) args[0], (String) args[1]); } } This gives us a roundabout 10% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: it must not be a private class it must not be a non-static inner class it must not be a CGLib proxy class the constructor to be used by Spring Data must not be private If any of these criteria match, Spring Data will fall back to entity instantiation via reflection. Property population: Once an instance of the entity has been created, Spring Data populates all remaining persistent properties of that class. Unless already populated by the entity’s constructor (i.e. consumed through its constructor argument list), the identifier property will be populated first to allow the resolution of cyclic object references. After that, all non-transient properties that have not already been populated by the constructor are set on the entity instance. For that we use the following algorithm: If the property is immutable but exposes a with… method (see below), we use the with… method to create a new entity instance with the new property value. If property access (i.e. access through getters and setters) is defined, we’re invoking the setter method. If the property is mutable we set the field directly. If the property is immutable we’re using the constructor to be used by persistence operations (see Object creation(#mapping.object-creation) ) to create a copy of the instance. By default, we set the field value directly. Property population internals Similarly to our optimizations in object construction(#mapping.object-creation.details) we also use Spring Data runtime generated accessor classes to interact with the entity instance. class Person { private final Long id; private String firstname; private @AccessType(Type.PROPERTY) String lastname; Person() { this.id = null; } Person(Long id, String firstname, String lastname) { // Field assignments } Person withId(Long id) { return new Person(id, this.firstname, this.lastame); } void setLastname(String lastname) { this.lastname = lastname; } } A generated Property Accessor class PersonPropertyAccessor implements PersistentPropertyAccessor { private static final MethodHandle firstname; (2) private Person person; (1) public void setProperty(PersistentProperty property, Object value) { String name = property.getName(); if (""firstname"".equals(name)) { firstname.invoke(person, (String) value); (2) } else if (""id"".equals(name)) { this.person = person.withId((Long) value); (3) } else if (""lastname"".equals(name)) { this.person.setLastname((String) value); (4) } } } 1 PropertyAccessor’s hold a mutable instance of the underlying object. This is, to enable mutations of otherwise immutable properties. 2 By default, Spring Data uses field-access to read and write property values. As per visibility rules of private fields, MethodHandles are used to interact with fields. 3 The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. Calling withId(…) creates a new Person object. All subsequent mutations will take place in the new instance leaving the previous untouched. 4 Using property-access allows direct method invocations without using MethodHandles . This gives us a roundabout 25% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: Types must not reside in the default or under the java package. Types and their constructors must be public Types that are inner classes must be static . The used Java Runtime must allow for declaring classes in the originating ClassLoader . Java 9 and newer impose certain limitations. By default, Spring Data attempts to use generated property accessors and falls back to reflection-based ones if a limitation is detected. Let’s have a look at the following entity: A sample entity class Person { private final @Id Long id; (1) private final String firstname, lastname; (2) private final LocalDate birthday; private final int age; (3) private String comment; (4) private @AccessType(Type.PROPERTY) String remarks; (5) static Person of(String firstname, String lastname, LocalDate birthday) { (6) return new Person(null, firstname, lastname, birthday, Period.between(birthday, LocalDate.now()).getYears()); } Person(Long id, String firstname, String lastname, LocalDate birthday, int age) { (6) this.id = id; this.firstname = firstname; this.lastname = lastname; this.birthday = birthday; this.age = age; } Person withId(Long id) { (1) return new Person(id, this.firstname, this.lastname, this.birthday, this.age); } void setRemarks(String remarks) { (5) this.remarks = remarks; } } 1 The identifier property is final but set to null in the constructor. The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. The original Person instance stays unchanged as a new one is created. The same pattern is usually applied for other properties that are store managed but might have to be changed for persistence operations. The wither method is optional as the persistence constructor (see 6) is effectively a copy constructor and setting the property will be translated into creating a fresh instance with the new identifier value applied. 2 The firstname and lastname properties are ordinary immutable properties potentially exposed through getters. 3 The age property is an immutable but derived one from the birthday property. With the design shown, the database value will trump the defaulting as Spring Data uses the only declared constructor. Even if the intent is that the calculation should be preferred, it’s important that this constructor also takes age as parameter (to potentially ignore it) as otherwise the property population step will attempt to set the age field and fail due to it being immutable and no with… method being present. 4 The comment property is mutable and is populated by setting its field directly. 5 The remarks property is mutable and is populated by invoking the setter method. 6 The class exposes a factory method and a constructor for object creation. The core idea here is to use factory methods instead of additional constructors to avoid the need for constructor disambiguation through @PersistenceCreator . Instead, defaulting of properties is handled within the factory method. If you want Spring Data to use the factory method for object instantiation, annotate it with @PersistenceCreator . General recommendations: Try to stick to immutable objects — Immutable objects are straightforward to create as materializing an object is then a matter of calling its constructor only. Also, this avoids your domain objects to be littered with setter methods that allow client code to manipulate the objects state. If you need those, prefer to make them package protected so that they can only be invoked by a limited amount of co-located types. Constructor-only materialization is up to 30% faster than properties population. Provide an all-args constructor — Even if you cannot or don’t want to model your entities as immutable values, there’s still value in providing a constructor that takes all properties of the entity as arguments, including the mutable ones, as this allows the object mapping to skip the property population for optimal performance. Use factory methods instead of overloaded constructors to avoid @PersistenceCreator — With an all-argument constructor needed for optimal performance, we usually want to expose more application use case specific constructors that omit things like auto-generated identifiers etc. It’s an established pattern to rather use static factory methods to expose these variants of the all-args constructor. Make sure you adhere to the constraints that allow the generated instantiator and property accessor classes to be used — For identifiers to be generated, still use a final field in combination with an all-arguments persistence constructor (preferred) or a with… method — Use Lombok to avoid boilerplate code — As persistence operations usually require a constructor taking all arguments, their declaration becomes a tedious repetition of boilerplate parameter to field assignments that can best be avoided by using Lombok’s @AllArgsConstructor . Overriding Properties: Java’s allows a flexible design of domain classes where a subclass could define a property that is already declared with the same name in its superclass. Consider the following example: public class SuperType { private CharSequence field; public SuperType(CharSequence field) { this.field = field; } public CharSequence getField() { return this.field; } public void setField(CharSequence field) { this.field = field; } } public class SubType extends SuperType { private String field; public SubType(String field) { super(field); this.field = field; } @Override public String getField() { return this.field; } public void setField(String field) { this.field = field; // optional super.setField(field); } } Both classes define a field using assignable types. SubType however shadows SuperType.field . Depending on the class design, using the constructor could be the only default approach to set SuperType.field . Alternatively, calling super.setField(…) in the setter could set the field in SuperType . All these mechanisms create conflicts to some degree because the properties share the same name yet might represent two distinct values. Spring Data skips super-type properties if types are not assignable. That is, the type of the overridden property must be assignable to its super-type property type to be registered as override, otherwise the super-type property is considered transient. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be generally set without making any further assumptions of the setter implementation. Kotlin support: Spring Data adapts specifics of Kotlin to allow object creation and mutation. Kotlin object creation: Kotlin classes are supported to be instantiated, all classes are immutable by default and require explicit property declarations to define mutable properties. Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a constructor that is annotated with @PersistenceCreator , it is used. If the type is a Kotlin data class(#mapping.kotlin) the primary constructor is used. If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. Consider the following data class Person : data class Person(val id: String, val name: String) The class above compiles to a typical class with an explicit constructor. We can customize this class by adding another constructor and annotate it with @PersistenceCreator to indicate a constructor preference: data class Person(var id: String, val name: String) { @PersistenceCreator constructor(id: String) : this(id, ""unknown"") } Kotlin supports parameter optionality by allowing default values to be used if a parameter is not provided. When Spring Data detects a constructor with parameter defaulting, then it leaves these parameters absent if the data store does not provide a value (or simply returns null ) so Kotlin can apply parameter defaulting.Consider the following class that applies parameter defaulting for name data class Person(var id: String, val name: String = ""unknown"") Every time the name parameter is either not part of the result or its value is null , then the name defaults to unknown . Delegated properties are not supported with Spring Data. The mapping metadata filters delegated properties for Kotlin Data classes. In all other cases you can exclude synthetic fields for delegated properties by annotating the property with @delegate:org.springframework.data.annotation.Transient . Property population of Kotlin data classes: In Kotlin, all classes are immutable by default and require explicit property declarations to define mutable properties. Consider the following data class Person : data class Person(val id: String, val name: String) This class is effectively immutable. It allows creating new instances as Kotlin generates a copy(…) method that creates new object instances copying all property values from the existing object and applying property values provided as arguments to the method. Kotlin Overriding Properties: Kotlin allows declaring property overrides(https://kotlinlang.org/docs/inheritance.html#overriding-properties) to alter properties in subclasses. open class SuperType(open var field: Int) class SubType(override var field: Int = 1) : SuperType(field) { } Such an arrangement renders two properties with the name field . Kotlin generates property accessors (getters and setters) for each property in each class. Effectively, the code looks like as follows: public class SuperType { private int field; public SuperType(int field) { this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } public final class SubType extends SuperType { private int field; public SubType(int field) { super(field); this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } Getters and setters on SubType set only SubType.field and not SuperType.field . In such an arrangement, using the constructor is the only default approach to set SuperType.field . Adding a method to SubType to set SuperType.field via this.SuperType.field = … is possible but falls outside of supported conventions. Property overrides create conflicts to some degree because the properties share the same name yet might represent two distinct values. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be set. Kotlin Value Classes: Kotlin Value Classes are designed for a more expressive domain model to make underlying concepts explicit. Spring Data can read and write types that define properties using Value Classes. Consider the following domain model: @JvmInline value class EmailAddress(val theAddress: String) (1) data class Contact(val id: String, val name:String, val emailAddress: EmailAddress) (2) 1 A simple value class with a non-nullable value type. 2 Data class defining a property using the EmailAddress value class. Non-nullable properties using non-primitive value types are flattened in the compiled class to the value type. Nullable primitive value types or nullable value-in-value types are represented with their wrapper type and that affects how value types are represented in the database."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/mapping.html","Object-to-Hash Mapping: The Redis Repository support persists Objects to Hashes. This requires an Object-to-Hash conversion which is done by a RedisConverter . The default implementation uses Converter for mapping property values to and from Redis native byte[] . Given the Person type from the previous sections, the default mapping looks like the following: _class = org.example.Person (1) id = e2c7dcee-b8cd-4424-883e-736ce564363e firstname = rand (2) lastname = al’thor address.city = emond's field (3) address.country = andor 1 The _class attribute is included on the root level as well as on any nested interface or abstract types. 2 Simple property values are mapped by path. 3 Properties of complex types are mapped by their dot path. Data Mapping and Type Conversion: This section explains how types are mapped to and from a Hash representation: Table 1. Default Mapping Rules Type Sample Mapped Value Simple Type (for example, String) String firstname = ""rand""; firstname = ""rand"" Byte array ( byte[] ) byte[] image = ""rand"".getBytes(); image = ""rand"" Complex Type (for example, Address) Address address = new Address(""emond’s field""); address.city = ""emond’s field"" List of Simple Type List<String> nicknames = asList(""dragon reborn"", ""lews therin""); nicknames.[0] = ""dragon reborn"", nicknames.[1] = ""lews therin"" Map of Simple Type Map<String, String> atts = asMap({""eye-color"", ""grey""}, {""…​ atts.[eye-color] = ""grey"", atts.[hair-color] = ""…​ List of Complex Type List<Address> addresses = asList(new Address(""em…​ addresses.[0].city = ""emond’s field"", addresses.[1].city = ""…​ Map of Complex Type Map<String, Address> addresses = asMap({""home"", new Address(""em…​ addresses.[home].city = ""emond’s field"", addresses.[work].city = ""…​ Due to the flat representation structure, Map keys need to be simple types, such as String or Number . Mapping behavior can be customized by registering the corresponding Converter in RedisCustomConversions . Those converters can take care of converting from and to a single byte[] as well as Map<String, byte[]> . The first one is suitable for (for example) converting a complex type to (for example) a binary JSON representation that still uses the default mappings hash structure. The second option offers full control over the resulting hash. Writing objects to a Redis hash deletes the content from the hash and re-creates the whole hash, so data that has not been mapped is lost. The following example shows two sample byte array converters: Example 1. Sample byte[] Converters @WritingConverter public class AddressToBytesConverter implements Converter<Address, byte[]> { private final Jackson2JsonRedisSerializer<Address> serializer; public AddressToBytesConverter() { serializer = new Jackson2JsonRedisSerializer<Address>(Address.class); serializer.setObjectMapper(new ObjectMapper()); } @Override public byte[] convert(Address value) { return serializer.serialize(value); } } @ReadingConverter public class BytesToAddressConverter implements Converter<byte[], Address> { private final Jackson2JsonRedisSerializer<Address> serializer; public BytesToAddressConverter() { serializer = new Jackson2JsonRedisSerializer<Address>(Address.class); serializer.setObjectMapper(new ObjectMapper()); } @Override public Address convert(byte[] value) { return serializer.deserialize(value); } } Using the preceding byte array Converter produces output similar to the following: _class = org.example.Person id = e2c7dcee-b8cd-4424-883e-736ce564363e firstname = rand lastname = al’thor address = { city : ""emond's field"", country : ""andor"" } The following example shows two examples of Map converters: Example 2. Sample Map<String, byte[]> Converters @WritingConverter public class AddressToMapConverter implements Converter<Address, Map<String, byte[]>> { @Override public Map<String, byte[]> convert(Address source) { return singletonMap(""ciudad"", source.getCity().getBytes()); } } @ReadingConverter public class MapToAddressConverter implements Converter<Map<String, byte[]>, Address> { @Override public Address convert(Map<String, byte[]> source) { return new Address(new String(source.get(""ciudad""))); } } Using the preceding Map Converter produces output similar to the following: _class = org.example.Person id = e2c7dcee-b8cd-4424-883e-736ce564363e firstname = rand lastname = al’thor ciudad = ""emond's field"" Custom conversions have no effect on index resolution. Secondary Indexes(indexes.html) are still created, even for custom converted types. Customizing Type Mapping: If you want to avoid writing the entire Java class name as type information and would rather like to use a key, you can use the @TypeAlias annotation on the entity class being persisted. If you need to customize the mapping even more, look at the TypeInformationMapper(https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/convert/TypeInformationMapper.html) interface. An instance of that interface can be configured at the DefaultRedisTypeMapper , which can be configured on MappingRedisConverter . The following example shows how to define a type alias for an entity: Example 3. Defining @TypeAlias for an entity @TypeAlias(""pers"") class Person { } The resulting document contains pers as the value in a _class field. Configuring Custom Type Mapping: The following example demonstrates how to configure a custom RedisTypeMapper in MappingRedisConverter : Example 4. Configuring a custom RedisTypeMapper via Spring Java Config class CustomRedisTypeMapper extends DefaultRedisTypeMapper { //implement custom type mapping here } @Configuration class SampleRedisConfiguration { @Bean public MappingRedisConverter redisConverter(RedisMappingContext mappingContext, RedisCustomConversions customConversions, ReferenceResolver referenceResolver) { MappingRedisConverter mappingRedisConverter = new MappingRedisConverter(mappingContext, null, referenceResolver, customTypeMapper()); mappingRedisConverter.setCustomConversions(customConversions); return mappingRedisConverter; } @Bean public RedisTypeMapper customTypeMapper() { return new CustomRedisTypeMapper(); } }"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/keyspaces.html","Keyspaces: Keyspaces define prefixes used to create the actual key for the Redis Hash. By default, the prefix is set to getClass().getName() . You can alter this default by setting @RedisHash on the aggregate root level or by setting up a programmatic configuration. However, the annotated keyspace supersedes any other configuration. The following example shows how to set the keyspace configuration with the @EnableRedisRepositories annotation: Example 1. Keyspace Setup via @EnableRedisRepositories @Configuration @EnableRedisRepositories(keyspaceConfiguration = MyKeyspaceConfiguration.class) public class ApplicationConfig { //... RedisConnectionFactory and RedisTemplate Bean definitions omitted public static class MyKeyspaceConfiguration extends KeyspaceConfiguration { @Override protected Iterable<KeyspaceSettings> initialConfiguration() { return Collections.singleton(new KeyspaceSettings(Person.class, ""people"")); } } } The following example shows how to programmatically set the keyspace: Example 2. Programmatic Keyspace setup @Configuration @EnableRedisRepositories public class ApplicationConfig { //... RedisConnectionFactory and RedisTemplate Bean definitions omitted @Bean public RedisMappingContext keyValueMappingContext() { return new RedisMappingContext( new MappingConfiguration(new IndexConfiguration(), new MyKeyspaceConfiguration())); } public static class MyKeyspaceConfiguration extends KeyspaceConfiguration { @Override protected Iterable<KeyspaceSettings> initialConfiguration() { return Collections.singleton(new KeyspaceSettings(Person.class, ""people"")); } } }"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/indexes.html","Secondary Indexes: Secondary indexes(https://redis.io/topics/indexes) are used to enable lookup operations based on native Redis structures. Values are written to the according indexes on every save and are removed when objects are deleted or expire(expirations.html) . Simple Property Index: Given the sample Person entity shown earlier, we can create an index for firstname by annotating the property with @Indexed , as shown in the following example: Example 1. Annotation driven indexing @RedisHash(""people"") public class Person { @Id String id; @Indexed String firstname; String lastname; Address address; } Indexes are built up for actual property values. Saving two Persons (for example, ""rand"" and ""aviendha"") results in setting up indexes similar to the following: SADD people:firstname:rand e2c7dcee-b8cd-4424-883e-736ce564363e SADD people:firstname:aviendha a9d4b3a0-50d3-4538-a2fc-f7fc2581ee56 It is also possible to have indexes on nested elements. Assume Address has a city property that is annotated with @Indexed . In that case, once person.address.city is not null , we have Sets for each city, as shown in the following example: SADD people:address.city:tear e2c7dcee-b8cd-4424-883e-736ce564363e Furthermore, the programmatic setup lets you define indexes on map keys and list properties, as shown in the following example: @RedisHash(""people"") public class Person { // ... other properties omitted Map<String, String> attributes; (1) Map<String, Person> relatives; (2) List<Address> addresses; (3) } 1 SADD people:attributes.map-key:map-value e2c7dcee-b8cd-4424-883e-736ce564363e 2 SADD people:relatives.map-key.firstname:tam e2c7dcee-b8cd-4424-883e-736ce564363e 3 SADD people:addresses.city:tear e2c7dcee-b8cd-4424-883e-736ce564363e Indexes cannot be resolved on References(usage.html#redis.repositories.references) . As with keyspaces, you can configure indexes without needing to annotate the actual domain type, as shown in the following example: Example 2. Index Setup with @EnableRedisRepositories @Configuration @EnableRedisRepositories(indexConfiguration = MyIndexConfiguration.class) public class ApplicationConfig { //... RedisConnectionFactory and RedisTemplate Bean definitions omitted public static class MyIndexConfiguration extends IndexConfiguration { @Override protected Iterable<IndexDefinition> initialConfiguration() { return Collections.singleton(new SimpleIndexDefinition(""people"", ""firstname"")); } } } Again, as with keyspaces, you can programmatically configure indexes, as shown in the following example: Example 3. Programmatic Index setup @Configuration @EnableRedisRepositories public class ApplicationConfig { //... RedisConnectionFactory and RedisTemplate Bean definitions omitted @Bean public RedisMappingContext keyValueMappingContext() { return new RedisMappingContext( new MappingConfiguration( new KeyspaceConfiguration(), new MyIndexConfiguration())); } public static class MyIndexConfiguration extends IndexConfiguration { @Override protected Iterable<IndexDefinition> initialConfiguration() { return Collections.singleton(new SimpleIndexDefinition(""people"", ""firstname"")); } } } Geospatial Index: Assume the Address type contains a location property of type Point that holds the geo coordinates of the particular address. By annotating the property with @GeoIndexed , Spring Data Redis adds those values by using Redis GEO commands, as shown in the following example: @RedisHash(""people"") public class Person { Address address; // ... other properties omitted } public class Address { @GeoIndexed Point location; // ... other properties omitted } public interface PersonRepository extends CrudRepository<Person, String> { List<Person> findByAddressLocationNear(Point point, Distance distance); (1) List<Person> findByAddressLocationWithin(Circle circle); (2) } Person rand = new Person(""rand"", ""al'thor""); rand.setAddress(new Address(new Point(13.361389D, 38.115556D))); repository.save(rand); (3) repository.findByAddressLocationNear(new Point(15D, 37D), new Distance(200, Metrics.KILOMETERS)); (4) 1 Query method declaration on a nested property, using Point and Distance . 2 Query method declaration on a nested property, using Circle to search within. 3 GEOADD people:address:location 13.361389 38.115556 e2c7dcee-b8cd-4424-883e-736ce564363e 4 GEORADIUS people:address:location 15.0 37.0 200.0 km In the preceding example the, longitude and latitude values are stored by using GEOADD that use the object’s id as the member’s name. The finder methods allow usage of Circle or Point, Distance combinations for querying those values. It is not possible to combine near and within with other criteria."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/expirations.html","Time To Live: Objects stored in Redis may be valid only for a certain amount of time. This is especially useful for persisting short-lived objects in Redis without having to remove them manually when they reach their end of life. The expiration time in seconds can be set with @RedisHash(timeToLive=…​) as well as by using KeyspaceConfiguration.KeyspaceSettings(../../api/java/org/springframework/data/redis/core/convert/KeyspaceConfiguration.KeyspaceSettings.html) (see Keyspaces(keyspaces.html) ). More flexible expiration times can be set by using the @TimeToLive annotation on either a numeric property or a method. However, do not apply @TimeToLive on both a method and a property within the same class. The following example shows the @TimeToLive annotation on a property and on a method: Example 1. Expirations public class TimeToLiveOnProperty { @Id private String id; @TimeToLive private Long expiration; } public class TimeToLiveOnMethod { @Id private String id; @TimeToLive public long getTimeToLive() { return new Random().nextLong(); } } Annotating a property explicitly with @TimeToLive reads back the actual TTL or PTTL value from Redis. -1 indicates that the object has no associated expiration. The repository implementation ensures subscription to Redis keyspace notifications(https://redis.io/topics/notifications) via RedisMessageListenerContainer(../../api/java/org/springframework/data/redis/listener/RedisMessageListenerContainer.html) . When the expiration is set to a positive value, the corresponding EXPIRE command is run. In addition to persisting the original, a phantom copy is persisted in Redis and set to expire five minutes after the original one. This is done to enable the Repository support to publish RedisKeyExpiredEvent(../../api/java/org/springframework/data/redis/core/RedisKeyExpiredEvent.html) , holding the expired value in Spring’s ApplicationEventPublisher whenever a key expires, even though the original values have already been removed. Expiry events are received on all connected applications that use Spring Data Redis repositories. By default, the key expiry listener is disabled when initializing the application. The startup mode can be adjusted in @EnableRedisRepositories or RedisKeyValueAdapter to start the listener with the application or upon the first insert of an entity with a TTL. See RedisKeyValueAdapter.EnableKeyspaceEvents(../../api/java/org/springframework/data/redis/core/RedisKeyValueAdapter.EnableKeyspaceEvents.html) for possible values. The RedisKeyExpiredEvent holds a copy of the expired domain object as well as the key. Delaying or disabling the expiry event listener startup impacts RedisKeyExpiredEvent publishing. A disabled event listener does not publish expiry events. A delayed startup can cause loss of events because of the delayed listener initialization. The keyspace notification message listener alters notify-keyspace-events settings in Redis, if those are not already set. Existing settings are not overridden, so you must set up those settings correctly (or leave them empty). Note that CONFIG is disabled on AWS ElastiCache, and enabling the listener leads to an error. To work around this behavior, set the keyspaceNotificationsConfigParameter parameter to an empty string. This prevents CONFIG command usage. Redis Pub/Sub messages are not persistent. If a key expires while the application is down, the expiry event is not processed, which may lead to secondary indexes containing references to the expired object. @EnableKeyspaceEvents(shadowCopy = OFF) disable storage of phantom copies and reduces data size within Redis. RedisKeyExpiredEvent will only contain the id of the expired key."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/queries.html","Redis-specific Query Methods: Query methods allow automatic derivation of simple finder queries from the method name, as shown in the following example: Example 1. Sample Repository finder Method public interface PersonRepository extends CrudRepository<Person, String> { List<Person> findByFirstname(String firstname); } Please make sure properties used in finder methods are set up for indexing. Query methods for Redis repositories support only queries for entities and collections of entities with paging. Using derived query methods might not always be sufficient to model the queries to run. RedisCallback offers more control over the actual matching of index structures or even custom indexes. To do so, provide a RedisCallback that returns a single or Iterable set of id values, as shown in the following example: Example 2. Sample finder using RedisCallback String user = //... List<RedisSession> sessionsByUser = template.find(new RedisCallback<Set<byte[]>>() { public Set<byte[]> doInRedis(RedisConnection connection) throws DataAccessException { return connection .sMembers(""sessions:securityContext.authentication.principal.username:"" + user); }}, RedisSession.class); The following table provides an overview of the keywords supported for Redis and what a method containing that keyword essentially translates to: Table 1. Supported keywords inside method names Keyword Sample Redis snippet And findByLastnameAndFirstname SINTER …:firstname:rand …:lastname:al’thor Or findByLastnameOrFirstname SUNION …:firstname:rand …:lastname:al’thor Is, Equals findByFirstname , findByFirstnameIs , findByFirstnameEquals SINTER …:firstname:rand IsTrue FindByAliveIsTrue SINTER …:alive:1 IsFalse findByAliveIsFalse SINTER …:alive:0 Top,First findFirst10ByFirstname , findTop5ByFirstname Sorting Query Method results: Redis repositories allow various approaches to define sorting order. Redis itself does not support in-flight sorting when retrieving hashes or sets. Therefore, Redis repository query methods construct a Comparator that is applied to the result before returning results as List . Let’s take a look at the following example: Example 3. Sorting Query Results interface PersonRepository extends RedisRepository<Person, String> { List<Person> findByFirstnameOrderByAgeDesc(String firstname); (1) List<Person> findByFirstname(String firstname, Sort sort); (2) } 1 Static sorting derived from method name. 2 Dynamic sorting using a method argument."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/query-by-example.html","Query by Example: Introduction: This chapter provides an introduction to Query by Example and explains how to use it. Query by Example (QBE) is a user-friendly querying technique with a simple interface. It allows dynamic query creation and does not require you to write queries that contain field names. In fact, Query by Example does not require you to write queries by using store-specific query languages at all. This chapter explains the core concepts of Query by Example. The information is pulled from the Spring Data Commons module. Depending on your database, String matching support can be limited. Usage: The Query by Example API consists of four parts: Probe: The actual example of a domain object with populated fields. ExampleMatcher : The ExampleMatcher carries details on how to match particular fields. It can be reused across multiple Examples. Example : An Example consists of the probe and the ExampleMatcher . It is used to create the query. FetchableFluentQuery : A FetchableFluentQuery offers a fluent API, that allows further customization of a query derived from an Example . Using the fluent API lets you specify ordering projection and result processing for your query. Query by Example is well suited for several use cases: Querying your data store with a set of static or dynamic constraints. Frequent refactoring of the domain objects without worrying about breaking existing queries. Working independently of the underlying data store API. Query by Example also has several limitations: No support for nested or grouped property constraints, such as firstname = ?0 or (firstname = ?1 and lastname = ?2) . Store-specific support on string matching. Depending on your databases, String matching can support starts/contains/ends/regex for strings. Exact matching for other property types. Before getting started with Query by Example, you need to have a domain object. To get started, create an interface for your repository, as shown in the following example: Sample Person object public class Person { @Id private String id; private String firstname; private String lastname; private Address address; // … getters and setters omitted } The preceding example shows a simple domain object. You can use it to create an Example . By default, fields having null values are ignored, and strings are matched by using the store specific defaults. Inclusion of properties into a Query by Example criteria is based on nullability. Properties using primitive types ( int , double , …) are always included unless the ExampleMatcher ignores the property path(#query-by-example.matchers) . Examples can be built by either using the of factory method or by using ExampleMatcher(#query-by-example.matchers) . Example is immutable. The following listing shows a simple Example: Example 1. Simple Example Person person = new Person(); (1) person.setFirstname(""Dave""); (2) Example<Person> example = Example.of(person); (3) 1 Create a new instance of the domain object. 2 Set the properties to query. 3 Create the Example . You can run the example queries by using repositories. To do so, let your repository interface extend QueryByExampleExecutor<T> . The following listing shows an excerpt from the QueryByExampleExecutor interface: The QueryByExampleExecutor public interface QueryByExampleExecutor<T> { <S extends T> S findOne(Example<S> example); <S extends T> Iterable<S> findAll(Example<S> example); // … more functionality omitted. } Example Matchers: Examples are not limited to default settings. You can specify your own defaults for string matching, null handling, and property-specific settings by using the ExampleMatcher , as shown in the following example: Example 2. Example matcher with customized matching Person person = new Person(); (1) person.setFirstname(""Dave""); (2) ExampleMatcher matcher = ExampleMatcher.matching() (3) .withIgnorePaths(""lastname"") (4) .withIncludeNullValues() (5) .withStringMatcher(StringMatcher.ENDING); (6) Example<Person> example = Example.of(person, matcher); (7) 1 Create a new instance of the domain object. 2 Set properties. 3 Create an ExampleMatcher to expect all values to match. It is usable at this stage even without further configuration. 4 Construct a new ExampleMatcher to ignore the lastname property path. 5 Construct a new ExampleMatcher to ignore the lastname property path and to include null values. 6 Construct a new ExampleMatcher to ignore the lastname property path, to include null values, and to perform suffix string matching. 7 Create a new Example based on the domain object and the configured ExampleMatcher . By default, the ExampleMatcher expects all values set on the probe to match. If you want to get results matching any of the predicates defined implicitly, use ExampleMatcher.matchingAny() . You can specify behavior for individual properties (such as ""firstname"" and ""lastname"" or, for nested properties, ""address.city""). You can tune it with matching options and case sensitivity, as shown in the following example: Configuring matcher options ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", endsWith()) .withMatcher(""lastname"", startsWith().ignoreCase()); } Another way to configure matcher options is to use lambdas (introduced in Java 8). This approach creates a callback that asks the implementor to modify the matcher. You need not return the matcher, because configuration options are held within the matcher instance. The following example shows a matcher that uses lambdas: Configuring matcher options with lambdas ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", match -> match.endsWith()) .withMatcher(""firstname"", match -> match.startsWith()); } Queries created by Example use a merged view of the configuration. Default matching settings can be set at the ExampleMatcher level, while individual settings can be applied to particular property paths. Settings that are set on ExampleMatcher are inherited by property path settings unless they are defined explicitly. Settings on a property patch have higher precedence than default settings. The following table describes the scope of the various ExampleMatcher settings: Table 1. Scope of ExampleMatcher settings Setting Scope Null-handling ExampleMatcher String matching ExampleMatcher and property path Ignoring properties Property path Case sensitivity ExampleMatcher and property path Value transformation Property path Fluent API: QueryByExampleExecutor offers one more method, which we did not mention so far: <S extends T, R> R findBy(Example<S> example, Function<FluentQuery.FetchableFluentQuery<S>, R> queryFunction) . As with other methods, it executes a query derived from an Example . However, with the second argument, you can control aspects of that execution that you cannot dynamically control otherwise. You do so by invoking the various methods of the FetchableFluentQuery in the second argument. sortBy lets you specify an ordering for your result. as lets you specify the type to which you want the result to be transformed. project limits the queried attributes. first , firstValue , one , oneValue , all , page , stream , count , and exists define what kind of result you get and how the query behaves when more than the expected number of results are available. Use the fluent API to get the last of potentially many results, ordered by lastname. Optional<Person> match = repository.findBy(example, q -> q .sortBy(Sort.by(""lastname"").descending()) .first() ); Running an Example: The following example uses Query by Example against a repository: Example 3. Query by Example using a Repository interface PersonRepository extends ListQueryByExampleExecutor<Person> { } class PersonService { @Autowired PersonRepository personRepository; List<Person> findPeople(Person probe) { return personRepository.findAll(Example.of(probe)); } } Redis Repositories support, with their secondary indexes, a subset of Spring Data’s Query by Example features. In particular, only exact, case-sensitive, and non-null values are used to construct a query. Secondary indexes use set-based operations (Set intersection, Set union) to determine matching keys. Adding a property to the query that is not indexed returns no result, because no index exists. Query by Example support inspects indexing configuration to include only properties in the query that are covered by an index. This is to prevent accidental inclusion of non-indexed properties. Case-insensitive queries and unsupported StringMatcher instances are rejected at runtime. The following list shows the supported Query by Example options: Case-sensitive, exact matching of simple and nested properties Any/All match modes Value transformation of the criteria value Exclusion of null values from the criteria The following list shows properties not supported by Query by Example: Case-insensitive matching Regex, prefix/contains/suffix String-matching Querying of Associations, Collection, and Map-like properties Inclusion of null values from the criteria findAll with sorting"
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/cluster.html","Redis Repositories Running on a Cluster: You can use the Redis repository support in a clustered Redis environment. See the “ Redis Cluster(../cluster.html) ” section for ConnectionFactory configuration details. Still, some additional configuration must be done, because the default key distribution spreads entities and secondary indexes through out the whole cluster and its slots. The following table shows the details of data on a cluster (based on previous examples): Key Type Slot Node people:e2c7dcee-b8cd-4424-883e-736ce564363e id for hash 15171 127.0.0.1:7381 people:a9d4b3a0-50d3-4538-a2fc-f7fc2581ee56 id for hash 7373 127.0.0.1:7380 people:firstname:rand index 1700 127.0.0.1:7379 Some commands (such as SINTER and SUNION ) can only be processed on the server side when all involved keys map to the same slot. Otherwise, computation has to be done on client side. Therefore, it is useful to pin keyspaces to a single slot, which lets make use of Redis server side computation right away. The following table shows what happens when you do (note the change in the slot column and the port value in the node column): Key Type Slot Node {people}:e2c7dcee-b8cd-4424-883e-736ce564363e id for hash 2399 127.0.0.1:7379 {people}:a9d4b3a0-50d3-4538-a2fc-f7fc2581ee56 id for hash 2399 127.0.0.1:7379 {people}:firstname:rand index 2399 127.0.0.1:7379 Define and pin keyspaces by using @RedisHash(""{yourkeyspace}"") to specific slots when you use Redis cluster."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/anatomy.html","Redis Repositories Anatomy: Redis as a store itself offers a very narrow low-level API leaving higher level functions, such as secondary indexes and query operations, up to the user. This section provides a more detailed view of commands issued by the repository abstraction for a better understanding of potential performance implications. Consider the following entity class as the starting point for all operations: Example 1. Example entity @RedisHash(""people"") public class Person { @Id String id; @Indexed String firstname; String lastname; Address hometown; } public class Address { @GeoIndexed Point location; } Insert new: repository.save(new Person(""rand"", ""al'thor"")); HMSET ""people:19315449-cda2-4f5c-b696-9cb8018fa1f9"" ""_class"" ""Person"" ""id"" ""19315449-cda2-4f5c-b696-9cb8018fa1f9"" ""firstname"" ""rand"" ""lastname"" ""al'thor"" (1) SADD ""people"" ""19315449-cda2-4f5c-b696-9cb8018fa1f9"" (2) SADD ""people:firstname:rand"" ""19315449-cda2-4f5c-b696-9cb8018fa1f9"" (3) SADD ""people:19315449-cda2-4f5c-b696-9cb8018fa1f9:idx"" ""people:firstname:rand"" (4) 1 Save the flattened entry as hash. 2 Add the key of the hash written in <1> to the helper index of entities in the same keyspace. 3 Add the key of the hash written in <2> to the secondary index of firstnames with the properties value. 4 Add the index of <3> to the set of helper structures for entry to keep track of indexes to clean on delete/update. Replace existing: repository.save(new Person(""e82908cf-e7d3-47c2-9eec-b4e0967ad0c9"", ""Dragon Reborn"", ""al'thor"")); DEL ""people:e82908cf-e7d3-47c2-9eec-b4e0967ad0c9"" (1) HMSET ""people:e82908cf-e7d3-47c2-9eec-b4e0967ad0c9"" ""_class"" ""Person"" ""id"" ""e82908cf-e7d3-47c2-9eec-b4e0967ad0c9"" ""firstname"" ""Dragon Reborn"" ""lastname"" ""al'thor"" (2) SADD ""people"" ""e82908cf-e7d3-47c2-9eec-b4e0967ad0c9"" (3) SMEMBERS ""people:e82908cf-e7d3-47c2-9eec-b4e0967ad0c9:idx"" (4) TYPE ""people:firstname:rand"" (5) SREM ""people:firstname:rand"" ""e82908cf-e7d3-47c2-9eec-b4e0967ad0c9"" (6) DEL ""people:e82908cf-e7d3-47c2-9eec-b4e0967ad0c9:idx"" (7) SADD ""people:firstname:Dragon Reborn"" ""e82908cf-e7d3-47c2-9eec-b4e0967ad0c9"" (8) SADD ""people:e82908cf-e7d3-47c2-9eec-b4e0967ad0c9:idx"" ""people:firstname:Dragon Reborn"" (9) 1 Remove the existing hash to avoid leftovers of hash keys potentially no longer present. 2 Save the flattened entry as hash. 3 Add the key of the hash written in <1> to the helper index of entities in the same keyspace. 4 Get existing index structures that might need to be updated. 5 Check if the index exists and what type it is (text, geo, …). 6 Remove a potentially existing key from the index. 7 Remove the helper holding index information. 8 Add the key of the hash added in <2> to the secondary index of firstnames with the properties value. 9 Add the index of <6> to the set of helper structures for entry to keep track of indexes to clean on delete/update. Save Geo Data: Geo indexes follow the same rules as normal text based ones but use geo structure to store values. Saving an entity that uses a Geo-indexed property results in the following commands: GEOADD ""people:hometown:location"" ""13.361389"" ""38.115556"" ""76900e94-b057-44bc-abcf-8126d51a621b"" (1) SADD ""people:76900e94-b057-44bc-abcf-8126d51a621b:idx"" ""people:hometown:location"" (2) 1 Add the key of the saved entry to the the geo index. 2 Keep track of the index structure. Find using simple index: repository.findByFirstname(""egwene""); SINTER ""people:firstname:egwene"" (1) HGETALL ""people:d70091b5-0b9a-4c0a-9551-519e61bc9ef3"" (2) HGETALL ... 1 Fetch keys contained in the secondary index. 2 Fetch each key returned by <1> individually. Find using Geo Index: repository.findByHometownLocationNear(new Point(15, 37), new Distance(200, KILOMETERS)); GEORADIUS ""people:hometown:location"" ""15.0"" ""37.0"" ""200.0"" ""km"" (1) HGETALL ""people:76900e94-b057-44bc-abcf-8126d51a621b"" (2) HGETALL ... 1 Fetch keys contained in the secondary index. 2 Fetch each key returned by <1> individually."
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/projections.html","Projections: Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> ."
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableRedisRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableRedisRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" />"
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/null-handling.html","Null Handling of Repository Methods: As of Spring Data 2.0, repository CRUD methods that return an individual aggregate instance use Java 8’s Optional to indicate the potential absence of a value. Besides that, Spring Data supports returning the following wrapper types on query methods: com.google.common.base.Optional scala.Option io.vavr.control.Option Alternatively, query methods can choose not to use a wrapper type at all. The absence of a query result is then indicated by returning null . Repository methods returning collections, collection alternatives, wrappers, and streams are guaranteed never to return null but rather the corresponding empty representation. See “ Repository query return types(query-return-types-reference.html) ” for details. Nullability Annotations: You can express nullability constraints for repository methods by using Spring Framework’s nullability annotations(https://docs.spring.io/spring-framework/reference/6.1/core/null-safety.html) . They provide a tooling-friendly approach and opt-in null checks during runtime, as follows: @NonNullApi(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNullApi.html) : Used on the package level to declare that the default behavior for parameters and return values is, respectively, neither to accept nor to produce null values. @NonNull(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNull.html) : Used on a parameter or return value that must not be null (not needed on a parameter and return value where @NonNullApi applies). @Nullable(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/Nullable.html) : Used on a parameter or return value that can be null . Spring annotations are meta-annotated with JSR 305(https://jcp.org/en/jsr/detail?id=305) annotations (a dormant but widely used JSR). JSR 305 meta-annotations let tooling vendors (such as IDEA(https://www.jetbrains.com/help/idea/nullable-and-notnull-annotations.html) , Eclipse(https://help.eclipse.org/latest/index.jsp?topic=/org.eclipse.jdt.doc.user/tasks/task-using_external_null_annotations.htm) , and Kotlin(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) ) provide null-safety support in a generic way, without having to hard-code support for Spring annotations. To enable runtime checking of nullability constraints for query methods, you need to activate non-nullability on the package level by using Spring’s @NonNullApi in package-info.java , as shown in the following example: Declaring Non-nullability in package-info.java @org.springframework.lang.NonNullApi package com.acme; Once non-null defaulting is in place, repository query method invocations get validated at runtime for nullability constraints. If a query result violates the defined constraint, an exception is thrown. This happens when the method would return null but is declared as non-nullable (the default with the annotation defined on the package in which the repository resides). If you want to opt-in to nullable results again, selectively use @Nullable on individual methods. Using the result wrapper types mentioned at the start of this section continues to work as expected: an empty result is translated into the value that represents absence. The following example shows a number of the techniques just described: Using different nullability constraints package com.acme; (1) import org.springframework.lang.Nullable; interface UserRepository extends Repository<User, Long> { User getByEmailAddress(EmailAddress emailAddress); (2) @Nullable User findByEmailAddress(@Nullable EmailAddress emailAdress); (3) Optional<User> findOptionalByEmailAddress(EmailAddress emailAddress); (4) } 1 The repository resides in a package (or sub-package) for which we have defined non-null behavior. 2 Throws an EmptyResultDataAccessException when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . 3 Returns null when the query does not produce a result. Also accepts null as the value for emailAddress . 4 Returns Optional.empty() when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . Nullability in Kotlin-based Repositories: Kotlin has the definition of nullability constraints(https://kotlinlang.org/docs/reference/null-safety.html) baked into the language. Kotlin code compiles to bytecode, which does not express nullability constraints through method signatures but rather through compiled-in metadata. Make sure to include the kotlin-reflect JAR in your project to enable introspection of Kotlin’s nullability constraints. Spring Data repositories use the language mechanism to define those constraints to apply the same runtime checks, as follows: Using nullability constraints on Kotlin repositories interface UserRepository : Repository<User, String> { fun findByUsername(username: String): User (1) fun findByFirstname(firstname: String?): User? (2) } 1 The method defines both the parameter and the result as non-nullable (the Kotlin default). The Kotlin compiler rejects method invocations that pass null to the method. If the query yields an empty result, an EmptyResultDataAccessException is thrown. 2 This method accepts null for the firstname parameter and returns null if the query does not produce a result."
"https://docs.spring.io/spring-data/redis/reference/3.3/redis/redis-repositories/cdi-integration.html","CDI Integration: Instances of the repository interfaces are usually created by a container, for which Spring is the most natural choice when working with Spring Data. Spring offers sophisticated for creating bean instances. Spring Data Redis ships with a custom CDI extension that lets you use the repository abstraction in CDI environments. The extension is part of the JAR, so, to activate it, drop the Spring Data Redis JAR into your classpath. You can then set up the infrastructure by implementing a CDI Producer for the RedisConnectionFactory(../../api/java/org/springframework/data/redis/connection/RedisConnectionFactory.html) and RedisOperations(../../api/java/org/springframework/data/redis/core/RedisOperations.html) , as shown in the following example: class RedisOperationsProducer { @Produces RedisConnectionFactory redisConnectionFactory() { LettuceConnectionFactory connectionFactory = new LettuceConnectionFactory(new RedisStandaloneConfiguration()); connectionFactory.afterPropertiesSet(); connectionFactory.start(); return connectionFactory; } void disposeRedisConnectionFactory(@Disposes RedisConnectionFactory redisConnectionFactory) throws Exception { if (redisConnectionFactory instanceof DisposableBean) { ((DisposableBean) redisConnectionFactory).destroy(); } } @Produces @ApplicationScoped RedisOperations<byte[], byte[]> redisOperationsProducer(RedisConnectionFactory redisConnectionFactory) { RedisTemplate<byte[], byte[]> template = new RedisTemplate<byte[], byte[]>(); template.setConnectionFactory(redisConnectionFactory); template.afterPropertiesSet(); return template; } } The necessary setup can vary, depending on your JavaEE environment. The Spring Data Redis CDI extension picks up all available repositories as CDI beans and creates a proxy for a Spring Data repository whenever a bean of a repository type is requested by the container. Thus, obtaining an instance of a Spring Data repository is a matter of declaring an @Injected property, as shown in the following example: class RepositoryClient { @Inject PersonRepository repository; public void businessMethod() { List<Person> people = repository.findAll(); } } A Redis Repository requires RedisKeyValueAdapter(../../api/java/org/springframework/data/redis/core/RedisKeyValueAdapter.html) and RedisKeyValueTemplate(../../api/java/org/springframework/data/redis/core/RedisKeyValueTemplate.html) instances. These beans are created and managed by the Spring Data CDI extension if no provided beans are found. You can, however, supply your own beans to configure the specific properties of RedisKeyValueAdapter(../../api/java/org/springframework/data/redis/core/RedisKeyValueAdapter.html) and RedisKeyValueTemplate(../../api/java/org/springframework/data/redis/core/RedisKeyValueTemplate.html) ."
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/redis/reference/3.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/redis/reference/3.3/observability.html","Observability: Getting insights from an application component about its operations, timing and relation to application code is crucial to understand latency. Spring Data Redis ships with a Micrometer integration through the Lettuce driver to collect observations during Redis interaction. Once the integration is set up, Micrometer will create meters and spans (for distributed tracing) for each Redis command. To enable the integration, apply the following configuration to LettuceClientConfiguration : @Configuration class ObservabilityConfiguration { @Bean public ClientResources clientResources(ObservationRegistry observationRegistry) { return ClientResources.builder() .tracing(new MicrometerTracingAdapter(observationRegistry, ""my-redis-cache"")) .build(); } @Bean public LettuceConnectionFactory lettuceConnectionFactory(ClientResources clientResources) { LettuceClientConfiguration clientConfig = LettuceClientConfiguration.builder() .clientResources(clientResources).build(); RedisConfiguration redisConfiguration = …; return new LettuceConnectionFactory(redisConfiguration, clientConfig); } } See also OpenTelemetry Semantic Conventions(https://opentelemetry.io/docs/reference/specification/trace/semantic_conventions/database/#redis) for further reference. Observability - Metrics: Below you can find a list of all metrics declared by this project. Redis Command Observation: Timer created around a Redis command execution. Metric name spring.data.redis . Type timer and base unit seconds . Fully qualified name of the enclosing class org.springframework.data.redis.connection.lettuce.observability.RedisObservation . Table 1. Low cardinality Keys Name Description db.operation Redis command value. db.redis.database_index Redis database index. db.system Database system. db.user Redis user. net.peer.name Name of the database host. net.peer.port Logical remote port number. net.sock.peer.addr Mongo peer address. net.sock.peer.port Mongo peer port. net.transport Network transport. Table 2. High cardinality Keys Name Description db.statement Redis statement. spring.data.redis.command.error Redis error response. Observability - Spans: Below you can find a list of all spans declared by this project. Redis Command Observation Span: Timer created around a Redis command execution. Span name spring.data.redis . Fully qualified name of the enclosing class org.springframework.data.redis.connection.lettuce.observability.RedisObservation . Table 3. Tag Keys Name Description db.operation Redis command value. db.redis.database_index Redis database index. db.statement Redis statement. db.system Database system. db.user Redis user. net.peer.name Name of the database host. net.peer.port Logical remote port number. net.sock.peer.addr Mongo peer address. net.sock.peer.port Mongo peer port. net.transport Network transport. spring.data.redis.command.error Redis error response."
"https://docs.spring.io/spring-data/redis/reference/3.3/appendix.html","Appendix: Schema: Spring Data Redis Schema (redis-namespace)(https://www.springframework.org/schema/redis/spring-redis-1.0.xsd) Supported Commands: Table 1. Redis commands supported by RedisTemplate Command Template Support APPEND X AUTH X BGREWRITEAOF X BGSAVE X BITCOUNT X BITFIELD X BITOP X BLPOP X BRPOP X BRPOPLPUSH X CLIENT KILL X CLIENT GETNAME X CLIENT LIST X CLIENT SETNAME X CLUSTER SLOTS - COMMAND - COMMAND COUNT - COMMAND GETKEYS - COMMAND INFO - CONFIG GET X CONFIG RESETSTAT X CONFIG REWRITE - CONFIG SET X DBSIZE X DEBUG OBJECT - DEBUG SEGFAULT - DECR X DECRBY X DEL X DISCARD X DUMP X ECHO X EVAL X EVALSHA X EXEC X EXISTS X EXPIRE X EXPIREAT X FLUSHALL X FLUSHDB X GEOADD X GEODIST X GEOHASH X GEOPOS X GEORADIUS X GEORADIUSBYMEMBER X GEOSEARCH X GEOSEARCHSTORE X GET X GETBIT X GETRANGE X GETSET X HDEL X HEXISTS X HGET X HGETALL X HINCRBY X HINCRBYFLOAT X HKEYS X HLEN X HMGET X HMSET X HSCAN X HSET X HSETNX X HVALS X INCR X INCRBY X INCRBYFLOAT X INFO X KEYS X LASTSAVE X LINDEX X LINSERT X LLEN X LPOP X LPUSH X LPUSHX X LRANGE X LREM X LSET X LTRIM X MGET X MIGRATE - MONITOR - MOVE X MSET X MSETNX X MULTI X OBJECT - PERSIST X PEXIPRE X PEXPIREAT X PFADD X PFCOUNT X PFMERGE X PING X PSETEX X PSUBSCRIBE X PTTL X PUBLISH X PUBSUB - PUBSUBSCRIBE - QUIT X RANDOMKEY X RENAME X RENAMENX X REPLICAOF X RESTORE X ROLE - RPOP X RPOPLPUSH X RPUSH X RPUSHX X SADD X SAVE X SCAN X SCARD X SCRIPT EXITS X SCRIPT FLUSH X SCRIPT KILL X SCRIPT LOAD X SDIFF X SDIFFSTORE X SELECT X SENTINEL FAILOVER X SENTINEL GET-MASTER-ADD-BY-NAME - SENTINEL MASTER - SENTINEL MASTERS X SENTINEL MONITOR X SENTINEL REMOVE X SENTINEL RESET - SENTINEL SET - SENTINEL SLAVES X SET X SETBIT X SETEX X SETNX X SETRANGE X SHUTDOWN X SINTER X SINTERSTORE X SISMEMBER X SLAVEOF X SLOWLOG - SMEMBERS X SMOVE X SORT X SPOP X SRANDMEMBER X SREM X SSCAN X STRLEN X SUBSCRIBE X SUNION X SUNIONSTORE X SYNC - TIME X TTL X TYPE X UNSUBSCRIBE X UNWATCH X WATCH X ZADD X ZCARD X ZCOUNT X ZINCRBY X ZINTERSTORE X ZLEXCOUNT - ZRANGE X ZRANGEBYLEX - ZREVRANGEBYLEX - ZRANGEBYSCORE X ZRANGESTORE X ZRANK X ZREM X ZREMRANGEBYLEX - ZREMRANGEBYRANK X ZREVRANGE X ZREVRANGEBYSCORE X ZREVRANK X ZSCAN X ZSCORE X ZUNINONSTORE X"
"https://docs.spring.io/spring-data/rest/reference/4.3/index.html","Spring Data REST: Spring Data REST exports Spring Data repositories as REST resources through WebMVC. It eases development of applications with a consistent programming model backed by Spring Data modules sources. Introduction(intro.html) Introduction to Spring Data REST and Examples Repository Resources(repository-resources.html) Exporting Repositories as REST Resources Representations(representations.html) Domain Object Representations (Object Mapping) Conditionals(etags-and-other-conditionals.html) Conditional Operations with ETag and other Headers Validation(validation.html) Validator Integration Events(events.html) Listening to REST Events Integration(integration.html) Integration with Spring Data REST components Metadata(metadata.html) ALPS and JSON Schema Security(security.html) Spring Security Integration Tools(tools.html) HAL Explorer Customizing(customizing-sdr.html) Tutorials and Recipes to customize Spring Data REST Wiki(https://github.com/spring-projects/spring-data-commons/wiki) What’s New, Upgrade Notes, Supported Versions, additional cross-version information. Jon Brisbin, Oliver Drotbohm, Greg Turnquist, Jay Bryant © 2008-2024 VMware, Inc. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically."
"https://docs.spring.io/spring-data/rest/reference/4.3/intro.html","Introduction: REST web services have become the number one means for application integration on the web. In its core, REST defines that a system that consists of resources with which clients interact. These resources are implemented in a hypermedia-driven way. Spring MVC(https://docs.spring.io/spring-framework/reference/6.1/web.html#spring-web) and Spring WebFlux(https://docs.spring.io/spring-framework/reference/6.1/web-reactive.html#spring-webflux) each offer a solid foundation to build these kinds of services. However, implementing even the simplest tenet of REST web services for a multi-domain object system can be quite tedious and result in a lot of boilerplate code. Spring Data REST builds on top of the Spring Data repositories and automatically exports those as REST resources. It leverages hypermedia to let clients automatically find functionality exposed by the repositories and integrate these resources into related hypermedia-based functionality. Section Summary: Upgrading Spring Data(introduction/upgrade.html) Getting started(introduction/getting-started.html) Using cURL to talk to Spring Data REST(introduction/example-api-usage-with-curl.html) Spring Data REST example projects(introduction/spring-data-rest-examples.html)"
"https://docs.spring.io/spring-data/rest/reference/4.3/introduction/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped."
"https://docs.spring.io/spring-data/rest/reference/4.3/introduction/getting-started.html","Getting started: Spring Data REST is itself a Spring MVC application and is designed in such a way that it should integrate with your existing Spring MVC applications with little effort. An existing (or future) layer of services can run alongside Spring Data REST with only minor additional work. Adding Spring Data REST to a Spring Boot Project: The simplest way to get to started is to build a Spring Boot application because Spring Boot has a starter for Spring Data REST and uses auto-configuration. The following example shows how to use Gradle to include Spring Data Rest in a Spring Boot project: Example 1. Spring Boot configuration with Gradle dependencies { ... compile(""org.springframework.boot:spring-boot-starter-data-rest"") ... } The following example shows how to use Maven to include Spring Data Rest in a Spring Boot project: Example 2. Spring Boot configuration with Maven <dependencies> ... <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-rest</artifactId> </dependency> ... </dependencies> You need not supply the version number if you use the Spring Boot Gradle plugin(https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#build-tool-plugins-gradle-plugin) or the Spring Boot Maven plugin(https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#build-tool-plugins-maven-plugin) . When you use Spring Boot, Spring Data REST gets configured automatically. Adding Spring Data REST to a Gradle project: To add Spring Data REST to a Gradle-based project, add the spring-data-rest-webmvc artifact to your compile-time dependencies, as follows: dependencies { … other project dependencies compile(""org.springframework.data:spring-data-rest-webmvc:4.3.4"") } Adding Spring Data REST to a Maven project: To add Spring Data REST to a Maven-based project, add the spring-data-rest-webmvc artifact to your compile-time dependencies, as follows: <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-rest-webmvc</artifactId> <version>4.3.4</version> </dependency> Configuring Spring Data REST: To install Spring Data REST alongside your existing Spring MVC application, you need to include the appropriate MVC configuration. Spring Data REST configuration is defined in a class called RepositoryRestMvcConfiguration and you can import that class into your application’s configuration. This step is unnecessary if you use Spring Boot’s auto-configuration. Spring Boot automatically enables Spring Data REST when you include spring-boot-starter-data-rest and, in your list of dependencies, your app is flagged with either @SpringBootApplication or @EnableAutoConfiguration . To customize the configuration, register a RepositoryRestConfigurer and implement or override the configure… -methods relevant to your use case. Make sure you also configure Spring Data repositories for the store you use. For details on that, see the reference documentation for the corresponding Spring Data module(https://projects.spring.io/spring-data/) . Basic Settings for Spring Data REST: This section covers the basic settings that you can manipulate when you configure a Spring Data REST application, including: Setting the Repository Detection Strategy(#getting-started.setting-repository-detection-strategy) Changing the Base URI(#getting-started.changing-base-uri) Changing Other Spring Data REST Properties(#getting-started.changing-other-properties) Setting the Repository Detection Strategy: Spring Data REST uses a RepositoryDetectionStrategy to determine whether a repository is exported as a REST resource. The RepositoryDiscoveryStrategies enumeration includes the following values: Table 1. Repository detection strategies Name Description DEFAULT Exposes all public repository interfaces but considers the exported flag of @(Repository)RestResource . ALL Exposes all repositories independently of type visibility and annotations. ANNOTATED Only repositories annotated with @(Repository)RestResource are exposed, unless their exported flag is set to false . VISIBILITY Only public repositories annotated are exposed. Changing the Base URI: By default, Spring Data REST serves up REST resources at the root URI, '/'. There are multiple ways to change the base path. With Spring Boot 1.2 and later versions, you can do change the base URI by setting a single property in application.properties , as follows: spring.data.rest.basePath=/api With Spring Boot 1.1 or earlier, or if you are not using Spring Boot, you can do the following: @Configuration class CustomRestMvcConfiguration { @Bean public RepositoryRestConfigurer repositoryRestConfigurer() { return new RepositoryRestConfigurer() { @Override public void configureRepositoryRestConfiguration(RepositoryRestConfiguration config, CorsRegistry cors) { config.setBasePath(""/api""); } }; } } Alternatively, you can register a custom implementation of RepositoryRestConfigurer as a Spring bean and make sure it gets picked up by component scanning, as follows: @Component public class CustomizedRestMvcConfiguration extends RepositoryRestConfigurer { @Override public void configureRepositoryRestConfiguration(RepositoryRestConfiguration config, CorsRegistry cors) { config.setBasePath(""/api""); } } Both of the preceding approaches change the base path to /api . Changing Other Spring Data REST Properties: You can alter the following properties: Table 2. Spring Boot configurable properties Property Description basePath the root URI for Spring Data REST defaultPageSize change the default for the number of items served in a single page maxPageSize change the maximum number of items in a single page pageParamName change the name of the query parameter for selecting pages limitParamName change the name of the query parameter for the number of items to show in a page sortParamName change the name of the query parameter for sorting defaultMediaType change the default media type to use when none is specified returnBodyOnCreate change whether a body should be returned when creating a new entity returnBodyOnUpdate change whether a body should be returned when updating an entity Starting the Application: At this point, you must also configure your key data store. Spring Data REST officially supports: Spring Data JPA(https://projects.spring.io/spring-data-jpa/) Spring Data MongoDB(https://projects.spring.io/spring-data-mongodb/) Spring Data Neo4j(https://projects.spring.io/spring-data-neo4j/) Spring Data GemFire(https://projects.spring.io/spring-data-gemfire/) Spring Data Cassandra(https://projects.spring.io/spring-data-cassandra/) The following Getting Started guides can help you get up and running quickly: Spring Data JPA(https://spring.io/guides/gs/accessing-data-rest/) Spring Data MongoDB(https://spring.io/guides/gs/accessing-mongodb-data-rest/) Spring Data Neo4j(https://spring.io/guides/gs/accessing-neo4j-data-rest/) Spring Data GemFire(https://spring.io/guides/gs/accessing-gemfire-data-rest/) These linked guides introduce how to add dependencies for the related data store, configure domain objects, and define repositories. You can run your application as either a Spring Boot app (with the links shown earlier) or configure it as a classic Spring MVC app. In general, Spring Data REST does not add functionality to a given data store. This means that, by definition, it should work with any Spring Data project that supports the repository programming model. The data stores listed above are the ones for which we have written integration tests to verify that Spring Data REST works with them. From this point, you can customize Spring Data REST(../customizing-sdr.html) with various options."
"https://docs.spring.io/spring-data/rest/reference/4.3/introduction/example-api-usage-with-curl.html","Using cURL to talk to Spring Data REST: This appendix contains a list of guides that demonstrate interacting with a Spring Data REST service over cURL: Accessing JPA Data with REST(https://spring.io/guides/gs/accessing-data-rest/) Accessing Neo4j Data with REST(https://spring.io/guides/gs/accessing-neo4j-data-rest/) Accessing MongoDB Data with REST(https://spring.io/guides/gs/accessing-mongodb-data-rest/) Accessing GemFire Data with REST(https://spring.io/guides/gs/accessing-gemfire-data-rest/)"
"https://docs.spring.io/spring-data/rest/reference/4.3/introduction/spring-data-rest-examples.html","Spring Data REST example projects: This appendix contains a list of Spring Data REST sample applications. The exact version of each example is not guaranteed to match the version of this reference manual. To get them all, visit github.com/spring-projects/spring-data-examples(https://github.com/spring-projects/spring-data-examples) and either clone or download a zipball. Doing so gives you example applications for all supported Spring Data projects. To see them, navigate to spring-data-examples/rest . Multi-store Example: This example(https://github.com/spring-projects/spring-data-examples/tree/master/rest/multi-store) shows how to mix together several underlying Spring Data projects. Projections: This example(https://github.com/spring-projects/spring-data-examples/tree/master/rest/projections) contains more detailed code you can use to explore projections(../projections-excerpts.html) . Spring Data REST with Spring Security: This example(https://github.com/spring-projects/spring-data-examples/tree/master/rest/security) shows how to secure a Spring Data REST(https://projects.spring.io/spring-data-rest) application in multiple ways with Spring Security(https://projects.spring.io/spring-security) . Starbucks example: This example(https://github.com/spring-projects/spring-data-examples/tree/master/rest/starbucks) exposes 10,843 Starbucks coffee shops through a RESTful API that allows access to the stores in a hypermedia-based way and exposes a resource to run a geo-location search for coffee shops."
"https://docs.spring.io/spring-data/rest/reference/4.3/repository-resources.html","Repository resources: Fundamentals: The core functionality of Spring Data REST is to export resources for Spring Data repositories. Thus, the core artifact to look at and potentially customize the way the exporting works is the repository interface. Consider the following repository interface: public interface OrderRepository extends CrudRepository<Order, Long> { } For this repository, Spring Data REST exposes a collection resource at /orders . The path is derived from the uncapitalized, pluralized, simple class name of the domain class being managed. It also exposes an item resource for each of the items managed by the repository under the URI template /orders/{id} . By default, the HTTP methods to interact with these resources map to the according methods of CrudRepository . Read more on that in the sections on collection resources(#repository-resources.collection-resource) and item resources(#repository-resources.item-resource) . Repository methods exposure: Which HTTP resources are exposed for a certain repository is mostly driven by the structure of the repository. In other words, the resource exposure will follow which methods you have exposed on the repository. If you extend CrudRepository you usually expose all methods required to expose all HTTP resources we can register by default. Each of the resources listed below will define which of the methods need to be present so that a particular HTTP method can be exposed for each of the resources. That means, that repositories that are not exposing those methods — either by not declaring them at all or explicitly using @RestResource(exported = false) — won’t expose those HTTP methods on those resources. For details on how to tweak the default method exposure or dedicated HTTP methods individually see Customizing supported HTTP methods(customizing-sdr.html#customizing-sdr.http-methods) . Default Status Codes: For the resources exposed, we use a set of default status codes: 200 OK : For plain GET requests. 201 Created : For POST and PUT requests that create new resources. 204 No Content : For PUT , PATCH , and DELETE requests when the configuration is set to not return response bodies for resource updates ( RepositoryRestConfiguration.setReturnBodyOnUpdate(…) ). If the configuration value is set to include responses for PUT , 200 OK is returned for updates, and 201 Created is returned for resource created through PUT . If the configuration values ( RepositoryRestConfiguration.returnBodyOnUpdate(…) and RepositoryRestConfiguration.returnBodyCreate(…) ) are explicitly set to null — which they are by default --, the presence of the HTTP Accept header is used to determine the response code. Read more on this in the detailed description of collection(#repository-resources.collection-resource.supported-methods.post) and item resources(#repository-resources.item-resource.supported-methods.put) . Resource Discoverability: A core principle of HATEOAS(https://github.com/spring-guides/understanding/tree/master/hateoas) is that resources should be discoverable through the publication of links that point to the available resources. There are a few competing de-facto standards of how to represent links in JSON. By default, Spring Data REST uses HAL(https://tools.ietf.org/html/draft-kelly-json-hal) to render responses. HAL defines the links to be contained in a property of the returned document. Resource discovery starts at the top level of the application. By issuing a request to the root URL under which the Spring Data REST application is deployed, the client can extract, from the returned JSON object, a set of links that represent the next level of resources that are available to the client. For example, to discover what resources are available at the root of the application, issue an HTTP GET to the root URL, as follows: curl -v http://localhost:8080/ < HTTP/1.1 200 OK < Content-Type: application/hal+json { ""_links"" : { ""orders"" : { ""href"" : ""http://localhost:8080/orders"" }, ""profile"" : { ""href"" : ""http://localhost:8080/api/alps"" } } } The property of the result document is an object that consists of keys representing the relation type, with nested link objects as specified in HAL. For more details about the profile link, see Application-Level Profile Semantics (ALPS)(metadata.html#metadata.alps) . The Collection Resource: Spring Data REST exposes a collection resource named after the uncapitalized, pluralized version of the domain class the exported repository is handling. Both the name of the resource and the path can be customized by using @RepositoryRestResource on the repository interface. Supported HTTP Methods: Collections resources support both GET and POST . All other HTTP methods cause a 405 Method Not Allowed . GET: Returns all entities the repository servers through its findAll(…) method. If the repository is a paging repository we include the pagination links if necessary and additional page metadata. Methods used for invocation: The following methods are used if present (descending order): findAll(Pageable) findAll(Sort) findAll() For more information on the default exposure of methods, see Repository methods exposure(#repository-resources.methods) . Parameters: If the repository has pagination capabilities, the resource takes the following parameters: page : The page number to access (0 indexed, defaults to 0). size : The page size requested (defaults to 20). sort : A collection of sort directives in the format ($propertyname,)+[asc|desc] ?. Custom Status Codes: The GET method has only one custom status code: 405 Method Not Allowed : If the findAll(…) methods were not exported (through @RestResource(exported = false) ) or are not present in the repository. Supported Media Types: The GET method supports the following media types: application/hal+json application/json Related Resources: The GET method supports a single link for discovering related resources: search : A search resource(#repository-resources.search-resource) is exposed if the backing repository exposes query methods. HEAD: The HEAD method returns whether the collection resource is available. It has no status codes, media types, or related resources. Methods used for invocation: The following methods are used if present (descending order): findAll(Pageable) findAll(Sort) findAll() For more information on the default exposure of methods, see Repository methods exposure(#repository-resources.methods) . POST: The POST method creates a new entity from the given request body. By default, whether the response contains a body is controlled by the Accept header sent with the request. If one is sent, a response body is created. If not, the response body is empty and a representation of the resource created can be obtained by following link contained in the Location response header. This behavior can be overridden by configuring RepositoryRestConfiguration.setReturnBodyOnCreate(…) accordingly. Methods used for invocation: The following methods are used if present (descending order): save(…) For more information on the default exposure of methods, see Repository methods exposure(#repository-resources.methods) . Custom Status Codes: The POST method has only one custom status code: 405 Method Not Allowed : If the save(…) methods were not exported (through @RestResource(exported = false) ) or are not present in the repository at all. Supported Media Types: The POST method supports the following media types: application/hal+json application/json The Item Resource: Spring Data REST exposes a resource for individual collection items as sub-resources of the collection resource. Supported HTTP Methods: Item resources generally support GET , PUT , PATCH , and DELETE , unless explicit configuration prevents that (see “ The Association Resource(#repository-resources.association-resource) ” for details). GET: The GET method returns a single entity. Methods used for invocation: The following methods are used if present (descending order): findById(…) For more information on the default exposure of methods, see Repository methods exposure(#repository-resources.methods) . Custom Status Codes: The GET method has only one custom status code: 405 Method Not Allowed : If the findOne(…) methods were not exported (through @RestResource(exported = false) ) or are not present in the repository. Supported Media Types: The GET method supports the following media types: application/hal+json application/json Related Resources: For every association of the domain type, we expose links named after the association property. You can customize this behavior by using @RestResource on the property. The related resources are of the association resource(#repository-resources.association-resource) type. HEAD: The HEAD method returns whether the item resource is available. It has no status codes, media types, or related resources. Methods used for invocation: The following methods are used if present (descending order): findById(…) For more information on the default exposure of methods, see Repository methods exposure(#repository-resources.methods) . PUT: The PUT method replaces the state of the target resource with the supplied request body. By default, whether the response contains a body is controlled by the Accept header sent with the request. If the request header is present, a response body and a status code of 200 OK is returned. If no header is present, the response body is empty and a successful request returns a status of 204 No Content . This behavior can be overridden by configuring RepositoryRestConfiguration.setReturnBodyOnUpdate(…) accordingly. Methods used for invocation: The following methods are used if present (descending order): save(…) For more information on the default exposure of methods, see Repository methods exposure(#repository-resources.methods) . Custom Status Codes: The PUT method has only one custom status code: 405 Method Not Allowed : If the save(…) methods were not exported (through @RestResource(exported = false) ) or is not present in the repository at all. Supported Media Types: The PUT method supports the following media types: application/hal+json application/json PATCH: The PATCH method is similar to the PUT method but partially updates the resources state. Methods used for invocation: The following methods are used if present (descending order): save(…) For more information on the default exposure of methods, see Repository methods exposure(#repository-resources.methods) . Custom Status Codes: The PATCH method has only one custom status code: 405 Method Not Allowed : If the save(…) methods were not exported (through @RestResource(exported = false) ) or are not present in the repository. Supported Media Types: The PATCH method supports the following media types: application/hal+json application/json application/patch+json(https://tools.ietf.org/html/rfc6902) application/merge-patch+json(https://tools.ietf.org/html/rfc7386) DELETE: The DELETE method deletes the resource exposed. By default, whether the response contains a body is controlled by the Accept header sent with the request. If the request header is present, a response body and a status code of 200 OK is returned. If no header is present, the response body is empty and a successful request returns a status of 204 No Content . This behavior can be overridden by configuring RepositoryRestConfiguration.setReturnBodyOnDelete(…) accordingly. Methods used for invocation: The following methods are used if present (descending order): delete(T) delete(ID) delete(Iterable) For more information on the default exposure of methods, see Repository methods exposure(#repository-resources.methods) . Custom Status Codes: The DELETE method has only one custom status code: 405 Method Not Allowed : If the delete(…) methods were not exported (through @RestResource(exported = false) ) or are not present in the repository. The Association Resource: Spring Data REST exposes sub-resources of every item resource for each of the associations the item resource has. The name and path of the resource defaults to the name of the association property and can be customized by using @RestResource on the association property. Supported HTTP Methods: The association resource supports the following media types: GET PUT POST DELETE GET: The GET method returns the state of the association resource. Supported Media Types: The GET method supports the following media types: application/hal+json application/json PUT: The PUT method binds the resource pointed to by the given URI(s) to the association resource (see Supported Media Types). Custom Status Codes: The PUT method has only one custom status code: 400 Bad Request : When multiple URIs were given for a to-one-association. Supported Media Types: The PUT method supports only one media type: text/uri-list: URIs pointing to the resource to bind to the association. POST: The POST method is supported only for collection associations. It adds a new element to the collection. Supported Media Types: The POST method supports only one media type: text/uri-list: URIs pointing to the resource to add to the association. DELETE: The DELETE method unbinds the association. Custom Status Codes: The POST method has only one custom status code: 405 Method Not Allowed : When the association is non-optional. The Search Resource: The search resource returns links for all query methods exposed by a repository. The path and name of the query method resources can be modified using @RestResource on the method declaration. Supported HTTP Methods: As the search resource is a read-only resource, it supports only the GET method. GET: The GET method returns a list of links pointing to the individual query method resources. Supported Media Types: The GET method supports the following media types: application/hal+json application/json Related Resources: For every query method declared in the repository, we expose a query method resource(#repository-resources.query-method-resource) . If the resource supports pagination, the URI pointing to it is a URI template containing the pagination parameters. HEAD: The HEAD method returns whether the search resource is available. A 404 return code indicates no query method resources are available. The Query Method Resource: The query method resource runs the exposed query through an individual query method on the repository interface. Supported HTTP Methods: As the query method resource is a read-only resource, it supports GET only. GET: The GET method returns the result of the query. Parameters: If the query method has pagination capabilities (indicated in the URI template pointing to the resource) the resource takes the following parameters: page : The page number to access (0 indexed, defaults to 0). size : The page size requested (defaults to 20). sort : A collection of sort directives in the format ($propertyname,)+[asc|desc] ?. Supported Media Types: The GET method supports the following media types: application/hal+json application/json HEAD: The HEAD method returns whether a query method resource is available."
"https://docs.spring.io/spring-data/rest/reference/4.3/paging-and-sorting.html","Paging and Sorting: This section documents Spring Data REST’s usage of the Spring Data Repository paging and sorting abstractions. To familiarize yourself with those features, see the Spring Data documentation for the repository implementation you use (such as Spring Data JPA). Paging: Rather than return everything from a large result set, Spring Data REST recognizes some URL parameters that influence the page size and the starting page number. If you extend PagingAndSortingRepository<T, ID> and access the list of all entities, you get links to the first 20 entities. To set the page size to any other number, add a size parameter, as follows: http://localhost:8080/people/?size=5 The preceding example sets the page size to 5. To use paging in your own query methods, you need to change the method signature to accept an additional Pageable parameter and return a Page or Slice rather than a List . For example, the following query method is exported to /people/search/nameStartsWith and supports paging: @RestResource(path = ""nameStartsWith"", rel = ""nameStartsWith"") public Page findByNameStartsWith(@Param(""name"") String name, Pageable p); The Spring Data REST exporter recognizes the returned Page / Slice and gives you the results in the body of the response, just as it would with a non-paged response, but additional links are added to the resource to represent the previous and next pages of data. Previous and Next Links: Each paged response returns links to the previous and next pages of results based on the current page by using the IANA-defined link relations prev(https://www.w3.org/TR/html5/links.html#link-type-prev) and next(https://www.w3.org/TR/html5/links.html#link-type-next) . If you are currently at the first page of results, however, no prev link is rendered. For the last page of results, no next link is rendered. Consider the following example, where we set the page size to 5: curl localhost:8080/people?size=5 { ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/persons{&sort,page,size}"", (1) ""templated"" : true }, ""next"" : { ""href"" : ""http://localhost:8080/persons?page=1&size=5{&sort}"", (2) ""templated"" : true } }, ""_embedded"" : { … data … }, ""page"" : { (3) ""size"" : 5, ""totalElements"" : 50, ""totalPages"" : 10, ""number"" : 0 } } At the top, we see _links : 1 The self link serves up the whole collection with some options. 2 The next link points to the next page, assuming the same page size. 3 At the bottom is extra data about the page settings, including the size of a page, total elements, total pages, and the page number you are currently viewing. When using tools such as curl on the command line, if you have a ampersand ( & ) in your statement, you need to wrap the whole URI in quotation marks. Note that the self and next URIs are, in fact, URI templates. They accept not only size , but also page and sort as optional flags. As mentioned earlier, the bottom of the HAL document includes a collection of details about the page. This extra information makes it easy for you to configure UI tools like sliders or indicators to reflect the user’s overall position when they view the data. For example, the document in the preceding example shows we are looking at the first page (with page numbers starting at 0). The following example shows What happens when we follow the next link: $ curl ""http://localhost:8080/persons?page=1&size=5"" { ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/persons{&sort,projection,page,size}"", ""templated"" : true }, ""next"" : { ""href"" : ""http://localhost:8080/persons?page=2&size=5{&sort,projection}"", (1) ""templated"" : true }, ""prev"" : { ""href"" : ""http://localhost:8080/persons?page=0&size=5{&sort,projection}"", (2) ""templated"" : true } }, ""_embedded"" : { ... data ... }, ""page"" : { ""size"" : 5, ""totalElements"" : 50, ""totalPages"" : 10, ""number"" : 1 (3) } } This looks very similar, except for the following differences: 1 The next link now points to yet another page, indicating its relative perspective to the self link. 2 A prev link now appears, giving us a path to the previous page. 3 The current number is now 1 (indicating the second page). This feature lets you map optional buttons on the screen to these hypermedia controls, letting you implement navigational features for the UI experience without having to hard code the URIs. In fact, the user can be empowered to pick from a list of page sizes, dynamically changing the content served, without having to rewrite the next and `prev controls at the top or bottom. Sorting: Spring Data REST recognizes sorting parameters that use the repository sorting support. To have your results sorted on a particular property, add a sort URL parameter with the name of the property on which you want to sort the results. You can control the direction of the sort by appending a comma ( , ) to the the property name plus either asc or desc . The following would use the findByNameStartsWith query method defined on the PersonRepository for all Person entities with names starting with the letter “K” and add sort data that orders the results on the name property in descending order: curl -v ""http://localhost:8080/people/search/nameStartsWith?name=K&sort=name,desc"" To sort the results by more than one property, keep adding as many sort=PROPERTY parameters as you need. They are added to the Pageable in the order in which they appear in the query string. Results can be sorted by top-level and nested properties. Use property path notation to express a nested sort property. Sorting by linkable associations (that is, links to top-level resources) is not supported."
"https://docs.spring.io/spring-data/rest/reference/4.3/projections-excerpts.html","Projections and Excerpts: Spring Data REST presents a default view of the domain model you export. However, sometimes, you may need to alter the view of that model for various reasons. This section covers how to define projections and excerpts to serve up simplified and reduced views of resources. Projections: Consider the following domain model: @Entity public class Person { @Id @GeneratedValue private Long id; private String firstName, lastName; @OneToOne private Address address; … } The Person object in the preceding example has several attributes: id is the primary key. firstName and lastName are data attributes. address is a link to another domain object. Now assume that we create a corresponding repository, as follows: interface PersonRepository extends CrudRepository<Person, Long> {} By default, Spring Data REST exports this domain object, including all of its attributes. firstName and lastName are exported as the plain data objects that they are. There are two options regarding the address attribute. One option is to also define a repository for Address objects, as follows: interface AddressRepository extends CrudRepository<Address, Long> {} In this situation, a Person resource renders the address attribute as a URI to its corresponding Address resource. If we were to look up “Frodo” in the system, we could expect to see a HAL document like this: { ""firstName"" : ""Frodo"", ""lastName"" : ""Baggins"", ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/persons/1"" }, ""address"" : { ""href"" : ""http://localhost:8080/persons/1/address"" } } } There is another way. If the Address domain object does not have its own repository definition, Spring Data REST includes the data fields inside the Person resource, as the following example shows: { ""firstName"" : ""Frodo"", ""lastName"" : ""Baggins"", ""address"" : { ""street"": ""Bag End"", ""state"": ""The Shire"", ""country"": ""Middle Earth"" }, ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/persons/1"" } } } But what if you do not want address details at all? Again, by default, Spring Data REST exports all of its attributes (except the id ). You can offer the consumer of your REST service an alternative by defining one or more projections. The following example shows a projection that does not include the address: @Projection(name = ""noAddresses"", types = { Person.class }) (1) interface NoAddresses { (2) String getFirstName(); (3) String getLastName(); (4) } 1 The @Projection annotation flags this as a projection. The name attribute provides the name of the projection, which we cover in more detail shortly. The types attributes targets this projection to apply only to Person objects. 2 It is a Java interface, making it declarative. 3 It exports the firstName . 4 It exports the lastName . The NoAddresses projection only has getters for firstName and lastName , meaning that it does not serve up any address information. Assuming you have a separate repository for Address resources, the default view from Spring Data REST differs slightly from the previous representation, as the following example shows: { ""firstName"" : ""Frodo"", ""lastName"" : ""Baggins"", ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/persons/1{?projection}"", (1) ""templated"" : true (2) }, ""address"" : { ""href"" : ""http://localhost:8080/persons/1/address"" } } } 1 This resource has a new option: {?projection} . 2 The self URI is a URI Template. To view the projection to the resource, look up localhost:8080/persons/1?projection=noAddresses(http://localhost:8080/persons/1?projection=noAddresses) . The value supplied to the projection query parameter is the same as that specified in @Projection(name = ""noAddress"") . It has nothing to do with the name of the projection’s interface. You can have multiple projections. See Projections(introduction/spring-data-rest-examples.html#spring-data-examples.projections) to see an example project. We encourage you to experiment with it. Spring Data REST finds projection definitions as follows: Any @Projection interface found in the same package as your entity definitions (or one of its sub-packages) is registered. You can manually register a projection by using RepositoryRestConfiguration.getProjectionConfiguration().addProjection(…) . In either case, the projection interface must have the @Projection annotation. Finding Existing Projections: Spring Data REST exposes Application-Level Profile Semantics (ALPS)(metadata.html#metadata.alps) documents, a micro metadata format. To view the ALPS metadata, follow the profile link exposed by the root resource. If you navigate down to the ALPS document for Person resources (which would be /alps/persons ), you can find many details about Person resources. Projections are listed, along with the details about the GET REST transition, in blocks similar to the following example: { … ""id"" : ""get-person"", (1) ""name"" : ""person"", ""type"" : ""SAFE"", ""rt"" : ""#person-representation"", ""descriptors"" : [ { ""name"" : ""projection"", (2) ""doc"" : { ""value"" : ""The projection that shall be applied when rendering the response. Acceptable values available in nested descriptors."", ""format"" : ""TEXT"" }, ""type"" : ""SEMANTIC"", ""descriptors"" : [ { ""name"" : ""noAddresses"", (3) ""type"" : ""SEMANTIC"", ""descriptors"" : [ { ""name"" : ""firstName"", (4) ""type"" : ""SEMANTIC"" }, { ""name"" : ""lastName"", (4) ""type"" : ""SEMANTIC"" } ] } ] } ] }, … 1 This part of the ALPS document shows details about GET and Person resources. 2 This part contais the projection options. 3 This part contains the noAddresses projection. 4 The actual attributes served up by this projection include firstName and lastName . Projection definitions are picked up and made available for clients if they are: Flagged with the @Projection annotation and located in the same package (or sub-package) of the domain type, OR Manually registered by using RepositoryRestConfiguration.getProjectionConfiguration().addProjection(…) . Bringing in Hidden Data: So far in this section, we have covered how projections can be used to reduce the information that is presented to the user. Projections can also bring in normally unseen data. For example, Spring Data REST ignores fields or getters that are marked up with @JsonIgnore annotations. Consider the following domain object: @Entity public class User { @Id @GeneratedValue private Long id; private String name; @JsonIgnore private String password; (1) private String[] roles; … 1 Jackson’s @JsonIgnore is used to prevent the password field from being serialized into JSON. The User class in the preceding example can be used to store user information as well as integration with Spring Security. If you create a UserRepository , the password field would normally have been exported, which is not good. In the preceding example, we prevent that from happening by applying Jackson’s @JsonIgnore on the password field. Jackson also does not serialize the field into JSON if @JsonIgnore is on the field’s corresponding getter function. However, projections introduce the ability to still serve this field. It is possible to create the following projection: @Projection(name = ""passwords"", types = { User.class }) interface PasswordProjection { String getPassword(); } If such a projection is created and used, it sidesteps the @JsonIgnore directive placed on User.password . This example may seem a bit contrived, but it is possible, with a richer domain model and many projections, to accidentally leak such details. Since Spring Data REST cannot discern the sensitivity of such data, it is up to you to avoid such situations. Projections can also generate virtual data. Imagine you had the following entity definition: @Entity public class Person { ... private String firstName; private String lastName; ... } You can create a projection that combines the two data fields in the preceding example together, as follows: @Projection(name = ""virtual"", types = { Person.class }) public interface VirtualProjection { @Value(""#{target.firstName} #{target.lastName}"") (1) String getFullName(); } 1 Spring’s @Value annotation lets you plug in a SpEL expression that takes the target object and splices together its firstName and lastName attributes to render a read-only fullName . Excerpts: An excerpt is a projection that is automatically applied to a resource collection. For example, you can alter the PersonRepository as follows: @RepositoryRestResource(excerptProjection = NoAddresses.class) interface PersonRepository extends CrudRepository<Person, Long> {} The preceding example directs Spring Data REST to use the NoAddresses projection when embedding Person resources into collections or related resources. Excerpt projections are not automatically applied to single resources. They have to be applied deliberately. Excerpt projections are meant to provide a default preview of collection data but not when fetching individual resources. See Why is an excerpt projection not applied automatically for a Spring Data REST item resource?(https://stackoverflow.com/questions/30220333/why-is-an-excerpt-projection-not-applied-automatically-for-a-spring-data-rest-it) for a discussion on the subject. In addition to altering the default rendering, excerpts have additional rendering options as shown in the next section. Excerpting Commonly Accessed Data: A common situation with REST services arises when you compose domain objects. For example, a Person is stored in one table and their related Address is stored in another. By default, Spring Data REST serves up the person’s address as a URI the client must navigate. But if it is common for consumers to always fetch this extra piece of data, an excerpt projection can put this extra piece of data inline, saving you an extra GET . To do so, you can define another excerpt projection, as follows: @Projection(name = ""inlineAddress"", types = { Person.class }) (1) interface InlineAddress { String getFirstName(); String getLastName(); Address getAddress(); (2) } 1 This projection has been named inlineAddress . 2 This projection adds getAddress , which returns the Address field. When used inside a projection, it causes the information to be included inline. You can plug it into the PersonRepository definition, as follows: @RepositoryRestResource(excerptProjection = InlineAddress.class) interface PersonRepository extends CrudRepository<Person, Long> {} Doing so causes the HAL document to appear as follows: { ""firstName"" : ""Frodo"", ""lastName"" : ""Baggins"", ""address"" : { (1) ""street"": ""Bag End"", ""state"": ""The Shire"", ""country"": ""Middle Earth"" }, ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/persons/1"" }, ""address"" : { (2) ""href"" : ""http://localhost:8080/persons/1/address"" } } } 1 The address data is directly included inline, so you do not have to navigate to get it. 2 The link to the Address resource is still provided, making it still possible to navigate to its own resource. Note that the preceding example is a mix of the examples shown earlier in this chapter. You may want to read back through them to follow the progression to the final example. Configuring @RepositoryRestResource(excerptProjection=…​) for a repository alters the default behavior. This can potentially cause breaking changes to consumers of your service if you have already made a release."
"https://docs.spring.io/spring-data/rest/reference/4.3/representations.html","Domain Object Representations (Object Mapping): Spring Data REST returns a representation of a domain object that corresponds to the Accept type specified in the HTTP request. Currently, only JSON representations are supported. Other representation types can be supported in the future by adding an appropriate converter and updating the controller methods with the appropriate content-type. Sometimes, the behavior of the Spring Data REST ObjectMapper (which has been specially configured to use intelligent serializers that can turn domain objects into links and back again) may not handle your domain model correctly. There are so many ways you can structure your data that you may find your own domain model is not translated to JSON correctly. It is also sometimes not practical in these cases to try and support a complex domain model in a generic way. Sometimes, depending on the complexity, it is not even possible to offer a generic solution. Adding Custom Serializers and Deserializers to Jackson’s ObjectMapper: To accommodate the largest percentage of use cases, Spring Data REST tries very hard to render your object graph correctly. It tries to serialize unmanaged beans as normal POJOs, and it tries to create links to managed beans where necessary. However, if your domain model does not easily lend itself to reading or writing plain JSON, you may want to configure Jackson’s ObjectMapper with your own custom mappings, serializers, and deserializers. Abstract Class Registration: One key configuration point you might need to hook into is when you use an abstract class (or an interface) in your domain model. By default, Jackson does not know what implementation to create for an interface. Consider the following example: @Entity public class MyEntity { @OneToMany private List<MyInterface> interfaces; } In a default configuration, Jackson has no idea what class to instantiate when POSTing new data to the exporter. This is something you need to tell Jackson either through an annotation, or (more cleanly) by registering a type mapping by using a Module . To add your own Jackson configuration to the ObjectMapper used by Spring Data REST, override the configureJacksonObjectMapper method. That method is passed an ObjectMapper instance that has a special module to handle serializing and deserializing PersistentEntity objects. You can register your own modules as well, as the following example shows: @Override protected void configureJacksonObjectMapper(ObjectMapper objectMapper) { objectMapper.registerModule(new SimpleModule(""MyCustomModule"") { @Override public void setupModule(SetupContext context) { context.addAbstractTypeResolver( new SimpleAbstractTypeResolver() .addMapping(MyInterface.class, MyInterfaceImpl.class)); } }); } Once you have access to the SetupContext object in your Module , you can do all sorts of cool things to configure Jackson’s JSON mapping. You can read more about how Module instances work on Jackson’s wiki(https://wiki.fasterxml.com/JacksonFeatureModules) . Adding Custom Serializers for Domain Types: If you want to serialize or deserialize a domain type in a special way, you can register your own implementations with Jackson’s ObjectMapper , and the Spring Data REST exporter transparently handles those domain objects correctly. To add serializers from your setupModule method implementation, you can do something like the following: @Override public void setupModule(SetupContext context) { SimpleSerializers serializers = new SimpleSerializers(); SimpleDeserializers deserializers = new SimpleDeserializers(); serializers.addSerializer(MyEntity.class, new MyEntitySerializer()); deserializers.addDeserializer(MyEntity.class, new MyEntityDeserializer()); context.addSerializers(serializers); context.addDeserializers(deserializers); }"
"https://docs.spring.io/spring-data/rest/reference/4.3/etags-and-other-conditionals.html","Conditional Operations with Headers: This section shows how Spring Data REST uses standard HTTP headers to enhance performance, conditionalize operations, and contribute to a more sophisticated frontend. ETag, If-Match, and If-None-Match Headers: The ETag header(https://tools.ietf.org/html/rfc7232#section-2.3) provides a way to tag resources. This can prevent clients from overriding each other while also making it possible to reduce unnecessary calls. Consider the following example: Example 1. A POJO with a version number class Sample { @Version Long version; (1) Sample(Long version) { this.version = version; } } 1 The @Version annotation (the JPA one in case you’re using Spring Data JPA, the Spring Data org.springframework.data.annotation.Version one for all other modules) flags this field as a version marker. The POJO in the preceding example, when served up as a REST resource by Spring Data REST, has an ETag header with the value of the version field. We can conditionally PUT , PATCH , or DELETE that resource if we supply a If-Match header such as the following: curl -v -X PATCH -H 'If-Match: <value of previous ETag>' ... Only if the resource’s current ETag state matches the If-Match header is the operation carried out. This safeguard prevents clients from stomping on each other. Two different clients can fetch the resource and have an identical ETag . If one client updates the resource, it gets a new ETag in the response. But the first client still has the old header. If that client attempts an update with the If-Match header, the update fails because they no longer match. Instead, that client receives an HTTP 412 Precondition Failed message to be sent back. The client can then catch up however is necessary. The term, “version,” may carry different semantics with different data stores and even different semantics within your application. Spring Data REST effectively delegates to the data store’s metamodel to discern if a field is versioned and, if so, only allows the listed updates if ETag elements match. The If-None-Match header(https://tools.ietf.org/html/rfc7232#section-3.2) provides an alternative. Instead of conditional updates, If-None-Match allows conditional queries. Consider the following example: curl -v -H 'If-None-Match: <value of previous etag>' ... The preceding command (by default) runs a GET . Spring Data REST checks for If-None-Match headers while doing a GET . If the header matches the ETag, it concludes that nothing has changed and, instead of sending a copy of the resource, sends back an HTTP 304 Not Modified status code. Semantically, it reads “If this supplied header value does not match the server-side version, send the whole resource. Otherwise, do not send anything.” This POJO is from an ETag -based unit test, so it does not have @Entity (JPA) or @Document (MongoDB) annotations, as expected in application code. It focuses solely on how a field with @Version results in an ETag header. If-Modified-Since header: The If-Modified-Since header(https://tools.ietf.org/html/rfc7232#section-3.3) provides a way to check whether a resource has been updated since the last request, which lets applications avoid resending the same data. Consider the following example: Example 2. The last modification date captured in a domain type @Document public class Receipt { public @Id String id; public @Version Long version; public @LastModifiedDate Date date; (1) public String saleItem; public BigDecimal amount; } 1 Spring Data Commons’s @LastModifiedDate annotation allows capturing this information in multiple formats (JodaTime’s DateTime , legacy Java Date and Calendar , JDK8 date/time types, and long / Long ). With the date field in the preceding example, Spring Data REST returns a Last-Modified header similar to the following: Last-Modified: Wed, 24 Jun 2015 20:28:15 GMT This value can be captured and used for subsequent queries to avoid fetching the same data twice when it has not been updated, as the following example shows: curl -H ""If-Modified-Since: Wed, 24 Jun 2015 20:28:15 GMT"" ... With the preceding command, you are asking that a resource be fetched only if it has changed since the specified time. If so, you get a revised Last-Modified header with which to update the client. If not, you receive an HTTP 304 Not Modified status code. The header is perfectly formatted to send back for a future query. Do not mix and match header value with different queries. Results could be disastrous. Use the header values ONLY when you request the exact same URI and parameters. Architecting a More Efficient Front End: ETag elements, combined with the If-Match and If-None-Match headers, let you build a front end that is more friendly to consumers' data plans and mobile battery lives. To do so: Identify the entities that need locking and add a version attribute. HTML5 nicely supports data-* attributes, so store the version in the DOM (somewhere such as an data-etag attribute). Identify the entries that would benefit from tracking the most recent updates. When fetching these resources, store the Last-Modified value in the DOM ( data-last-modified perhaps). When fetching resources, also embed self URIs in your DOM nodes (perhaps data-uri or data-self ) so that it is easy to go back to the resource. Adjust PUT / PATCH / DELETE operations to use If-Match and also handle HTTP 412 Precondition Failed status codes. Adjust GET operations to use If-None-Match and If-Modified-Since and handle HTTP 304 Not Modified status codes. By embedding ETag elements and Last-Modified values in your DOM (or perhaps elsewhere for a native mobile app), you can then reduce the consumption of data and battery power by not retrieving the same thing over and over. You can also avoid colliding with other clients and, instead, be alerted when you need to reconcile differences. In this fashion, with just a little tweaking on your front end and some entity-level edits, the backend serves up time-sensitive details you can cash in on when building a customer-friendly client."
"https://docs.spring.io/spring-data/rest/reference/4.3/validation.html","Validation: There are two ways to register a Validator instance in Spring Data REST: wire it by bean name or register the validator manually. For the majority of cases, the simple bean name prefix style is sufficient. In order to tell Spring Data REST you want a particular Validator assigned to a particular event, prefix the bean name with the event in question. For example, to validate instances of the Person class before new ones are saved into the repository, you would declare an instance of a Validator<Person> in your ApplicationContext with a bean name of beforeCreatePersonValidator . Since the beforeCreate prefix matches a known Spring Data REST event, that validator is wired to the correct event. Assigning Validators Manually: If you would rather not use the bean name prefix approach, you need to register an instance of your validator with the bean whose job it is to invoke validators after the correct event. In your configuration that implements RepositoryRestConfigurer , override the configureValidatingRepositoryEventListener method and call addValidator on the ValidatingRepositoryEventListener , passing the event on which you want this validator to be triggered and an instance of the validator. The following example shows how to do so: @Override void configureValidatingRepositoryEventListener(ValidatingRepositoryEventListener v) { v.addValidator(""beforeSave"", new BeforeSaveValidator()); }"
"https://docs.spring.io/spring-data/rest/reference/4.3/events.html","Events: The REST exporter emits eight different events throughout the process of working with an entity: BeforeCreateEvent(api/java/org/springframework/data/rest/core/event/BeforeCreateEvent.html) AfterCreateEvent(api/java/org/springframework/data/rest/core/event/AfterCreateEvent.html) BeforeSaveEvent(api/java/org/springframework/data/rest/core/event/BeforeSaveEvent.html) AfterSaveEvent(api/java/org/springframework/data/rest/core/event/AfterSaveEvent.html) BeforeLinkSaveEvent(api/java/org/springframework/data/rest/core/event/BeforeLinkSaveEvent.html) AfterLinkSaveEvent(api/java/org/springframework/data/rest/core/event/AfterLinkSaveEvent.html) BeforeDeleteEvent(api/java/org/springframework/data/rest/core/event/BeforeDeleteEvent.html) AfterDeleteEvent(api/java/org/springframework/data/rest/core/event/AfterDeleteEvent.html) Writing an ApplicationListener: You can subclass an abstract class that listens for these kinds of events and calls the appropriate method based on the event type. To do so, override the methods for the events in question, as follows: public class BeforeSaveEventListener extends AbstractRepositoryEventListener { @Override public void onBeforeSave(Object entity) { ... logic to handle inspecting the entity before the Repository saves it } @Override public void onAfterDelete(Object entity) { ... send a message that this entity has been deleted } } One thing to note with this approach, however, is that it makes no distinction based on the type of the entity. You have to inspect that yourself. Writing an Annotated Handler: Another approach is to use an annotated handler, which filters events based on domain type. To declare a handler, create a POJO and put the @RepositoryEventHandler annotation on it. This tells the BeanPostProcessor that this class needs to be inspected for handler methods. Once the BeanPostProcessor finds a bean with this annotation, it iterates over the exposed methods and looks for annotations that correspond to the event in question. For example, to handle BeforeSaveEvent instances in an annotated POJO for different kinds of domain types, you could define your class as follows: @RepositoryEventHandler (1) public class PersonEventHandler { @HandleBeforeSave public void handlePersonSave(Person p) { // … you can now deal with Person in a type-safe way } @HandleBeforeSave public void handleProfileSave(Profile p) { // … you can now deal with Profile in a type-safe way } } 1 It’s possible to narrow the types to which this handler applies by using (for example) @RepositoryEventHandler(Person.class) . The domain type whose events you are interested in is determined from the type of the first parameter of the annotated methods. To register your event handler, either mark the class with one of Spring’s @Component stereotypes (so that it can be picked up by @SpringBootApplication or @ComponentScan ) or declare an instance of your annotated bean in your ApplicationContext . Then the BeanPostProcessor that is created in RepositoryRestMvcConfiguration inspects the bean for handlers and wires them to the correct events. The following example shows how to create an event handler for the Person class: @Configuration public class RepositoryConfiguration { @Bean PersonEventHandler personEventHandler() { return new PersonEventHandler(); } } Spring Data REST events are customized Spring application events(https://docs.spring.io/spring-framework/reference/6.1/core.html#context-functionality-events) . By default, Spring events are synchronous, unless they get republished across a boundary (such as issuing a WebSocket event or crossing into a thread)."
"https://docs.spring.io/spring-data/rest/reference/4.3/integration.html","Integration: This section details various ways to integrate with Spring Data REST components, whether from a Spring application that is using Spring Data REST or from other means. Programmatic Links: Sometimes you need to add links to exported resources in your own custom-built Spring MVC controllers. There are three basic levels of linking available: Manually assembling links. Using Spring HATEOAS’s LinkBuilder(https://docs.spring.io/spring-hateoas/docs/current/reference/html/#fundamentals.obtaining-links.builder) with linkTo() , slash() , and so on. Using Spring Data REST’s implementation of RepositoryEntityLinks(api/java/org/springframework/data/rest/webmvc/support/RepositoryEntityLinks.html) . The first suggestion is terrible and should be avoided at all costs. It makes your code brittle and high-risk. The second is handy when creating links to other hand-written Spring MVC controllers. The last one, which we explore in the rest of this section, is good for looking up resource links that are exported by Spring Data REST. Consider the following class ,which uses Spring’s autowiring: public class MyWebApp { private RepositoryEntityLinks entityLinks; @Autowired public MyWebApp(RepositoryEntityLinks entityLinks) { this.entityLinks = entityLinks; } } With the class in the preceding example, you can use the following operations: Table 1. Ways to link to exported resources Method Description entityLinks.linkToCollectionResource(Person.class) Provide a link to the collection resource of the specified type ( Person , in this case). entityLinks.linkToItemResource(Person.class, 1) Provide a link to a single resource. entityLinks.linkToPagedResource(Person.class, new PageRequest(…​)) Provide a link to a paged resource. entityLinks.linksToSearchResources(Person.class) Provides a list of links for all the finder methods exposed by the corresponding repository. entityLinks.linkToSearchResource(Person.class, ""findByLastName"") Provide a finder link by rel (that is, the name of the finder). All of the search-based links support extra parameters for paging and sorting. See RepositoryEntityLinks(api/java/org/springframework/data/rest/webmvc/support/RepositoryEntityLinks.html) for the details. There is also linkFor(Class<?> type) , but that returns a Spring HATEOAS LinkBuilder , which returns you to the lower level API. Try to use the other ones first."
"https://docs.spring.io/spring-data/rest/reference/4.3/metadata.html","Metadata: This section details the various forms of metadata provided by a Spring Data REST-based application. Application-Level Profile Semantics (ALPS): ALPS(http://alps.io/) is a data format for defining simple descriptions of application-level semantics, similar in complexity to HTML microformats. An ALPS document can be used as a profile to explain the application semantics of a document with an application-agnostic media type (such as HTML, HAL, Collection+JSON, Siren, etc.). This increases the reusability of profile documents across media types. — M. Admundsen / L. Richardson / M. Foster https://tools.ietf.org/html/draft-amundsen-richardson-foster-alps-00 Spring Data REST provides an ALPS document for every exported repository. It contains information about both the RESTful transitions and the attributes of each repository. At the root of a Spring Data REST app is a profile link. Assuming you had an app with both persons and related addresses , the root document would be as follows: { ""_links"" : { ""persons"" : { ""href"" : ""http://localhost:8080/persons"" }, ""addresses"" : { ""href"" : ""http://localhost:8080/addresses"" }, ""profile"" : { ""href"" : ""http://localhost:8080/profile"" } } } A profile link, as defined in RFC 6906(https://tools.ietf.org/html/rfc6906) , is a place to include application-level details. The ALPS draft spec(https://tools.ietf.org/html/draft-amundsen-richardson-foster-alps-00) is meant to define a particular profile format, which we explore later in this section. If you navigate into the profile link at localhost:8080/profile , you see content resembling the following: { ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/profile"" }, ""persons"" : { ""href"" : ""http://localhost:8080/profile/persons"" }, ""addresses"" : { ""href"" : ""http://localhost:8080/profile/addresses"" } } } At the root level, profile is a single link and cannot serve up more than one application profile. That is why you must navigate to /profile to find a link for each resource’s metadata. If you navigate to /profile/persons and look at the profile data for a Person resource, you see content resembling the following example: { ""version"" : ""1.0"", ""descriptors"" : [ { ""id"" : ""person-representation"", (1) ""descriptors"" : [ { ""name"" : ""firstName"", ""type"" : ""SEMANTIC"" }, { ""name"" : ""lastName"", ""type"" : ""SEMANTIC"" }, { ""name"" : ""id"", ""type"" : ""SEMANTIC"" }, { ""name"" : ""address"", ""type"" : ""SAFE"", ""rt"" : ""http://localhost:8080/profile/addresses#address"" } ] }, { ""id"" : ""create-persons"", (2) ""name"" : ""persons"", (3) ""type"" : ""UNSAFE"", (4) ""rt"" : ""#person-representation"" (5) }, { ""id"" : ""get-persons"", ""name"" : ""persons"", ""type"" : ""SAFE"", ""rt"" : ""#person-representation"" }, { ""id"" : ""delete-person"", ""name"" : ""person"", ""type"" : ""IDEMPOTENT"", ""rt"" : ""#person-representation"" }, { ""id"" : ""patch-person"", ""name"" : ""person"", ""type"" : ""UNSAFE"", ""rt"" : ""#person-representation"" }, { ""id"" : ""update-person"", ""name"" : ""person"", ""type"" : ""IDEMPOTENT"", ""rt"" : ""#person-representation"" }, { ""id"" : ""get-person"", ""name"" : ""person"", ""type"" : ""SAFE"", ""rt"" : ""#person-representation"" } ] } 1 A detailed listing of the attributes of a Person resource, identified as #person-representation , lists the names of the attributes. 2 The supported operations. This one indicates how to create a new Person . 3 The name is persons , which indicates (because it is plural) that a POST should be applied to the whole collection, not a single person . 4 The type is UNSAFE , because this operation can alter the state of the system. 5 The rt is #person-representation , which indicates that returned resource type will be Person resource. This JSON document has a media type of application/alps+json . This is different from the previous JSON document, which had a media type of application/hal+json . These formats are different and governed by different specs. You can also find a profile link in the collection of _links when you examine a collection resource, as the following example shows: { ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/persons"" (1) }, ... other links ... ""profile"" : { ""href"" : ""http://localhost:8080/profile/persons"" (2) } }, ... } 1 This HAL document respresents the Person collection. 2 It has a profile link to the same URI for metadata. Again, by default, the profile link serves up ALPS. However, if you use an Accept header(https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.1) , it can serve application/alps+json . Hypermedia Control Types: ALPS displays types for each hypermedia control. They include: Table 1. ALPS types Type Description SEMANTIC A state element (such as HTML.SPAN , HTML.INPUT , and others). SAFE A hypermedia control that triggers a safe, idempotent state transition (such as GET or HEAD ). IDEMPOTENT A hypermedia control that triggers an unsafe, idempotent state transition (such as PUT or DELETE ). UNSAFE A hypermedia control that triggers an unsafe, non-idempotent state transition (such as POST ). In the representation section shown earlier, bits of data from the application are marked as being SEMANTIC . The address field is a link that involves a safe GET to retrieve. Consequently, it is marked as being SAFE . Hypermedia operations themselves map onto the types as shown in the preceding table. ALPS with Projections: If you define any projections, they are also listed in the ALPS metadata. Assuming we also defined inlineAddress and noAddresses , they would appear inside the relevant operations. (See “ Projections(projections-excerpts.html#projections-excerpts.projections) ” for the definitions and discussion of these two projections.) That is GET would appear in the operations for the whole collection, and GET would appear in the operations for a single resource. The following example shows the alternate version of the get-persons subsection: ... { ""id"" : ""get-persons"", ""name"" : ""persons"", ""type"" : ""SAFE"", ""rt"" : ""#person-representation"", ""descriptors"" : [ { (1) ""name"" : ""projection"", ""doc"" : { ""value"" : ""The projection that shall be applied when rendering the response. Acceptable values available in nested descriptors."", ""format"" : ""TEXT"" }, ""type"" : ""SEMANTIC"", ""descriptors"" : [ { ""name"" : ""inlineAddress"", (2) ""type"" : ""SEMANTIC"", ""descriptors"" : [ { ""name"" : ""address"", ""type"" : ""SEMANTIC"" }, { ""name"" : ""firstName"", ""type"" : ""SEMANTIC"" }, { ""name"" : ""lastName"", ""type"" : ""SEMANTIC"" } ] }, { ""name"" : ""noAddresses"", (3) ""type"" : ""SEMANTIC"", ""descriptors"" : [ { ""name"" : ""firstName"", ""type"" : ""SEMANTIC"" }, { ""name"" : ""lastName"", ""type"" : ""SEMANTIC"" } ] } ] } ] } ... 1 A new attribute, descriptors , appears, containing an array with one entry, projection . 2 Inside the projection.descriptors , we can see inLineAddress . It render address , firstName , and lastName . Relationships rendered inside a projection result in including the data fields inline. 3 noAddresses serves up a subset that contains firstName and lastName . With all this information, a client can deduce not only the available RESTful transitions but also, to some degree, the data elements needed to interact with the back end. Adding Custom Details to Your ALPS Descriptions: You can create custom messages that appear in your ALPS metadata. To do so, create rest-messages.properties , as follows: rest.description.person=A collection of people rest.description.person.id=primary key used internally to store a person (not for RESTful usage) rest.description.person.firstName=Person's first name rest.description.person.lastName=Person's last name rest.description.person.address=Person's address These rest.description.* properties define details to display for a Person resource. They alter the ALPS format of the person-representation , as follows: ... { ""id"" : ""person-representation"", ""doc"" : { ""value"" : ""A collection of people"", (1) ""format"" : ""TEXT"" }, ""descriptors"" : [ { ""name"" : ""firstName"", ""doc"" : { ""value"" : ""Person's first name"", (2) ""format"" : ""TEXT"" }, ""type"" : ""SEMANTIC"" }, { ""name"" : ""lastName"", ""doc"" : { ""value"" : ""Person's last name"", (3) ""format"" : ""TEXT"" }, ""type"" : ""SEMANTIC"" }, { ""name"" : ""id"", ""doc"" : { ""value"" : ""primary key used internally to store a person (not for RESTful usage)"", (4) ""format"" : ""TEXT"" }, ""type"" : ""SEMANTIC"" }, { ""name"" : ""address"", ""doc"" : { ""value"" : ""Person's address"", (5) ""format"" : ""TEXT"" }, ""type"" : ""SAFE"", ""rt"" : ""http://localhost:8080/profile/addresses#address"" } ] } ... 1 The value of rest.description.person maps into the whole representation. 2 The value of rest.description.person.firstName maps to the firstName attribute. 3 The value of rest.description.person.lastName maps to the lastName attribute. 4 The value of rest.description.person.id maps to the id attribute, a field not normally displayed. 5 The value of rest.description.person.address maps to the address attribute. Supplying these property settings causes each field to have an extra doc attribute. Spring MVC (which is the essence of a Spring Data REST application) supports locales, meaning you can bundle up multiple properties files with different messages. JSON Schema: JSON Schema(https://json-schema.org/) is another form of metadata supported by Spring Data REST. Per their website, JSON Schema has the following advantages: Describes your existing data format Clear, human- and machine-readable documentation Complete structural validation, useful for automated testing and validating client-submitted data As shown in the previous section(#metadata.alps) , you can reach this data by navigating from the root URI to the profile link. { ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/profile"" }, ""persons"" : { ""href"" : ""http://localhost:8080/profile/persons"" }, ""addresses"" : { ""href"" : ""http://localhost:8080/profile/addresses"" } } } These links are the same as shown earlier. To retrieve JSON Schema, you can invoke them with the following Accept header: application/schema+json . In this case, if you ran curl -H 'Accept:application/schema+json' localhost:8080/profile/persons(http://localhost:8080/profile/persons) , you would see output resembling the following: { ""title"" : ""org.springframework.data.rest.webmvc.jpa.Person"", (1) ""properties"" : { (2) ""firstName"" : { ""readOnly"" : false, ""type"" : ""string"" }, ""lastName"" : { ""readOnly"" : false, ""type"" : ""string"" }, ""siblings"" : { ""readOnly"" : false, ""type"" : ""string"", ""format"" : ""uri"" }, ""created"" : { ""readOnly"" : false, ""type"" : ""string"", ""format"" : ""date-time"" }, ""father"" : { ""readOnly"" : false, ""type"" : ""string"", ""format"" : ""uri"" }, ""weight"" : { ""readOnly"" : false, ""type"" : ""integer"" }, ""height"" : { ""readOnly"" : false, ""type"" : ""integer"" } }, ""descriptors"" : { }, ""type"" : ""object"", ""$schema"" : ""https://json-schema.org/draft-04/schema#"" } 1 The type that was exported 2 A listing of properties There are more details if your resources have links to other resources. You can also find a profile link in the collection of _links when you examine a collection resource, as the following example shows: { ""_links"" : { ""self"" : { ""href"" : ""http://localhost:8080/persons"" (1) }, ... other links ... ""profile"" : { ""href"" : ""http://localhost:8080/profile/persons"" (2) } }, ... } 1 This HAL document respresents the Person collection. 2 It has a profile link to the same URI for metadata. Again, the profile link serves ALPS(#metadata.alps) by default. If you supply it with an Accept header(https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.1) of application/schema+json , it renders the JSON Schema representation."
"https://docs.spring.io/spring-data/rest/reference/4.3/security.html","Security: Spring Data REST works quite well with Spring Security. This section shows examples of how to secure your Spring Data REST services with method-level security. @Pre and @Post Security: The following example from Spring Data REST’s test suite shows Spring Security’s PreAuthorization model(https://docs.spring.io/spring-security/reference/servlet/authorization/expression-based.html#_access_control_using_preauthorize_and_postauthorize) (the most sophisticated security model): Example 1. spring-data-rest-tests/spring-data-rest-tests-security/src/test/java/org/springframework/data/rest/tests/security/PreAuthorizedOrderRepository.java @PreAuthorize(""hasRole('ROLE_USER')"") (1) public interface PreAuthorizedOrderRepository extends CrudRepository<Order, UUID> { @PreAuthorize(""hasRole('ROLE_ADMIN')"") @Override Optional<Order> findById(UUID id); @PreAuthorize(""hasRole('ROLE_ADMIN')"") (2) @Override void deleteById(UUID aLong); @PreAuthorize(""hasRole('ROLE_ADMIN')"") @Override void delete(Order order); @PreAuthorize(""hasRole('ROLE_ADMIN')"") @Override void deleteAll(Iterable<? extends Order> orders); @PreAuthorize(""hasRole('ROLE_ADMIN')"") @Override void deleteAll(); } 1 This Spring Security annotation secures the entire repository. The Spring Security SpEL expression(https://docs.spring.io/spring-security/reference/servlet/authorization/expression-based.html) indicates that the principal must have ROLE_USER in its collection of roles. 2 To change method-level settings, you must override the method signature and apply a Spring Security annotation. In this case, the method overrides the repository-level settings with the requirement that the user have ROLE_ADMIN to perform a delete. The preceding example shows a standard Spring Data repository definition extending CrudRepository with some key changes: the specification of particular roles to access the various methods: Repository and method level security settings do not combine. Instead, method-level settings override repository level settings. The previous example illustrates that CrudRepository , in fact, has four delete methods. You must override all delete methods to properly secure it. @Secured security: The following example shows Spring Security’s older @Secured annotation, which is purely role-based: Example 2. spring-data-rest-tests/spring-data-rest-tests-security/src/test/java/org/springframework/data/rest/tests/security/SecuredPersonRepository.java @Secured(""ROLE_USER"") (1) @RepositoryRestResource(collectionResourceRel = ""people"", path = ""people"") public interface SecuredPersonRepository extends CrudRepository<Person, UUID> { @Secured(""ROLE_ADMIN"") (2) @Override void deleteById(UUID aLong); @Secured(""ROLE_ADMIN"") @Override void delete(Person person); @Secured(""ROLE_ADMIN"") @Override void deleteAll(Iterable<? extends Person> persons); @Secured(""ROLE_ADMIN"") @Override void deleteAll(); } 1 This results in the same security check as the previous example but has less flexibility. It allows only roles as the means to restrict access. 2 Again, this shows that delete methods require ROLE_ADMIN . If you start with a new project or first apply Spring Security, @PreAuthorize is the recommended solution. If are already using Spring Security with @Secured in other parts of your app, you can continue on that path without rewriting everything. Enabling Method-level Security: To configure method-level security, here is a brief snippet from Spring Data REST’s test suite: Example 3. spring-data-rest-tests/spring-data-rest-tests-security/src/test/java/org/springframework/data/rest/tests/security/SecurityConfiguration.java @Configuration (1) @EnableWebSecurity @EnableMethodSecurity(securedEnabled = true, prePostEnabled = true) (2) class SecurityConfiguration { (3) ... } 1 This is a Spring configuration class. 2 It uses Spring Security’s @EnableGlobalMethodSecurity annotation to enable both @Secured and @Pre / @Post support. NOTE: You don’t have to use both. This particular case is used to prove both versions work with Spring Data REST. 3 This class extends Spring Security’s WebSecurityConfigurerAdapter which is used for pure Java configuration of security. The rest of the configuration class is not listed, because it follows standard practices(https://docs.spring.io/spring-security/reference/servlet/configuration/java.html) that you can read about in the Spring Security reference docs."
"https://docs.spring.io/spring-data/rest/reference/4.3/tools.html","Tools: The HAL Explorer: Kai Tödter has created a useful application: HAL Explorer ( Git Repo(https://github.com/toedter/hal-explorer) , Reference Doc(https://toedter.github.io/hal-explorer/release/reference-doc/) , Demo(https://toedter.github.io/hal-explorer/release/hal-explorer/#theme=Cosmo&uri=examples/examples.hal-forms.json) ). It is an Angular based web application that lets you easily explore HAL and HAL-FORMS based HTTP responses. It also supports Spring profiles generated by Spring Data REST. You can point it at any Spring Data REST API and use it to navigate the app and create new resources. Instead of pulling down the files, embedding them in your application, and crafting a Spring MVC controller to serve them up, all you need to do is add a single dependency. The following listing shows how to add the dependency in Maven: <dependencies> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-rest-hal-explorer</artifactId> </dependency> </dependencies> The following listing shows how to add the dependency in Gradle: dependencies { implementation 'org.springframework.data:spring-data-rest-hal-explorer' } If you use Spring Boot or the Spring Data BOM (bill of materials), you do not need to specify the version. This dependency auto-configures the HAL Explorer to be served up when you visit your application’s root URI in a browser. (NOTE: localhost:8080/api(http://localhost:8080/api) was plugged into the browser, and it redirected to the URL shown in the following image.) The preceding screenshot shows the root path of the API. On the right side are details from the response, including headers, and the body (a HAL document). The HAL Explorer reads the links from the response and puts them in a list on the left side. You can either click on the green GET button and navigate to one of the collections, or click on the other buttons to make changes (POST, PUT, PATCH) or delete resources. The HAL Explorer understands URI Templates . Whenever a link contains a URI template, a modal dialog pops up where you can enter the template parameters. If you click Go! without entering anything, the variables are essentially ignored. For situations like Projections and Excerpts(projections-excerpts.html) or Paging and Sorting(paging-and-sorting.html) , this can be useful. When you click on a NON-GET button with a + or a > sign on it, a modal dialog appears. It shows the HTTP method belonging to the clicked button. You can fill the body and submit the new JSON document. Below the URI and HTTP method are the fields. These are automatically supplied, depending on the metadata of the resources, which was automatically generated by Spring Data REST. If you update your domain objects, the pop-up reflects it, as the following image shows:"
"https://docs.spring.io/spring-data/rest/reference/4.3/customizing-sdr.html","Customizing Spring Data REST: There are many options to tailor Spring Data REST. These subsections show how. Customizing Item Resource URIs: By default, the URI for item resources are comprised of the path segment used for the collection resource with the database identifier appended. That lets you use the repository’s findOne(…) method to lookup entity instances. As of Spring Data REST 2.5, this can be customized by using configuration API on RepositoryRestConfiguration (preferred on Java 8) or by registering an implementation of EntityLookup as a Spring bean in your application. Spring Data REST picks those up and tweaks the URI generation according to their implementation. Assume a User with a username property that uniquely identifies it. Further assume that we have a Optional<User> findByUsername(String username) method on the corresponding repository. On Java 8, we can register the mapping methods as method references to tweak the URI creation, as follows: @Component public class SpringDataRestCustomization implements RepositoryRestConfigurer { @Override public void configureRepositoryRestConfiguration(RepositoryRestConfiguration config) { config.withEntityLookup() .forRepository(UserRepository.class) .withIdMapping(User::getUsername) .withLookup(UserRepository::findByUsername); } } forRepository(…) takes the repository type as the first argument, a method reference mapping the repositories domain type to some target type as the second argument, and another method reference to map that value back by using the repository mentioned as the first argument. If you are not running Java 8 or better, you could use the method, but it would require a few quite verbose anonymous inner classes. On older Java versions, you should probably prefer implementing a UserEntityLookup that resembles the following: @Component public class UserEntityLookup extends EntityLookupSupport<User> { private final UserRepository repository; public UserEntityLookup(UserRepository repository) { this.repository = repository; } @Override public Serializable getResourceIdentifier(User entity) { return entity.getUsername(); } @Override public Object lookupEntity(Serializable id) { return repository.findByUsername(id.toString()); } } Notice how getResourceIdentifier(…) returns the username to be used by the URI creation. To load entity instances by the value returned from that method, we now implement lookupEntity(…) by using the query method available on the UserRepository . Customizing repository exposure: By default, all public Spring Data repositories are used to expose HTTP resources as described in Repository resources(repository-resources.html) . Package protected repository interfaces are excluded from this list, as you express its functionality is only visible to the package internally. This can be customized by explicitly setting a RepositoryDetectionStrategy (usually through the enum RepositoryDetectionStrategies ) on RepositoryRestConfiguration . The following values can be configured: ALL — exposes all Spring Data repositories regardless of their Java visibility or annotation configuration. DEFAULT — exposes public Spring Data repositories or ones explicitly annotated with @RepositoryRestResource and its exported attribute not set to false . VISIBILITY — exposes only public Spring Data repositories regardless of annotation configuration. ANNOTATED — only exposes Spring Data repositories explicitly annotated with @RepositoryRestResource and its exported attribute not set to false . If you need custom rules to apply, simply implement RepositoryDetectionStrategy manually. Customizing supported HTTP methods: Customizing default exposure: By default, Spring Data REST exposes HTTP resources and methods as described in Repository resources(repository-resources.html) based on which CRUD methods the repository exposes. The repositories don’t need to extend CrudRepository but can also selectively declare methods described in aforementioned section and the resource exposure will follow. E.g. if a repository does not expose a delete(…) method, an HTTP DELETE will not be supported for item resources. If you need to declare a method for internal use but don’t want it to trigger the HTTP method exposure, the repository method can be annotated with @RestResource(exported = false) . Which methods to annotate like that to remove support for which HTTP method is described in Repository resources(repository-resources.html) . Sometimes managing the exposure on the method level is not fine-grained enough. E.g. the save(…) method is used to back POST on collection resources, as well as PUT and PATCH on item resources. To selectively define which HTTP methods are supposed to be exposed, you can use RepositoryRestConfiguration.getExposureConfiguration() . The class exposes a Lambda based API to define both global and type-based rules: ExposureConfiguration config = repositoryRestConfiguration.getExposureConfiguration(); config.forDomainType(User.class).disablePutForCreation(); (1) config.withItemExposure((metadata, httpMethods) -> httpMethods.disable(HttpMethod.PATCH)); (2) 1 Disables the support for HTTP PUT to create item resources directly. 2 Disables the support for HTTP PATCH on all item resources."
"https://docs.spring.io/spring-data/rest/reference/4.3/customizing/configuring-the-rest-url-path.html","Configuring the REST URL Path: You can configure the segments of the URL path under which the resources of a JPA repository are exported. To do so, add an annotation at the class level or at the query method level. By default, the exporter exposes your CrudRepository by using the name of the domain class. Spring Data REST also applies the Evo Inflector(https://github.com/atteo/evo-inflector) to pluralize this word. Consider the following repository definition: interface PersonRepository extends CrudRepository<Person, Long> {} The repository defined by the preceding example is exposed at localhost:8080/persons/(http://localhost:8080/persons/) . To change how the repository is exported, add a @RestResource annotation at the class level, as the following example shows: @RepositoryRestResource(path = ""people"") interface PersonRepository extends CrudRepository<Person, Long> {} The repository defined by the preceding example is accessible at localhost:8080/people/(http://localhost:8080/people/) . If you have query methods defined, those also default to being exposed by their name, as the following example shows: interface PersonRepository extends CrudRepository<Person, Long> { List<Person> findByName(String name); } The method in the preceding example is exposed at localhost:8080/persons/search/findByName(http://localhost:8080/persons/search/findByName) . All query method resources are exposed under the search resource. To change the segment of the URL under which this query method is exposed, you can use the @RestResource annotation again, as the following example shows: @RepositoryRestResource(path = ""people"") interface PersonRepository extends CrudRepository<Person, Long> { @RestResource(path = ""names"") List<Person> findByName(String name); } Now the query method in the preceding example is exposed at localhost:8080/people/search/names(http://localhost:8080/people/search/names) . Handling rel Attributes: Since these resources are all discoverable, you can also affect how the rel attribute is displayed in the links sent out by the exporter. For instance, in the default configuration, if you issue a request to localhost:8080/persons/search(http://localhost:8080/persons/search) to find out what query methods are exposed, you get back a list of links similar to the following: { ""_links"" : { ""findByName"" : { ""href"" : ""http://localhost:8080/persons/search/findByName"" } } } To change the rel value, use the rel property on the @RestResource annotation, as the following example shows: @RepositoryRestResource(path = ""people"") interface PersonRepository extends CrudRepository<Person, Long> { @RestResource(path = ""names"", rel = ""names"") List<Person> findByName(String name); } The preceding example results in the following link value: { ""_links"" : { ""names"" : { ""href"" : ""http://localhost:8080/persons/search/names"" } } } These snippets of JSON assume you use Spring Data REST’s default format of HAL(https://stateless.co/hal_specification.html) . You can turn off HAL, which would cause the output to look different. However, your ability to override rel names is totally independent of the rendering format. You can change the rel of a repository, as the following example shows: @RepositoryRestResource(path = ""people"", rel = ""people"") interface PersonRepository extends CrudRepository<Person, Long> { @RestResource(path = ""names"", rel = ""names"") List<Person> findByName(String name); } Altering the rel of a repository changes the top-level name, as the following example output shows: { ""_links"" : { ""people"" : { ""href"" : ""http://localhost:8080/people"" }, … } } In the top level fragment shown in the preceding output: path = ""people"" changed the value in href from /persons to /people . rel = ""people"" changed the name of that link from persons to people . When you navigate to the search resource of this repository, the finder method’s @RestResource annotation has altered the path, as follows: { ""_links"" : { ""names"" : { ""href"" : ""http://localhost:8080/people/search/names"" } } } This collection of annotations in your repository definition has caused the following changes: The Repository-level annotation’s path = ""people"" is reflected in the base URI with /people . The inclusion of a finder method provides you with /people/search . path = ""names"" creates a URI of /people/search/names . rel = ""names"" changes the name of that link from findByNames to names . Hiding Certain Repositories, Query Methods, or Fields: You may not want a certain repository, a query method on a repository, or a field of your entity to be exported at all. Examples include hiding fields like password on a User object and similar sensitive data. To tell the exporter to not export these items, annotate them with @RestResource and set exported = false . For example, to skip exporting a repository, you could create a repository definition similar to the following example: @RepositoryRestResource(exported = false) interface PersonRepository extends CrudRepository<Person, Long> {} To skip exporting a query method, you can annotate the query method with @RestResource(exported = false) , as follows: @RepositoryRestResource(path = ""people"", rel = ""people"") interface PersonRepository extends CrudRepository<Person, Long> { @RestResource(exported = false) List<Person> findByName(String name); } Similarly, to skip exporting a field, you can annotate the field with @RestResource(exported = false) , as follows: @Entity public class Person { @Id @GeneratedValue private Long id; @OneToMany @RestResource(exported = false) private Map<String, Profile> profiles; } Projections provide the means to change what is exported and effectively side-step these settings(#projections-excerpts.hidden-data) . If you create any projections against the same domain object, be sure to NOT export the fields. Hiding Repository CRUD Methods: If you do not want to expose a save or delete method on your CrudRepository , you can use the @RestResource(exported = false) setting by overriding the method you want to turn off and placing the annotation on the overridden version. For example, to prevent HTTP users from invoking the delete methods of CrudRepository , override all of them and add the annotation to the overridden methods, as follows: @RepositoryRestResource(path = ""people"", rel = ""people"") interface PersonRepository extends CrudRepository<Person, Long> { @Override @RestResource(exported = false) void delete(Long id); @Override @RestResource(exported = false) void delete(Person entity); } It is important that you override both delete methods. In the interest of faster runtime performance, the exporter currently uses a somewhat naive algorithm for determining which CRUD method to use. You cannot currently turn off the version of delete that takes an ID but export the version that takes an entity instance. For the time being, you can either export the delete methods or not. If you want turn them off, keep in mind that you have to annotate both versions with exported = false ."
"https://docs.spring.io/spring-data/rest/reference/4.3/customizing/adding-sdr-to-spring-mvc-app.html","Adding Spring Data REST to an Existing Spring MVC Application: The following steps are unnecessary if you use Spring Boot. For Boot applications, adding spring-boot-starter-data-rest automatically adds Spring Data REST to your application. You can integrate Spring Data REST with an existing Spring MVC application. In your Spring MVC configuration (most likely where you configure your MVC resources), add a bean reference to the Java configuration class that is responsible for configuring the RepositoryRestController . The class name is org.springframework.data.rest.webmvc.RepositoryRestMvcConfiguration . The following example shows how to use an @Import annotation to add the proper reference: The configuration would look like: Java import org.springframework.context.annotation.Import; import org.springframework.data.rest.webmvc.RepositoryRestMvcConfiguration; @Configuration @Import(RepositoryRestMvcConfiguration.class) public class MyApplicationConfiguration { … } XML <bean class=""org.springframework.data.rest.webmvc.config.RepositoryRestMvcConfiguration""/> When your ApplicationContext comes across this bean definition, it bootstraps the necessary Spring MVC resources to fully configure the controller for exporting the repositories it finds in that ApplicationContext and any parent contexts. More on Required Configuration: Spring Data REST depends on a couple Spring MVC resources that must be configured correctly for it to work inside an existing Spring MVC application. We tried to isolate those resources from whatever similar resources already exist within your application, but it may be that you want to customize some of the behavior of Spring Data REST by modifying these MVC components. You should pay special attention to configuring RepositoryRestHandlerMapping , covered in the next section. RepositoryRestHandlerMapping: We register a custom HandlerMapping instance that responds only to the RepositoryRestController and only if a path is meant to be handled by Spring Data REST. In order to keep paths that are meant to be handled by your application separate from those handled by Spring Data REST, this custom HandlerMapping class inspects the URL path and checks to see if a repository has been exported under that name. If it has, the custom HandlerMapping class lets the request be handled by Spring Data REST. If there is no Repository exported under that name, it returns null , which means “let other HandlerMapping instances try to service this request”. The Spring Data REST HandlerMapping is configured with order=(Ordered.LOWEST_PRECEDENCE - 100) , which means it is usually first in line when it comes time to map a URL path. Your existing application never gets a chance to service a request that is meant for a repository. For example, if you have a repository exported under the name of person , then all requests to your application that start with /person are handled by Spring Data REST, and your application never sees that request. If your repository is exported under a different name (such as people ), however, then requests to /people go to Spring Data REST and requests to /person are handled by your application."
"https://docs.spring.io/spring-data/rest/reference/4.3/customizing/overriding-sdr-response-handlers.html","Overriding Spring Data REST Response Handlers: Sometimes, you may want to write a custom handler for a specific resource. To take advantage of Spring Data REST’s settings, message converters, exception handling, and more, use the @RepositoryRestController annotation instead of a standard Spring MVC @Controller or @RestController . Controllers annotated with @RepositoryRestController are served from the API base path defined in RepositoryRestConfiguration.setBasePath , which is used by all other RESTful endpoints (for example, /api ). The following example shows how to use the @RepositoryRestController annotation: @RepositoryRestController class ScannerController { private final ScannerRepository repository; ScannerController(ScannerRepository repository) { (1) this.repository = repository; } @GetMapping(path = ""/scanners/search/producers"") (2) ResponseEntity<?> getProducers() { List<String> producers = repository.listProducers(); (3) // do some intermediate processing, logging, etc. with the producers CollectionModel<String> resources = CollectionModel.of(producers); (4) resources.add(linkTo(methodOn(ScannerController.class).getProducers()).withSelfRel()); (5) // add other links as needed return ResponseEntity.ok(resources); (6) } } 1 This example uses constructor injection. 2 This handler plugs in a custom handler method as query method resource 3 This handler uses the underlying repository to fetch data, but then does some form of post processing before returning the final data set to the client. 4 The results of type T need to be wrapped up in a Spring HATEOAS CollectionModel<T> object to return a collection. EntityModel<T> or RepresentationModel<T> are suitable wrappers for a single item, respectively. 5 Add a link back to this exact method as a self link. 6 Returning the collection by using Spring MVC’s ResponseEntity wrapper ensures that the collection is properly wrapped and rendered in the proper accept type. CollectionModel is for a collection, while EntityModel — or the more general class RepresentationModel — is for a single item. These types can be combined. If you know the links for each item in a collection, use CollectionModel<EntityModel<String>> (or whatever the core domain type is rather than String ). Doing so lets you assemble links for each item as well as for the whole collection. In this example, the combined path is RepositoryRestConfiguration.getBasePath() + /scanners/search/producers . Obtaining Aggregate References: For custom controllers receiving PUT and POST requests, the request body usually contains a JSON document that will use URIs to express references to other resources. For GET requests, those references are handed in via a request parameter. As of Spring Data REST 4.1, we provide AggregateReference<T, ID> to be used as handler method parameter type to capture such references and resolve them into either the referenced aggregate’s identifier, the aggregate itself or a jMolecules Association . All you need to do is declare an @RequestParam of that type and then consume either the identifier or the fully resolved aggregate. @RepositoryRestController class ScannerController { private final ScannerRepository repository; ScannerController(ScannerRepository repository) { this.repository = repository; } @GetMapping(path = ""/scanners"") ResponseEntity<?> getProducers( @RequestParam AggregateReference<Producer, ProducerIdentifier> producer) { var identifier = producer.resolveRequiredId(); // Alternatively var aggregate = producer.resolveRequiredAggregate(); } // Alternatively @GetMapping(path = ""/scanners"") ResponseEntity<?> getProducers( @RequestParam AssociationAggregateReference<Producer, ProducerIdentifier> producer) { var association = producer.resolveRequiredAssociation(); } } In case you are using jMolecules, AssociationAggregateReference also allows you to obtain an Association . While both of the abstraction assume the value for the parameter to be a URI matching the scheme that Spring Data REST uses to expose item resources, that source value resolution can be customized by calling ….withIdSource(…) on the reference instance to provide a function to extract the identifier value to be used for aggregate resolution eventually from the UriComponents obtained from the URI received. @RepositoryRestController VS. @BasePathAwareController: If you are not interested in entity-specific operations but still want to build custom operations underneath basePath , such as Spring MVC views, resources, and others, use @BasePathAwareController . If you’re using @RepositoryRestController on your custom controller, it will only handle the request if your request mappings blend into the URI space used by the repository. It will also apply the following extra functionality to the controller methods: CORS configuration according as defined for the repository mapped to the base path segment used in the request mapping of the handler method. Apply an OpenEntityManagerInViewInterceptor if JPA is used to make sure you can access properties marked as to be resolved lazily. If you use @Controller or @RestController for anything, that code is totally outside the scope of Spring Data REST. This extends to request handling, message converters, exception handling, and other uses."
"https://docs.spring.io/spring-data/rest/reference/4.3/customizing/customizing-json-output.html","Customizing the JSON Output: Sometimes in your application, you need to provide links to other resources from a particular entity. For example, a Customer response might be enriched with links to a current shopping cart or links to manage resources related to that entity. Spring Data REST provides integration with Spring HATEOAS(https://github.com/spring-projects/spring-hateoas) and provides an extension hook that lets you alter the representation of resources that go out to the client. The RepresentationModelProcessor Interface: Spring HATEOAS defines a RepresentationModelProcessor<> interface for processing entities. All beans of type RepresentationModelProcessor<EntityModel<T>> are automatically picked up by the Spring Data REST exporter and triggered when serializing an entity of type T . For example, to define a processor for a Person entity, add a @Bean similar to the following (which is taken from the Spring Data REST tests) to your ApplicationContext : @Bean public RepresentationModelProcessor<EntityModel<Person>> personProcessor() { return new RepresentationModelProcessor<EntityModel<Person>>() { @Override public EntityModel<Person> process(EntityModel<Person> model) { model.add(new Link(""http://localhost:8080/people"", ""added-link"")); return model; } }; } The preceding example hard codes a link to localhost:8080/people(http://localhost:8080/people) . If you have a Spring MVC endpoint inside your app to which you wish to link, consider using Spring HATEOAS’s linkTo(…​)(https://docs.spring.io/spring-hateoas/docs/current/reference/html/#fundamentals.obtaining-links.builder.methods) method to avoid managing the URL. Adding Links: You can add links to the default representation of an entity by calling model.add(Link) , as the preceding example shows. Any links you add to the EntityModel are added to the final output. Customizing the Representation: The Spring Data REST exporter runs any discovered RepresentationModelProcessor instances before it creates the output representation. It does so by registering a Converter<Entity, EntityModel> instance with an internal ConversionService . This is the component responsible for creating the links to referenced entities (such as those objects under the _links property in the object’s JSON representation). It takes an @Entity and iterates over its properties, creating links for those properties that are managed by a Repository and copying across any embedded or simple properties. If your project needs to have output in a different format, however, you can completely replace the default outgoing JSON representation with your own. If you register your own ConversionService in the ApplicationContext and register your own Converter<Entity, EntityModel> , you can return a EntityModel implementation of your choosing."
"https://docs.spring.io/spring-data/rest/reference/4.3/customizing/custom-jackson-deserialization.html","Adding Custom Serializers and Deserializers to Jackson’s ObjectMapper: Sometimes, the behavior of the Spring Data REST ObjectMapper (which has been specially configured to use intelligent serializers that can turn domain objects into links and back again) may not handle your domain model correctly. You can structure your data in so many ways that you may find your own domain model does not correctly translate to JSON. It is also sometimes not practical in these cases to support a complex domain model in a generic way. Sometimes, depending on the complexity, it is not even possible to offer a generic solution. To accommodate the largest percentage of the use cases, Spring Data REST tries to render your object graph correctly. It tries to serialize unmanaged beans as normal POJOs, and tries to create links to managed beans where necessary. However, if your domain model does not easily lend itself to reading or writing plain JSON, you may want to configure Jackson’s ObjectMapper with your own custom type mappings and (de)serializers. Abstract Class Registration: One key configuration point you might need to hook into is when you use an abstract class (or an interface) in your domain model. Jackson does not, by default, know what implementation to create for an interface. Consider the following example: @Entity public class MyEntity { @OneToMany private List<MyInterface> interfaces; } In a default configuration, Jackson has no idea what class to instantiate when POSTing new data to the exporter. You need to tell Jackson either through an annotation or, more cleanly, by registering a type mapping by using a Module(https://wiki.fasterxml.com/JacksonFeatureModules) . Any Module bean declared within the scope of your ApplicationContext is picked up by the exporter and registered with its ObjectMapper . To add this special abstract class type mapping, you can create a Module bean and, in the setupModule method, add an appropriate TypeResolver , as follows: public class MyCustomModule extends SimpleModule { private MyCustomModule() { super(""MyCustomModule"", new Version(1, 0, 0, ""SNAPSHOT"")); } @Override public void setupModule(SetupContext context) { context.addAbstractTypeResolver( new SimpleAbstractTypeResolver().addMapping(MyInterface.class, MyInterfaceImpl.class)); } } Once you have access to the SetupContext object in your Module , you can do all sorts of cool things to configure Jackon’s JSON mapping. You can read more about how Modules work on Jackson’s wiki(https://wiki.fasterxml.com/JacksonFeatureModules) . Adding Custom Serializers for Domain Types: If you want to serialize or deserialize a domain type in a special way, you can register your own implementations with Jackson’s ObjectMapper . Then the Spring Data REST exporter transparently handles those domain objects correctly. To add serializers from your setupModule method implementation, you can do something like the following: public class MyCustomModule extends SimpleModule { … @Override public void setupModule(SetupContext context) { SimpleSerializers serializers = new SimpleSerializers(); SimpleDeserializers deserializers = new SimpleDeserializers(); serializers.addSerializer(MyEntity.class, new MyEntitySerializer()); deserializers.addDeserializer(MyEntity.class, new MyEntityDeserializer()); context.addSerializers(serializers); context.addDeserializers(deserializers); } } Thanks to the custom module shown in the preceding example, Spring Data REST correctly handles your domain objects when they are too complex for the 80% generic use case that Spring Data REST tries to cover."
"https://docs.spring.io/spring-data/rest/reference/4.3/customizing/configuring-cors.html","Configuring CORS: For security reasons, browsers prohibit AJAX calls to resources residing outside the current origin. When working with client-side HTTP requests issued by a browser, you want to enable specific HTTP resources to be accessible. Spring Data REST, as of 2.6, supports Cross-Origin Resource Sharing(https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) (CORS) through Spring’s CORS(https://docs.spring.io/spring-framework/reference/6.1/web.html#mvc-cors) support. Repository Interface CORS Configuration: You can add a @CrossOrigin annotation to your repository interfaces to enable CORS for the whole repository. By default, @CrossOrigin allows all origins and HTTP methods. The following example shows a cross-origin repository interface definition: @CrossOrigin interface PersonRepository extends CrudRepository<Person, Long> {} In the preceding example, CORS support is enabled for the whole PersonRepository . @CrossOrigin provides attributes to configure CORS support, as the following example shows: @CrossOrigin(origins = ""http://domain2.example"", methods = { RequestMethod.GET, RequestMethod.POST, RequestMethod.DELETE }, maxAge = 3600) interface PersonRepository extends CrudRepository<Person, Long> {} The preceding example enables CORS support for the whole PersonRepository by providing one origin, restricted to the GET , POST , and DELETE methods and with a max age of 3600 seconds. Repository REST Controller Method CORS Configuration: Spring Data REST fully supports Spring Web MVC’s controller method configuration(https://docs.spring.io/spring-framework/reference/6.1/web.html#mvc-cors-controller) on custom REST controllers that share repository base paths, as the following example shows: @RepositoryRestController public class PersonController { @CrossOrigin(maxAge = 3600) @RequestMapping(path = ""/people/xml/{id}"", method = RequestMethod.GET, produces = MediaType.APPLICATION_XML_VALUE) public Person retrieve(@PathVariable Long id) { // … } } Controllers annotated with @RepositoryRestController inherit @CrossOrigin configuration from their associated repositories. Global CORS Configuration: In addition to fine-grained, annotation-based configuration, you probably want to define some global CORS configuration as well. This is similar to Spring Web MVC’S CORS configuration but can be declared within Spring Data REST and combined with fine-grained @CrossOrigin configuration. By default, all origins and GET , HEAD , and POST methods are allowed. Existing Spring Web MVC CORS configuration is not applied to Spring Data REST. The following example sets an allowed origin, adds the PUT and DELETE HTTP methods, adds and exposes some headers, and sets a maximum age of an hour: @Component public class SpringDataRestCustomization implements RepositoryRestConfigurer { @Override public void configureRepositoryRestConfiguration(RepositoryRestConfiguration config, CorsRegistry cors) { cors.addMapping(""/person/**"") .allowedOrigins(""http://domain2.example"") .allowedMethods(""PUT"", ""DELETE"") .allowedHeaders(""header1"", ""header2"", ""header3"") .exposedHeaders(""header1"", ""header2"") .allowCredentials(false).maxAge(3600); } }"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/index.html","Spring Data Cassandra: Spring Data for Apache Cassandra provides repository support for the Apache Cassandra database. It eases development of applications with a consistent programming model that need to access Cassandra data sources. Cassandra(cassandra.html) Apache Cassandra support and connectivity Repositories(repositories.html) Apache Cassandra Repositories Observability(observability.html) Observability Integration Kotlin(kotlin.html) Kotlin support Migration(migration-guides.html) Migration Guides Wiki(https://github.com/spring-projects/spring-data-commons/wiki) What’s New, Upgrade Notes, Supported Versions, additional cross-version information. David Webb, Matthew Adams, John Blum, Mark Paluch, Jay Bryant © 2008-2024 VMware, Inc. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/commons/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/migration-guides.html","Migration Guides: This section contains version-specific migration guides explaining how to upgrade between two versions. Section Summary: Migration Guide from 1.x to 2.x(migration-guide/migration-guide-1.5-to-2.0.html) Migration Guide from 2.x to 3.x(migration-guide/migration-guide-2.2-to-3.0.html) Migration Guide from 3.x to 4.x(migration-guide/migration-guide-3.0-to-4.0.html) Migration Guide from 4.x to 4.3(migration-guide/migration-guide-4.0-to-4.3.html)"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/migration-guide/migration-guide-1.5-to-2.0.html","Migration Guide from 1.x to 2.x: Spring Data for Apache Cassandra 2.0 introduces a set of breaking changes when upgrading from earlier versions: Merged the spring-cql and spring-data-cassandra modules into a single module. Separated asynchronous and synchronous operations in CqlOperations and CassandraOperations into dedicated interfaces and templates. Revised the CqlTemplate API to align with JdbcTemplate . Removed the CassandraOperations.selectBySimpleIds method. Used better names for CassandraRepository . Removed SD Cassandra ConsistencyLevel and RetryPolicy types in favor of DataStax ConsistencyLevel and RetryPolicy types. Refactored CQL specifications to value objects and configurators. Refactored QueryOptions to be immutable objects. Refactored CassandraPersistentProperty to single-column. Deprecations: Deprecated QueryOptionsBuilder.readTimeout(long, TimeUnit) in favor of QueryOptionsBuilder.readTimeout(Duration) . Deprecated CustomConversions in favor of CassandraCustomConversions . Deprecated BasicCassandraMappingContext in favor of CassandraMappingContext . Deprecated o.s.d.c.core.cql.CachedPreparedStatementCreator in favor of o.s.d.c.core.cql.support.CachedPreparedStatementCreator . Deprecated CqlTemplate.getSession() in favor of getSessionFactory() . Deprecated CqlIdentifier.cqlId(…) and KeyspaceIdentifier.ksId(…) in favor of the .of(…) methods. Deprecated constructors of QueryOptions in favor of their builders. Deprecated TypedIdCassandraRepository in favor of CassandraRepository Merged Spring CQL and Spring Data Cassandra Modules: Spring CQL and Spring Data Cassandra are now merged into a single module. The standalone spring-cql module is no longer available. You can find all types merged into spring-data-cassandra . The following listing shows how to include spring-data-cassandra in your maven dependencies: <dependencies> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-cassandra</artifactId> <version>4.3.4</version> </dependency> </dependencies> With the merge, we merged all CQL packages into Spring Data Cassandra: Moved o.s.d.cql into o.s.d.cassandra.core.cql . Merged o.s.d.cql with o.s.d.cassandra.config and flattened the XML and Java subpackages. Moved CassandraExceptionTranslator and CqlExceptionTranslator to o.s.d.c.core.cql . Moved Cassandra exceptions o.s.d.c.support.exception to o.s.d.cassandra . Moved o.s.d.c.convert to o.s.d.c.core.convert (affects converters). Moved o.s.d.c.mapping to o.s.d.c.core.mapping (affects mapping annotations). Moved MapId from o.s.d.c.repository to o.s.d.c.core.mapping . [[revised-cqltemplate/cassandratemplate]] == Revised CqlTemplate / CassandraTemplate We split CqlTemplate and CassandraTemplate in three ways: CassandraTemplate is no longer a CqlTemplate but uses an instance that allows reuse and fine-grained control over fetch size, consistency levels, and retry policies. You can obtain the CqlOperations through CassandraTemplate.getCqlOperations() . Because of the change, dependency injection of CqlTemplate requires additional bean setup. CqlTemplate now reflects basic CQL operations instead of mixing high-level and low-level API calls (such as count(…) versus execute(…) ) and the reduced method set is aligned with Spring Frameworks’s JdbcTemplate with its convenient callback interfaces. Asynchronous methods are re-implemented on AsyncCqlTemplate and AsyncCassandraTemplate by using ListenableFuture . We removed Cancellable and the various async callback listeners. ListenableFuture is a flexible approach and allows transition into a CompletableFuture . Removed CassandraOperations.selectBySimpleIds(): The method was removed because it did not support complex IDs. The newly introduced query DSL allows mapped and complex id’s for single column Id’s, as the following example shows: cassandraTemplate.select(Query.query(Criteria.where(""id"").in(…)), Person.class) Better names for CassandraRepository: We renamed CassandraRepository and TypedIdCassandraRepository to align Spring Data Cassandra naming with other Spring Data modules: Renamed CassandraRepository to MapIdCassandraRepository Renamed TypedIdCassandraRepository to CassandraRepository Introduced TypedIdCassandraRepository , extending CassandraRepository as a deprecated type to ease migration Removed SD Cassandra ConsistencyLevel and RetryPolicy types in favor of DataStax ConsistencyLevel and RetryPolicy types: Spring Data Cassandra ConsistencyLevel and RetryPolicy have been removed. Please use the types provided by the DataStax driver. The Spring Data Cassandra types restricted usage of available features provided in and allowed by the Cassandra native driver. As a result, the Spring Data Cassandra’s types required an update each time newer functionality was introduced by the driver. Refactored CQL Specifications to Value Objects and Configurators: As much as possible, CQL specification types are now value types (such as FieldSpecification , AlterColumnSpecification ), and objects are constructed by static factory methods. This allows immutability for simple value objects. Configurator objects (such as AlterTableSpecification ) that operate on mandatory properties (such as a table name or keyspace name) are initially constructed through a a static factory method and allow further configuration until the desired state is created. Refactored QueryOptions to be Immutable Objects: QueryOptions and WriteOptions are now immutable and can be created through builders. Methods accepting QueryOptions enforce non-null objects, which are available from static empty() factory methods. The following example shows how to use QueryOptions.builder() : QueryOptions queryOptions = QueryOptions.builder() .consistencyLevel(ConsistencyLevel.ANY) .retryPolicy(FallthroughRetryPolicy.INSTANCE) .readTimeout(Duration.ofSeconds(10)) .fetchSize(10) .tracing(true) .build(); Refactored CassandraPersistentProperty to Single-column: This change affects You only if you operate directly on the mapping model. CassandraPersistentProperty allowed previously multiple column names to be bound for composite primary key use. Columns of a CassandraPersistentProperty are now reduced to a single column. Resolved composite primary keys map to a class through MappingContext.getRequiredPersistentEntity(…) ."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/migration-guide/migration-guide-2.2-to-3.0.html","Migration Guide from 2.x to 3.x: Spring Data for Apache Cassandra 3.0 introduces a set of breaking changes when upgrading from earlier versions. Review dependencies: Upgrading to Spring Data Cassandra requires an upgrade to the DataStax Driver version 4. Upgrading to the new driver comes with transitive dependency changes, most notably, Google Guava is bundled and shaded by the driver. Check out the DataStax Java Driver for Apache Cassandra 4 Upgrade Guide(https://docs.datastax.com/en/developer/java-driver/4.3/upgrade_guide/) for details on the Driver-related changes. Adapt Configuration: DataStax Java Driver 4 merges Cluster and Session objects into a single CqlSession object, therefore, all Cluster -related API was removed. The configuration was revised in large parts by removing most configuration items that were moved into DriverConfigLoader that is mostly file-based. This means that SocketOptions , AddressTranslator and many more options are configured now through other means. If you’re using XML-based configuration, make sure to migrate all configuration files from the cql namespace ( www.springframework.org/schema/cql(http://www.springframework.org/schema/cql) www.springframework.org/schema/cql/spring-cql.xsd(https://www.springframework.org/schema/cql/spring-cql.xsd) ) to the cassandra namespace ( www.springframework.org/schema/data/cassandra(http://www.springframework.org/schema/data/cassandra) www.springframework.org/schema/data/cassandra/spring-cassandra.xsd(https://www.springframework.org/schema/data/cassandra/spring-cassandra.xsd) ). To reflect the change in configuration builders, ClusterBuilderConfigurer was renamed to SessionBuilderConfigurer accepting now CqlSessionBuilder instead of the Cluster.Builder . Make sure to also provide the local data center in your configuration as it is required to properly configure load balancing. Connectivity: The configuration elements for Cluster ( cassandra:cluster ) and Session ( cassandra:session ) were merged into a single CqlSession ( cassandra:session ) element that configures both, the keyspace and endpoints. With the upgrade, schema support was moved to a new namespace element: cassandra:session-factory that provides a SessionFactory bean. Example 1. Cluster, Session and Schema Configuration in version 2: <cassandra:cluster contact-points=""localhost"" port=""9042""> <cassandra:keyspace action=""CREATE_DROP"" name=""mykeyspace"" /> </cassandra:cluster> <cassandra:session keyspace-name=""mykeyspace"" schema-action=""CREATE""> <cassandra:startup-cql>CREATE TABLE …</cassandra:startup-cql> </cassandra:session> Example 2. Session and Schema Configuration in version 3: <cassandra:session contact-points=""localhost"" port=""9042"" keyspace=""mykeyspace"" local-datacenter=""datacenter1""> <cassandra:keyspace action=""CREATE_DROP"" name=""mykeyspace"" /> </cassandra:session> <cassandra:session-factory schema-action=""CREATE""> <cassandra:script location=""classpath:/schema.cql""/> </cassandra:session-factory> Spring Data Cassandra 3.0 no longer registers default Mapping Context, Context and Template API beans when using XML namespace configuration. The defaulting should be applied on application or Spring Boot level. Template API: Spring Data for Apache Cassandra encapsulates most of the changes that come with the driver upgrade as the Template API and repository support if your application mainly interacts with mapped entities or primitive Java types. We generally recommend to create CqlTemplate and CassandraTemplate objects by using SessionFactory as the factory usage allows synchronization for schema creation and introduces a level of flexibility when working with multiple databases. Example 3. Template API configuration in version 2: <cql:template session-ref=""…"" /> <cassandra:template session-ref=""…"" cassandra-converter-ref=""…""/> Example 4. Template API configuration in version 3: <cassandra:session-factory /> <cassandra:cql-template session-factory-ref=""…"" /> <cassandra:template session-factory-ref=""…"" cassandra-converter-ref=""…""/> You will have to adapt your code in all places, where you use DataStax driver API directly. Typical cases include: Implementations of ResultSetExtractor Implementations of RowCallbackHandler Implementations of RowMapper Implementations of PreparedStatementCreator including async and reactive variants Calls to CqlTemplate.queryForResultSet(…) Calling methods that accept Statement Changes in AsyncCqlTemplate: DataStax driver 4 has changed the result type of queries that are run asynchronously. To reflect these changes, you need to adapt your code that provides: Implementations of AsyncSessionCallback Implementations of AsyncPreparedStatementCreator Result set extraction requires a new interface for DataStax' AsyncResultSet . AsyncCqlTemplate now uses AsyncResultSetExtractor in places where it used previously ResultSetExtractor . Note that AsyncResultSetExtractor.extractData(…) returns a Future instead of a scalar object so a migration of code comes with the possibility to use fully non-blocking code in the extractor. Data model migrations: Your data model may require updates if you use the following features: @CassandraType forceQuote in @Table , @Column , @PrimaryKeyColumn , @PrimaryKey and @UserDefinedType Properties using java.lang.Date Properties using UDTValue or TupleValue @CassandraType: DataStax driver 4 no longer ships with a Name enumeration to describe the Cassandra type. We decided to re-introduce the enumeration with CassandraType.Name . Make sure to update your imports to use the newly introduced replacement type. Force Quote: This flag is now deprecated, and we recommend not to use it any longer. Spring Data for Apache Cassandra internally uses the driver’s CqlIdentifier that ensures quoting where it’s required. Property Types: DataStax driver 4 no longer uses java.lang.Date . Please upgrade your data model to use java.time.LocalDateTime . Please also migrate raw UDT and tuple types to the new driver types UdtValue respective TupleValue . Other changes: Driver’s ConsistencyLevel constant class was removed and reintroduced as DefaultConsistencyLevel . @Consistency was adapted to DefaultConsistencyLevel . RetryPolicy on QueryOptions and …CqlTemplate types was removed without replacement. Drivers’s PagingState type was removed. Paging state now uses ByteBuffer . SimpleUserTypeResolver accepts CqlSession instead of Cluster . SimpleTupleTypeFactory was migrated to enum . SimpleTupleTypeFactory.INSTANCE no longer requires a Cluster / CqlSession context. Introduction of StatementBuilder to functionally build statements as the QueryBuilder API uses immutable statement types. Session bean renamed from session to cassandraSession and SessionFactory bean renamed from sessionFactory to cassandraSessionFactory . ReactiveSession bean renamed from reactiveSession to reactiveCassandraSession and ReactiveSessionFactory bean renamed from reactiveSessionFactory to reactiveCassandraSessionFactory . ReactiveSessionFactory.getSession() now returns a Mono<ReactiveSession> . Previously it returned just ReactiveSession . Data type resolution was moved into ColumnTypeResolver so all DataType -related methods were moved from CassandraPersistentEntity / CassandraPersistentProperty into ColumnTypeResolver (affected methods are MappingContext.getDataType(…) , CassandraPersistentProperty.getDataType() , CassandraPersistentEntity.getUserType() , and CassandraPersistentEntity.getTupleType() ). Schema creation was moved from MappingContext to SchemaFactory (affected methods are CassandraMappingContext.getCreateTableSpecificationFor(…) , CassandraMappingContext.getCreateIndexSpecificationsFor(…) , and CassandraMappingContext.getCreateUserTypeSpecificationFor(…) ). Deprecations: CassandraCqlSessionFactoryBean , use CqlSessionFactoryBean instead. KeyspaceIdentifier and CqlIdentifier , use com.datastax.oss.driver.api.core.CqlIdentifier instead. CassandraSessionFactoryBean , use CqlSessionFactoryBean instead. AbstractCqlTemplateConfiguration , use AbstractSessionConfiguration instead. AbstractSessionConfiguration.getClusterName() , use AbstractSessionConfiguration.getSessionName() instead. CodecRegistryTupleTypeFactory , use SimpleTupleTypeFactory instead. Spring Data’s CqlIdentifier , use the driver CqlIdentifier instead. forceQuote attributes as quoting is no longer required. CqlIdentifier properly escapes reserved keywords and takes care of case-sensitivity. fetchSize on QueryOptions and …CqlTemplate types was deprecated, use pageSize instead CassandraMappingContext.setUserTypeResolver(…) , CassandraMappingContext.setCodecRegistry(…) , and CassandraMappingContext.setCustomConversions(…) : Configure these properties on CassandraConverter . TupleTypeFactory and CassandraMappingContext.setTupleTypeFactory(…) : TupleTypeFactory is no longer used as the Cassandra driver ships with a DataTypes.tupleOf(…) factory method. Schema creation via CqlSessionFactoryBean ( cassandra:session ) is deprecated. Keyspace creation via CqlSessionFactoryBean ( cassandra:session ) is not affected. Removals: Configuration API: PoolingOptionsFactoryBean SocketOptionsFactoryBean CassandraClusterFactoryBean CassandraClusterParser CassandraCqlClusterFactoryBean CassandraCqlClusterParser CassandraCqlSessionParser AbstractClusterConfiguration ClusterBuilderConfigurer (use SessionBuilderConfigurer instead Utilities: GuavaListenableFutureAdapter QueryOptions and WriteOptions constructor taking ConsistencyLevel and RetryPolicy arguments. Use the builder in conjunction of execution profiles as replacement. CassandraAccessor.setRetryPolicy(…) and ReactiveCqlTemplate.setRetryPolicy(…) methods. Use execution profiles as replacement. Namespace support: cql namespace ( www.springframework.org/schema/cql(http://www.springframework.org/schema/cql) , use www.springframework.org/schema/data/cassandra(http://www.springframework.org/schema/data/cassandra) instead) cassandra:cluster (endpoint properties merged to cassandra:session ) cql:template , use cassandra:cql-template instead Removed implicit bean registrations Mapping Context, Context and Template API beans. These must be declared explicitly. Additions: Configuration API: CqlSessionFactoryBean InitializeKeyspaceBeanDefinitionParser SessionFactoryFactoryBean including schema creation via KeyspacePopulator KeyspacePopulator and SessionFactoryInitializer to initialize a keyspace Namespace support: cassandra:cluster (endpoint properties merged to cassandra:session ) cassandra:initialize-keyspace namespace support cassandra:session-factory with cassandra:script support"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/migration-guide/migration-guide-3.0-to-4.0.html","Migration Guide from 3.x to 4.x: Spring Data for Apache Cassandra 4.0 introduces a set of breaking changes when upgrading from earlier versions. Asynchronous Template API: With the deprecation of ListenableFuture , AsyncCqlOperations and AsyncCassandraOperations and their dependant classes were migrated to CompletableFuture . If your application heavily depends on ListenableFuture and you cannot easily migrate to CompletableFuture then we suggest switching to the legacy Async…Operations types in the legacy subpackage. That is org.springframework.data.cassandra.core.cql.legacy for AsyncCqlOperations and org.springframework.data.cassandra.core.legacy for AsyncCassandraOperations ."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/migration-guide/migration-guide-4.0-to-4.3.html","Migration Guide from 4.x to 4.3: Spring Data for Apache Cassandra 4.3 has migrated the com.datastax.oss groupId to org.apache.cassandra . Migration of the Datastax driver into Apache: With the migration of the Datastax driver into the Apache foundation, you need to update coordinates of the driver in your code. Consider the following example showing a potential previous state of a Maven project configuration: Example 1. pom.xml Example up to 4.2.x <dependency> <groupId>com.datastax.oss</groupId> <artifactId>java-driver-core</artifactId> </dependency> <dependency> <groupId>com.datastax.oss</groupId> <artifactId>java-driver-query-builder</artifactId> </dependency> With upgrading the groupId from com.datastax.oss to org.apache.cassandra your project configuration would look like: Example 2. pom.xml Example since to 4.3.x <dependency> <groupId>org.apache.cassandra</groupId> <artifactId>java-driver-core</artifactId> </dependency> <dependency> <groupId>org.apache.cassandra</groupId> <artifactId>java-driver-query-builder</artifactId> </dependency>"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra.html","Cassandra Support: This part of the reference documentation explains the core functionality offered by Spring Data for Apache Cassandra. Spring Data support for Apache Cassandra contains a wide range of features: Spring configuration support with Java-based @Configuration classes or the XML namespace(cassandra/configuration.html) . The CqlTemplate, AsyncCqlTemplate, and ReactiveCqlTemplate(cassandra/cql-template.html) helper classes that increases productivity by properly handling common Cassandra data access operations. The CassandraTemplate, AsyncCassandraTemplate, and ReactiveCassandraTemplate(cassandra/template.html) helper classes that provide object mapping between CQL Tables and POJOs. Exception translation(cassandra/cql-template.html#exception-translation) into Spring’s portable Data Access Exception Hierarchy(https://docs.spring.io/spring-framework/reference/6.1data-access.html#dao-exceptions) . Feature rich object mapping(object-mapping.html) integrated with Spring’s Conversion Service(https://docs.spring.io/spring-framework/reference/6.1core.html#core-convert) . Annotation-based mapping(object-mapping.html#mapping.usage-annotations) metadata that is extensible to support other metadata formats. Java-based query, criteria, and update DSLs(cassandra/template.html#cassandra.template.query) . Automatic implementation of imperative and reactive Repository interfaces(repositories.html) including support for custom query methods(repositories/custom-implementations.html) . Abstractions: Spring Data for Apache Cassandra allows interaction on both the CQL and the entity level. The value provided by the Spring Data for Apache Cassandra abstraction is perhaps best shown by the sequence of actions outlined in the table below. The table shows which actions Spring take care of and which actions are the responsibility of you, the application developer. Table 1. Spring Data for Apache Cassandra - who does what? Action Spring You Define connection parameters. X Open the connection. X Specify the CQL statement. X Declare parameters and provide parameter values X Prepare and run the statement. X Set up the loop to iterate through the results (if any). X Do the work for each iteration. X Process any exception. X Close the Session. X The core CQL support takes care of all the low-level details that can make Cassandra and CQL such a tedious API with which to develop. Using mapped entity objects allows schema generation, object mapping, and repository support. Choosing an Approach for Cassandra Database Access: You can choose among several approaches to use as a basis for your Cassandra database access. Spring’s support for Apache Cassandra comes in different flavors. Once you start using one of these approaches, you can still mix and match to include a feature from a different approach. The following approaches work well: CqlTemplate(cassandra/cql-template.html) and ReactiveCqlTemplate(cassandra/reactive-cassandra.html) are the classic Spring CQL approach and the most popular. This is the “lowest-level” approach. Note that components like CassandraTemplate use CqlTemplate under-the-hood. CassandraTemplate(cassandra/template.html) wraps a CqlTemplate to provide query result-to-object mapping and the use of SELECT , INSERT , UPDATE , and DELETE methods instead of writing CQL statements. This approach provides better documentation and ease of use. ReactiveCassandraTemplate(cassandra/reactive-cassandra.html) wraps a ReactiveCqlTemplate to provide query result-to-object mapping and the use of SELECT , INSERT , UPDATE , and DELETE methods instead of writing CQL statements. This approach provides better documentation and ease of use. Repository Abstraction lets you create repository declarations in your data access layer. The goal of Spring Data’s repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. For most data-oriented tasks, you can use the [Reactive|Async]CassandraTemplate or the Repository support, both of which use the rich object-mapping functionality. [Reactive|Async]CqlTemplate is commonly used to increment counters or perform ad-hoc CRUD operations. [Reactive|Async]CqlTemplate also provides callback methods that make it easy to get low-level API objects, such as com.datastax.oss.driver.api.core.CqlSession , which lets you communicate directly with Cassandra. Spring Data for Apache Cassandra uses consistent naming conventions on objects in various APIs to those found in the DataStax Java Driver so that they are familiar and so that you can map your existing knowledge onto the Spring APIs. Section Summary: Getting Started(cassandra/getting-started.html) Connecting to Cassandra with Spring(cassandra/configuration.html) Schema Management(cassandra/schema-management.html) CQL Template API(cassandra/cql-template.html) Reactive Infrastructure(cassandra/reactive-cassandra.html) Persisting Entities(cassandra/template.html) Prepared Statements(cassandra/prepared-statements.html) Mapping(object-mapping.html) Type-based Converter(cassandra/converters.html) Property-based Converters(cassandra/property-converters.html) Lifecycle Events(cassandra/events.html) Auditing Configuration for Cassandra(cassandra/auditing.html) Value Expressions Fundamentals(cassandra/value-expressions.html)"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/getting-started.html","Getting Started: Spring Data for Apache Cassandra requires Apache Cassandra 2.1 or later and Datastax Java Driver 4.0 or later. An easy way to quickly set up and bootstrap a working environment is to create a Spring-based project in Spring Tools(https://spring.io/tools) or use start.spring.io(https://start.spring.io/#!type=maven-project&dependencies=data-cassandra) . Examples Repository: To get a feel for how the library works, you can download and play around with several examples(https://github.com/spring-projects/spring-data-examples) . Hello World: First, you need to set up a running Apache Cassandra server. See the Apache Cassandra Quick Start Guide(https://cassandra.apache.org/doc/latest/getting_started/index.html) for an explanation on how to start Apache Cassandra. Once installed, starting Cassandra is typically a matter of executing the following command: CASSANDRA_HOME/bin/cassandra -f . To create a Spring project in STS, go to File → New → Spring Template Project → Simple Spring Utility Project and press Yes when prompted. Then enter a project and a package name, such as org.spring.data.cassandra.example . Then you can add the following dependency declaration to your pom.xml file’s dependencies section. <dependencies> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-cassandra</artifactId> <version>4.3.4</version> </dependency> </dependencies> Also, you should change the version of Spring in the pom.xml file to be as follows: <spring.version>6.1.13</spring.version> If using a milestone release instead of a GA release, you also need to add the location of the Spring Milestone repository for Maven to your pom.xml file so that it is at the same level of your <dependencies/> element, as follows: <repositories> <repository> <id>spring-milestone</id> <name>Spring Maven MILESTONE Repository</name> <url>https://repo.spring.io/milestone</url> </repository> </repositories> The repository is also browseable here(https://repo.spring.io/milestone/org/springframework/data/) . You can also browse all Spring repositories here(https://repo.spring.io/webapp/#/home) . Now you can create a simple Java application that stores and reads a domain object to and from Cassandra. To do so, first create a simple domain object class to persist, as the following example shows: package org.springframework.data.cassandra.example; import org.springframework.data.cassandra.core.mapping.PrimaryKey; import org.springframework.data.cassandra.core.mapping.Table; @Table public class Person { @PrimaryKey private final String id; private final String name; private final int age; public Person(String id, String name, int age) { this.id = id; this.name = name; this.age = age; } public String getId() { return id; } private String getName() { return name; } private int getAge() { return age; } @Override public String toString() { return String.format(""{ @type = %1$s, id = %2$s, name = %3$s, age = %4$d }"", getClass().getName(), getId(), getName(), getAge()); } } Next, create the main application to run, as the following example shows: Imperative Reactive package org.springframework.data.cassandra.example; import java.util.UUID; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import org.springframework.data.cassandra.core.CassandraOperations; import org.springframework.data.cassandra.core.CassandraTemplate; import org.springframework.data.cassandra.core.query.Criteria; import org.springframework.data.cassandra.core.query.Query; import com.datastax.oss.driver.api.core.CqlSession; public class CassandraApplication { private static final Log LOG = LogFactory.getLog(CassandraApplication.class); private static Person newPerson(String name, int age) { return new Person(UUID.randomUUID().toString(), name, age); } public static void main(String[] args) { CqlSession cqlSession = CqlSession.builder().withKeyspace(""mykeyspace"").build(); CassandraOperations template = new CassandraTemplate(cqlSession); Person jonDoe = template.insert(newPerson(""Jon Doe"", 40)); LOG.info(template.selectOne(Query.query(Criteria.where(""id"").is(jonDoe.getId())), Person.class).getId()); template.truncate(Person.class); cqlSession.close(); } } package org.springframework.data.cassandra.example; import reactor.core.publisher.Mono; import java.util.UUID; import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; import org.springframework.data.cassandra.core.ReactiveCassandraOperations; import org.springframework.data.cassandra.core.ReactiveCassandraTemplate; import org.springframework.data.cassandra.core.cql.session.DefaultBridgedReactiveSession; import org.springframework.data.cassandra.core.query.Criteria; import org.springframework.data.cassandra.core.query.Query; import com.datastax.oss.driver.api.core.CqlSession; public class ReactiveCassandraApplication { private static final Log LOG = LogFactory.getLog(ReactiveCassandraApplication.class); private static Person newPerson(String name, int age) { return new Person(UUID.randomUUID().toString(), name, age); } public static void main(String[] args) { CqlSession cqlSession = CqlSession.builder().withKeyspace(""mykeyspace"").build(); ReactiveCassandraOperations template = new ReactiveCassandraTemplate(new DefaultBridgedReactiveSession(cqlSession)); Mono<Person> jonDoe = template.insert(newPerson(""Jon Doe"", 40)); jonDoe.flatMap(it -> template.selectOne(Query.query(Criteria.where(""id"").is(it.getId())), Person.class)) .doOnNext(it -> LOG.info(it.toString())) .then(template.truncate(Person.class)) .block(); cqlSession.close(); } } Even in this simple example, there are a few notable things to point out: You can create an instance of CassandraTemplate(../api/java/org/springframework/data/cassandra/core/CassandraTemplate.html) (or ReactiveCassandraTemplate(../api/java/org/springframework/data/cassandra/core/ReactiveCassandraTemplate.html) for reactive usage) with a Cassandra CqlSession . You must annotate your POJO as a Cassandra @Table entity and also annotate the @PrimaryKey . Optionally, you can override these mapping names to match your Cassandra database table and column names. You can either use raw CQL or the Driver QueryBuilder API to construct your queries."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/configuration.html","Connecting to Cassandra with Spring: One of the first tasks when using Apache Cassandra with Spring is to create a com.datastax.oss.driver.api.core.CqlSession object by using the Spring IoC container. You can do so either by using Java-based bean metadata or by using XML-based bean metadata. These are discussed in the following sections. For those not familiar with how to configure the Spring container using Java-based bean metadata instead of XML-based metadata, see the high-level introduction in the reference docs here(https://docs.spring.io/spring/docs/3.2.x/spring-framework-reference/html/new-in-3.0.html#new-java-configuration) as well as the detailed documentation here(https://docs.spring.io/spring-framework/reference/6.1core.html#beans-java-instantiating-container) . Registering a Session Instance by using Java-based Metadata: The following example shows how to use Java-based bean metadata to register an instance of a com.datastax.oss.driver.api.core.CqlSession : Example 1. Registering a com.datastax.oss.driver.api.core.CqlSession object by using Java-based bean metadata @Configuration public class AppConfig { /* * Use the standard Cassandra driver API to create a com.datastax.oss.driver.api.core.CqlSession instance. */ public @Bean CqlSession session() { return CqlSession.builder().withKeyspace(""mykeyspace"").build(); } } This approach lets you use the standard com.datastax.oss.driver.api.core.CqlSession API that you may already know. An alternative is to register an instance of com.datastax.oss.driver.api.core.CqlSession with the container by using Spring’s CqlSessionFactoryBean . As compared to instantiating a com.datastax.oss.driver.api.core.CqlSession instance directly, the FactoryBean approach has the added advantage of also providing the container with an ExceptionTranslator implementation that translates Cassandra exceptions to exceptions in Spring’s portable DataAccessException hierarchy. This hierarchy and the use of @Repository is described in Spring’s DAO support features(https://docs.spring.io/spring-framework/reference/6.1data-access.html) . The following example shows Java-based factory class usage: Example 2. Registering a com.datastax.oss.driver.api.core.CqlSession object by using Spring’s CqlSessionFactoryBean : @Configuration public class FactoryBeanAppConfig { /* * Factory bean that creates the com.datastax.oss.driver.api.core.CqlSession instance */ @Bean public CqlSessionFactoryBean session() { CqlSessionFactoryBean session = new CqlSessionFactoryBean(); session.setContactPoints(""localhost""); session.setKeyspaceName(""mykeyspace""); return session; } } Using CassandraTemplate with object mapping and repository support requires a CassandraTemplate , CassandraMappingContext , CassandraConverter , and enabling repository support. The following example shows how to register components to configure object mapping and repository support: Example 3. Registering components to configure object mapping and repository support @Configuration @EnableCassandraRepositories(basePackages = { ""org.springframework.data.cassandra.example"" }) public class CassandraConfig { @Bean public CqlSessionFactoryBean session() { CqlSessionFactoryBean session = new CqlSessionFactoryBean(); session.setContactPoints(""localhost""); session.setKeyspaceName(""mykeyspace""); return session; } @Bean public SessionFactoryFactoryBean sessionFactory(CqlSession session, CassandraConverter converter) { SessionFactoryFactoryBean sessionFactory = new SessionFactoryFactoryBean(); sessionFactory.setSession(session); sessionFactory.setConverter(converter); sessionFactory.setSchemaAction(SchemaAction.NONE); return sessionFactory; } @Bean public CassandraMappingContext mappingContext() { return new CassandraMappingContext(); } @Bean public CassandraConverter converter(CqlSession cqlSession, CassandraMappingContext mappingContext) { MappingCassandraConverter cassandraConverter = new MappingCassandraConverter(mappingContext); cassandraConverter.setUserTypeResolver(new SimpleUserTypeResolver(cqlSession)); return cassandraConverter; } @Bean public CassandraOperations cassandraTemplate(SessionFactory sessionFactory, CassandraConverter converter) { return new CassandraTemplate(sessionFactory, converter); } } Creating configuration classes that register Spring Data for Apache Cassandra components can be an exhausting challenge, so Spring Data for Apache Cassandra comes with a pre-built configuration support class. Classes that extend from AbstractCassandraConfiguration register beans for Spring Data for Apache Cassandra use. AbstractCassandraConfiguration lets you provide various configuration options, such as initial entities, default query options, pooling options, socket options, and many more. AbstractCassandraConfiguration also supports you with schema generation based on initial entities, if any are provided. Extending from AbstractCassandraConfiguration requires you to at least provide the keyspace name by implementing the getKeyspaceName method. The following example shows how to register beans by using AbstractCassandraConfiguration : Example 4. Registering Spring Data for Apache Cassandra beans by using AbstractCassandraConfiguration @Configuration public class CassandraConfiguration extends AbstractCassandraConfiguration { /* * Provide a contact point to the configuration. */ @Override public String getContactPoints() { return ""localhost""; } /* * Provide a keyspace name to the configuration. */ @Override public String getKeyspaceName() { return ""mykeyspace""; } } Abstract…Configuration classes wire all the necessary beans for using Cassandra from your application. The configuration assumes a single CqlSession and wires it through SessionFactory into the related components such as CqlTemplate . If you want to customize the creation of the CqlSession , then you can provide a SessionBuilderConfigurer function to customize CqlSessionBuilder . This is useful to provide e.g. a Cloud Connection Bundle for Astra. Example 5. Connecting to Astra through AbstractCassandraConfiguration @Configuration public class CustomizedCassandraConfiguration extends AbstractCassandraConfiguration { /* * Customize the CqlSession through CqlSessionBuilder. */ @Override protected SessionBuilderConfigurer getSessionBuilderConfigurer() { Path connectBundlePath = …; return builder -> builder .withCloudSecureConnectBundle(Path.of(connectBundlePath)); } /* * Provide a keyspace name to the configuration. */ @Override public String getKeyspaceName() { return ""mykeyspace""; } } XML Configuration: This section describes how to configure Spring Data Cassandra with XML. While we still support Namespace Configuration, we generally recommend using Java-based Configuration(#cassandra.cassandra-java-config) . Externalizing Connection Properties: To externalize connection properties, you should first create a properties file that contains the information needed to connect to Cassandra. contactpoints and keyspace are the required fields. The following example shows our properties file, called cassandra.properties : cassandra.contactpoints=10.1.55.80:9042,10.1.55.81:9042 cassandra.keyspace=showcase In the next two examples, we use Spring to load these properties into the Spring context. Registering a Session Instance by using XML-based Metadata: While you can use Spring’s traditional <beans/> XML namespace to register an instance of com.datastax.oss.driver.api.core.CqlSession with the container, the XML can be quite verbose, because it is general purpose. XML namespaces are a better alternative to configuring commonly used objects, such as the CqlSession instance. The cassandra namespace let you create a CqlSession instance. The following example shows how to configure the cassandra namespace: Example 6. XML schema to configure Cassandra by using the cassandra namespace <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:cassandra=""http://www.springframework.org/schema/data/cassandra"" xsi:schemaLocation="" http://www.springframework.org/schema/data/cassandra https://www.springframework.org/schema/data/cassandra/spring-cassandra.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <!-- Default bean name is 'cassandraSession' --> <cassandra:session contact-points=""localhost"" port=""9042""> <cassandra:keyspace action=""CREATE_DROP"" name=""mykeyspace"" /> </cassandra:session> <cassandra:session-factory> <cassandra:script location=""classpath:/org/springframework/data/cassandra/config/schema.cql""/> </cassandra:session-factory> </beans> The XML configuration elements for more advanced Cassandra configuration are shown below. These elements all use default bean names to keep the configuration code clean and readable. While the preceding example shows how easy it is to configure Spring to connect to Cassandra, there are many other options. Basically, any option available with the DataStax Java Driver is also available in the Spring Data for Apache Cassandra configuration. This includes but is not limited to authentication, load-balancing policies, retry policies, and pooling options. All of the Spring Data for Apache Cassandra method names and XML elements are named exactly (or as close as possible) like the configuration options on the driver so that mapping any existing driver configuration should be straight forward. The following example shows how to configure Spring Data components by using XML Example 7. Configuring Spring Data components by using XML <!-- Loads the properties into the Spring Context and uses them to fill in placeholders in the bean definitions --> <context:property-placeholder location=""classpath:cassandra.properties"" /> <!-- REQUIRED: The Cassandra Session --> <cassandra:session contact-points=""${cassandra.contactpoints}"" keyspace-name=""${cassandra.keyspace}"" /> <!-- REQUIRED: The default Cassandra mapping context used by `CassandraConverter` --> <cassandra:mapping> <cassandra:user-type-resolver keyspace-name=""${cassandra.keyspace}"" /> </cassandra:mapping> <!-- REQUIRED: The default Cassandra converter used by `CassandraTemplate` --> <cassandra:converter /> <!-- REQUIRED: The Cassandra template is the foundation of all Spring Data Cassandra --> <cassandra:template id=""cassandraTemplate"" /> <!-- OPTIONAL: If you use Spring Data for Apache Cassandra repositories, add your base packages to scan here --> <cassandra:repositories base-package=""org.spring.cassandra.example.repo"" />"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/schema-management.html","Schema Management: Apache Cassandra is a data store that requires a schema definition prior to any data interaction. Spring Data for Apache Cassandra can support you with schema creation. Keyspaces and Lifecycle Scripts: The first thing to start with is a Cassandra keyspace. A keyspace is a logical grouping of tables that share the same replication factor and replication strategy. Keyspace management is located in the CqlSession configuration, which has the KeyspaceSpecification and startup and shutdown CQL script execution. Declaring a keyspace with a specification allows creating and dropping of the Keyspace. It derives CQL from the specification so that you need not write CQL yourself. The following example specifies a Cassandra keyspace by using XML: Example 1. Specifying a Cassandra keyspace Java @Configuration public class CreateKeyspaceConfiguration extends AbstractCassandraConfiguration implements BeanClassLoaderAware { @Override protected List<CreateKeyspaceSpecification> getKeyspaceCreations() { CreateKeyspaceSpecification specification = CreateKeyspaceSpecification.createKeyspace(""my_keyspace"") .with(KeyspaceOption.DURABLE_WRITES, true) .withNetworkReplication(DataCenterReplication.of(""foo"", 1), DataCenterReplication.of(""bar"", 2)); return Arrays.asList(specification); } @Override protected List<DropKeyspaceSpecification> getKeyspaceDrops() { return Arrays.asList(DropKeyspaceSpecification.dropKeyspace(""my_keyspace"")); } // ... } XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:cassandra=""http://www.springframework.org/schema/data/cassandra"" xsi:schemaLocation="" http://www.springframework.org/schema/data/cassandra https://www.springframework.org/schema/data/cassandra/spring-cassandra.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <cassandra:session> <cassandra:keyspace action=""CREATE_DROP"" durable-writes=""true"" name=""my_keyspace""> <cassandra:replication class=""NETWORK_TOPOLOGY_STRATEGY""> <cassandra:data-center name=""foo"" replication-factor=""1"" /> <cassandra:data-center name=""bar"" replication-factor=""2"" /> </cassandra:replication> </cassandra:keyspace> </cassandra:session> </beans> Keyspace creation allows rapid bootstrapping without the need of external keyspace management. This can be useful for certain scenarios but should be used with care. Dropping a keyspace on application shutdown removes the keyspace and all data from the tables in the keyspace. Initializing a SessionFactory: The org.springframework.data.cassandra.core.cql.session.init package provides support for initializing an existing SessionFactory . You may sometimes need to initialize a keyspace that runs on a server somewhere. Initializing a Keyspace: You can provide arbitrary CQL that is executed on CqlSession initialization and shutdown in the configured keyspace, as the following Java configuration example shows: Java @Configuration public class KeyspacePopulatorConfiguration extends AbstractCassandraConfiguration { @Nullable @Override protected KeyspacePopulator keyspacePopulator() { return new ResourceKeyspacePopulator(new ClassPathResource(""com/foo/cql/db-schema.cql""), new ClassPathResource(""com/foo/cql/db-test-data.cql"")); } @Nullable @Override protected KeyspacePopulator keyspaceCleaner() { return new ResourceKeyspacePopulator(scriptOf(""DROP TABLE my_table;"")); } // ... } XML <cassandra:initialize-keyspace session-factory-ref=""cassandraSessionFactory""> <cassandra:script location=""classpath:com/foo/cql/db-schema.cql""/> <cassandra:script location=""classpath:com/foo/cql/db-test-data.cql""/> </cassandra:initialize-keyspace> The preceding example runs the two specified scripts against the keyspace. The first script creates a schema, and the second populates tables with a test data set. The script locations can also be patterns with wildcards in the usual Ant style used for resources in Spring (for example, classpath*:/com/foo/**/cql/*-data.cql ). If you use a pattern, the scripts are run in the lexical order of their URL or filename. The default behavior of the keyspace initializer is to unconditionally run the provided scripts. This may not always be what you want — for instance, if you run the scripts against a keyspace that already has test data in it. The likelihood of accidentally deleting data is reduced by following the common pattern (shown earlier) of creating the tables first and then inserting the data. The first step fails if the tables already exist. However, to gain more control over the creation and deletion of existing data, the XML namespace provides a few additional options. The first is a flag to switch the initialization on and off. You can set this according to the environment (such as pulling a boolean value from system properties or from an environment bean). The following example gets a value from a system property: <cassandra:initialize-keyspace session-factory-ref=""cassandraSessionFactory"" enabled=""#{systemProperties.INITIALIZE_KEYSPACE}""> (1) <cassandra:script location=""...""/> </cassandra:initialize-database> 1 Get the value for enabled from a system property called INITIALIZE_KEYSPACE . The second option to control what happens with existing data is to be more tolerant of failures. To this end, you can control the ability of the initializer to ignore certain errors in the CQL it executes from the scripts, as the following example shows: Java @Configuration public class KeyspacePopulatorFailureConfiguration extends AbstractCassandraConfiguration { @Nullable @Override protected KeyspacePopulator keyspacePopulator() { ResourceKeyspacePopulator populator = new ResourceKeyspacePopulator( new ClassPathResource(""com/foo/cql/db-schema.cql"")); populator.setIgnoreFailedDrops(true); return populator; } // ... } XML <cassandra:initialize-keyspace session-factory-ref=""cassandraSessionFactory"" ignore-failures=""DROPS""> <cassandra:script location=""...""/> </cassandra:initialize-database> In the preceding example, we are saying that we expect that, sometimes, the scripts are run against an empty keyspace, and there are some DROP statements in the scripts that would, therefore, fail. So failed CQL DROP statements will be ignored, but other failures will cause an exception. This is useful if you don’t want tu use support DROP …​ IF EXISTS (or similar) but you want to unconditionally remove all test data before re-creating it. In that case the first script is usually a set of DROP statements, followed by a set of CREATE statements. The ignore-failures option can be set to NONE (the default), DROPS (ignore failed drops), or ALL (ignore all failures). Each statement should be separated by ; or a new line if the ; character is not present at all in the script. You can control that globally or script by script, as the following example shows: Java @Configuration public class SessionFactoryInitializerConfiguration extends AbstractCassandraConfiguration { @Bean SessionFactoryInitializer sessionFactoryInitializer(SessionFactory sessionFactory) { SessionFactoryInitializer initializer = new SessionFactoryInitializer(); initializer.setSessionFactory(sessionFactory); ResourceKeyspacePopulator populator1 = new ResourceKeyspacePopulator(); populator1.setSeparator("";""); populator1.setScripts(new ClassPathResource(""com/myapp/cql/db-schema.cql"")); ResourceKeyspacePopulator populator2 = new ResourceKeyspacePopulator(); populator2.setSeparator(""@@""); populator2.setScripts(new ClassPathResource(""classpath:com/myapp/cql/db-test-data-1.cql""), // new ClassPathResource(""classpath:com/myapp/cql/db-test-data-2.cql"")); initializer.setKeyspacePopulator(new CompositeKeyspacePopulator(populator1, populator2)); return initializer; } // ... } XML <cassandra:initialize-keyspace session-factory-ref=""cassandraSessionFactory"" separator=""@@""> <cassandra:script location=""classpath:com/myapp/cql/db-schema.cql"" separator="";""/> <cassandra:script location=""classpath:com/myapp/cql/db-test-data-1.cql""/> <cassandra:script location=""classpath:com/myapp/cql/db-test-data-2.cql""/> </cassandra:initialize-keyspace> In this example, the two test-data scripts use @@ as statement separator and only the db-schema.cql uses ; . This configuration specifies that the default separator is @@ and overrides that default for the db-schema script. If you need more control than you get from the XML namespace, you can use the SessionFactoryInitializer directly and define it as a component in your application. Initialization of Other Components that Depend on the Keyspace: A large class of applications (those that do not use the database until after the Spring context has started) can use the database initializer with no further complications. If your application is not one of those, you might need to read the rest of this section. The database initializer depends on a SessionFactory instance and runs the scripts provided in its initialization callback (analogous to an init-method in an XML bean definition, a @PostConstruct method in a component, or the afterPropertiesSet() method in a component that implements InitializingBean ). If other beans depend on the same data source and use the session factory in an initialization callback, there might be a problem because the data has not yet been initialized. A common example of this is a cache that initializes eagerly and loads data from the database on application startup. To get around this issue, you have two options: change your cache initialization strategy to a later phase or ensure that the keyspace initializer is initialized first. Changing your cache initialization strategy might be easy if the application is in your control and not otherwise. Some suggestions for how to implement this include: Make the cache initialize lazily on first usage, which improves application startup time. Have your cache or a separate component that initializes the cache implement Lifecycle or SmartLifecycle . When the application context starts, you can automatically start a SmartLifecycle by setting its autoStartup flag, and you can manually start a Lifecycle by calling ConfigurableApplicationContext.start() on the enclosing context. Use a Spring ApplicationEvent or similar custom observer mechanism to trigger the cache initialization. ContextRefreshedEvent is always published by the context when it is ready for use (after all beans have been initialized), so that is often a useful hook (this is how the SmartLifecycle works by default). Ensuring that the keyspace initializer is initialized first can also be easy. Some suggestions on how to implement this include: Rely on the default behavior of the Spring BeanFactory , which is that beans are initialized in registration order. You can easily arrange that by adopting the common practice of a set of <import/> elements in XML configuration that order your application modules and ensuring that the database and database initialization are listed first. Separate the SessionFactory and the business components that use it and control their startup order by putting them in separate ApplicationContext instances (for example, the parent context contains the SessionFactory , and the child context contains the business components). This structure is common in Spring web applications but can be more generally applied. Use the Schema management for Tables and User-defined Types(#cassandra.schema-management.tables) to initialize the keyspace using Spring Data Cassandra’s built-in schema generator. Tables and User-defined Types: Spring Data for Apache Cassandra approaches data access with mapped entity classes that fit your data model. You can use these entity classes to create Cassandra table specifications and user type definitions. Schema creation is tied to CqlSession initialization by SchemaAction . The following actions are supported: SchemaAction.NONE : No tables or types are created or dropped. This is the default setting. SchemaAction.CREATE : Create tables, indexes, and user-defined types from entities annotated with @Table and types annotated with @UserDefinedType . Existing tables or types cause an error if you tried to create the type. SchemaAction.CREATE_IF_NOT_EXISTS : Like SchemaAction.CREATE but with IF NOT EXISTS applied. Existing tables or types do not cause any errors but may remain stale. SchemaAction.RECREATE : Drops and recreates existing tables and types that are known to be used. Tables and types that are not configured in the application are not dropped. SchemaAction.RECREATE_DROP_UNUSED : Drops all tables and types and recreates only known tables and types. SchemaAction.RECREATE and SchemaAction.RECREATE_DROP_UNUSED drop your tables and lose all data. RECREATE_DROP_UNUSED also drops tables and types that are not known to the application. Enabling Tables and User-Defined Types for Schema Management: Metadata-based Mapping(../object-mapping.html#mapping.usage) explains object mapping with conventions and annotations. To prevent unwanted classes from being created as a table or a type, schema management is only active for entities annotated with @Table and user-defined types annotated with @UserDefinedType . Entities are discovered by scanning the classpath. Entity scanning requires one or more base packages. Tuple-typed columns that use TupleValue do not provide any typing details. Consequently, you must annotate such column properties with @CassandraType(type = TUPLE, typeArguments = …) to specify the desired column type. The following example shows how to specify entity base packages in XML configuration: Example 2. Specifying entity base packages Java @Configuration public class EntityBasePackagesConfiguration extends AbstractCassandraConfiguration { @Override public String[] getEntityBasePackages() { return new String[] { ""com.foo"", ""com.bar"" }; } // ... } XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:cassandra=""http://www.springframework.org/schema/data/cassandra"" xsi:schemaLocation="" http://www.springframework.org/schema/data/cassandra https://www.springframework.org/schema/data/cassandra/spring-cassandra.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <cassandra:mapping entity-base-packages=""com.foo,com.bar""/> </beans>"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/cql-template.html","CQL Template API: The CqlTemplate(../api/java/org/springframework/data/cassandra/core/cql/CqlTemplate.html) class (and its reactive variant ReactiveCqlTemplate(../api/java/org/springframework/data/cassandra/core/cql/ReactiveCqlTemplate.html) ) is the central class in the core CQL package. It handles the creation and release of resources. It performs the basic tasks of the core CQL workflow, such as statement creation and execution, and leaves application code to provide CQL and extract results. The CqlTemplate class executes CQL queries and update statements, performs iteration over ResultSet instances and extraction of returned parameter values. It also catches CQL exceptions and translates them to the generic, more informative, exception hierarchy defined in the org.springframework.dao package. When you use the CqlTemplate for your code, you need only implement callback interfaces, which have a clearly defined contract. Given a CqlSession , the PreparedStatementCreator(../api/java/org/springframework/data/cassandra/core/cql/PreparedStatementCreator.html) callback interface creates a prepared statement(prepared-statements.html#cassandra.template.prepared-statements.cql) with the provided CQL and any necessary parameter arguments. The RowCallbackHandler(../api/java/org/springframework/data/cassandra/core/cql/RowCallbackHandler.html) interface extracts values from each row of a ResultSet . The CqlTemplate(../api/java/org/springframework/data/cassandra/core/cql/CqlTemplate.html) can be used within a DAO implementation through direct instantiation with a SessionFactory(../api/java/org/springframework/data/cassandra/SessionFactory.html) reference or be configured in the Spring container and given to DAOs as a bean reference. CqlTemplate is a foundational building block for CassandraTemplate(template.html) . All CQL issued by this class is logged at the DEBUG level under the category corresponding to the fully-qualified class name of the template instance (typically CqlTemplate , but it may be different if you use a custom subclass of the CqlTemplate class). You can control fetch size, consistency level, and retry policy defaults by configuring these parameters on the CQL API instances: CqlTemplate(../api/java/org/springframework/data/cassandra/core/cql/CqlTemplate.html) , AsyncCqlTemplate(../api/java/org/springframework/data/cassandra/core/cql/AsyncCqlTemplate.html) , and ReactiveCqlTemplate(../api/java/org/springframework/data/cassandra/core/cql/ReactiveCqlTemplate.html) . Defaults apply if the particular query option is not set. CqlTemplate comes in different execution model flavors. The basic CqlTemplate uses a blocking execution model. You can use AsyncCqlTemplate for asynchronous execution and synchronization with ListenableFuture instances or ReactiveCqlTemplate for reactive execution. Examples of CqlTemplate Class Usage: This section provides some examples of the CqlTemplate class in action. These examples are not an exhaustive list of all functionality exposed by the CqlTemplate . See the Javadoc(../api/java/org/springframework/data/cassandra/core/cql/CqlTemplate.html) for that. Querying (SELECT) with CqlTemplate: The following query gets the number of rows in a table: Imperative Reactive int rowCount = cqlTemplate.queryForObject(""SELECT COUNT(*) FROM t_actor"", Integer.class); Mono<Integer> rowCount = reactiveCqlTemplate.queryForObject(""SELECT COUNT(*) FROM t_actor"", Integer.class); The following query uses a bind variable: Imperative Reactive int countOfActorsNamedJoe = cqlTemplate.queryForObject( ""SELECT COUNT(*) FROM t_actor WHERE first_name = ?"", Integer.class, ""Joe""); Mono<Integer> countOfActorsNamedJoe = reactiveCqlTemplate.queryForObject( ""SELECT COUNT(*) FROM t_actor WHERE first_name = ?"", Integer.class, ""Joe""); The following example queries for a String : Imperative Reactive String lastName = cqlTemplate.queryForObject( ""SELECT last_name FROM t_actor WHERE id = ?"", String.class, 1212L); Mono<String> lastName = reactiveCqlTemplate.queryForObject( ""SELECT last_name FROM t_actor WHERE id = ?"", String.class, 1212L); The following example queries and populates a single domain object: Imperative Reactive Actor actor = cqlTemplate.queryForObject(""SELECT first_name, last_name FROM t_actor WHERE id = ?"", new RowMapper<Actor>() { public Actor mapRow(Row row, int rowNum) { Actor actor = new Actor(); actor.setFirstName(row.getString(""first_name"")); actor.setLastName(row.getString(""last_name"")); return actor; } }, 1212L); Mono<Actor> actor = reactiveCqlTemplate.queryForObject( ""SELECT first_name, last_name FROM t_actor WHERE id = ?"", new RowMapper<Actor>() { public Actor mapRow(Row row, int rowNum) { Actor actor = new Actor(); actor.setFirstName(row.getString(""first_name"")); actor.setLastName(row.getString(""last_name"")); return actor; }}, 1212L); The following example queries and populates multiple domain objects: Imperative Reactive List<Actor> actors = cqlTemplate.query( ""SELECT first_name, last_name FROM t_actor"", new RowMapper<Actor>() { public Actor mapRow(Row row, int rowNum) { Actor actor = new Actor(); actor.setFirstName(row.getString(""first_name"")); actor.setLastName(row.getString(""last_name"")); return actor; } }); Flux<Actor> actors = reactiveCqlTemplate.query( ""SELECT first_name, last_name FROM t_actor"", new RowMapper<Actor>() { public Actor mapRow(Row row, int rowNum) { Actor actor = new Actor(); actor.setFirstName(row.getString(""first_name"")); actor.setLastName(row.getString(""last_name"")); return actor; } }); If the last two snippets of code actually existed in the same application, it would make sense to remove the duplication present in the two RowMapper anonymous inner classes and extract them out into a single class (typically a static nested class) that can then be referenced by DAO methods. For example, it might be better to write the last code snippet as follows: Imperative Reactive List<Actor> findAllActors() { return cqlTemplate.query(""SELECT first_name, last_name FROM t_actor"", ActorMapper.INSTANCE); } enum ActorMapper implements RowMapper<Actor> { INSTANCE; public Actor mapRow(Row row, int rowNum) { Actor actor = new Actor(); actor.setFirstName(row.getString(""first_name"")); actor.setLastName(row.getString(""last_name"")); return actor; } } Flux<Actor> findAllActors() { return reactiveCqlTemplate.query(""SELECT first_name, last_name FROM t_actor"", ActorMapper.INSTANCE); } enum ActorMapper implements RowMapper<Actor> { INSTANCE; public Actor mapRow(Row row, int rowNum) { Actor actor = new Actor(); actor.setFirstName(row.getString(""first_name"")); actor.setLastName(row.getString(""last_name"")); return actor; } } INSERT, UPDATE, and DELETE with CqlTemplate: You can use the execute(…) method to perform INSERT , UPDATE , and DELETE operations. Parameter values are usually provided as variable arguments or, alternatively, as an object array. The following example shows how to perform an INSERT operation with CqlTemplate : Imperative Reactive cqlTemplate.execute( ""INSERT INTO t_actor (first_name, last_name) VALUES (?, ?)"", ""Leonor"", ""Watling""); Mono<Boolean> applied = reactiveCqlTemplate.execute( ""INSERT INTO t_actor (first_name, last_name) VALUES (?, ?)"", ""Leonor"", ""Watling""); The following example shows how to perform an UPDATE operation with CqlTemplate : Imperative Reactive cqlTemplate.execute( ""UPDATE t_actor SET last_name = ? WHERE id = ?"", ""Banjo"", 5276L); Mono<Boolean> applied = reactiveCqlTemplate.execute( ""UPDATE t_actor SET last_name = ? WHERE id = ?"", ""Banjo"", 5276L); The following example shows how to perform an DELETE operation with CqlTemplate : Imperative Reactive cqlTemplate.execute( ""DELETE FROM t_actor WHERE id = ?"", 5276L); Mono<Boolean> applied = reactiveCqlTemplate.execute( ""DELETE FROM actor WHERE id = ?"", actorId); Other CqlTemplate operations: You can use the execute(..) method to execute any arbitrary CQL. As a result, the method is often used for DDL statements. It is heavily overloaded with variants that take callback interfaces, bind variable arrays, and so on. The following example shows how to create and drop a table by using different API objects that are all passed to the execute() methods: cqlTemplate.execute(""CREATE TABLE test_table (id uuid primary key, event text)""); DropTableSpecification dropper = DropTableSpecification.dropTable(""test_table""); String cql = DropTableCqlGenerator.toCql(dropper); cqlTemplate.execute(cql); Controlling Cassandra Connections: Applications connect to Apache Cassandra by using CqlSession objects. A Cassandra CqlSession keeps track of multiple connections to the individual nodes and is designed to be a thread-safe, long-lived object. Usually, you can use a single CqlSession for the whole application. Spring acquires a Cassandra CqlSession through a SessionFactory . SessionFactory is part of Spring Data for Apache Cassandra and is a generalized connection factory. It lets the container or framework hide connection handling and routing issues from the application code. The following example shows how to configure a default SessionFactory : Imperative Reactive CqlSession session = … // get a Cassandra Session CqlTemplate template = new CqlTemplate(); template.setSessionFactory(new DefaultSessionFactory(session)); CqlSession session = … // get a Cassandra Session ReactiveCqlTemplate template = new ReactiveCqlTemplate(new DefaultBridgedReactiveSession(session)); CqlTemplate and other Template API implementations obtain a CqlSession for each operation. Due to their long-lived nature, sessions are not closed after invoking the desired operation. Responsibility for proper resource disposal lies with the container or framework that uses the session. You can find various SessionFactory implementations within the org.springframework.data.cassandra.core.cql.session package. Exception Translation: The Spring Framework provides exception translation for a wide variety of database and mapping technologies. This has traditionally been for JDBC and JPA. Spring Data for Apache Cassandra extends this feature to Apache Cassandra by providing an implementation of the org.springframework.dao.support.PersistenceExceptionTranslator interface. The motivation behind mapping to Spring’s consistent data access exception hierarchy(https://docs.spring.io/spring-framework/reference/6.1html/dao.html#dao-exceptions) is to let you write portable and descriptive exception handling code without resorting to coding against and handling specific Cassandra exceptions. All of Spring’s data access exceptions are inherited from the DataAccessException class, so you can be sure that you can catch all database-related exceptions within a single try-catch block. ReactiveCqlTemplate and ReactiveCassandraTemplate propagate exceptions as early as possible. Exceptions that occur during the processing of the reactive sequence are emitted as error signals."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/reactive-cassandra.html","Reactive Infrastructure: The reactive Cassandra support contains a wide range of features: Spring configuration support using Java-based @Configuration classes. ReactiveCqlTemplate helper class that increases productivity by properly handling common Cassandra data access operations. ReactiveCassandraTemplate helper class that increases productivity by using ReactiveCassandraOperations in a reactive manner. It includes integrated object mapping between tables and POJOs. Exception translation into Spring’s portable Data Access Exception Hierarchy(https://docs.spring.io/spring-framework/reference/6.1data-access.html#dao-exceptions) . Feature rich object mapping integrated with Spring’s Conversion Service(https://docs.spring.io/spring-framework/reference/6.1core.html#core-convert) . Java-based Query, Criteria, and Update DSLs. Automatic implementation of Repository interfaces, including support for custom finder methods. For most data-oriented tasks, you can use the ReactiveCassandraTemplate or the repository support, which use the rich object mapping functionality. ReactiveCqlTemplate is commonly used to increment counters or perform ad-hoc CRUD operations. ReactiveCqlTemplate also provides callback methods that make it easy to get low-level API objects, such as com.datastax.oss.driver.api.core.CqlSession , which let you communicate directly with Cassandra. Spring Data for Apache Cassandra uses consistent naming conventions on objects in various APIs to those found in the DataStax Java Driver so that they are immediately familiar and so that you can map your existing knowledge onto the Spring APIs. Reactive usage is broken up into two phases: Composition and Execution. Calling repository methods lets you compose a reactive sequence by obtaining Publisher instances and applying operators. No I/O happens until you subscribe. Passing the reactive sequence to a reactive execution infrastructure, such as Spring WebFlux(https://docs.spring.io/spring-framework/reference/6.1web.html#web-reactive) or Vert.x(https://vertx.io/docs/vertx-reactive-streams/java/) ), subscribes to the publisher and initiate the actual execution. See the Project reactor documentation(https://projectreactor.io/docs/core/release/reference/#reactive.subscribe) for more detail. Reactive Composition Libraries: The reactive space offers various reactive composition libraries. The most common libraries are RxJava(https://github.com/ReactiveX/RxJava) and Project Reactor(https://projectreactor.io/) . Spring Data for Apache Cassandra is built on top of the DataStax Cassandra Driver(https://github.com/datastax/java-driver) . The driver is not reactive but the asynchronous capabilities allow us to adopt and expose the Publisher APIs to provide maximum interoperability by relying on the Reactive Streams(https://www.reactive-streams.org/) initiative. Static APIs, such as ReactiveCassandraOperations , are provided by using Project Reactor’s Flux and Mono types. Project Reactor offers various adapters to convert reactive wrapper types ( Flux to Observable and back), but conversion can easily clutter your code."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/template.html","Persisting Entities: The CassandraTemplate class (and its reactive variant ReactiveCassandraTemplate ), located in the org.springframework.data.cassandra package, is the central class in Spring’s Cassandra support and provides a rich feature set to interact with the database. The template offers convenience operations to create, update, delete, and query Cassandra, and provides a mapping between your domain objects and rows in Cassandra tables. Once configured, a template instance is thread-safe and can be reused across multiple instances. The mapping between rows in Cassandra and application domain classes is done by delegating to an implementation of the CassandraConverter interface. Spring provides a default implementation, MappingCassandraConverter , but you can also write your own custom converter. See the section on Cassandra conversion(../object-mapping.html) for more detailed information. The CassandraTemplate class implements the CassandraOperations interface and its reactive variant ReactiveCassandraTemplate implements ReactiveCassandraOperations . In as much as possible, the methods on [Reactive]CassandraOperations are named after methods available in Cassandra to make the API familiar to developers who are already familiar with Cassandra. For example, you can find methods such as select , insert , delete , and update . The design goal was to make it as easy as possible to transition between the use of the base Cassandra driver and [Reactive]CassandraOperations . A major difference between the two APIs is that CassandraOperations can be passed domain objects instead of CQL and query objects. The preferred way to reference operations on a [Reactive]CassandraTemplate instance is through the [Reactive]CassandraOperations interface. The default converter implementation used by [Reactive]CassandraTemplate is MappingCassandraConverter . While MappingCassandraConverter can use additional metadata to specify the mapping of objects to rows, it can also convert objects that contain no additional metadata by using some conventions for the mapping of fields and table names. These conventions, as well as the use of mapping annotations, are explained in the “Mapping” chapter(../object-mapping.html) . Another central feature of [Reactive]CassandraTemplate is exception translation of exceptions thrown in the Cassandra Java driver into Spring’s portable Data Access Exception hierarchy. See the section on exception translation(cql-template.html#exception-translation) for more information. The Template API has different execution model flavors. The basic CassandraTemplate uses a blocking (imperative-synchronous) execution model. You can use AsyncCassandraTemplate for asynchronous execution and synchronization with ListenableFuture instances or ReactiveCassandraTemplate for reactive execution. Instantiating CassandraTemplate: CassandraTemplate should always be configured as a Spring bean, although we show an example earlier where you can instantiate it directly. However, because we are assuming the context of making a Spring module, we assume the presence of the Spring container. There are two ways to get a CassandraTemplate , depending on how you load you Spring ApplicationContext : Autowiring(#cassandra-template-autowiring) Bean Lookup with ApplicationContext(#cassandra-template-bean-lookup-applicationcontext) Autowiring: You can autowire a [Reactive]CassandraOperations into your project, as the following example shows: Imperative Reactive @Autowired private CassandraOperations cassandraOperations; @Autowired private ReactiveCassandraOperations reactiveCassandraOperations; As with all Spring autowiring, this assumes there is only one bean of type [Reactive]CassandraOperations in the ApplicationContext . If you have multiple [Reactive]CassandraTemplate beans (which is the case if you work with multiple keyspaces in the same project), then you can use the @Qualifier annotation to designate the bean you want to autowire. Imperative Reactive @Autowired @Qualifier(""keyspaceOneTemplateBeanId"") private CassandraOperations cassandraOperations; @Autowired @Qualifier(""keyspaceOneTemplateBeanId"") private ReactiveCassandraOperations reactiveCassandraOperations; Bean Lookup with ApplicationContext: You can also look up the [Reactive]CassandraTemplate bean from the ApplicationContext , as shown in the following example: Imperative Reactive CassandraOperations cassandraOperations = applicationContext.getBean(""cassandraTemplate"", CassandraOperations.class); ReactiveCassandraOperations cassandraOperations = applicationContext.getBean(""ReactiveCassandraOperations"", ReactiveCassandraOperations.class); Querying Rows: You can express your queries by using the Query and Criteria classes, which have method names that reflect the native Cassandra predicate operator names, such as lt , lte , is , and others. The Query and Criteria classes follow a fluent API style so that you can easily chain together multiple method criteria and queries while having easy-to-understand code. Static imports are used in Java when creating Query and Criteria instances to improve readability. Querying Rows in a Table: In earlier sections, we saw how to retrieve a single object by using the selectOneById method on [Reactive]CassandraTemplate . Doing so returns a single domain object. We can also query for a collection of rows to be returned as a list of domain objects. Assuming we have a number of Person objects with name and age values stored as rows in a table and that each person has an account balance, we can now run a query by using the following code: Querying for rows using [Reactive]CassandraTemplate Imperative Reactive import static org.springframework.data.cassandra.core.query.Criteria.where; import static org.springframework.data.cassandra.core.query.Query.query; … List<Person> result = cassandraTemplate.select(query(where(""age"").is(50)) .and(where(""balance"").gt(1000.00d)).withAllowFiltering(), Person.class); import static org.springframework.data.cassandra.core.query.Criteria.where; import static org.springframework.data.cassandra.core.query.Query.query; … Flux<Person> result = reactiveCassandraTemplate.select(query(where(""age"").is(50)) .and(where(""balance"").gt(1000.00d)).withAllowFiltering(), Person.class); The select , selectOne , and stream methods take a Query object as a parameter. This object defines the criteria and options used to perform the query. The criteria is specified by using a Criteria object that has a static factory method named where that instantiates a new Criteria object. We recommend using a static import for org.springframework.data.cassandra.core.query.Criteria.where and Query.query , to make the query more readable. This query should return a list of Person objects that meet the specified criteria. The Criteria class has the following methods that correspond to the operators provided in Apache Cassandra: Methods for the Criteria class: CriteriaDefinition gt (Object value) : Creates a criterion by using the > operator. CriteriaDefinition gte (Object value) : Creates a criterion by using the >= operator. CriteriaDefinition in (Object…​ values) : Creates a criterion by using the IN operator for a varargs argument. CriteriaDefinition in (Collection<?> collection) : Creates a criterion by using the IN operator using a collection. CriteriaDefinition is (Object value) : Creates a criterion by using field matching ( column = value ). CriteriaDefinition lt (Object value) : Creates a criterion by using the < operator. CriteriaDefinition lte (Object value) : Creates a criterion by using the ⇐ operator. CriteriaDefinition like (Object value) : Creates a criterion by using the LIKE operator. CriteriaDefinition contains (Object value) : Creates a criterion by using the CONTAINS operator. CriteriaDefinition containsKey (Object key) : Creates a criterion by using the CONTAINS KEY operator. Criteria is immutable once created. Methods for the Query class: The Query class has some additional methods that you can use to provide options for the query: Query by (CriteriaDefinition…​ criteria) : Used to create a Query object. Query and (CriteriaDefinition criteria) : Used to add additional criteria to the query. Query columns (Columns columns) : Used to define columns to be included in the query results. Query limit (Limit limit) : Used to limit the size of the returned results to the provided limit (used SELECT limiting). Query limit (long limit) : Used to limit the size of the returned results to the provided limit (used SELECT limiting). Query pageRequest (Pageable pageRequest) : Used to associate Sort , PagingState , and fetchSize with the query (used for paging). Query pagingState (ByteBuffer pagingState) : Used to associate a ByteBuffer with the query (used for paging). Query queryOptions (QueryOptions queryOptions) : Used to associate QueryOptions with the query. Query sort (Sort sort) : Used to provide a sort definition for the results. Query withAllowFiltering () : Used to render ALLOW FILTERING queries. Query is immutable once created. Invoking methods creates new immutable (intermediate) Query objects. Methods for Querying for Rows: The Query class has the following methods that return rows: List<T> select (Query query, Class<T> entityClass) : Query for a list of objects of type T from the table. T selectOne (Query query, Class<T> entityClass) : Query for a single object of type T from the table. Slice<T> slice (Query query, Class<T> entityClass) : Starts or continues paging by querying for a Slice of objects of type T from the table. Stream<T> stream (Query query, Class<T> entityClass) : Query for a stream of objects of type T from the table. List<T> select (String cql, Class<T> entityClass) : Ad-hoc query for a list of objects of type T from the table by providing a CQL statement. T selectOne (String cql, Class<T> entityClass) : Ad-hoc query for a single object of type T from the table by providing a CQL statement. Stream<T> stream (String cql, Class<T> entityClass) : Ad-hoc query for a stream of objects of type T from the table by providing a CQL statement. The query methods must specify the target type T that is returned. Fluent Template API: The [Reactive]CassandraOperations interface is one of the central components when it comes to more low-level interaction with Apache Cassandra. It offers a wide range of methods. You can find multiple overloads for every method. Most of them cover optional (nullable) parts of the API. FluentCassandraOperations and its reactive variant ReactiveFluentCassandraOperations provide a more narrow interface for common methods of [Reactive]CassandraOperations providing a more readable, fluent API. The entry points ( query(…) , insert(…) , update(…) , and delete(…) ) follow a natural naming scheme based on the operation to execute. Moving on from the entry point, the API is designed to offer only context-dependent methods that guide the developer towards a terminating method that invokes the actual [Reactive]CassandraOperations . The following example shows the fluent API: Imperative Reactive List<SWCharacter> all = ops.query(SWCharacter.class) .inTable(""star_wars"") (1) .all(); 1 Skip this step if SWCharacter defines the table name with @Table or if using the class name as the table name is not a problem Flux<SWCharacter> all = ops.query(SWCharacter.class) .inTable(""star_wars"") (1) .all(); 1 Skip this step if SWCharacter defines the table name with @Table or if using the class name as the table name is not a problem If a table in Cassandra holds entities of different types, such as a Jedi within a Table of SWCharacters , you can use different types to map the query result. You can use as(Class<?> targetType) to map results to a different target type, while query(Class<?> entityType) still applies to the query and table name. The following example uses the query and as methods: Imperative Reactive List<Jedi> all = ops.query(SWCharacter.class) (1) .as(Jedi.class) (2) .matching(query(where(""jedi"").is(true))) .all(); 1 The query fields are mapped against the SWCharacter type. 2 Resulting rows are mapped into Jedi . Flux<Jedi> all = ops.query(SWCharacter.class) (1) .as(Jedi.class) (2) .matching(query(where(""jedi"").is(true))) .all(); 1 The query fields are mapped against the SWCharacter type. 2 Resulting rows are mapped into Jedi . You can directly apply Projections(../repositories/projections.html) to resulting documents by providing only the interface type through as(Class<?>) . The terminating methods ( first() , one() , all() , and stream() ) handle switching between retrieving a single entity and retrieving multiple entities as List or Stream and similar operations. The new fluent template API methods (that is, query(..) , insert(..) , update(..) , and delete(..) ) use effectively thread-safe supporting objects to compose the CQL statement. However, it comes at the added cost of additional young-gen JVM heap overhead, since the design is based on final fields for the various CQL statement components and construction on mutation. You should be careful when possibly inserting or deleting a large number of objects (such as inside of a loop, for instance). Saving, Updating, and Removing Rows: [Reactive]CassandraTemplate provides a simple way for you to save, update, and delete your domain objects and map those objects to tables managed in Cassandra. Type Mapping: Spring Data for Apache Cassandra relies on the DataStax Java driver’s CodecRegistry to ensure type support. As types are added or changed, the Spring Data for Apache Cassandra module continues to function without requiring changes. See CQL data types(https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cql_data_types_c.html) and “ Data Mapping and Type Conversion(../object-mapping.html#mapping-conversion) ” for the current type mapping matrix. Methods for Inserting and Updating rows: [Reactive]CassandraTemplate has several convenient methods for saving and inserting your objects. To have more fine-grained control over the conversion process, you can register Spring Converter instances with the MappingCassandraConverter (for example, Converter<Row, Person> ). The difference between insert and update operations is that INSERT operations do not insert null values. The simple case of using the INSERT operation is to save a POJO. In this case, the table name is determined by the simple class name (not the fully qualified class name). The table to store the object can be overridden by using mapping metadata. When inserting or updating, the id property must be set. Apache Cassandra has no means to generate an ID. The following example uses the save operation and retrieves its contents: Inserting and retrieving objects by using the [Reactive]CassandraTemplate Imperative Reactive import static org.springframework.data.cassandra.core.query.Criteria.where; import static org.springframework.data.cassandra.core.query.Query.query; … Person bob = new Person(""Bob"", 33); cassandraTemplate.insert(bob); Person queriedBob = cassandraTemplate.selectOneById(query(where(""age"").is(33)), Person.class); import static org.springframework.data.cassandra.core.query.Criteria.where; import static org.springframework.data.cassandra.core.query.Query.query; … Person bob = new Person(""Bob"", 33); cassandraTemplate.insert(bob); Mono<Person> queriedBob = reactiveCassandraTemplate.selectOneById(query(where(""age"").is(33)), Person.class); You can use the following operations to insert and save: void insert (Object objectToSave) : Inserts the object in an Apache Cassandra table. WriteResult insert (Object objectToSave, InsertOptions options) : Inserts the object in an Apache Cassandra table and applies InsertOptions . You can use the following update operations: void update (Object objectToSave) : Updates the object in an Apache Cassandra table. WriteResult update (Object objectToSave, UpdateOptions options) : Updates the object in an Apache Cassandra table and applies UpdateOptions . You can also use the old fashioned way and write your own CQL statements, as the following example shows: Imperative Reactive String cql = ""INSERT INTO person (age, name) VALUES (39, 'Bob')""; cassandraTemplate().getCqlOperations().execute(cql); String cql = ""INSERT INTO person (age, name) VALUES (39, 'Bob')""; Mono<Boolean> applied = reactiveCassandraTemplate.getReactiveCqlOperations().execute(cql); You can also configure additional options such as TTL, consistency level, and lightweight transactions when using InsertOptions and UpdateOptions . Which Table Are My Rows Inserted into?: You can manage the table name that is used for operating on the tables in two ways. The default table name is the simple class name changed to start with a lower-case letter. So, an instance of the com.example.Person class would be stored in the person table. The second way is to specify a table name in the @Table annotation. Inserting, Updating, and Deleting Individual Objects in a Batch: The Cassandra protocol supports inserting a collection of rows in one operation by using a batch. The following methods in the [Reactive]CassandraTemplate interface support this functionality: batchOps : Creates a new [Reactive]CassandraBatchOperations to populate the batch. [Reactive]CassandraBatchOperations insert : Takes a single object, an array (var-args), or an Iterable of objects to insert. update : Takes a single object, an array (var-args), or an Iterable of objects to update. delete : Takes a single object, an array (var-args), or an Iterable of objects to delete. withTimestamp : Applies a TTL to the batch. execute : Executes the batch. Updating Rows in a Table: For updates, you can select to update a number of rows. The following example shows updating a single account object by adding a one-time $50.00 bonus to the balance with the + assignment: Updating rows using [Reactive]CasandraTemplate Imperative Reactive import static org.springframework.data.cassandra.core.query.Criteria.where; import org.springframework.data.cassandra.core.query.Query; import org.springframework.data.cassandra.core.query.Update; … boolean applied = cassandraTemplate.update(Query.query(where(""id"").is(""foo"")), Update.create().increment(""balance"", 50.00), Account.class); import static org.springframework.data.cassandra.core.query.Criteria.where; import org.springframework.data.cassandra.core.query.Query; import org.springframework.data.cassandra.core.query.Update; … Mono<Boolean> wasApplied = reactiveCassandraTemplate.update(Query.query(where(""id"").is(""foo"")), Update.create().increment(""balance"", 50.00), Account.class); In addition to the Query discussed earlier, we provide the update definition by using an Update object. The Update class has methods that match the update assignments available for Apache Cassandra. Most methods return the Update object to provide a fluent API for code styling purposes. Methods for Executing Updates for Rows: The update method can update rows, as follows: boolean update (Query query, Update update, Class<?> entityClass) : Updates a selection of objects in the Apache Cassandra table. Methods for the Update class: The Update class can be used with a little 'syntax sugar', as its methods are meant to be chained together. Also, you can kick-start the creation of a new Update instance with the static method public static Update update(String key, Object value) and by using static imports. The Update class has the following methods: AddToBuilder addTo (String columnName) AddToBuilder entry-point: Update prepend(Object value) : Prepends a collection value to the existing collection by using the + update assignment. Update prependAll(Object…​ values) : Prepends all collection values to the existing collection by using the + update assignment. Update append(Object value) : Appends a collection value to the existing collection by using the + update assignment. Update append(Object…​ values) : Appends all collection values to the existing collection by using the + update assignment. Update entry(Object key, Object value) : Adds a map entry by using the + update assignment. Update addAll(Map<? extends Object, ? extends Object> map) : Adds all map entries to the map by using the + update assignment. Update remove (String columnName, Object value) : Removes the value from the collection by using the - update assignment. Update clear (String columnName) : Clears the collection. Update increment (String columnName, Number delta) : Updates by using the + update assignment. Update decrement (String columnName, Number delta) : Updates by using the - update assignment. Update set (String columnName, Object value) : Updates by using the = update assignment. SetBuilder set (String columnName) SetBuilder entry-point: Update atIndex(int index).to(Object value) : Sets a collection at the given index to a value using the = update assignment. Update atKey(String object).to(Object value) : Sets a map entry at the given key to a value the = update assignment. The following listing shows a few update examples: // UPDATE … SET key = 'Spring Data'; Update.update(""key"", ""Spring Data"") // UPDATE … SET key[5] = 'Spring Data'; Update.empty().set(""key"").atIndex(5).to(""Spring Data""); // UPDATE … SET key = key + ['Spring', 'DATA']; Update.empty().addTo(""key"").appendAll(""Spring"", ""Data""); Note that Update is immutable once created. Invoking methods creates new immutable (intermediate) Update objects. Methods for Removing Rows: You can use the following overloaded methods to remove an object from the database: boolean delete (Query query, Class<?> entityClass) : Deletes the objects selected by Query . T delete (T entity) : Deletes the given object. T delete (T entity, QueryOptions queryOptions) : Deletes the given object applying QueryOptions . boolean deleteById (Object id, Class<?> entityClass) : Deletes the object using the given Id. Optimistic Locking: The @Version annotation provides syntax similar to that of JPA in the context of Cassandra and makes sure updates are only applied to rows with a matching version. Optimistic Locking leverages Cassandra’s lightweight transactions to conditionally insert, update and delete rows. Therefore, INSERT statements are executed with the IF NOT EXISTS condition. For updates and deletes, the actual value of the version property is added to the UPDATE condition in such a way that the modification does not have any effect if another operation altered the row in the meantime. In that case, an OptimisticLockingFailureException is thrown. The following example shows these features: @Table class Person { @Id String id; String firstname; String lastname; @Version Long version; } Person daenerys = template.insert(new Person(""Daenerys"")); (1) Person tmp = template.findOne(query(where(""id"").is(daenerys.getId())), Person.class); (2) daenerys.setLastname(""Targaryen""); template.save(daenerys); (3) template.save(tmp); // throws OptimisticLockingFailureException (4) 1 Intially insert document. version is set to 0 . 2 Load the just inserted document. version is still 0 . 3 Update the document with version = 0 . Set the lastname and bump version to 1 . 4 Try to update the previously loaded document that still has version = 0 . The operation fails with an OptimisticLockingFailureException , as the current version is 1 . Optimistic Locking is only supported with single-entity operations and not for batch operations."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/prepared-statements.html","Prepared Statements: CQL statements that are executed multiple times can be prepared and stored in a PreparedStatement object to improve query performance. Both, the driver and Cassandra maintain a mapping of PreparedStatement queries to their metadata. You can use prepared statements through the following abstractions: CqlTemplate, AsyncCqlTemplate, or ReactiveCqlTemplate(cql-template.html) through the choice of API CassandraTemplate, AsyncCassandraTemplate, or ReactiveCassandraTemplate(template.html) by enabling prepared statements Cassandra repositories(../repositories.html) as they are built on top of the Template API Using CqlTemplate: The CqlTemplate class (and its asynchronous and reactive variants) offers various methods accepting static CQL, Statement objects and PreparedStatementCreator . Methods accepting static CQL without additional arguments typically run the CQL statement as-is without further processing. Methods accepting static CQL in combination with an arguments array (such as execute(String cql, Object…​ args) and queryForRows(String cql, Object…​ args) ) use prepared statements. Internally, these methods create a PreparedStatementCreator and PreparedStatementBinder objects to prepare the statement and later on to bind values to the statement to run it. Spring Data Cassandra generally uses index-based parameter bindings for prepared statements. Since Cassandra Driver version 4, prepared statements are cached on the driver level which removes the need to keep track of prepared statements in the application. The following example shows how to issue a query with a parametrized prepared statement: Imperative Reactive String lastName = cqlTemplate.queryForObject( ""SELECT last_name FROM t_actor WHERE id = ?"", String.class, 1212L); Mono<String> lastName = reactiveCqlTemplate.queryForObject( ""SELECT last_name FROM t_actor WHERE id = ?"", String.class, 1212L); In cases where you require more control over statement preparation and parameter binding (for example, using named binding parameters), you can fully control prepared statement creation and parameter binding by calling query methods with PreparedStatementCreator and PreparedStatementBinder arguments: Imperative Reactive List<String> lastNames = cqlTemplate.query( session -> session.prepare(""SELECT last_name FROM t_actor WHERE id = ?""), ps -> ps.bind(1212L), (row, rowNum) -> row.getString(0)); Flux<String> lastNames = reactiveCqlTemplate.query( session -> session.prepare(""SELECT last_name FROM t_actor WHERE id = ?""), ps -> ps.bind(1212L), (row, rowNum) -> row.getString(0)); Spring Data Cassandra ships with classes supporting that pattern in the cql package: SimplePreparedStatementCreator - utility class to create a prepared statement. ArgumentPreparedStatementBinder - utility class to bind arguments to a prepared statement. Using CassandraTemplate: The CassandraTemplate class is built on top of CqlTemplate to provide a higher level of abstraction. The use of prepared statements can be controlled directly on CassandraTemplate (and its asynchronous and reactive variants) by calling setUsePreparedStatements(false) respective setUsePreparedStatements(true) . Note that the use of prepared statements by CassandraTemplate is enabled by default. The following example shows the use of methods that generate and that accept CQL: Imperative Reactive template.setUsePreparedStatements(true); Actor actorByQuery = template.selectOne(query(where(""id"").is(42)), Actor.class); Actor actorByStatement = template.selectOne( SimpleStatement.newInstance(""SELECT id, name FROM actor WHERE id = ?"", 42), Actor.class); template.setUsePreparedStatements(true); Mono<Actor> actorByQuery = template.selectOne(query(where(""id"").is(42)), Actor.class); Mono<Actor> actorByStatement = template.selectOne( SimpleStatement.newInstance(""SELECT id, name FROM actor WHERE id = ?"", 42), Actor.class); Calling entity-bound methods such as select(Query, Class<T>) or update(Query, Update, Class<T>) build CQL statements themselves to perform the intended operations. Some CassandraTemplate methods (such as select(Statement<?>, Class<T>) ) also accepts CQL Statement objects as part of their API. It’s possible to participate in prepared statements when calling methods accepting a Statement with a SimpleStatement object. The template API extracts the query string and parameters (positional and named parameters) and uses these to prepare, bind, and run the statement. Non- SimpleStatement objects cannot be used with prepared statements. Caching Prepared Statements: Since Cassandra driver 4.0, prepared statements are cached by the CqlSession cache so it is okay to prepare the same string twice. Previous versions required caching of prepared statements outside of the driver. See also the Driver documentation on Prepared Statements(https://docs.datastax.com/en/developer/java-driver/latest/manual/core/statements/prepared/) for further reference."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/object-mapping.html","Mapping: Rich object mapping support is provided by the MappingCassandraConverter . MappingCassandraConverter has a rich metadata model that provides a complete feature set of functionality to map domain objects to CQL tables. The mapping metadata model is populated by using annotations on your domain objects. However, the infrastructure is not limited to using annotations as the only source of metadata. The MappingCassandraConverter also lets you map domain objects to tables without providing any additional metadata, by following a set of conventions. In this chapter, we describe the features of the MappingCassandraConverter , how to use conventions for mapping domain objects to tables, and how to override those conventions with annotation-based mapping metadata. Object Mapping Fundamentals: This section covers the fundamentals of Spring Data object mapping, object creation, field and property access, mutability and immutability. Note, that this section only applies to Spring Data modules that do not use the object mapping of the underlying data store (like JPA). Also be sure to consult the store-specific sections for store-specific object mapping, like indexes, customizing column or field names or the like. Core responsibility of the Spring Data object mapping is to create instances of domain objects and map the store-native data structures onto those. This means we need two fundamental steps: Instance creation by using one of the constructors exposed. Instance population to materialize all exposed properties. Object creation: Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. The value resolution assumes constructor/factory method argument names to match the property names of the entity, i.e. the resolution will be performed as if the property was to be populated, including all customizations in mapping (different datastore column or field name etc.). This also requires either parameter names information available in the class file or an @ConstructorProperties annotation being present on the constructor. The value resolution can be customized by using Spring Framework’s @Value value annotation using a store-specific SpEL expression. Please consult the section on store specific mappings for further details. Object creation internals To avoid the overhead of reflection, Spring Data object creation uses a factory class generated at runtime by default, which will call the domain classes constructor directly. I.e. for this example type: class Person { Person(String firstname, String lastname) { … } } we will create a factory class semantically equivalent to this one at runtime: class PersonObjectInstantiator implements ObjectInstantiator { Object newInstance(Object... args) { return new Person((String) args[0], (String) args[1]); } } This gives us a roundabout 10% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: it must not be a private class it must not be a non-static inner class it must not be a CGLib proxy class the constructor to be used by Spring Data must not be private If any of these criteria match, Spring Data will fall back to entity instantiation via reflection. Property population: Once an instance of the entity has been created, Spring Data populates all remaining persistent properties of that class. Unless already populated by the entity’s constructor (i.e. consumed through its constructor argument list), the identifier property will be populated first to allow the resolution of cyclic object references. After that, all non-transient properties that have not already been populated by the constructor are set on the entity instance. For that we use the following algorithm: If the property is immutable but exposes a with… method (see below), we use the with… method to create a new entity instance with the new property value. If property access (i.e. access through getters and setters) is defined, we’re invoking the setter method. If the property is mutable we set the field directly. If the property is immutable we’re using the constructor to be used by persistence operations (see Object creation(#mapping.object-creation) ) to create a copy of the instance. By default, we set the field value directly. Property population internals Similarly to our optimizations in object construction(#mapping.object-creation.details) we also use Spring Data runtime generated accessor classes to interact with the entity instance. class Person { private final Long id; private String firstname; private @AccessType(Type.PROPERTY) String lastname; Person() { this.id = null; } Person(Long id, String firstname, String lastname) { // Field assignments } Person withId(Long id) { return new Person(id, this.firstname, this.lastame); } void setLastname(String lastname) { this.lastname = lastname; } } A generated Property Accessor class PersonPropertyAccessor implements PersistentPropertyAccessor { private static final MethodHandle firstname; (2) private Person person; (1) public void setProperty(PersistentProperty property, Object value) { String name = property.getName(); if (""firstname"".equals(name)) { firstname.invoke(person, (String) value); (2) } else if (""id"".equals(name)) { this.person = person.withId((Long) value); (3) } else if (""lastname"".equals(name)) { this.person.setLastname((String) value); (4) } } } 1 PropertyAccessor’s hold a mutable instance of the underlying object. This is, to enable mutations of otherwise immutable properties. 2 By default, Spring Data uses field-access to read and write property values. As per visibility rules of private fields, MethodHandles are used to interact with fields. 3 The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. Calling withId(…) creates a new Person object. All subsequent mutations will take place in the new instance leaving the previous untouched. 4 Using property-access allows direct method invocations without using MethodHandles . This gives us a roundabout 25% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: Types must not reside in the default or under the java package. Types and their constructors must be public Types that are inner classes must be static . The used Java Runtime must allow for declaring classes in the originating ClassLoader . Java 9 and newer impose certain limitations. By default, Spring Data attempts to use generated property accessors and falls back to reflection-based ones if a limitation is detected. Let’s have a look at the following entity: A sample entity class Person { private final @Id Long id; (1) private final String firstname, lastname; (2) private final LocalDate birthday; private final int age; (3) private String comment; (4) private @AccessType(Type.PROPERTY) String remarks; (5) static Person of(String firstname, String lastname, LocalDate birthday) { (6) return new Person(null, firstname, lastname, birthday, Period.between(birthday, LocalDate.now()).getYears()); } Person(Long id, String firstname, String lastname, LocalDate birthday, int age) { (6) this.id = id; this.firstname = firstname; this.lastname = lastname; this.birthday = birthday; this.age = age; } Person withId(Long id) { (1) return new Person(id, this.firstname, this.lastname, this.birthday, this.age); } void setRemarks(String remarks) { (5) this.remarks = remarks; } } 1 The identifier property is final but set to null in the constructor. The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. The original Person instance stays unchanged as a new one is created. The same pattern is usually applied for other properties that are store managed but might have to be changed for persistence operations. The wither method is optional as the persistence constructor (see 6) is effectively a copy constructor and setting the property will be translated into creating a fresh instance with the new identifier value applied. 2 The firstname and lastname properties are ordinary immutable properties potentially exposed through getters. 3 The age property is an immutable but derived one from the birthday property. With the design shown, the database value will trump the defaulting as Spring Data uses the only declared constructor. Even if the intent is that the calculation should be preferred, it’s important that this constructor also takes age as parameter (to potentially ignore it) as otherwise the property population step will attempt to set the age field and fail due to it being immutable and no with… method being present. 4 The comment property is mutable and is populated by setting its field directly. 5 The remarks property is mutable and is populated by invoking the setter method. 6 The class exposes a factory method and a constructor for object creation. The core idea here is to use factory methods instead of additional constructors to avoid the need for constructor disambiguation through @PersistenceCreator . Instead, defaulting of properties is handled within the factory method. If you want Spring Data to use the factory method for object instantiation, annotate it with @PersistenceCreator . General recommendations: Try to stick to immutable objects — Immutable objects are straightforward to create as materializing an object is then a matter of calling its constructor only. Also, this avoids your domain objects to be littered with setter methods that allow client code to manipulate the objects state. If you need those, prefer to make them package protected so that they can only be invoked by a limited amount of co-located types. Constructor-only materialization is up to 30% faster than properties population. Provide an all-args constructor — Even if you cannot or don’t want to model your entities as immutable values, there’s still value in providing a constructor that takes all properties of the entity as arguments, including the mutable ones, as this allows the object mapping to skip the property population for optimal performance. Use factory methods instead of overloaded constructors to avoid @PersistenceCreator — With an all-argument constructor needed for optimal performance, we usually want to expose more application use case specific constructors that omit things like auto-generated identifiers etc. It’s an established pattern to rather use static factory methods to expose these variants of the all-args constructor. Make sure you adhere to the constraints that allow the generated instantiator and property accessor classes to be used — For identifiers to be generated, still use a final field in combination with an all-arguments persistence constructor (preferred) or a with… method — Use Lombok to avoid boilerplate code — As persistence operations usually require a constructor taking all arguments, their declaration becomes a tedious repetition of boilerplate parameter to field assignments that can best be avoided by using Lombok’s @AllArgsConstructor . Overriding Properties: Java’s allows a flexible design of domain classes where a subclass could define a property that is already declared with the same name in its superclass. Consider the following example: public class SuperType { private CharSequence field; public SuperType(CharSequence field) { this.field = field; } public CharSequence getField() { return this.field; } public void setField(CharSequence field) { this.field = field; } } public class SubType extends SuperType { private String field; public SubType(String field) { super(field); this.field = field; } @Override public String getField() { return this.field; } public void setField(String field) { this.field = field; // optional super.setField(field); } } Both classes define a field using assignable types. SubType however shadows SuperType.field . Depending on the class design, using the constructor could be the only default approach to set SuperType.field . Alternatively, calling super.setField(…) in the setter could set the field in SuperType . All these mechanisms create conflicts to some degree because the properties share the same name yet might represent two distinct values. Spring Data skips super-type properties if types are not assignable. That is, the type of the overridden property must be assignable to its super-type property type to be registered as override, otherwise the super-type property is considered transient. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be generally set without making any further assumptions of the setter implementation. Kotlin support: Spring Data adapts specifics of Kotlin to allow object creation and mutation. Kotlin object creation: Kotlin classes are supported to be instantiated, all classes are immutable by default and require explicit property declarations to define mutable properties. Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a constructor that is annotated with @PersistenceCreator , it is used. If the type is a Kotlin data class(#mapping.kotlin) the primary constructor is used. If there is a single static factory method annotated with @PersistenceCreator then it is used. If there is a single constructor, it is used. If there are multiple constructors and exactly one is annotated with @PersistenceCreator , it is used. If the type is a Java Record the canonical constructor is used. If there’s a no-argument constructor, it is used. Other constructors will be ignored. Consider the following data class Person : data class Person(val id: String, val name: String) The class above compiles to a typical class with an explicit constructor. We can customize this class by adding another constructor and annotate it with @PersistenceCreator to indicate a constructor preference: data class Person(var id: String, val name: String) { @PersistenceCreator constructor(id: String) : this(id, ""unknown"") } Kotlin supports parameter optionality by allowing default values to be used if a parameter is not provided. When Spring Data detects a constructor with parameter defaulting, then it leaves these parameters absent if the data store does not provide a value (or simply returns null ) so Kotlin can apply parameter defaulting.Consider the following class that applies parameter defaulting for name data class Person(var id: String, val name: String = ""unknown"") Every time the name parameter is either not part of the result or its value is null , then the name defaults to unknown . Delegated properties are not supported with Spring Data. The mapping metadata filters delegated properties for Kotlin Data classes. In all other cases you can exclude synthetic fields for delegated properties by annotating the property with @delegate:org.springframework.data.annotation.Transient . Property population of Kotlin data classes: In Kotlin, all classes are immutable by default and require explicit property declarations to define mutable properties. Consider the following data class Person : data class Person(val id: String, val name: String) This class is effectively immutable. It allows creating new instances as Kotlin generates a copy(…) method that creates new object instances copying all property values from the existing object and applying property values provided as arguments to the method. Kotlin Overriding Properties: Kotlin allows declaring property overrides(https://kotlinlang.org/docs/inheritance.html#overriding-properties) to alter properties in subclasses. open class SuperType(open var field: Int) class SubType(override var field: Int = 1) : SuperType(field) { } Such an arrangement renders two properties with the name field . Kotlin generates property accessors (getters and setters) for each property in each class. Effectively, the code looks like as follows: public class SuperType { private int field; public SuperType(int field) { this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } public final class SubType extends SuperType { private int field; public SubType(int field) { super(field); this.field = field; } public int getField() { return this.field; } public void setField(int field) { this.field = field; } } Getters and setters on SubType set only SubType.field and not SuperType.field . In such an arrangement, using the constructor is the only default approach to set SuperType.field . Adding a method to SubType to set SuperType.field via this.SuperType.field = … is possible but falls outside of supported conventions. Property overrides create conflicts to some degree because the properties share the same name yet might represent two distinct values. We generally recommend using distinct property names. Spring Data modules generally support overridden properties holding different values. From a programming model perspective there are a few things to consider: Which property should be persisted (default to all declared properties)? You can exclude properties by annotating these with @Transient . How to represent properties in your data store? Using the same field/column name for different values typically leads to corrupt data so you should annotate least one of the properties using an explicit field/column name. Using @AccessType(PROPERTY) cannot be used as the super-property cannot be set. Kotlin Value Classes: Kotlin Value Classes are designed for a more expressive domain model to make underlying concepts explicit. Spring Data can read and write types that define properties using Value Classes. Consider the following domain model: @JvmInline value class EmailAddress(val theAddress: String) (1) data class Contact(val id: String, val name:String, val emailAddress: EmailAddress) (2) 1 A simple value class with a non-nullable value type. 2 Data class defining a property using the EmailAddress value class. Non-nullable properties using non-primitive value types are flattened in the compiled class to the value type. Nullable primitive value types or nullable value-in-value types are represented with their wrapper type and that affects how value types are represented in the database. Data Mapping and Type Conversion: This section explains how types are mapped to and from an Apache Cassandra representation. Spring Data for Apache Cassandra supports several types that are provided by Apache Cassandra. In addition to these types, Spring Data for Apache Cassandra provides a set of built-in converters to map additional types. You can provide your own custom converters to adjust type conversion. See “ Overriding Default Mapping with Custom Converters(cassandra/converters.html) ” for further details. The following table maps Spring Data types to Cassandra types: Table 1. Type Type Cassandra types String text (default), varchar , ascii double , Double double float , Float float long , Long bigint (default), counter int , Integer int short , Short smallint byte , Byte tinyint boolean , Boolean boolean BigInteger varint BigDecimal decimal java.util.Date timestamp com.datastax.driver.core.LocalDate date InetAddress inet ByteBuffer blob java.util.UUID uuid TupleValue , mapped Tuple Types tuple<…> UDTValue , mapped User-Defined Types user type java.util.Map<K, V> map java.util.List<E> list java.util.Set<E> set Enum text (default), bigint , varint , int , smallint , tinyint LocalDate (Joda, Java 8, JSR310-BackPort) date LocalTime + (Joda, Java 8, JSR310-BackPort) time LocalDateTime , LocalTime , Instant (Joda, Java 8, JSR310-BackPort) timestamp ZoneId (Java 8, JSR310-BackPort) text Each supported type maps to a default Cassandra data type(https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/cql_data_types_c.html) . Java types can be mapped to other Cassandra types by using @CassandraType , as the following example shows: Example 1. Enum mapping to numeric types @Table public class EnumToOrdinalMapping { @PrimaryKey String id; @CassandraType(type = Name.INT) Condition asOrdinal; } public enum Condition { NEW, USED } Convention-based Mapping: MappingCassandraConverter uses a few conventions for mapping domain objects to CQL tables when no additional mapping metadata is provided. The conventions are: The simple (short) Java class name is mapped to the table name by being changed to lower case. For example, com.bigbank.SavingsAccount maps to a table named savingsaccount . The converter uses any registered Spring Converter instances to override the default mapping of object properties to tables columns. The properties of an object are used to convert to and from columns in the table. You can adjust conventions by configuring a NamingStrategy on CassandraMappingContext . Naming strategy objects implement the convention by which a table, column or user-defined type is derived from an entity class and from an actual property. The following example shows how to configure a NamingStrategy : Example 2. Configuring NamingStrategy on CassandraMappingContext CassandraMappingContext context = new CassandraMappingContext(); // default naming strategy context.setNamingStrategy(NamingStrategy.INSTANCE); // snake_case converted to upper case (SNAKE_CASE) context.setNamingStrategy(NamingStrategy.SNAKE_CASE.transform(String::toUpperCase)); Mapping Configuration: Unless explicitly configured, an instance of MappingCassandraConverter is created by default when creating a CassandraTemplate . You can create your own instance of the MappingCassandraConverter to tell it where to scan the classpath at startup for your domain classes to extract metadata and construct indexes. Also, by creating your own instance, you can register Spring Converter instances to use for mapping specific classes to and from the database. The following example configuration class sets up Cassandra mapping support: Example 3. @Configuration class to configure Cassandra mapping support @Configuration public class SchemaConfiguration extends AbstractCassandraConfiguration { @Override protected String getKeyspaceName() { return ""bigbank""; } // the following are optional @Override public CassandraCustomConversions customConversions() { return CassandraCustomConversions.create(config -> { config.registerConverter(new PersonReadConverter())); config.registerConverter(new PersonWriteConverter())); }); } @Override public SchemaAction getSchemaAction() { return SchemaAction.RECREATE; } // other methods omitted... } AbstractCassandraConfiguration requires you to implement methods that define a keyspace. AbstractCassandraConfiguration also has a method named getEntityBasePackages(…) . You can override it to tell the converter where to scan for classes annotated with the @Table annotation. You can add additional converters to the MappingCassandraConverter by overriding the customConversions method. AbstractCassandraConfiguration creates a CassandraTemplate instance and registers it with the container under the name of cassandraTemplate . Metadata-based Mapping: To take full advantage of the object mapping functionality inside the Spring Data for Apache Cassandra support, you should annotate your mapped domain objects with the @Table annotation. Doing so lets the classpath scanner find and pre-process your domain objects to extract the necessary metadata. Only annotated entities are used to perform schema actions. In the worst case, a SchemaAction.RECREATE_DROP_UNUSED operation drops your tables and you lose your data. The following example shows a simple domain object: Example 4. Example domain object package com.mycompany.domain; @Table public class Person { @Id private String id; @CassandraType(type = Name.VARINT) private Integer ssn; private String firstName; private String lastName; } The @Id annotation tells the mapper which property you want to use for the Cassandra primary key. Composite primary keys can require a slightly different data model. Working with Primary Keys: Cassandra requires at least one partition key field for a CQL table. A table can additionally declare one or more clustering key fields. When your CQL table has a composite primary key, you must create a @PrimaryKeyClass to define the structure of the composite primary key. In this context, “composite primary key” means one or more partition columns optionally combined with one or more clustering columns. Primary keys can make use of any singular simple Cassandra type or mapped user-defined Type. Collection-typed primary keys are not supported. Simple Primary Keys: A simple primary key consists of one partition key field within an entity class. Since it is one field only, we safely can assume it is a partition key. The following listing shows a CQL table defined in Cassandra with a primary key of user_id : Example 5. CQL Table defined in Cassandra CREATE TABLE user ( user_id text, firstname text, lastname text, PRIMARY KEY (user_id)) ; The following example shows a Java class annotated such that it corresponds to the Cassandra defined in the previous listing: Example 6. Annotated Entity @Table(value = ""login_event"") public class LoginEvent { @PrimaryKey(""user_id"") private String userId; private String firstname; private String lastname; // getters and setters omitted } Composite Keys: Composite primary keys (or compound keys) consist of more than one primary key field. That said, a composite primary key can consist of multiple partition keys, a partition key and a clustering key, or a multitude of primary key fields. Composite keys can be represented in two ways with Spring Data for Apache Cassandra: Embedded in an entity. By using @PrimaryKeyClass . The simplest form of a composite key is a key with one partition key and one clustering key. The following example shows a CQL statement to represent the table and its composite key: Example 7. CQL Table with a Composite Primary Key CREATE TABLE login_event( person_id text, event_code int, event_time timestamp, ip_address text, PRIMARY KEY (person_id, event_code, event_time)) WITH CLUSTERING ORDER BY (event_time DESC) ; Flat Composite Primary Keys: Flat composite primary keys are embedded inside the entity as flat fields. Primary key fields are annotated with @PrimaryKeyColumn . Selection requires either a query to contain predicates for the individual fields or the use of MapId . The following example shows a class with a flat composite primary key: Example 8. Using a flat composite primary key @Table(value = ""login_event"") class LoginEvent { @PrimaryKeyColumn(name = ""person_id"", ordinal = 0, type = PrimaryKeyType.PARTITIONED) private String personId; @PrimaryKeyColumn(name = ""event_code"", ordinal = 1, type = PrimaryKeyType.PARTITIONED) private int eventCode; @PrimaryKeyColumn(name = ""event_time"", ordinal = 2, type = PrimaryKeyType.CLUSTERED, ordering = Ordering.DESCENDING) private LocalDateTime eventTime; @Column(""ip_address"") private String ipAddress; // getters and setters omitted } Primary Key Class: A primary key class is a composite primary key class that is mapped to multiple fields or properties of the entity. It is annotated with @PrimaryKeyClass and should define equals and hashCode methods. The semantics of value equality for these methods should be consistent with the database equality for the database types to which the key is mapped. Primary key classes can be used with repositories (as the Id type) and to represent an entity’s identity in a single complex object. The following example shows a composite primary key class: Example 9. Composite primary key class @PrimaryKeyClass class LoginEventKey implements Serializable { @PrimaryKeyColumn(name = ""person_id"", ordinal = 0, type = PrimaryKeyType.PARTITIONED) private String personId; @PrimaryKeyColumn(name = ""event_code"", ordinal = 1, type = PrimaryKeyType.PARTITIONED) private int eventCode; @PrimaryKeyColumn(name = ""event_time"", ordinal = 2, type = PrimaryKeyType.CLUSTERED, ordering = Ordering.DESCENDING) private LocalDateTime eventTime; // other methods omitted } The following example shows how to use a composite primary key: Example 10. Using a composite primary key @Table(value = ""login_event"") public class LoginEvent { @PrimaryKey private LoginEventKey key; @Column(""ip_address"") private String ipAddress; // getters and setters omitted } Embedded Entity Support: Embedded entities are used to design value objects in your Java domain model whose properties are flattened out into the table. In the following example you see, that User.name is annotated with @Embedded . The consequence of this is that all properties of UserName are folded into the user table which consists of 3 columns ( user_id , firstname , lastname ). Embedded entities may only contain simple property types. It is not possible to nest an embedded entity into another embedded one. However, if the firstname and lastname column values are actually null within the result set, the entire property name will be set to null according to the onEmpty of @Embedded , which null s objects when all nested properties are null . Opposite to this behavior USE_EMPTY tries to create a new instance using either a default constructor or one that accepts nullable parameter values from the result set. Example 11. Sample Code of embedding objects public class User { @PrimaryKey(""user_id"") private String userId; @Embedded(onEmpty = USE_NULL) (1) UserName name; } public class UserName { private String firstname; private String lastname; } 1 Property is null if firstname and lastname are null . Use onEmpty=USE_EMPTY to instantiate UserName with a potential null value for its properties. You can embed a value object multiple times in an entity by using the optional prefix element of the @Embedded annotation. This element represents a prefix and is prepended to each column name in the embedded object. Note that properties will overwrite each other if multiple properties render to the same column name. Make use of the shortcuts @Embedded.Nullable and @Embedded.Empty for @Embedded(onEmpty = USE_NULL) and @Embedded(onEmpty = USE_EMPTY) to reduce verbosity and simultaneously set JSR-305 @javax.annotation.Nonnull accordingly. public class MyEntity { @Id Integer id; @Embedded.Nullable (1) EmbeddedEntity embeddedEntity; } 1 Shortcut for @Embedded(onEmpty = USE_NULL) . Mapping Annotation Overview: The MappingCassandraConverter can use metadata to drive the mapping of objects to rows in a Cassandra table. An overview of the annotations follows: @Id : Applied at the field or property level to mark the property used for identity purposes. @Table : Applied at the class level to indicate that this class is a candidate for mapping to the database. You can specify the name of the table where the object is stored. @PrimaryKey : Similar to @Id but lets you specify the column name. @PrimaryKeyColumn : Cassandra-specific annotation for primary key columns that lets you specify primary key column attributes, such as for clustered or partitioned. Can be used on single and multiple attributes to indicate either a single or a composite (compound) primary key. If used on a property within the entity, make sure to apply the @Id annotation as well. @PrimaryKeyClass : Applied at the class level to indicate that this class is a compound primary key class. Must be referenced with @PrimaryKey in the entity class. @Transient : By default, all private fields are mapped to the row. This annotation excludes the field where it is applied from being stored in the database. Transient properties cannot be used within a persistence constructor as the converter cannot materialize a value for the constructor argument. @PersistenceConstructor : Marks a given constructor — even a package protected one — to use when instantiating the object from the database. Constructor arguments are mapped by name to the key values in the retrieved row. @Value : This annotation is part of the Spring Framework . Within the mapping framework it can be applied to constructor arguments. This lets you use a Spring Expression Language statement to transform a key’s value retrieved in the database before it is used to construct a domain object. In order to reference a property of a given Row / UdtValue / TupleValue one has to use expressions like: @Value(""#root.getString(0)"") where root refers to the root of the given document. @ReadOnlyProperty : Applies at the field level to mark a property as read-only. Entity-bound insert and update statements do not include this property. @Column : Applied at the field level. Describes the column name as it is represented in the Cassandra table, thus letting the name differ from the field name of the class. Can be used on constructor arguments to customize the column name during constructor creation. @Embedded : Applied at the field level. Enables embedded object usage for types mapped to a table or a user-defined type. Properties of the embedded object are flattened into the structure of its parent. @Indexed : Applied at the field level. Describes the index to be created at session initialization. @SASI : Applied at the field level. Allows SASI index creation during session initialization. @CassandraType : Applied at the field level to specify a Cassandra data type. Types are derived from the property declaration by default. @Frozen : Applied at the field level to class-types and parametrized types. Declares a frozen UDT column or frozen collection like List<@Frozen UserDefinedPersonType> . @UserDefinedType : Applied at the type level to specify a Cassandra User-defined Data Type (UDT). Types are derived from the declaration by default. @Tuple : Applied at the type level to use a type as a mapped tuple. @Element : Applied at the field level to specify element or field ordinals within a mapped tuple. Types are derived from the property declaration by default. Can be used on constructor arguments to customize tuple element ordinals during constructor creation. @Version : Applied at field level is used for optimistic locking and checked for modification on save operations. The initial value is zero which is bumped automatically on every update. The mapping metadata infrastructure is defined in the separate, spring-data-commons project that is both technology- and data store-agnostic. The following example shows a more complex mapping: Example 12. Mapped Person class @Table(""my_person"") public class Person { @PrimaryKeyClass public static class Key implements Serializable { @PrimaryKeyColumn(ordinal = 0, type = PrimaryKeyType.PARTITIONED) private String type; @PrimaryKeyColumn(ordinal = 1, type = PrimaryKeyType.PARTITIONED) private String value; @PrimaryKeyColumn(name = ""correlated_type"", ordinal = 2, type = PrimaryKeyType.CLUSTERED) private String correlatedType; // other getters/setters omitted } @PrimaryKey private Person.Key key; @CassandraType(type = CassandraType.Name.VARINT) private Integer ssn; @Column(""f_name"") private String firstName; @Column @Indexed private String lastName; private Address address; @CassandraType(type = CassandraType.Name.UDT, userTypeName = ""myusertype"") private UdtValue usertype; private Coordinates coordinates; @Transient private Integer accountTotal; @CassandraType(type = CassandraType.Name.SET, typeArguments = CassandraType.Name.BIGINT) private Set<Long> timestamps; private Map<@Indexed String, InetAddress> sessions; public Person(Integer ssn) { this.ssn = ssn; } public Person.Key getKey() { return key; } // no setter for Id. (getter is only exposed for some unit testing) public Integer getSsn() { return ssn; } public void setFirstName(String firstName) { this.firstName = firstName; } // other getters/setters omitted } The following example shows how to map a UDT Address : Example 13. Mapped User-Defined Type Address @UserDefinedType(""address"") public class Address { @CassandraType(type = CassandraType.Name.VARCHAR) private String street; private String city; private Set<String> zipcodes; @CassandraType(type = CassandraType.Name.SET, typeArguments = CassandraType.Name.BIGINT) private List<Long> timestamps; // other getters/setters omitted } Working with User-Defined Types requires a UserTypeResolver that is configured with the mapping context. See the configuration chapter(cassandra/configuration.html) for how to configure a UserTypeResolver . The following example shows how map a tuple: Example 14. Mapped Tuple @Tuple class Coordinates { @Element(0) @CassandraType(type = CassandraType.Name.VARCHAR) private String description; @Element(1) private long longitude; @Element(2) private long latitude; // other getters/setters omitted } Index Creation: You can annotate particular entity properties with @Indexed or @SASI if you wish to create secondary indexes on application startup. Index creation creates simple secondary indexes for scalar types, user-defined types, and collection types. You can configure a SASI Index to apply an analyzer, such as StandardAnalyzer or NonTokenizingAnalyzer (by using @StandardAnalyzed and @NonTokenizingAnalyzed , respectively). Map types distinguish between ENTRY , KEYS , and VALUES indexes. Index creation derives the index type from the annotated element. The following example shows a number of ways to create an index: Example 15. Variants of map indexing @Table class PersonWithIndexes { @Id private String key; @SASI @StandardAnalyzed private String names; @Indexed(""indexed_map"") private Map<String, String> entries; private Map<@Indexed String, String> keys; private Map<String, @Indexed String> values; // … } The @Indexed annotation can be applied to single properties of embedded entities or along side with the @Embedded annotation, in which case all properties of the embedded are indexed. Index creation on session initialization may have a severe performance impact on application startup."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/converters.html","Custom Conversions: The following example of a Spring Converter implementation converts from a String to a custom Email value object: @ReadingConverter public class EmailReadConverter implements Converter<String, Email> { public Email convert(String source) { return Email.valueOf(source); } } If you write a Converter whose source and target type are native types, we cannot determine whether we should consider it as a reading or a writing converter. Registering the converter instance as both might lead to unwanted results. For example, a Converter<String, Long> is ambiguous, although it probably does not make sense to try to convert all String instances into Long instances when writing. To let you force the infrastructure to register a converter for only one way, we provide @ReadingConverter and @WritingConverter annotations to be used in the converter implementation. Converters are subject to explicit registration as instances are not picked up from a classpath or container scan to avoid unwanted registration with a conversion service and the side effects resulting from such a registration. Converters are registered with CustomConversions as the central facility that allows registration and querying for registered converters based on source- and target type. CustomConversions ships with a pre-defined set of converter registrations: JSR-310 Converters for conversion between java.time , java.util.Date and String types. Default converters for local temporal types (e.g. LocalDateTime to java.util.Date ) rely on system-default timezone settings to convert between those types. You can override the default converter, by registering your own converter. Converter Disambiguation: Generally, we inspect the Converter implementations for the source and target types they convert from and to. Depending on whether one of those is a type the underlying data access API can handle natively, we register the converter instance as a reading or a writing converter. The following examples show a writing- and a read converter (note the difference is in the order of the qualifiers on Converter ): // Write converter as only the target type is one that can be handled natively class MyConverter implements Converter<Person, String> { … } // Read converter as only the source type is one that can be handled natively class MyConverter implements Converter<String, Person> { … } Overriding Default Mapping with Custom Converters: To have more fine-grained control over the mapping process, you can register Spring Converters with CassandraConverter implementations, such as MappingCassandraConverter . MappingCassandraConverter first checks to see whether any Spring Converters can handle a specific class before attempting to map the object itself. To ""'hijack'"" the normal mapping strategies of the MappingCassandraConverter (perhaps for increased performance or other custom mapping needs), you need to create an implementation of the Spring Converter interface and register it with the MappingCassandraConverter . Saving by Using a Registered Spring Converter: You can combine converting and saving in a single process, basically using the converter to do the saving. The following example uses a Converter to convert a Person object to a java.lang.String with Jackson 2: class PersonWriteConverter implements Converter<Person, String> { public String convert(Person source) { try { return new ObjectMapper().writeValueAsString(source); } catch (IOException e) { throw new IllegalStateException(e); } } } Reading by Using a Spring Converter: Similar to how you can combine saving and converting, you can also combine reading and converting. The following example uses a Converter that converts a java.lang.String into a Person object with Jackson 2: class PersonReadConverter implements Converter<String, Person> { public Person convert(String source) { if (StringUtils.hasText(source)) { try { return new ObjectMapper().readValue(source, Person.class); } catch (IOException e) { throw new IllegalStateException(e); } } return null; } } Registering Spring Converters with CassandraConverter: Spring Data for Apache Cassandra Java configuration provides a convenient way to register Spring Converter instances: MappingCassandraConverter . The following configuration snippet shows how to manually register converters as well as configure CustomConversions : @Configuration public class ConverterConfiguration extends AbstractCassandraConfiguration { @Override public CassandraCustomConversions customConversions() { return CassandraCustomConversions.create(config -> { config.registerConverter(new PersonReadConverter())); config.registerConverter(new PersonWriteConverter())); }); } // other methods omitted... }"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/property-converters.html","Property-based Converters: While type-based conversion(converters.html) already offers ways to influence the conversion and representation of certain types within the target store, it has limitations when only certain values or properties of a particular type should be considered for conversion. Property-based converters allow configuring conversion rules on a per-property basis, either declaratively (via @ValueConverter ) or programmatically (by registering a PropertyValueConverter for a specific property). A PropertyValueConverter can transform a given value into its store representation (write) and back (read) as the following listing shows. The additional ValueConversionContext provides additional information, such as mapping metadata and direct read and write methods. Example 1. A simple PropertyValueConverter class ReversingValueConverter implements PropertyValueConverter<String, String, ValueConversionContext> { @Override public String read(String value, ValueConversionContext context) { return reverse(value); } @Override public String write(String value, ValueConversionContext context) { return reverse(value); } } You can obtain PropertyValueConverter instances from CustomConversions#getPropertyValueConverter(…) by delegating to PropertyValueConversions , typically by using a PropertyValueConverterFactory to provide the actual converter. Depending on your application’s needs, you can chain or decorate multiple instances of PropertyValueConverterFactory — for example, to apply caching. By default, Spring Data Cassandra uses a caching implementation that can serve types with a default constructor or enum values. A set of predefined factories is available through the factory methods in PropertyValueConverterFactory . You can use PropertyValueConverterFactory.beanFactoryAware(…) to obtain a PropertyValueConverter instance from an ApplicationContext . You can change the default behavior through ConverterConfiguration . Declarative Value Converter: The most straight forward usage of a PropertyValueConverter is by annotating properties with the @ValueConverter annotation that defines the converter type: Example 2. Declarative PropertyValueConverter class Person { @ValueConverter(ReversingValueConverter.class) String ssn; } Programmatic Value Converter Registration: Programmatic registration registers PropertyValueConverter instances for properties within an entity model by using a PropertyValueConverterRegistrar , as the following example shows. The difference between declarative registration and programmatic registration is that programmatic registration happens entirely outside the entity model. Such an approach is useful if you cannot or do not want to annotate the entity model. Example 3. Programmatic PropertyValueConverter registration PropertyValueConverterRegistrar registrar = new PropertyValueConverterRegistrar(); registrar.registerConverter(Address.class, ""street"", new PropertyValueConverter() { … }); (1) // type safe registration registrar.registerConverter(Person.class, Person::getSsn()) (2) .writing(value -> encrypt(value)) .reading(value -> decrypt(value)); 1 Register a converter for the field identified by its name. 2 Type safe variant that allows to register a converter and its conversion functions. This method uses class proxies to determine the property. Make sure that neither the class nor the accessors are final as otherwise this approach doesn’t work. Dot notation (such as registerConverter(Person.class, ""address.street"", …) ) for nagivating across properties into nested objects is not supported when registering converters. Schema derivation can only derive the column type from a registered converter if the converter is a PropertyValueConverter class. Generics cannot be determined from lambdas and using a lambda will fall back to the property type. CassandraValueConverter offers a pre-typed PropertyValueConverter interface that uses CassandraConversionContext . CassandraCustomConversions configuration: By default, CassandraCustomConversions can handle declarative value converters, depending on the configured PropertyValueConverterFactory . CassandraConverterConfigurationAdapter helps you to set up programmatic value conversions or define the PropertyValueConverterFactory to be used or to register converters. Example 4. Configuration Sample CassandraCustomConversions conversions = CassandraCustomConversions.create(adapter -> { adapter.registerConverter(…); adapter.configurePropertyConversions(registrar -> { registrar.registerConverter(Person.class, ""name"", String.class) .writing((from, ctx) -> …) .reading((from, ctx) -> …); }); });"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/events.html","Lifecycle Events: The Cassandra mapping framework has several built-in org.springframework.context.ApplicationEvent events that your application can respond to by registering special beans in the ApplicationContext . Being based on Spring’s application context event infrastructure lets other products, such as Spring Integration, easily receive these events as they are a well known eventing mechanism in Spring-based applications. To intercept an object before it goes into the database, you can register a subclass of AbstractCassandraEventListener(../api/java/org/springframework/data/cassandra/core/mapping/event/AbstractCassandraEventListener.html) that overrides the onBeforeSave(…) method. When the event is dispatched, your listener is called and passed the domain object (which is a Java entity). Entity lifecycle events can be costly and you may notice a change in the performance profile when loading large result sets. You can disable lifecycle events on the Template API(../api/java/org/springframework/data/cassandra/core/CassandraTemplate.html#setEntityLifecycleEventsEnabled(boolean)) . The following example uses the onBeforeSave method: class BeforeSaveListener extends AbstractCassandraEventListener<Person> { @Override public void onBeforeSave(BeforeSaveEvent<Person> event) { // … change values, delete them, whatever … } } Declaring these beans in your Spring ApplicationContext will cause them to be invoked whenever the event is dispatched. The AbstractCassandraEventListener(../api/java/org/springframework/data/cassandra/core/mapping/event/AbstractCassandraEventListener.html) has the following callback methods: onBeforeSave : Called in CassandraTemplate.insert(…) and .update(…) operations before inserting or updating a row in the database but after creating the Statement . onAfterSave : Called in CassandraTemplate…insert(…) and .update(…) operations after inserting or updating a row in the database. onBeforeDelete : Called in CassandraTemplate.delete(…) operations before deleting row from the database. onAfterDelete : Called in CassandraTemplate.delete(…) operations after deleting row from the database. onAfterLoad : Called in the CassandraTemplate.select(…) , .slice(…) , and .stream(…) methods after each row is retrieved from the database. onAfterConvert : Called in the CassandraTemplate.select(…) , .slice(…) , and .stream(…) methods after converting a row retrieved from the database to a POJO. Lifecycle events are emitted only for root-level types. Complex types used as properties within an aggregate root are not subject to event publication. Entity Callbacks: The Spring Data infrastructure provides hooks for modifying an entity before and after certain methods are invoked. Those so called EntityCallback instances provide a convenient way to check and potentially modify an entity in a callback fashioned style. An EntityCallback looks pretty much like a specialized ApplicationListener . Some Spring Data modules publish store specific events (such as BeforeSaveEvent ) that allow modifying the given entity. In some cases, such as when working with immutable types, these events can cause trouble. Also, event publishing relies on ApplicationEventMulticaster . If configuring that with an asynchronous TaskExecutor it can lead to unpredictable outcomes, as event processing can be forked onto a Thread. Entity callbacks provide integration points with both synchronous and reactive APIs to guarantee in-order execution at well-defined checkpoints within the processing chain, returning a potentially modified entity or an reactive wrapper type. Entity callbacks are typically separated by API type. This separation means that a synchronous API considers only synchronous entity callbacks and a reactive implementation considers only reactive entity callbacks. The Entity Callback API has been introduced with Spring Data Commons 2.2. It is the recommended way of applying entity modifications. Existing store specific ApplicationEvents are still published before the invoking potentially registered EntityCallback instances. Implementing Entity Callbacks: An EntityCallback is directly associated with its domain type through its generic type argument. Each Spring Data module typically ships with a set of predefined EntityCallback interfaces covering the entity lifecycle. Anatomy of an EntityCallback @FunctionalInterface public interface BeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked before a domain object is saved. * Can return either the same or a modified instance. * * @return the domain object to be persisted. */ (1) T onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called before an entity is saved. Returns a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Anatomy of a reactive EntityCallback @FunctionalInterface public interface ReactiveBeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked on subscription, before a domain object is saved. * The returned Publisher can emit either the same or a modified instance. * * @return Publisher emitting the domain object to be persisted. */ (1) Publisher<T> onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called on subscription, before an entity is saved. Emits a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Optional entity callback parameters are defined by the implementing Spring Data module and inferred from call site of EntityCallback.callback() . Implement the interface suiting your application needs like shown in the example below: Example BeforeSaveCallback class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { (1) if(collection == ""user"") { return // ... } return // ... } @Override public int getOrder() { return 100; (2) } } 1 Callback implementation according to your requirements. 2 Potentially order the entity callback if multiple ones for the same domain type exist. Ordering follows lowest precedence. Registering Entity Callbacks: EntityCallback beans are picked up by the store specific implementations in case they are registered in the ApplicationContext . Most template APIs already implement ApplicationContextAware and therefore have access to the ApplicationContext The following example explains a collection of valid entity callback registrations: Example EntityCallback Bean registration @Order(1) (1) @Component class First implements BeforeSaveCallback<Person> { @Override public Person onBeforeSave(Person person) { return // ... } } @Component class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { // ... } @Override public int getOrder() { return 100; (2) } } @Configuration public class EntityCallbackConfiguration { @Bean BeforeSaveCallback<Person> unorderedLambdaReceiverCallback() { (3) return (BeforeSaveCallback<Person>) it -> // ... } } @Component class UserCallbacks implements BeforeConvertCallback<User>, BeforeSaveCallback<User> { (4) @Override public Person onBeforeConvert(User user) { return // ... } @Override public Person onBeforeSave(User user) { return // ... } } 1 BeforeSaveCallback receiving its order from the @Order annotation. 2 BeforeSaveCallback receiving its order via the Ordered interface implementation. 3 BeforeSaveCallback using a lambda expression. Unordered by default and invoked last. Note that callbacks implemented by a lambda expression do not expose typing information hence invoking these with a non-assignable entity affects the callback throughput. Use a class or enum to enable type filtering for the callback bean. 4 Combine multiple entity callback interfaces in a single implementation class. Store-specific EntityCallbacks: Spring Data for Apache Cassandra uses the EntityCallback API for its auditing support and reacts on the following callbacks. Table 1. Supported Entity Callbacks Callback Method Description Order ReactiveBeforeConvertCallback BeforeConvertCallback onBeforeConvert(T entity, CqlIdentifier tableName) Invoked before a domain object is converted to Statement . Domain objects can be updated to include the change in the Statement . Ordered.LOWEST_PRECEDENCE ReactiveAuditingEntityCallback AuditingEntityCallback onBeforeConvert(Object entity, CqlIdentifier tableName) Marks an auditable entity created or modified 100 ReactiveBeforeSaveCallback BeforeSaveCallback onBeforeSave(T entity, CqlIdentifier tableName, Statement statement) Invoked before a domain object is saved. Can modify the target object after the Statement has been created. The provided statement contains all mapped entity information but changes to the domain object are not included in the Statement . Ordered.LOWEST_PRECEDENCE"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/auditing.html","Auditing Configuration for Cassandra: To activate auditing functionality, create a configuration as the following example shows: Activating auditing through configuration Java XML @Configuration @EnableCassandraAuditing class Config { @Bean public AuditorAware<AuditableUser> myAuditorProvider() { return new AuditorAwareImpl(); } } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:cassandra=""http://www.springframework.org/schema/data/cassandra"" xsi:schemaLocation="" http://www.springframework.org/schema/data/cassandra https://www.springframework.org/schema/data/cassandra/spring-cassandra.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <cassandra:auditing mapping-context-ref=""customMappingContext"" auditor-aware-ref=""yourAuditorAwareImpl""/> </beans> If you expose a bean of type AuditorAware to the ApplicationContext , the auditing infrastructure picks it up automatically and uses it to determine the current user to be set on domain types. If you have multiple implementations registered in the ApplicationContext , you can select the one to be used by explicitly setting the auditorAwareRef attribute of @EnableCassandraAuditing . To enable auditing, leveraging a reactive programming model, use the @EnableReactiveCassandraAuditing annotation. If you expose a bean of type ReactiveAuditorAware to the ApplicationContext , the auditing infrastructure picks it up automatically and uses it to determine the current user to be set on domain types. If you have multiple implementations registered in the ApplicationContext , you can select the one to be used by explicitly setting the auditorAwareRef attribute of @EnableReactiveCassandraAuditing . Example 1. Activating reactive auditing using JavaConfig @Configuration @EnableReactiveCassandraAuditing class Config { @Bean public ReactiveAuditorAware<AuditableUser> myAuditorProvider() { return new AuditorAwareImpl(); } }"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/value-expressions.html","Value Expressions Fundamentals: Value Expressions are a combination of Spring Expression Language (SpEL)(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) and Property Placeholder Resolution(https://docs.spring.io/spring-framework/reference/6.1/core/beans/environment.html#beans-placeholder-resolution-in-statements) . They combine powerful evaluation of programmatic expressions with the simplicity to resort to property-placeholder resolution to obtain values from the Environment such as configuration properties. Expressions are expected to be defined by a trusted input such as an annotation value and not to be determined from user input. The following code demonstrates how to use expressions in the context of annotations. Example 1. Annotation Usage @Document(""orders-#{tenantService.getOrderCollection()}-${tenant-config.suffix}"") class Order { // … } Value Expressions can be defined from a sole SpEL Expression, a Property Placeholder or a composite expression mixing various expressions including literals. Example 2. Expression Examples #{tenantService.getOrderCollection()} (1) #{(1+1) + '-hello-world'} (2) ${tenant-config.suffix} (3) orders-${tenant-config.suffix} (4) #{tenantService.getOrderCollection()}-${tenant-config.suffix} (5) 1 Value Expression using a single SpEL Expression. 2 Value Expression using a static SpEL Expression evaluating to 2-hello-world . 3 Value Expression using a single Property Placeholder. 4 Composite expression comprised of the literal orders- and the Property Placeholder ${tenant-config.suffix} . 5 Composite expression using SpEL, Property Placeholders and literals. Using value expressions introduces a lot of flexibility to your code. Doing so requires evaluation of the expression on each usage and, therefore, value expression evaluation has an impact on the performance profile. Parsing and Evaluation: Value Expressions are parsed by the ValueExpressionParser API. Instances of ValueExpression are thread-safe and can be cached for later use to avoid repeated parsing. The following example shows the Value Expression API usage: Parsing and Evaluation Java Kotlin ValueParserConfiguration configuration = SpelExpressionParser::new; ValueEvaluationContext context = ValueEvaluationContext.of(environment, evaluationContext); ValueExpressionParser parser = ValueExpressionParser.create(configuration); ValueExpression expression = parser.parse(""Hello, World""); Object result = expression.evaluate(context); val configuration = ValueParserConfiguration { SpelExpressionParser() } val context = ValueEvaluationContext.of(environment, evaluationContext) val parser = ValueExpressionParser.create(configuration) val expression: ValueExpression = parser.parse(""Hello, World"") val result: Any = expression.evaluate(context) SpEL Expressions: SpEL Expressions(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) follow the Template style where the expression is expected to be enclosed within the #{…} format. Expressions are evaluated using an EvaluationContext that is provided by EvaluationContextProvider . The context itself is a powerful StandardEvaluationContext allowing a wide range of operations, access to static types and context extensions. Make sure to parse and evaluate only expressions from trusted sources such as annotations. Accepting user-provided expressions can create an entry path to exploit the application context and your system resulting in a potential security vulnerability. Extending the Evaluation Context: EvaluationContextProvider and its reactive variant ReactiveEvaluationContextProvider provide access to an EvaluationContext . ExtensionAwareEvaluationContextProvider and its reactive variant ReactiveExtensionAwareEvaluationContextProvider are default implementations that determine context extensions from an application context, specifically ListableBeanFactory . Extensions implement either EvaluationContextExtension or ReactiveEvaluationContextExtension to provide extension support to hydrate EvaluationContext . That are a root object, properties and functions (top-level methods). The following example shows a context extension that provides a root object, properties, functions and an aliased function. Implementing a EvaluationContextExtension Java Kotlin @Component public class MyExtension implements EvaluationContextExtension { @Override public String getExtensionId() { return ""my-extension""; } @Override public Object getRootObject() { return new CustomExtensionRootObject(); } @Override public Map<String, Object> getProperties() { Map<String, Object> properties = new HashMap<>(); properties.put(""key"", ""Hello""); return properties; } @Override public Map<String, Function> getFunctions() { Map<String, Function> functions = new HashMap<>(); try { functions.put(""aliasedMethod"", new Function(getClass().getMethod(""extensionMethod""))); return functions; } catch (Exception o_O) { throw new RuntimeException(o_O); } } public static String extensionMethod() { return ""Hello World""; } public static int add(int i1, int i2) { return i1 + i2; } } public class CustomExtensionRootObject { public boolean rootObjectInstanceMethod() { return true; } } @Component class MyExtension : EvaluationContextExtension { override fun getExtensionId(): String { return ""my-extension"" } override fun getRootObject(): Any? { return CustomExtensionRootObject() } override fun getProperties(): Map<String, Any> { val properties: MutableMap<String, Any> = HashMap() properties[""key""] = ""Hello"" return properties } override fun getFunctions(): Map<String, Function> { val functions: MutableMap<String, Function> = HashMap() try { functions[""aliasedMethod""] = Function(javaClass.getMethod(""extensionMethod"")) return functions } catch (o_O: Exception) { throw RuntimeException(o_O) } } companion object { fun extensionMethod(): String { return ""Hello World"" } fun add(i1: Int, i2: Int): Int { return i1 + i2 } } } class CustomExtensionRootObject { fun rootObjectInstanceMethod(): Boolean { return true } } Once the above shown extension is registered, you can use its exported methods, properties and root object to evaluate SpEL expressions: Example 3. Expression Evaluation Examples #{add(1, 2)} (1) #{extensionMethod()} (2) #{aliasedMethod()} (3) #{key} (4) #{rootObjectInstanceMethod()} (5) 1 Invoke the method add declared by MyExtension resulting in 3 as the method adds both numeric parameters and returns the sum. 2 Invoke the method extensionMethod declared by MyExtension resulting in Hello World . 3 Invoke the method aliasedMethod . The method is exposed as function and redirects into the method extensionMethod declared by MyExtension resulting in Hello World . 4 Evaluate the key property resulting in Hello . 5 Invoke the method rootObjectInstanceMethod on the root object instance CustomExtensionRootObject . You can find real-life context extensions at SecurityEvaluationContextExtension(https://github.com/spring-projects/spring-security/blob/main/data/src/main/java/org/springframework/security/data/repository/query/SecurityEvaluationContextExtension.java) . Property Placeholders: Property placeholders following the form ${…} refer to properties provided typically by a PropertySource through Environment . Properties are useful to resolve against system properties, application configuration files, environment configuration or property sources contributed by secret management systems. You can find more details on the property placeholders in Spring Framework’s documentation on @Value usage(https://docs.spring.io/spring-framework/reference/6.1/core/beans/annotation-config/value-annotations.html#page-title) ."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories.html","Repositories: This chapter explains the basic foundations of Spring Data repositories and Cassandra specifics. Before continuing to the Cassandra specifics, make sure you have a sound understanding of the basic concepts. The goal of the Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. Section Summary: Core concepts(repositories/core-concepts.html) Defining Repository Interfaces(repositories/definition.html) Cassandra Repositories(cassandra/repositories/repositories.html) Creating Repository Instances(repositories/create-instances.html) Defining Query Methods(repositories/query-methods-details.html) Cassandra-specific Query Methods(cassandra/repositories/query-methods.html) Projections(repositories/projections.html) Custom Repository Implementations(repositories/custom-implementations.html) Publishing Events from Aggregate Roots(repositories/core-domain-events.html) Null Handling of Repository Methods(repositories/null-handling.html) CDI Integration(cassandra/repositories/cdi-integration.html) Repository query keywords(repositories/query-keywords-reference.html) Repository query return types(repositories/query-return-types-reference.html)"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); } Entity State Detection Strategies: The following table describes the strategies that Spring Data offers for detecting whether an entity is new: Table 1. Options for detection whether an entity is new in Spring Data @Id -Property inspection (the default) By default, Spring Data inspects the identifier property of the given entity. If the identifier property is null or 0 in case of primitive types, then the entity is assumed to be new. Otherwise, it is assumed to not be new. @Version -Property inspection If a property annotated with @Version is present and null , or in case of a version property of primitive type 0 the entity is considered new. If the version property is present but has a different value, the entity is considered to not be new. If no version property is present Spring Data falls back to inspection of the identifier property. Implementing Persistable If an entity implements Persistable , Spring Data delegates the new detection to the isNew(…) method of the entity. See the Javadoc(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//index.html?org/springframework/data/domain/Persistable.html) for details. Note: Properties of Persistable will get detected and persisted if you use AccessType.PROPERTY . To avoid that, use @Transient . Providing a custom EntityInformation implementation You can customize the EntityInformation abstraction used in the repository base implementation by creating a subclass of the module specific repository factory and overriding the getEntityInformation(…) method. You then have to register the custom implementation of module specific repository factory as a Spring bean. Note that this should rarely be necessary. Cassandra provides no means to generate identifiers upon inserting data. As consequence, entities must be associated with identifier values. Spring Data defaults to identifier inspection to determine whether an entity is new. If you want to use auditing(../cassandra/auditing.html) make sure to either use Optimistic Locking(../cassandra/template.html#cassandra.template.optimistic-locking) or implement Persistable for proper entity state detection."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/repositories/repositories.html","Cassandra Repositories: To access domain entities stored in Apache Cassandra, you can use Spring Data’s sophisticated repository support, which significantly eases implementing DAOs. To do so, create an interface for your repository, as the following example shows: Example 1. Sample Person entity @Table public class Person { @Id private String id; private String firstname; private String lastname; // … getters and setters omitted } Note that the entity has a property named id of type String . The default conversion mechanism used in MappingCassandraConverter (which backs the repository support) regards properties named id as being the row ID. The following example shows a repository definition to persist Person entities: Basic repository interface to persist Person entities Imperative Reactive interface PersonRepository extends CrudRepository<Person, String> { // additional custom finder methods go here } interface PersonRepository extends ReactiveCrudRepository<Person, String> { // additional custom finder methods go here } Right now, the interface in the preceding example serves only typing purposes, but we add additional methods to it later. Next, in your Spring configuration, add the following (if you use Java for configuration): If you want to use Java configuration, use the @EnableCassandraRepositories respective @EnableReactiveCassandraRepositories annotation. The annotation carries the same attributes as the namespace element. If no base package is configured, the infrastructure scans the package of the annotated configuration class. The following example show how to the different configuration approaches: Configuration for repositories Imperative Java Configuration XML Reactive Java Configuration @Configuration @EnableCassandraRepositories class ApplicationConfig extends AbstractCassandraConfiguration { @Override protected String getKeyspaceName() { return ""keyspace""; } public String[] getEntityBasePackages() { return new String[] { ""com.oreilly.springdata.cassandra"" }; } } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:cassandra=""http://www.springframework.org/schema/data/cassandra"" xsi:schemaLocation="" http://www.springframework.org/schema/data/cassandra https://www.springframework.org/schema/data/cassandra/spring-cassandra.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <cassandra:session port=""9042"" keyspace-name=""keyspaceName""/> <cassandra:mapping entity-base-packages=""com.acme. .entities""> </cassandra:mapping> <cassandra:converter/> <cassandra:template/> <cassandra:repositories base-package=""com.acme. .entities""/> </beans> @Configuration @EnableReactiveCassandraRepositories class ApplicationConfig extends AbstractReactiveCassandraConfiguration { @Override protected String getKeyspaceName() { return ""keyspace""; } public String[] getEntityBasePackages() { return new String[] { ""com.oreilly.springdata.cassandra"" }; } } The cassandra:repositories namespace element causes the base packages to be scanned for interfaces that extend CrudRepository and create Spring beans for each one found. By default, the repositories are wired with a CassandraTemplate Spring bean called cassandraTemplate , so you only need to configure cassandra-template-ref explicitly if you deviate from this convention. Because our domain repository extends CrudRepository respective ReactiveCrudRepository , it provides you with basic CRUD operations. Working with the repository instance is a matter of injecting the repository as a dependency into a client, as the following example does by autowiring PersonRepository : Basic access to Person entities Imperative Reactive @ExtendWith(SpringExtension.class) class PersonRepositoryTests { @Autowired PersonRepository repository; @Test void readsPersonTableCorrectly() { List<Person> persons = repository.findAll(); assertThat(persons.isEmpty()).isFalse(); } } public class PersonRepositoryTests { @Autowired ReactivePersonRepository repository; @Test public void sortsElementsCorrectly() { Flux<Person> people = repository.findAll(Sort.by(new Order(ASC, ""lastname""))); } } Cassandra repositories support paging and sorting for paginated and sorted access to the entities. Cassandra paging requires a paging state to forward-only navigate through pages. A Slice keeps track of the current paging state and allows for creation of a Pageable to request the next page. The following example shows how to set up paging access to Person entities: Paging access to Person entities Imperative Reactive @ExtendWith(SpringExtension.class) class PersonRepositoryTests { @Autowired PersonRepository repository; @Test void readsPagesCorrectly() { Slice<Person> firstBatch = repository.findAll(CassandraPageRequest.first(10)); assertThat(firstBatch).hasSize(10); Slice<Person> nextBatch = repository.findAll(firstBatch.nextPageable()); // … } } @ExtendWith(SpringExtension.class) class PersonRepositoryTests { @Autowired PersonRepository repository; @Test void readsPagesCorrectly() { Mono<Slice<Person>> firstBatch = repository.findAll(CassandraPageRequest.first(10)); Mono<Slice<Person>> nextBatch = firstBatch.flatMap(it -> repository.findAll(it.nextPageable())); // … } }} Cassandra repositories do not extend PagingAndSortingRepository , because classic paging patterns using limit/offset are not applicable to Cassandra. The preceding example creates an application context with Spring’s unit test support, which performs annotation-based dependency injection into the test class. Inside the test cases (the test methods), we use the repository to query the data store. We invoke the repository query method that requests all Person instances. Reactive Repositories: Spring Data’s repository abstraction is a dynamic API that is mostly defined by you and your requirements as you declare query methods. Reactive Cassandra repositories can be implemented by using either RxJava or Project Reactor wrapper types by extending from one of the library-specific repository interfaces: ReactiveCrudRepository ReactiveSortingRepository RxJava3CrudRepository RxJava3SortingRepository Spring Data converts reactive wrapper types behind the scenes so that you can stick to your favorite composition library."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/create-instances.html","Creating Repository Instances: This section covers how to create instances and bean definitions for the defined repository interfaces. Java Configuration: Use the store-specific @EnableCassandraRepositories annotation on a Java configuration class to define a configuration for repository activation. For an introduction to Java-based configuration of the Spring container, see JavaConfig in the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/java.html) . A sample configuration to enable Spring Data repositories resembles the following: Sample annotation-based repository configuration @Configuration @EnableJpaRepositories(""com.acme.repositories"") class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } The preceding example uses the JPA-specific annotation, which you would change according to the store module you actually use. The same applies to the definition of the EntityManagerFactory bean. See the sections covering the store-specific configuration. XML Configuration: Each Spring Data module includes a repositories element that lets you define a base package that Spring scans for you, as shown in the following example: Enabling Spring Data repositories via XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans:beans xmlns:beans=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans:beans> In the preceding example, Spring is instructed to scan com.acme.repositories and all its sub-packages for interfaces extending Repository or one of its sub-interfaces. For each interface found, the infrastructure registers the persistence technology-specific FactoryBean to create the appropriate proxies that handle invocations of the query methods. Each bean is registered under a bean name that is derived from the interface name, so an interface of UserRepository would be registered under userRepository . Bean names for nested repository interfaces are prefixed with their enclosing type name. The base package attribute allows wildcards so that you can define a pattern of scanned packages. Using Filters: By default, the infrastructure picks up every interface that extends the persistence technology-specific Repository sub-interface located under the configured base package and creates a bean instance for it. However, you might want more fine-grained control over which interfaces have bean instances created for them. To do so, use filter elements inside the repository declaration. The semantics are exactly equivalent to the elements in Spring’s component filters. For details, see the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/classpath-scanning.html) for these elements. For example, to exclude certain interfaces from instantiation as repository beans, you could use the following configuration: Using filters Java XML @Configuration @EnableCassandraRepositories(basePackages = ""com.acme.repositories"", includeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeRepository"") }, excludeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeOtherRepository"") }) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } <repositories base-package=""com.acme.repositories""> <context:include-filter type=""regex"" expression="".*SomeRepository"" /> <context:exclude-filter type=""regex"" expression="".*SomeOtherRepository"" /> </repositories> The preceding example includes all interfaces ending with SomeRepository and excludes those ending with SomeOtherRepository from being instantiated. Standalone Usage: You can also use the repository infrastructure outside of a Spring container — for example, in CDI environments.You still need some Spring libraries in your classpath, but, generally, you can set up repositories programmatically as well.The Spring Data modules that provide repository support ship with a persistence technology-specific RepositoryFactory that you can use, as follows: Standalone usage of the repository factory RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class);"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/query-methods-details.html","Defining Query Methods: The repository proxy has two ways to derive a store-specific query from the method name: By deriving the query from the method name directly. By using a manually defined query. Available options depend on the actual store. However, there must be a strategy that decides what actual query is created. The next section describes the available options. Query Lookup Strategies: The following strategies are available for the repository infrastructure to resolve the query. With XML configuration, you can configure the strategy at the namespace through the query-lookup-strategy attribute. For Java configuration, you can use the queryLookupStrategy attribute of the EnableCassandraRepositories annotation. Some strategies may not be supported for particular datastores. CREATE attempts to construct a store-specific query from the query method name. The general approach is to remove a given set of well known prefixes from the method name and parse the rest of the method. You can read more about query construction in “ Query Creation(#repositories.query-methods.query-creation) ”. USE_DECLARED_QUERY tries to find a declared query and throws an exception if it cannot find one. The query can be defined by an annotation somewhere or declared by other means. See the documentation of the specific store to find available options for that store. If the repository infrastructure does not find a declared query for the method at bootstrap time, it fails. CREATE_IF_NOT_FOUND (the default) combines CREATE and USE_DECLARED_QUERY . It looks up a declared query first, and, if no declared query is found, it creates a custom method name-based query. This is the default lookup strategy and, thus, is used if you do not configure anything explicitly. It allows quick query definition by method names but also custom-tuning of these queries by introducing declared queries as needed. Query Creation: The query builder mechanism built into the Spring Data repository infrastructure is useful for building constraining queries over entities of the repository. The following example shows how to create a number of queries: Query creation from method names interface PersonRepository extends Repository<Person, Long> { List<Person> findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // Enables the distinct flag for the query List<Person> findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List<Person> findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); // Enabling ignoring case for an individual property List<Person> findByLastnameIgnoreCase(String lastname); // Enabling ignoring case for all suitable properties List<Person> findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // Enabling static ORDER BY for a query List<Person> findByLastnameOrderByFirstnameAsc(String lastname); List<Person> findByLastnameOrderByFirstnameDesc(String lastname); } Parsing query method names is divided into subject and predicate. The first part ( find…By , exists…By ) defines the subject of the query, the second part forms the predicate. The introducing clause (subject) can contain further expressions. Any text between find (or other introducing keywords) and By is considered to be descriptive unless using one of the result-limiting keywords such as a Distinct to set a distinct flag on the query to be created or Top/First to limit query results(#repositories.limit-query-result) . The appendix contains the full list of query method subject keywords(query-keywords-reference.html#appendix.query.method.subject) and query method predicate keywords including sorting and letter-casing modifiers(query-keywords-reference.html#appendix.query.method.predicate) . However, the first By acts as a delimiter to indicate the start of the actual criteria predicate. At a very basic level, you can define conditions on entity properties and concatenate them with And and Or . The actual result of parsing the method depends on the persistence store for which you create the query. However, there are some general things to notice: The expressions are usually property traversals combined with operators that can be concatenated. You can combine property expressions with AND and OR . You also get support for operators such as Between , LessThan , GreaterThan , and Like for the property expressions. The supported operators can vary by datastore, so consult the appropriate part of your reference documentation. The method parser supports setting an IgnoreCase flag for individual properties (for example, findByLastnameIgnoreCase(…) ) or for all properties of a type that supports ignoring case (usually String instances — for example, findByLastnameAndFirstnameAllIgnoreCase(…) ). Whether ignoring cases is supported may vary by store, so consult the relevant sections in the reference documentation for the store-specific query method. You can apply static ordering by appending an OrderBy clause to the query method that references a property and by providing a sorting direction ( Asc or Desc ). To create a query method that supports dynamic sorting, see “ Paging, Iterating Large Results, Sorting & Limiting(#repositories.special-parameters) ”. Property Expressions: Property expressions can refer only to a direct property of the managed entity, as shown in the preceding example. At query creation time, you already make sure that the parsed property is a property of the managed domain class. However, you can also define constraints by traversing nested properties. Consider the following method signature: List<Person> findByAddressZipCode(ZipCode zipCode); Assume a Person has an Address with a ZipCode . In that case, the method creates the x.address.zipCode property traversal. The resolution algorithm starts by interpreting the entire part ( AddressZipCode ) as the property and checks the domain class for a property with that name (uncapitalized). If the algorithm succeeds, it uses that property. If not, the algorithm splits up the source at the camel-case parts from the right side into a head and a tail and tries to find the corresponding property — in our example, AddressZip and Code . If the algorithm finds a property with that head, it takes the tail and continues building the tree down from there, splitting the tail up in the way just described. If the first split does not match, the algorithm moves the split point to the left ( Address , ZipCode ) and continues. Although this should work for most cases, it is possible for the algorithm to select the wrong property. Suppose the Person class has an addressZip property as well. The algorithm would match in the first split round already, choose the wrong property, and fail (as the type of addressZip probably has no code property). To resolve this ambiguity you can use _ inside your method name to manually define traversal points. So our method name would be as follows: List<Person> findByAddress_ZipCode(ZipCode zipCode); Because we treat underscores ( _ ) as a reserved character, we strongly advise to follow standard Java naming conventions (that is, not using underscores in property names but applying camel case instead). Field Names starting with underscore: Field names may start with underscores like String _name . Make sure to preserve the _ as in _name and use double _ to split nested paths like user__name . Upper Case Field Names: Field names that are all uppercase can be used as such. Nested paths if applicable require splitting via _ as in USER_name . Field Names with 2nd uppercase letter: Field names that consist of a starting lower case letter followed by an uppercase one like String qCode can be resolved by starting with two upper case letters as in QCode . Please be aware of potential path ambiguities. Path Ambiguities: In the following sample the arrangement of properties qCode and q , with q containing a property called code , creates an ambiguity for the path QCode . record Container(String qCode, Code q) {} record Code(String code) {} Since a direct match on a property is considered first, any potential nested paths will not be considered and the algorithm picks the qCode field. In order to select the code field in q the underscore notation Q_Code is required. Repository Methods Returning Collections or Iterables: Query methods that return multiple results can use standard Java Iterable , List , and Set . Beyond that, we support returning Spring Data’s Streamable , a custom extension of Iterable , as well as collection types provided by Vavr(https://www.vavr.io/) . Refer to the appendix explaining all possible query method return types(query-return-types-reference.html#appendix.query.return.types) . Using Streamable as Query Method Return Type: You can use Streamable as alternative to Iterable or any collection type. It provides convenience methods to access a non-parallel Stream (missing from Iterable ) and the ability to directly ….filter(…) and ….map(…) over the elements and concatenate the Streamable to others: Using Streamable to combine query method results interface PersonRepository extends Repository<Person, Long> { Streamable<Person> findByFirstnameContaining(String firstname); Streamable<Person> findByLastnameContaining(String lastname); } Streamable<Person> result = repository.findByFirstnameContaining(""av"") .and(repository.findByLastnameContaining(""ea"")); Returning Custom Streamable Wrapper Types: Providing dedicated wrapper types for collections is a commonly used pattern to provide an API for a query result that returns multiple elements. Usually, these types are used by invoking a repository method returning a collection-like type and creating an instance of the wrapper type manually. You can avoid that additional step as Spring Data lets you use these wrapper types as query method return types if they meet the following criteria: The type implements Streamable . The type exposes either a constructor or a static factory method named of(…) or valueOf(…) that takes Streamable as an argument. The following listing shows an example: class Product { (1) MonetaryAmount getPrice() { … } } @RequiredArgsConstructor(staticName = ""of"") class Products implements Streamable<Product> { (2) private final Streamable<Product> streamable; public MonetaryAmount getTotal() { (3) return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); } @Override public Iterator<Product> iterator() { (4) return streamable.iterator(); } } interface ProductRepository implements Repository<Product, Long> { Products findAllByDescriptionContaining(String text); (5) } 1 A Product entity that exposes API to access the product’s price. 2 A wrapper type for a Streamable<Product> that can be constructed by using Products.of(…) (factory method created with the Lombok annotation). A standard constructor taking the Streamable<Product> will do as well. 3 The wrapper type exposes an additional API, calculating new values on the Streamable<Product> . 4 Implement the Streamable interface and delegate to the actual result. 5 That wrapper type Products can be used directly as a query method return type. You do not need to return Streamable<Product> and manually wrap it after the query in the repository client. Support for Vavr Collections: Vavr(https://www.vavr.io/) is a library that embraces functional programming concepts in Java. It ships with a custom set of collection types that you can use as query method return types, as the following table shows: Vavr collection type Used Vavr implementation type Valid Java source types io.vavr.collection.Seq io.vavr.collection.List java.util.Iterable io.vavr.collection.Set io.vavr.collection.LinkedHashSet java.util.Iterable io.vavr.collection.Map io.vavr.collection.LinkedHashMap java.util.Map You can use the types in the first column (or subtypes thereof) as query method return types and get the types in the second column used as implementation type, depending on the Java type of the actual query result (third column). Alternatively, you can declare Traversable (the Vavr Iterable equivalent), and we then derive the implementation class from the actual return value. That is, a java.util.List is turned into a Vavr List or Seq , a java.util.Set becomes a Vavr LinkedHashSet Set , and so on. Streaming Query Results: You can process the results of query methods incrementally by using a Java 8 Stream<T> as the return type. Instead of wrapping the query results in a Stream , data store-specific methods are used to perform the streaming, as shown in the following example: Stream the result of a query with Java 8 Stream<T> @Query(""select u from User u"") Stream<User> findAllByCustomQueryAndStream(); Stream<User> readAllByFirstnameNotNull(); @Query(""select u from User u"") Stream<User> streamAllPaged(Pageable pageable); A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example: Working with a Stream<T> result in a try-with-resources block try (Stream<User> stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } Not all Spring Data modules currently support Stream<T> as a return type. Asynchronous Query Results: You can run repository queries asynchronously by using Spring’s asynchronous method running capability(https://docs.spring.io/spring-framework/reference/6.1/integration/scheduling.html) . This means the method returns immediately upon invocation while the actual query occurs in a task that has been submitted to a Spring TaskExecutor . Asynchronous queries differ from reactive queries and should not be mixed. See the store-specific documentation for more details on reactive support. The following example shows a number of asynchronous queries: @Async Future<User> findByFirstname(String firstname); (1) @Async CompletableFuture<User> findOneByFirstname(String firstname); (2) 1 Use java.util.concurrent.Future as the return type. 2 Use a Java 8 java.util.concurrent.CompletableFuture as the return type. Paging, Iterating Large Results, Sorting & Limiting: To handle parameters in your query, define method parameters as already seen in the preceding examples. Besides that, the infrastructure recognizes certain specific types like Pageable , Sort and Limit , to apply pagination, sorting and limiting to your queries dynamically. The following example demonstrates these features: Using Pageable , Slice , Sort and Limit in query methods Page<User> findByLastname(String lastname, Pageable pageable); Slice<User> findByLastname(String lastname, Pageable pageable); List<User> findByLastname(String lastname, Sort sort); List<User> findByLastname(String lastname, Sort sort, Limit limit); List<User> findByLastname(String lastname, Pageable pageable); APIs taking Sort , Pageable and Limit expect non- null values to be handed into methods. If you do not want to apply any sorting or pagination, use Sort.unsorted() , Pageable.unpaged() and Limit.unlimited() . The first method lets you pass an org.springframework.data.domain.Pageable instance to the query method to dynamically add paging to your statically defined query. A Page knows about the total number of elements and pages available. It does so by the infrastructure triggering a count query to calculate the overall number. As this might be expensive (depending on the store used), you can instead return a Slice . A Slice knows only about whether a next Slice is available, which might be sufficient when walking through a larger result set. Sorting options are handled through the Pageable instance, too. If you need only sorting, add an org.springframework.data.domain.Sort parameter to your method. As you can see, returning a List is also possible. In this case, the additional metadata required to build the actual Page instance is not created (which, in turn, means that the additional count query that would have been necessary is not issued). Rather, it restricts the query to look up only the given range of entities. To find out how many pages you get for an entire query, you have to trigger an additional count query. By default, this query is derived from the query you actually trigger. Special parameters may only be used once within a query method. Some special parameters described above are mutually exclusive. Please consider the following list of invalid parameter combinations. Parameters Example Reason Pageable and Sort findBy…​(Pageable page, Sort sort) Pageable already defines Sort Pageable and Limit findBy…​(Pageable page, Limit limit) Pageable already defines a limit. The Top keyword used to limit results can be used to along with Pageable whereas Top defines the total maximum of results, whereas the Pageable parameter may reduce this number. Which Method is Appropriate?: The value provided by the Spring Data abstractions is perhaps best shown by the possible query method return types outlined in the following table below. The table shows which types you can return from a query method Table 1. Consuming Large Query Results Method Amount of Data Fetched Query Structure Constraints List<T>(#repositories.collections-and-iterables) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Streamable<T>(#repositories.collections-and-iterables.streamable) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Stream<T>(#repositories.query-streaming) Chunked (one-by-one or in batches) depending on Stream consumption. Single query using typically cursors. Streams must be closed after usage to avoid resource leaks. Flux<T> Chunked (one-by-one or in batches) depending on Flux consumption. Single query using typically cursors. Store module must provide reactive infrastructure. Slice<T> Pageable.getPageSize() + 1 at Pageable.getOffset() One to many queries fetching data starting at Pageable.getOffset() applying limiting. A Slice can only navigate to the next Slice . Slice provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Window provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Page<T> Pageable.getPageSize() at Pageable.getOffset() One to many queries starting at Pageable.getOffset() applying limiting. Additionally, COUNT(…) query to determine the total number of elements can be required. Often times, COUNT(…) queries are required that are costly. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Paging and Sorting: You can define simple sorting expressions by using property names. You can concatenate expressions to collect multiple criteria into one expression. Defining sort expressions Sort sort = Sort.by(""firstname"").ascending() .and(Sort.by(""lastname"").descending()); For a more type-safe way to define sort expressions, start with the type for which to define the sort expression and use method references to define the properties on which to sort. Defining sort expressions by using the type-safe API TypedSort<Person> person = Sort.sort(Person.class); Sort sort = person.by(Person::getFirstname).ascending() .and(person.by(Person::getLastname).descending()); TypedSort.by(…) makes use of runtime proxies by (typically) using CGlib, which may interfere with native image compilation when using tools such as Graal VM Native. If your store implementation supports Querydsl, you can also use the generated metamodel types to define sort expressions: Defining sort expressions by using the Querydsl API QSort sort = QSort.by(QPerson.firstname.asc()) .and(QSort.by(QPerson.lastname.desc())); Limiting Query Results: In addition to paging it is possible to limit the result size using a dedicated Limit parameter. You can also limit the results of query methods by using the First or Top keywords, which you can use interchangeably but may not be mixed with a Limit parameter. You can append an optional numeric value to Top or First to specify the maximum result size to be returned. If the number is left out, a result size of 1 is assumed. The following example shows how to limit the query size: Limiting the result size of a query with Top and First List<User> findByLastname(Limit limit); User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page<User> queryFirst10ByLastname(String lastname, Pageable pageable); Slice<User> findTop3ByLastname(String lastname, Pageable pageable); List<User> findFirst10ByLastname(String lastname, Sort sort); List<User> findTop10ByLastname(String lastname, Pageable pageable); The limiting expressions also support the Distinct keyword for datastores that support distinct queries. Also, for the queries that limit the result set to one instance, wrapping the result into with the Optional keyword is supported. If pagination or slicing is applied to a limiting query pagination (and the calculation of the number of available pages), it is applied within the limited result. Limiting the results in combination with dynamic sorting by using a Sort parameter lets you express query methods for the 'K' smallest as well as for the 'K' biggest elements."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/repositories/query-methods.html","Cassandra-specific Query Methods: This chapter explains Cassandra-specific query methods. This documentation uses imperative types. By using reactive return types, the same semantics apply to reactive repositories as well. Most of the data access operations you usually trigger on a repository result in a query being executed against the Apache Cassandra database. Defining such a query is a matter of declaring a method on the repository interface. The following example shows a number of such method declarations: PersonRepository with query methods Imperative Reactive interface PersonRepository extends CrudRepository<Person, String> { List<Person> findByLastname(String lastname); (1) Slice<Person> findByFirstname(String firstname, Pageable pageRequest); (2) Window<Person> findByFirstname(String firstname, CassandraScrollPosition pos, Limit limit); (3) List<Person> findByFirstname(String firstname, QueryOptions opts); (4) List<Person> findByFirstname(String firstname, Sort sort); (5) List<Person> findByFirstname(String firstname, Limit limit); (6) Person findByShippingAddress(Address address); (7) Person findFirstByShippingAddress(Address address); (8) Stream<Person> findAllBy(); (9) @AllowFiltering List<Person> findAllByAge(int age); (10) } 1 The method shows a query for all people with the given lastname . The query is derived from parsing the method name for constraints, which can be concatenated with And . Thus, the method name results in a query expression of SELECT * FROM person WHERE lastname = 'lastname' . 2 Applies pagination to a query. You can equip your method signature with a Pageable parameter and let the method return a Slice instance, and we automatically page the query accordingly. 3 Applies scrolling to a query. Scrolling wraps Cassandra’s PagingState into CassandraScrollPosition and allows dynamic limiting. You can also use findTop… for a static limit. 4 Passing a QueryOptions object applies the query options to the resulting query before its execution. 5 Applies dynamic sorting to a query. You can add a Sort parameter to your method signature, and Spring Data automatically applies ordering to the query. 6 Applies dynamic result limiting to a query. Query results can be limited using SELECT … LIMIT . 7 Shows that you can query based on properties that are not a primitive type by using Converter instances registered in CustomConversions . Throws IncorrectResultSizeDataAccessException if more than one match is found. 8 Uses the First keyword to restrict the query to only the first result. Unlike the preceding method, this method does not throw an exception if more than one match is found. 9 Uses a Java 8 Stream to read and convert individual elements while iterating the stream. 10 Shows a query method annotated with @AllowFiltering , to allow server-side filtering. interface ReactivePersonRepository extends ReactiveSortingRepository<Person, Long> { Flux<Person> findByFirstname(String firstname); (1) Flux<Person> findByFirstname(Publisher<String> firstname); (2) Mono<Person> findByFirstnameAndLastname(String firstname, String lastname); (3) Mono<Person> findFirstByFirstname(String firstname); (4) @AllowFiltering Flux<Person> findByAge(int age); (5) } 1 A query for all people with the given firstname . The query is derived by parsing the method name for constraints, which can be concatenated with And and Or . Thus, the method name results in a query expression of SELECT * FROM person WHERE firstname = :firstname . 2 A query for all people with the given firstname once the firstname is emitted from the given Publisher . 3 Find a single entity for the given criteria. Completes with IncorrectResultSizeDataAccessException on non-unique results. 4 Unlike the preceding query, the first entity is always emitted even if the query yields more result rows. 5 A query method annotated with @AllowFiltering , which allows server-side filtering. Querying non-primary key properties requires secondary indexes. The following table shows short examples of the keywords that you can use in query methods: Table 1. Supported keywords for query methods Keyword Sample Logical result After findByBirthdateAfter(Date date) birthdate > date GreaterThan findByAgeGreaterThan(int age) age > age GreaterThanEqual findByAgeGreaterThanEqual(int age) age >= age Before findByBirthdateBefore(Date date) birthdate < date LessThan findByAgeLessThan(int age) age < age LessThanEqual findByAgeLessThanEqual(int age) age ⇐ age Between findByAgeBetween(int from, int to) and findByAgeBetween(Range<Integer> range) age > from AND age < to and lower / upper bounds ( > / >= & < / ⇐ ) according to Range In findByAgeIn(Collection ages) age IN (ages…​) Like , StartingWith , EndingWith findByFirstnameLike(String name) firstname LIKE (name as like expression) Containing on String findByFirstnameContaining(String name) firstname LIKE (name as like expression) Containing on Collection findByAddressesContaining(Address address) addresses CONTAINING address (No keyword) findByFirstname(String name) firstname = name IsTrue , True findByActiveIsTrue() active = true IsFalse , False findByActiveIsFalse() active = false Repository Delete Queries: The keywords in the preceding table can be used in conjunction with delete…By to create queries that delete matching documents. Imperative Reactive interface PersonRepository extends Repository<Person, String> { void deleteWithoutResultByLastname(String lastname); boolean deleteByLastname(String lastname); } interface PersonRepository extends Repository<Person, String> { Mono<Void> deleteWithoutResultByLastname(String lastname); Mono<Boolean> deleteByLastname(String lastname); } Delete queries return whether the query was applied or terminate without returning a value using void . Query Options: You can specify query options for query methods by passing a QueryOptions object. The options apply to the query before the actual query execution. QueryOptions is treated as a non-query parameter and is not considered to be a query parameter value. Query options apply to derived and string @Query repository methods. To statically set the consistency level, use the @Consistency annotation on query methods. The declared consistency level is applied to the query each time it is executed. The following example sets the consistency level to ConsistencyLevel.LOCAL_ONE : Imperative Reactive interface PersonRepository extends CrudRepository<Person, String> { @Consistency(ConsistencyLevel.LOCAL_ONE) List<Person> findByLastname(String lastname); List<Person> findByFirstname(String firstname, QueryOptions options); } interface PersonRepository extends ReactiveCrudRepository<Person, String> { @Consistency(ConsistencyLevel.LOCAL_ONE) Flux<Person> findByLastname(String lastname); Flux<Person> findByFirstname(String firstname, QueryOptions options); } The DataStax Cassandra documentation includes a good discussion of the available consistency levels(https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlConfigConsistency.html) . You can control fetch size, consistency level, and retry policy defaults by configuring the following parameters on the CQL API instances: CqlTemplate , AsyncCqlTemplate , and ReactiveCqlTemplate . Defaults apply if the particular query option is not set."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/projections.html","Projections: Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> ."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableCassandraRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableCassandraRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" />"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/null-handling.html","Null Handling of Repository Methods: As of Spring Data 2.0, repository CRUD methods that return an individual aggregate instance use Java 8’s Optional to indicate the potential absence of a value. Besides that, Spring Data supports returning the following wrapper types on query methods: com.google.common.base.Optional scala.Option io.vavr.control.Option Alternatively, query methods can choose not to use a wrapper type at all. The absence of a query result is then indicated by returning null . Repository methods returning collections, collection alternatives, wrappers, and streams are guaranteed never to return null but rather the corresponding empty representation. See “ Repository query return types(query-return-types-reference.html) ” for details. Nullability Annotations: You can express nullability constraints for repository methods by using Spring Framework’s nullability annotations(https://docs.spring.io/spring-framework/reference/6.1/core/null-safety.html) . They provide a tooling-friendly approach and opt-in null checks during runtime, as follows: @NonNullApi(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNullApi.html) : Used on the package level to declare that the default behavior for parameters and return values is, respectively, neither to accept nor to produce null values. @NonNull(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNull.html) : Used on a parameter or return value that must not be null (not needed on a parameter and return value where @NonNullApi applies). @Nullable(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/Nullable.html) : Used on a parameter or return value that can be null . Spring annotations are meta-annotated with JSR 305(https://jcp.org/en/jsr/detail?id=305) annotations (a dormant but widely used JSR). JSR 305 meta-annotations let tooling vendors (such as IDEA(https://www.jetbrains.com/help/idea/nullable-and-notnull-annotations.html) , Eclipse(https://help.eclipse.org/latest/index.jsp?topic=/org.eclipse.jdt.doc.user/tasks/task-using_external_null_annotations.htm) , and Kotlin(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) ) provide null-safety support in a generic way, without having to hard-code support for Spring annotations. To enable runtime checking of nullability constraints for query methods, you need to activate non-nullability on the package level by using Spring’s @NonNullApi in package-info.java , as shown in the following example: Declaring Non-nullability in package-info.java @org.springframework.lang.NonNullApi package com.acme; Once non-null defaulting is in place, repository query method invocations get validated at runtime for nullability constraints. If a query result violates the defined constraint, an exception is thrown. This happens when the method would return null but is declared as non-nullable (the default with the annotation defined on the package in which the repository resides). If you want to opt-in to nullable results again, selectively use @Nullable on individual methods. Using the result wrapper types mentioned at the start of this section continues to work as expected: an empty result is translated into the value that represents absence. The following example shows a number of the techniques just described: Using different nullability constraints package com.acme; (1) import org.springframework.lang.Nullable; interface UserRepository extends Repository<User, Long> { User getByEmailAddress(EmailAddress emailAddress); (2) @Nullable User findByEmailAddress(@Nullable EmailAddress emailAdress); (3) Optional<User> findOptionalByEmailAddress(EmailAddress emailAddress); (4) } 1 The repository resides in a package (or sub-package) for which we have defined non-null behavior. 2 Throws an EmptyResultDataAccessException when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . 3 Returns null when the query does not produce a result. Also accepts null as the value for emailAddress . 4 Returns Optional.empty() when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . Nullability in Kotlin-based Repositories: Kotlin has the definition of nullability constraints(https://kotlinlang.org/docs/reference/null-safety.html) baked into the language. Kotlin code compiles to bytecode, which does not express nullability constraints through method signatures but rather through compiled-in metadata. Make sure to include the kotlin-reflect JAR in your project to enable introspection of Kotlin’s nullability constraints. Spring Data repositories use the language mechanism to define those constraints to apply the same runtime checks, as follows: Using nullability constraints on Kotlin repositories interface UserRepository : Repository<User, String> { fun findByUsername(username: String): User (1) fun findByFirstname(firstname: String?): User? (2) } 1 The method defines both the parameter and the result as non-nullable (the Kotlin default). The Kotlin compiler rejects method invocations that pass null to the method. If the query yields an empty result, an EmptyResultDataAccessException is thrown. 2 This method accepts null for the firstname parameter and returns null if the query does not produce a result."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/cassandra/repositories/cdi-integration.html","CDI Integration: Instances of the repository interfaces are usually created by a container, and the Spring container is the most natural choice when working with Spring Data. Spring Data for Apache Cassandra ships with a custom CDI extension that allows using the repository abstraction in CDI environments. The extension is part of the JAR.To activate it, drop the Spring Data for Apache Cassandra JAR into your classpath. You can now set up the infrastructure by implementing a CDI Producer for the CassandraTemplate , as the following examlpe shows: class CassandraTemplateProducer { @Produces @Singleton public CqlSession createSession() { return CqlSession.builder().withKeyspace(""my-keyspace"").build(); } @Produces @ApplicationScoped public CassandraOperations createCassandraOperations(CqlSession session) { CassandraMappingContext mappingContext = new CassandraMappingContext(); mappingContext.afterPropertiesSet(); MappingCassandraConverter cassandraConverter = new MappingCassandraConverter(mappingContext); cassandraConverter.setUserTypeResolver(new SimpleUserTypeResolver(session)); cassandraConverter.afterPropertiesSet(); return new CassandraAdminTemplate(session, cassandraConverter); } public void close(@Disposes CqlSession session) { session.close(); } } The Spring Data for Apache Cassandra CDI extension picks up CassandraOperations as a CDI bean and creates a proxy for a Spring Data repository whenever a bean of a repository type is requested by the container. Thus, obtaining an instance of a Spring Data repository is a matter of declaring an injected property, as the following example shows: class RepositoryClient { @Inject PersonRepository repository; public void businessMethod() { List<Person> people = repository.findAll(); } }"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/observability.html","Observability: Getting insights from an application component about its operations, timing and relation to application code is crucial to understand latency. Spring Data Cassandra ships with a Micrometer instrumentation through the Cassandra driver to collect observations during Cassandra interaction. Once the integration is set up, Micrometer will create meters and spans (for distributed tracing) for each Cassandra statement. To enable the instrumentation, apply the following configuration to your application: @Configuration class ObservabilityConfiguration { @Bean public ObservableCqlSessionFactoryBean observableCqlSession(CqlSessionBuilder builder, ObservationRegistry registry) { return new ObservableCqlSessionFactoryBean(builder, registry); (1) } @Bean public ObservableReactiveSessionFactoryBean observableReactiveSession(CqlSession session, ObservationRegistry registry) { return new ObservableReactiveSessionFactoryBean(session, registry); (2) } } 1 Wraps the CQL session object to observe Cassandra statement execution. Also, registers ObservationRequestTracker.INSTANCE with the CqlSessionBuilder . 2 Wraps a CQL session object to observe reactive Cassandra statement execution. Both, ObservableCqlSessionFactoryBean(api/java/org/springframework/data/cassandra/observability/ObservableCqlSessionFactoryBean.html) and ObservableReactiveSessionFactoryBean(api/java/org/springframework/data/cassandra/observability/ObservableReactiveSessionFactoryBean.html) support configuration of CassandraObservationConvention(api/java/org/springframework/data/cassandra/observability/CassandraObservationConvention.html) . See also OpenTelemetry Semantic Conventions(https://opentelemetry.io/docs/reference/specification/trace/semantic_conventions/database/#cassandra) for further reference. Conventions: Below you can find a list of all GlobalObservabilityConventions and ObservabilityConventions declared by this project. Table 1. ObservationConvention implementations ObservationConvention Class Name Applicable ObservationContext Class Name org.springframework.data.cassandra.observability.DefaultCassandraObservationConvention n/a Metrics: Below you can find a list of all metrics declared by this project. Cassandra Query Observation: Create an io.micrometer.observation.Observation for Cassandra-based queries. Metric name spring.data.cassandra.query . Type timer and base unit seconds . Fully qualified name of the enclosing class org.springframework.data.cassandra.observability.CassandraObservation . Table 2. Low cardinality Keys Name Description db.cassandra.coordinator.dc db.cassandra.coordinator.id db.name Name of the Cassandra keyspace. db.operation The database operation. db.system Database system. net.peer.name Name of the database host. net.peer.port Logical remote port number. net.sock.peer.addr Cassandra peer address. net.sock.peer.port Cassandra peer port. net.transport Network transport. spring.data.cassandra.methodName The method name spring.data.cassandra.sessionName Cassandra session Table 3. High cardinality Keys Name Description db.cassandra.consistency_level db.cassandra.idempotence db.cassandra.page_size db.statement A key-value containing Cassandra CQL. spring.data.cassandra.node[%s].error A tag containing error that occurred for the given node. (since the name contains %s the final value will be resolved at runtime) Spans: Below you can find a list of all spans declared by this project. Cassandra Query Observation Span: Create an io.micrometer.observation.Observation for Cassandra-based queries. Span name spring.data.cassandra.query . Fully qualified name of the enclosing class org.springframework.data.cassandra.observability.CassandraObservation . Table 4. Tag Keys Name Description db.cassandra.consistency_level db.cassandra.coordinator.dc db.cassandra.coordinator.id db.cassandra.idempotence db.cassandra.page_size db.name Name of the Cassandra keyspace. db.operation The database operation. db.statement A key-value containing Cassandra CQL. db.system Database system. net.peer.name Name of the database host. net.peer.port Logical remote port number. net.sock.peer.addr Cassandra peer address. net.sock.peer.port Cassandra peer port. net.transport Network transport. spring.data.cassandra.methodName The method name spring.data.cassandra.node[%s].error A tag containing error that occurred for the given node. (since the name contains %s the final value will be resolved at runtime) spring.data.cassandra.sessionName Cassandra session"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/kotlin.html","Kotlin Support: Kotlin(https://kotlinlang.org) is a statically typed language that targets the JVM (and other platforms) which allows writing concise and elegant code while providing excellent interoperability(https://kotlinlang.org/docs/reference/java-interop.html) with existing libraries written in Java. Spring Data provides first-class support for Kotlin and lets developers write Kotlin applications almost as if Spring Data was a Kotlin native framework. The easiest way to build a Spring application with Kotlin is to leverage Spring Boot and its dedicated Kotlin support(https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-kotlin.html) . This comprehensive tutorial(https://spring.io/guides/tutorials/spring-boot-kotlin/) will teach you how to build Spring Boot applications with Kotlin using start.spring.io(https://start.spring.io/#!language=kotlin&type=gradle-project) . Section Summary: Requirements(kotlin/requirements.html) Null Safety(kotlin/null-safety.html) Object Mapping(kotlin/object-mapping.html) Extensions(kotlin/extensions.html) Coroutines(kotlin/coroutines.html)"
"https://docs.spring.io/spring-data/cassandra/reference/4.3/kotlin/requirements.html","Requirements: Spring Data supports Kotlin 1.3 and requires kotlin-stdlib (or one of its variants, such as kotlin-stdlib-jdk8 ) and kotlin-reflect to be present on the classpath. Those are provided by default if you bootstrap a Kotlin project via start.spring.io(https://start.spring.io/#!language=kotlin&type=gradle-project) ."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/kotlin/null-safety.html","Null Safety: One of Kotlin’s key features is null safety(https://kotlinlang.org/docs/null-safety.html) , which cleanly deals with null values at compile time. This makes applications safer through nullability declarations and the expression of “value or no value” semantics without paying the cost of wrappers, such as Optional . (Kotlin allows using functional constructs with nullable values. See this comprehensive guide to Kotlin null safety(https://www.baeldung.com/kotlin/null-safety) .) Although Java does not let you express null safety in its type system, Spring Data API is annotated with JSR-305(https://jcp.org/en/jsr/detail?id=305) tooling friendly annotations declared in the org.springframework.lang package. By default, types from Java APIs used in Kotlin are recognized as platform types(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) , for which null checks are relaxed. Kotlin support for JSR-305 annotations(https://kotlinlang.org/docs/reference/java-interop.html#jsr-305-support) and Spring nullability annotations provide null safety for the whole Spring Data API to Kotlin developers, with the advantage of dealing with null related issues at compile time. See Null Handling of Repository Methods(../repositories/null-handling.html) how null safety applies to Spring Data Repositories. You can configure JSR-305 checks by adding the -Xjsr305 compiler flag with the following options: -Xjsr305={strict|warn|ignore} . For Kotlin versions 1.1+, the default behavior is the same as -Xjsr305=warn . The strict value is required take Spring Data API null-safety into account. Kotlin types inferred from Spring API but should be used with the knowledge that Spring API nullability declaration could evolve, even between minor releases and that more checks may be added in the future. Generic type arguments, varargs, and array elements nullability are not supported yet, but should be in an upcoming release."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/kotlin/object-mapping.html","Object Mapping: See Kotlin support(../object-mapping.html#mapping.kotlin) for details on how Kotlin objects are materialized."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/kotlin/extensions.html","Extensions: Kotlin extensions(https://kotlinlang.org/docs/reference/extensions.html) provide the ability to extend existing classes with additional functionality. Spring Data Kotlin APIs use these extensions to add new Kotlin-specific conveniences to existing Spring APIs. Keep in mind that Kotlin extensions need to be imported to be used. Similar to static imports, an IDE should automatically suggest the import in most cases. For example, Kotlin reified type parameters(https://kotlinlang.org/docs/reference/inline-functions.html#reified-type-parameters) provide a workaround for JVM generics type erasure(https://docs.oracle.com/javase/tutorial/java/generics/erasure.html) , and Spring Data provides some extensions to take advantage of this feature. This allows for a better Kotlin API. To retrieve a list of SWCharacter objects in Java, you would normally write the following: Flux<SWCharacter> characters = template.query(SWCharacter.class).inTable(""star-wars"").all() With Kotlin and the Spring Data extensions, you can instead write the following: val characters = template.query<SWCharacter>().inTable(""star-wars"").all() // or (both are equivalent) val characters : Flux<SWCharacter> = template.query().inTable(""star-wars"").all() As in Java, characters in Kotlin is strongly typed, but Kotlin’s clever type inference allows for shorter syntax. Spring Data for Apache Cassandra provides the following extensions: Reified generics support for CassandraOperations (including async and reactive variants), CqlOperations (including async and reactive variants) FluentCassandraOperations , ReactiveFluentCassandraOperations , Criteria , and Query . [kotlin.coroutines](#kotlin.coroutines) extensions for ReactiveFluentCassandraOperations ."
"https://docs.spring.io/spring-data/cassandra/reference/4.3/kotlin/coroutines.html","Coroutines: Kotlin Coroutines(https://kotlinlang.org/docs/reference/coroutines-overview.html) are instances of suspendable computations allowing to write non-blocking code imperatively. On language side, suspend functions provides an abstraction for asynchronous operations while on library side kotlinx.coroutines(https://github.com/Kotlin/kotlinx.coroutines) provides functions like async { }(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/async.html) and types like Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) . Spring Data modules provide support for Coroutines on the following scope: Deferred(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/-deferred/index.html) and Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) return values support in Kotlin extensions Dependencies: Coroutines support is enabled when kotlinx-coroutines-core , kotlinx-coroutines-reactive and kotlinx-coroutines-reactor dependencies are in the classpath: Dependencies to add in Maven pom.xml <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-core</artifactId> </dependency> <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-reactive</artifactId> </dependency> <dependency> <groupId>org.jetbrains.kotlinx</groupId> <artifactId>kotlinx-coroutines-reactor</artifactId> </dependency> Supported versions 1.3.0 and above. How Reactive translates to Coroutines?: For return values, the translation from Reactive to Coroutines APIs is the following: fun handler(): Mono<Void> becomes suspend fun handler() fun handler(): Mono<T> becomes suspend fun handler(): T or suspend fun handler(): T? depending on if the Mono can be empty or not (with the advantage of being more statically typed) fun handler(): Flux<T> becomes fun handler(): Flow<T> Flow(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/index.html) is Flux equivalent in Coroutines world, suitable for hot or cold stream, finite or infinite streams, with the following main differences: Flow is push-based while Flux is push-pull hybrid Backpressure is implemented via suspending functions Flow has only a single suspending collect method(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/-flow/collect.html) and operators are implemented as extensions(https://kotlinlang.org/docs/reference/extensions.html) Operators are easy to implement(https://github.com/Kotlin/kotlinx.coroutines/tree/master/kotlinx-coroutines-core/common/src/flow/operators) thanks to Coroutines Extensions allow adding custom operators to Flow Collect operations are suspending functions map operator(https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines.flow/map.html) supports asynchronous operation (no need for flatMap ) since it takes a suspending function parameter Read this blog post about Going Reactive with Spring, Coroutines and Kotlin Flow(https://spring.io/blog/2019/04/12/going-reactive-with-spring-coroutines-and-kotlin-flow) for more details, including how to run code concurrently with Coroutines. Repositories: Here is an example of a Coroutines repository: interface CoroutineRepository : CoroutineCrudRepository<User, String> { suspend fun findOne(id: String): User fun findByFirstname(firstname: String): Flow<User> suspend fun findAllByFirstname(id: String): List<User> } Coroutines repositories are built on reactive repositories to expose the non-blocking nature of data access through Kotlin’s Coroutines. Methods on a Coroutines repository can be backed either by a query method or a custom implementation. Invoking a custom implementation method propagates the Coroutines invocation to the actual implementation method if the custom method is suspend -able without requiring the implementation method to return a reactive type such as Mono or Flux . Note that depending on the method declaration the coroutine context may or may not be available. To retain access to the context, either declare your method using suspend or return a type that enables context propagation such as Flow . suspend fun findOne(id: String): User : Retrieve the data once and synchronously by suspending. fun findByFirstname(firstname: String): Flow<User> : Retrieve a stream of data. The Flow is created eagerly while data is fetched upon Flow interaction ( Flow.collect(…) ). fun getUser(): User : Retrieve data once blocking the thread and without context propagation. This should be avoided. Coroutines repositories are only discovered when the repository extends the CoroutineCrudRepository interface."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/index.html","Spring Data Elasticsearch: Spring Data Elasticsearch provides repository support for the Elasticsearch database. It eases development of applications with a consistent programming model that need to access Elasticsearch data sources. Versions(elasticsearch/versions.html) Version Compatibility Matrix Clients(elasticsearch/clients.html) Elasticsearch Client Configuration Elasticsearch(elasticsearch.html) Elasticsearch support Repositories(repositories.html) Elasticsearch Repositories Migration(migration-guides.html) Migration Guides Wiki(https://github.com/spring-projects/spring-data-commons/wiki) What’s New, Upgrade Notes, Supported Versions, additional cross-version information. BioMed Central Development Team; Oliver Drotbohm; Greg Turnquist; Christoph Strobl; Peter-Josef Meisch © 2008-2024 VMware, Inc. Copies of this document may be made for your own use and for distribution to others, provided that you do not charge any fee for such copies and further provided that each copy contains this Copyright Notice, whether distributed in print or electronically."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/commons/upgrade.html","Upgrading Spring Data: Instructions for how to upgrade from earlier versions of Spring Data are provided on the project wiki(https://github.com/spring-projects/spring-data-commons/wiki) . Follow the links in the release notes section(https://github.com/spring-projects/spring-data-commons/wiki#release-notes) to find the version that you want to upgrade to. Upgrading instructions are always the first item in the release notes. If you are more than one release behind, please make sure that you also review the release notes of the versions that you jumped."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides.html","Migration Guides: This section contains version-specific migration guides explaining how to upgrade across versions. Section Summary: Upgrading from 3.2.x to 4.0.x(migration-guides/migration-guide-3.2-4.0.html) Upgrading from 4.0.x to 4.1.x(migration-guides/migration-guide-4.0-4.1.html) Upgrading from 4.1.x to 4.2.x(migration-guides/migration-guide-4.1-4.2.html) Upgrading from 4.2.x to 4.3.x(migration-guides/migration-guide-4.2-4.3.html) Upgrading from 4.3.x to 4.4.x(migration-guides/migration-guide-4.3-4.4.html) Upgrading from 4.4.x to 5.0.x(migration-guides/migration-guide-4.4-5.0.html) Upgrading from 5.0.x to 5.1.x(migration-guides/migration-guide-5.0-5.1.html) Upgrading from 5.1.x to 5.2.x(migration-guides/migration-guide-5.1-5.2.html) Upgrading from 5.2.x to 5.3.x(migration-guides/migration-guide-5.2-5.3.html)"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-3.2-4.0.html","Upgrading from 3.2.x to 4.0.x: This section describes breaking changes from version 3.2.x to 4.0.x and how removed features can be replaced by new introduced features. Removal of the used Jackson Mapper: One of the changes in version 4.0.x is that Spring Data Elasticsearch does not use the Jackson Mapper anymore to map an entity to the JSON representation needed for Elasticsearch (see Elasticsearch Object Mapping(../elasticsearch/object-mapping.html) ). In version 3.2.x the Jackson Mapper was the default that was used. It was possible to switch to the meta-model based converter (named ElasticsearchEntityMapper ) by explicitly configuring it ( Meta Model Object Mapping(../elasticsearch/object-mapping.html#elasticsearch.mapping.meta-model) ). In version 4.0.x the meta-model based converter is the only one that is available and does not need to be configured explicitly. If you had a custom configuration to enable the meta-model converter by providing a bean like this: @Bean @Override public EntityMapper entityMapper() { ElasticsearchEntityMapper entityMapper = new ElasticsearchEntityMapper( elasticsearchMappingContext(), new DefaultConversionService() ); entityMapper.setConversions(elasticsearchCustomConversions()); return entityMapper; } You now have to remove this bean, the ElasticsearchEntityMapper interface has been removed. Entity configuration Some users had custom Jackson annotations on the entity class, for example in order to define a custom name for the mapped document in Elasticsearch or to configure date conversions. These are not taken into account anymore. The needed functionality is now provided with Spring Data Elasticsearch’s @Field annotation. Please see Mapping Annotation Overview(../elasticsearch/object-mapping.html#elasticsearch.mapping.meta-model.annotations) for detailed information. Removal of implicit index name from query objects: In 3.2.x the different query classes like IndexQuery or SearchQuery had properties that were taking the index name or index names that they were operating upon.If these were not set, the passed in entity was inspected to retrieve the index name that was set in the @Document annotation. In 4.0.x the index name(s) must now be provided in an additional parameter of type IndexCoordinates .By separating this, it now is possible to use one query object against different indices. So for example the following code: IndexQuery indexQuery = new IndexQueryBuilder() .withId(person.getId().toString()) .withObject(person) .build(); String documentId = elasticsearchOperations.index(indexQuery); must be changed to: IndexCoordinates indexCoordinates = elasticsearchOperations.getIndexCoordinatesFor(person.getClass()); IndexQuery indexQuery = new IndexQueryBuilder() .withId(person.getId().toString()) .withObject(person) .build(); String documentId = elasticsearchOperations.index(indexQuery, indexCoordinates); To make it easier to work with entities and use the index name that is contained in the entitie’s @Document annotation, new methods have been added like DocumentOperations.save(T entity) ; The new Operations interfaces: In version 3.2 there was the ElasticsearchOperations interface that defined all the methods for the ElasticsearchTemplate class. In version 4 the functions have been split into different interfaces, aligning these interfaces with the Elasticsearch API: DocumentOperations are the functions related documents like saving, or deleting SearchOperations contains the functions to search in Elasticsearch IndexOperations define the functions to operate on indexes, like index creation or mappings creation. ElasticsearchOperations now extends DocumentOperations and SearchOperations and has methods get access to an IndexOperations instance. All the functions from the ElasticsearchOperations interface in version 3.2 that are now moved to the IndexOperations interface are still available, they are marked as deprecated and have default implementations that delegate to the new implementation: /** * Create an index for given indexName. * * @param indexName the name of the index * @return {@literal true} if the index was created * @deprecated since 4.0, use {@link IndexOperations#create()} */ @Deprecated default boolean createIndex(String indexName) { return indexOps(IndexCoordinates.of(indexName)).create(); } Deprecations: Methods and classes: Many functions and classes have been deprecated. These functions still work, but the Javadocs show with what they should be replaced. Example from ElasticsearchOperations /* * Retrieves an object from an index. * * @param query the query defining the id of the object to get * @param clazz the type of the object to be returned * @return the found object * @deprecated since 4.0, use {@link #get(String, Class, IndexCoordinates)} */ @Deprecated @Nullable <T> T queryForObject(GetQuery query, Class<T> clazz); Elasticsearch deprecations: Since version 7 the Elasticsearch TransportClient is deprecated, it will be removed with Elasticsearch version 8. Spring Data Elasticsearch deprecates the ElasticsearchTemplate class which uses the TransportClient in version 4.0. Mapping types were removed from Elasticsearch 7, they still exist as deprecated values in the Spring Data @Document annotation and the IndexCoordinates class but they are not used anymore internally. Removals: As already described, the ElasticsearchEntityMapper interface has been removed. The SearchQuery interface has been merged into it’s base interface Query , so it’s occurrences can just be replaced with Query . The method org.springframework.data.elasticsearch.core.ElasticsearchOperations.query(SearchQuery query, ResultsExtractor<T> resultsExtractor); and the org.springframework.data.elasticsearch.core.ResultsExtractor interface have been removed. These could be used to parse the result from Elasticsearch for cases in which the response mapping done with the Jackson based mapper was not enough. Since version 4.0, there are the new Search Result Types(../elasticsearch/template.html#elasticsearch.operations.searchresulttypes) to return the information from an Elasticsearch response, so there is no need to expose this low level functionality. The low level methods startScroll , continueScroll and clearScroll have been removed from the ElasticsearchOperations interface. For low level scroll API access, there now are searchScrollStart , searchScrollContinue and searchScrollClear methods on the ElasticsearchRestTemplate class."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-4.0-4.1.html","Upgrading from 4.0.x to 4.1.x: This section describes breaking changes from version 4.0.x to 4.1.x and how removed features can be replaced by new introduced features. Deprecations: Definition of the id property It is possible to define a property of en entity as the id property by naming it either id or document . This behaviour is now deprecated and will produce a warning. Please use the @Id annotation to mark a property as being the id property. Index mappings In the ReactiveElasticsearchClient.Indices interface the updateMapping methods are deprecated in favour of the putMapping methods. They do the same, but putMapping is consistent with the naming in the Elasticsearch API: Alias handling In the IndexOperations interface the methods addAlias(AliasQuery) , removeAlias(AliasQuery) and queryForAlias() have been deprecated. The new methods alias(AliasAction) , getAliases(String…​) and getAliasesForIndex(String…​) offer more functionality and a cleaner API. Parent-ID Usage of a parent-id has been removed from Elasticsearch since version 6. We now deprecate the corresponding fields and methods. Removals: Type mappings The type mappings parameters of the @Document annotation and the IndexCoordinates object were removed. They had been deprecated in Spring Data Elasticsearch 4.0 and their values weren’t used anymore. Breaking Changes: Return types of ReactiveElasticsearchClient.Indices methods: The methods in the ReactiveElasticsearchClient.Indices were not used up to now. With the introduction of the ReactiveIndexOperations it became necessary to change some of the return types: the createIndex variants now return a Mono<Boolean> instead of a Mono<Void> to signal successful index creation. the updateMapping variants now return a Mono<Boolean> instead of a Mono<Void> to signal successful mappings storage. Return types of DocumentOperations.bulkIndex methods: These methods were returning a List<String> containing the ids of the new indexed records. Now they return a List<IndexedObjectInformation> ; these objects contain the id and information about optimistic locking (seq_no and primary_term)"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-4.1-4.2.html","Upgrading from 4.1.x to 4.2.x: This section describes breaking changes from version 4.1.x to 4.2.x and how removed features can be replaced by new introduced features. Deprecations: @Document parameters: The parameters of the @Document annotation that are relevant for the index settings ( useServerConfiguration , shards . replicas , refreshIntervall and indexStoretype ) have been moved to the @Setting annotation. Use in @Document is still possible but deprecated. Removals: The @Score annotation that was used to set the score return value in an entity was deprecated in version 4.0 and has been removed. Score values are returned in the SearchHit instances that encapsulate the returned entities. The org.springframework.data.elasticsearch.ElasticsearchException class has been removed. The remaining usages have been replaced with org.springframework.data.mapping.MappingException and org.springframework.dao.InvalidDataAccessApiUsageException . The deprecated ScoredPage , ScrolledPage @AggregatedPage and implementations has been removed. The deprecated GetQuery and DeleteQuery have been removed. The deprecated find methods from ReactiveSearchOperations and ReactiveDocumentOperations have been removed. Breaking Changes: RefreshPolicy: Enum package changed: It was possible in 4.1 to configure the refresh policy for the ReactiveElasticsearchTemplate by overriding the method AbstractReactiveElasticsearchConfiguration.refreshPolicy() in a custom configuration class. The return value of this method was an instance of the class org.elasticsearch.action.support.WriteRequest.RefreshPolicy . Now the configuration must return org.springframework.data.elasticsearch.core.RefreshPolicy . This enum has the same values and triggers the same behaviour as before, so only the import statement has to be adjusted. Refresh behaviour: ElasticsearchOperations and ReactiveElasticsearchOperations now explicitly use the RefreshPolicy set on the template for write requests if not null. If the refresh policy is null, then nothing special is done, so the cluster defaults are used. ElasticsearchOperations was always using the cluster default before this version. The provided implementations for ElasticsearchRepository and ReactiveElasticsearchRepository will do an explicit refresh when the refresh policy is null. This is the same behaviour as in previous versions. If a refresh policy is set, then it will be used by the repositories as well. Refresh configuration: When configuring Spring Data Elasticsearch like described in Elasticsearch Clients(../elasticsearch/clients.html) by using ElasticsearchConfigurationSupport , AbstractElasticsearchConfiguration or AbstractReactiveElasticsearchConfiguration the refresh policy will be initialized to null . Previously the reactive code initialized this to IMMEDIATE , now reactive and non-reactive code show the same behaviour. Method return types: delete methods that take a Query: The reactive methods previously returned a Mono<Long> with the number of deleted documents, the non reactive versions were void. They now return a Mono<ByQueryResponse> which contains much more detailed information about the deleted documents and errors that might have occurred. multiget methods: The implementations of multiget previousl only returned the found entities in a List<T> for non-reactive implementations and in a Flux<T> for reactive implementations. If the request contained ids that were not found, the information that these are missing was not available. The user needed to compare the returned ids to the requested ones to find which ones were missing. Now the multiget methods return a MultiGetItem for every requested id. This contains information about failures (like non existing indices) and the information if the item existed (then it is contained in the `MultiGetItem) or not."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-4.2-4.3.html","Upgrading from 4.2.x to 4.3.x: This section describes breaking changes from version 4.2.x to 4.3.x and how removed features can be replaced by new introduced features. Elasticsearch is working on a new Client that will replace the RestHighLevelClient because the RestHighLevelClient uses code from Elasticsearch core libraries which are not Apache 2 licensed anymore. Spring Data Elasticsearch is preparing for this change as well. This means that internally the implementations for the *Operations interfaces need to change - which should be no problem if users program against the interfaces like ElasticsearchOperations or ReactiveElasticsearchOperations . If you are using the implementation classes like ElasticsearchRestTemplate directly, you will need to adapt to these changes. Spring Data Elasticsearch also removes or replaces the use of classes from the org.elasticsearch packages in it’s API classes and methods, only using them in the implementation where the access to Elasticsearch is implemented. For the user that means, that some enum classes that were used are replaced by enums that live in org.springframework.data.elasticsearch with the same values, these are internally mapped onto the Elasticsearch ones. Places where classes are used that cannot easily be replaced, this usage is marked as deprecated, we are working on replacements. Check the sections on Deprecations(#elasticsearch-migration-guide-4.2-4.3.deprecations) and Breaking Changes(#elasticsearch-migration-guide-4.2-4.3.breaking-changes) for further details. Deprecations: suggest methods: In SearchOperations , and so in ElasticsearchOperations as well, the suggest methods taking a org.elasticsearch.search.suggest.SuggestBuilder as argument and returning a org.elasticsearch.action.search.SearchResponse have been deprecated. Use SearchHits<T> search(Query query, Class<T> clazz) instead, passing in a NativeSearchQuery which can contain a SuggestBuilder and read the suggest results from the returned SearchHit<T> . In ReactiveSearchOperations the new suggest methods return a Mono<org.springframework.data.elasticsearch.core.suggest.response.Suggest> now. Here as well the old methods are deprecated. Breaking Changes: Removal of org.elasticsearch classes from the API.: In the org.springframework.data.elasticsearch.annotations.CompletionContext annotation the property type() has changed from org.elasticsearch.search.suggest.completion.context.ContextMapping.Type to org.springframework.data.elasticsearch.annotations.CompletionContext.ContextMappingType , the available enum values are the same. In the org.springframework.data.elasticsearch.annotations.Document annotation the versionType() property has changed to org.springframework.data.elasticsearch.annotations.Document.VersionType , the available enum values are the same. In the org.springframework.data.elasticsearch.core.query.Query interface the searchType() property has changed to org.springframework.data.elasticsearch.core.query.Query.SearchType , the available enum values are the same. In the org.springframework.data.elasticsearch.core.query.Query interface the return value of timeout() was changed to java.time.Duration . The SearchHits<T>`class does not contain the `org.elasticsearch.search.aggregations.Aggregations anymore. Instead it now contains an instance of the org.springframework.data.elasticsearch.core.AggregationsContainer<T> class where T is the concrete aggregations type from the underlying client that is used. Currently this will be a org .springframework.data.elasticsearch.core.clients.elasticsearch7.ElasticsearchAggregations object; later different implementations will be available. The same change has been done to the ReactiveSearchOperations.aggregate() functions, the now return a Flux<AggregationContainer<?>> . Programs using the aggregations need to be changed to cast the returned value to the appropriate class to further proces it. methods that might have thrown a org.elasticsearch.ElasticsearchStatusException now will throw org.springframework.data.elasticsearch.RestStatusException instead. Handling of field and sourceFilter properties of Query: Up to version 4.2 the fields property of a Query was interpreted and added to the include list of the sourceFilter . This was not correct, as these are different things for Elasticsearch. This has been corrected. As a consequence code might not work anymore that relies on using fields to specify which fields should be returned from the document’s _source' and should be changed to use the `sourceFilter . search_type default value: The default value for the search_type in Elasticsearch is query_then_fetch . This now is also set as default value in the Query implementations, it was previously set to dfs_query_then_fetch . BulkOptions changes: Some properties of the org.springframework.data.elasticsearch.core.query.BulkOptions class have changed their type: the type of the timeout property has been changed to java.time.Duration . the type of the`refreshPolicy` property has been changed to org.springframework.data.elasticsearch.core.RefreshPolicy . IndicesOptions change: Spring Data Elasticsearch now uses org.springframework.data.elasticsearch.core.query.IndicesOptions instead of org.elasticsearch.action.support.IndicesOptions . Completion classes: The classes from the package org.springframework.data.elasticsearch.core.completion have been moved to org.springframework.data.elasticsearch.core.suggest . Other renamings: The org.springframework.data.elasticsearch.core.mapping.ElasticsearchPersistentPropertyConverter interface has been renamed to org.springframework.data.elasticsearch.core.mapping.PropertyValueConverter . Likewise the implementations classes named XXPersistentPropertyConverter have been renamed to XXPropertyValueConverter ."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-4.3-4.4.html","Upgrading from 4.3.x to 4.4.x: This section describes breaking changes from version 4.3.x to 4.4.x and how removed features can be replaced by new introduced features. Deprecations: org.springframework.data.elasticsearch.core.ReactiveElasticsearchOperations: The method <T> Publisher<T> execute(ClientCallback<Publisher<T>> callback) has been deprecated. As there now are multiple implementations using different client libraries the execute method is still available in the different implementations, but there is no more method in the interface, because there is no common callback interface for the different clients. Breaking Changes: Removal of deprecated classes: org.springframework.data.elasticsearch.core.ElasticsearchTemplate has been removed: As of version 4.4 Spring Data Elasticsearch does not use the TransportClient from Elasticsearch anymore (which itself is deprecated since Elasticsearch 7.0). This means that the org.springframework.data.elasticsearch.core.ElasticsearchTemplate class which was deprecated since Spring Data Elasticsearch 4.0 has been removed. This was the implementation of the ElasticsearchOperations interface that was using the TransportClient . Connections to Elasticsearch must be made using either the imperative ElasticsearchRestTemplate or the reactive ReactiveElasticsearchTemplate . Package changes: In 4.3 two classes ( ElasticsearchAggregations and ElasticsearchAggregation ) had been moved to the org.springframework.data.elasticsearch.core.clients.elasticsearch7 package in preparation for the integration of the new Elasticsearch client. The were moved back to the org.springframework.data.elasticsearch.core package as we keep the classes use the old Elasticsearch client where they were. Behaviour change: The ReactiveElasticsearchTemplate , when created directly or by Spring Boot configuration had a default refresh policy of IMMEDIATE. This could cause performance issues on heavy indexing and was different than the default behaviour of Elasticsearch. This has been changed to that now the default refresh policy is NONE. When the ReactiveElasticsearchTemplate was provided by using the configuration like described in Reactive REST Client(../elasticsearch/clients.html#elasticsearch.clients.reactiverestclient) the default refresh policy already was set to NONE. New Elasticsearch client: Elasticsearch has introduced it’s new ElasticsearchClient and has deprecated the previous RestHighLevelClient . Spring Data Elasticsearch 4.4 still uses the old client as the default client for the following reasons: The new client forces applications to use the jakarta.json.spi.JsonProvider package whereas Spring Boot will stick to javax.json.spi.JsonProvider until version 3. So switching the default implementaiton in Spring Data Elasticsearch can only come with Spring Data Elasticsearch 5 (Spring Data 3, Spring 6). There are still some bugs in the Elasticsearch client which need to be resolved The implementation using the new client in Spring Data Elasticsearch is not yet complete, due to limited resources working on that - remember Spring Data Elasticsearch is a community driven project that lives from public contributions. How to use the new client: The implementation using the new client is not complete, some operations will throw a java.lang.UnsupportedOperationException or might throw NPE (for example when the Elasticsearch cannot parse a response from the server, this still happens sometimes) Use the new client to test the implementations but do not use it in productive code yet! In order to try and use the new client the following steps are necessary: Make sure not to configure the existing default client: If using Spring Boot, exclude Spring Data Elasticsearch from the autoconfiguration @SpringBootApplication(exclude = ElasticsearchDataAutoConfiguration.class) public class SpringdataElasticTestApplication { // ... } Remove Spring Data Elasticsearch related properties from your application configuration. If Spring Data Elasticsearch was configured using a programmatic configuration (see Elasticsearch Clients(../elasticsearch/clients.html) ), remove these beans from the Spring application context. Add dependencies: The dependencies for the new Elasticsearch client are still optional in Spring Data Elasticsearch so they need to be added explicitly: <dependencies> <dependency> <groupId>co.elastic.clients</groupId> <artifactId>elasticsearch-java</artifactId> <version>7.17.3</version> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.elasticsearch.client</groupId> <artifactId>elasticsearch-rest-client</artifactId> <!-- is Apache 2--> <version>7.17.3</version> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> </exclusions> </dependency> </dependencies> When using Spring Boot, it is necessary to set the following property in the pom.xml . <properties> <jakarta-json.version>2.0.1</jakarta-json.version> </properties> New configuration classes: Imperative style: In order configure Spring Data Elasticsearch to use the new client, it is necessary to create a configuration bean that derives from org.springframework.data.elasticsearch.client.elc.ElasticsearchConfiguration : @Configuration public class NewRestClientConfig extends ElasticsearchConfiguration { @Override public ClientConfiguration clientConfiguration() { return ClientConfiguration.builder() // .connectedTo(""localhost:9200"") // .build(); } } The configuration is done in the same way as with the old client, but it is not necessary anymore to create more than the configuration bean. With this configuration, the following beans will be available in the Spring application context: a RestClient bean, that is the configured low level RestClient that is used by the Elasticsearch client an ElasticsearchClient bean, this is the new client that uses the RestClient an ElasticsearchOperations bean, available with the bean names elasticsearchOperations and elasticsearchTemplate , this uses the ElasticsearchClient Reactive style: To use the new client in a reactive environment the only difference is the class from which to derive the configuration: @Configuration public class NewRestClientConfig extends ReactiveElasticsearchConfiguration { @Override public ClientConfiguration clientConfiguration() { return ClientConfiguration.builder() // .connectedTo(""localhost:9200"") // .build(); } } With this configuration, the following beans will be available in the Spring application context: a RestClient bean, that is the configured low level RestClient that is used by the Elasticsearch client an ReactiveElasticsearchClient bean, this is the new reactive client that uses the RestClient an ReactiveElasticsearchOperations bean, available with the bean names reactiveElasticsearchOperations and reactiveElasticsearchTemplate , this uses the ReactiveElasticsearchClient"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-4.4-5.0.html","Upgrading from 4.4.x to 5.0.x: This section describes breaking changes from version 4.4.x to 5.0.x and how removed features can be replaced by new introduced features. Deprecations: Custom trace level logging: Logging by setting the property logging.level.org.springframework.data.elasticsearch.client.WIRE=trace is deprecated now, the Elasticsearch RestClient provides a better solution that can be activated by setting the logging level of the tracer package to ""trace"". org.springframework.data.elasticsearch.client.erhlc package: See Package changes(#elasticsearch-migration-guide-4.4-5.0.breaking-changes-packages) , all classes in this package have been deprecated, as the default client implementations to use are the ones based on the new Java Client from Elasticsearch, see New Elasticsearch client(#elasticsearch-migration-guide-4.4-5.0.new-clients) Removal of deprecated code: DateFormat.none and DateFormat.custom had been deprecated since version 4.2 and have been removed. The properties of @Document that were deprecated since 4.2 have been removed. Use the @Settings annotation for these. @DynamicMapping and @DynamicMappingValue have been removed. Use @Document.dynamic or @Field.dynamic instead. Breaking Changes: Removal of deprecated calls: suggest calls in operations interfaces have been removed: Both SearchOperations and ReactiveSearchOperations had deprecated calls that were using Elasticsearch classes as parameters. These now have been removed and so the dependency on Elasticsearch classes in these APIs has been cleaned. Package changes: All the classes that are using or depend on the deprecated Elasticsearch RestHighLevelClient have been moved to the package org.springframework.data.elasticsearch.client.erhlc . By this change we now have a clear separation of code using the old deprecated Elasticsearch libraries, code using the new Elasticsearch client and code that is independent of the client implementation. Also the reactive implementation that was provided up to now has been moved here, as this implementation contains code that was copied and adapted from Elasticsearch libraries. If you are using ElasticsearchRestTemplate directly and not the ElasticsearchOperations interface you’ll need to adjust your imports as well. When working with the NativeSearchQuery class, you’ll need to switch to the NativeQuery class, which can take a Query instance coming from the new Elasticsearch client libraries. You’ll find plenty of examples in the test code. Conversion to Java 17 records: The following classes have been converted to Record , you might need to adjust the use of getter methods from getProp() to prop() : org.springframework.data.elasticsearch.core.AbstractReactiveElasticsearchTemplate.IndexResponseMetaData org.springframework.data.elasticsearch.core.ActiveShardCount org.springframework.data.elasticsearch.support.Version org.springframework.data.elasticsearch.support.ScoreDoc org.springframework.data.elasticsearch.core.query.ScriptData org.springframework.data.elasticsearch.core.query.SeqNoPrimaryTerm New HttpHeaders class: Until version 4.4 the client configuration used the HttpHeaders class from the org.springframework:spring-web project. This introduces a dependency on that artifact. Users that do not use spring-web then face an error as this class cannot be found. In version 5.0 we introduce our own HttpHeaders to configure the clients. So if you are using headers in the client configuration, you need to replace org.springframework.http.HttpHeaders with org.springframework.data.elasticsearch.support.HttpHeaders . Hint: You can pass a org.springframework.http .HttpHeaders to the addAll() method of org.springframework.data.elasticsearch.support.HttpHeaders . New Elasticsearch client: Spring Data Elasticsearch now uses the new ElasticsearchClient and has deprecated the use of the previous RestHighLevelClient . Imperative style configuration: To configure Spring Data Elasticsearch to use the new client, it is necessary to create a configuration bean that derives from org.springframework.data.elasticsearch.client.elc.ElasticsearchConfiguration : @Configuration public class NewRestClientConfig extends ElasticsearchConfiguration { @Override public ClientConfiguration clientConfiguration() { return ClientConfiguration.builder() // .connectedTo(""localhost:9200"") // .build(); } } The configuration is done in the same way as with the old client, but it is not necessary anymore to create more than the configuration bean. With this configuration, the following beans will be available in the Spring application context: a RestClient bean, that is the configured low level RestClient that is used by the Elasticsearch client an ElasticsearchClient bean, this is the new client that uses the RestClient an ElasticsearchOperations bean, available with the bean names elasticsearchOperations and elasticsearchTemplate , this uses the ElasticsearchClient Reactive style configuration: To use the new client in a reactive environment the only difference is the class from which to derive the configuration: @Configuration public class NewRestClientConfig extends ReactiveElasticsearchConfiguration { @Override public ClientConfiguration clientConfiguration() { return ClientConfiguration.builder() // .connectedTo(""localhost:9200"") // .build(); } } With this configuration, the following beans will be available in the Spring application context: a RestClient bean, that is the configured low level RestClient that is used by the Elasticsearch client an ReactiveElasticsearchClient bean, this is the new reactive client that uses the RestClient an ReactiveElasticsearchOperations bean, available with the bean names reactiveElasticsearchOperations and reactiveElasticsearchTemplate , this uses the ReactiveElasticsearchClient Still want to use the old client?: The old deprecated RestHighLevelClient can still be used, but you will need to add the dependency explicitly to your application as Spring Data Elasticsearch does not pull it in automatically anymore: <!-- include the RHLC, specify version explicitly --> <dependency> <groupId>org.elasticsearch.client</groupId> <artifactId>elasticsearch-rest-high-level-client</artifactId> <version>7.17.5</version> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> </exclusions> </dependency> Make sure to specify the version 7.17.6 explicitly, otherwise maven will resolve to 8.5.0, and this does not exist."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-5.0-5.1.html","Upgrading from 5.0.x to 5.1.x: This section describes breaking changes from version 5.0.x to 5.1.x and how removed features can be replaced by new introduced features. Breaking Changes: In the org.springframework.data.elasticsearch.core.index.AliasData class, which is used for alias information returned from Elasticsearch, the property filter (of type Document ) is replaced by filterQuery which is of type org.springframework.data.elasticsearch.core.query.Query . org.springframework.data.elasticsearch.annotations.Similarity was an enum class until 5.1. This enum was used in the @Field annotation to specify a similarity value. But besides the values defined by the enum, it is possible to have similarities with custom names in Elasticsearch. Therefore, the annotation property was changed from the type of the enum to a simple String . The previous enum values like Similarity.Default do still exist as String constants, so existing code will compile unmodified. Adaptions are necessary when this enum was used at other places than as a property of the @Field annotation. Deprecations: template functions: The functions in the IndexOperations and ReactiverIndexOperations to manage index templates that were introduced in Spring Data Elasticsearch 4.1 have been deprecated. They were using the old Elasticsearch API that was deprecated in Elasticsearch version 7.8. Please use the new functions that are based on the composable index template API instead."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-5.1-5.2.html","Upgrading from 5.1.x to 5.2.x: This section describes breaking changes from version 5.1.x to 5.2.x and how removed features can be replaced by new introduced features. Breaking Changes: Bulk failures: In the org.springframework.data.elasticsearch.BulkFailureException class, the return type of the getFailedDocuments is changed from Map<String, String> to Map<String, FailureDetails> , which allows to get additional details about failure reasons. The definition of the FailureDetails class (inner to BulkFailureException ): public record FailureDetails(Integer status, String errorMessage) { } scripted and runtime fields: The classes org.springframework.data.elasticsearch.core.RuntimeField and org.springframework.data.elasticsearch.core.query.ScriptType have been moved to the subpackage org.springframework.data.elasticsearch.core.query . The type parameter of the ScriptData constructor is not nullable any longer. Deprecations: Removal of deprecated code: All the code using the old deprecated RestHighLevelClient has been removed. The default Elasticsearch client used since version 5.0 is the (not so) new Elasticsearch Java client. The org.springframework.data.elasticsearch.client.ClientLogger class has been removed. This logger was configured with the org.springframework.data.elasticsearch.client.WIRE setting, but was not working with all clients. From version 5 on, use the trace logger available in the Elasticsearch Java client, see Client Logging(../elasticsearch/clients.html#elasticsearch.clients.logging) . The method org.springframework.data.elasticsearch.core.ElasticsearchOperations.stringIdRepresentation(Object) has been removed, use the convertId(Object) method defined in the same interface instead. The class org.springframework.data.elasticsearch.core.Range has been removed, use org.springframework.data.domain.Range instead. The methods org.springframework.data.elasticsearch.core.query.IndexQuery.getParentId() and `setParentId(String) have been removed, they weren’t used anymore and were no-ops. It has been removed from the org.springframework.data.elasticsearch.core.query.IndexQuery class as well."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/migration-guides/migration-guide-5.2-5.3.html","Upgrading from 5.2.x to 5.3.x: This section describes breaking changes from version 5.2.x to 5.3.x and how removed features can be replaced by new introduced features. Breaking Changes: During the parameter replacement in @Query annotated repository methods previous versions wrote the String ""null"" into the query that was sent to Elasticsearch when the actual parameter value was null . As Elasticsearch does not store null values, this behaviour could lead to problems, for example whent the fields to be searched contains the string ""null"" . In Version 5.3 a null value in a parameter will cause a ConversionException to be thrown. If you are using ""null"" as the null_value defined in a field mapping, then pass that string into the query instead of a Java null . Deprecations: Removals: The deprecated classes org.springframework.data.elasticsearch.ELCQueries and org.springframework.data.elasticsearch.client.elc.QueryBuilders have been removed, use org.springframework.data.elasticsearch.client.elc.Queries instead."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch.html","Elasticsearch Support: Spring Data support for Elasticsearch contains a wide range of features: Spring configuration support for various Elasticsearch clients(elasticsearch/clients.html) . The ElasticsearchTemplate and ReactiveElasticsearchTemplate(elasticsearch/template.html) helper classes that provide object mapping between ES index operations and POJOs. Exception translation(elasticsearch/template.html#exception-translation) into Spring’s portable Data Access Exception Hierarchy(https://docs.spring.io/spring-framework/reference/6.1data-access.html#dao-exceptions) . Feature rich object mapping(elasticsearch/object-mapping.html) integrated with Spring’s Conversion Service(https://docs.spring.io/spring-framework/reference/6.1core.html#core-convert) . Annotation-based mapping(elasticsearch/object-mapping.html#elasticsearch.mapping.meta-model.annotations) metadata that is extensible to support other metadata formats. Java-based query, criteria, and update DSLs(elasticsearch/template.html#cassandra.template.query) . Automatic implementation of imperative and reactive Repository interfaces(repositories.html) including support for custom query methods(repositories/custom-implementations.html) . For most data-oriented tasks, you can use the [Reactive]ElasticsearchTemplate or the Repository support, both of which use the rich object-mapping functionality. Spring Data Elasticsearch uses consistent naming conventions on objects in various APIs to those found in the DataStax Java Driver so that they are familiar and so that you can map your existing knowledge onto the Spring APIs. Section Summary: Elasticsearch Clients(elasticsearch/clients.html) Elasticsearch Object Mapping(elasticsearch/object-mapping.html) Elasticsearch Operations(elasticsearch/template.html) Reactive Elasticsearch Operations(elasticsearch/reactive-template.html) Entity Callbacks(elasticsearch/entity-callbacks.html) Elasticsearch Auditing(elasticsearch/auditing.html) Join-Type implementation(elasticsearch/join-types.html) Routing values(elasticsearch/routing.html) Miscellaneous Elasticsearch Operation Support(elasticsearch/misc.html) Scripted and runtime fields(elasticsearch/scripted-and-runtime-fields.html)"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/clients.html","Elasticsearch Clients: This chapter illustrates configuration and usage of supported Elasticsearch client implementations. Spring Data Elasticsearch operates upon an Elasticsearch client (provided by Elasticsearch client libraries) that is connected to a single Elasticsearch node or a cluster. Although the Elasticsearch Client can be used directly to work with the cluster, applications using Spring Data Elasticsearch normally use the higher level abstractions of Elasticsearch Operations(template.html) and Elasticsearch Repositories(repositories/elasticsearch-repositories.html) . Imperative Rest Client: To use the imperative (non-reactive) client, a configuration bean must be configured like this: import org.springframework.data.elasticsearch.client.elc.ElasticsearchConfiguration; @Configuration public class MyClientConfig extends ElasticsearchConfiguration { @Override public ClientConfiguration clientConfiguration() { return ClientConfiguration.builder() (1) .connectedTo(""localhost:9200"") .build(); } } 1 for a detailed description of the builder methods see Client Configuration(#elasticsearch.clients.configuration) The ElasticsearchConfiguration(../api/java/org/springframework/data/elasticsearch/client/elc/ElasticsearchConfiguration.html) ] class allows further configuration by overriding for example the jsonpMapper() or transportOptions() methods. The following beans can then be injected in other Spring components: import org.springframework.beans.factory.annotation.Autowired;@Autowired ElasticsearchOperations operations; (1) @Autowired ElasticsearchClient elasticsearchClient; (2) @Autowired RestClient restClient; (3) @Autowired JsonpMapper jsonpMapper; (4) 1 an implementation of ElasticsearchOperations(../api/java/org/springframework/data/elasticsearch/core/ElasticsearchOperations.html) 2 the co.elastic.clients.elasticsearch.ElasticsearchClient that is used. 3 the low level RestClient from the Elasticsearch libraries 4 the JsonpMapper user by the Elasticsearch Transport Basically one should just use the ElasticsearchOperations(../api/java/org/springframework/data/elasticsearch/core/ElasticsearchOperations.html) to interact with the Elasticsearch cluster. When using repositories, this instance is used under the hood as well. Reactive Rest Client: When working with the reactive stack, the configuration must be derived from a different class: import org.springframework.data.elasticsearch.client.elc.ReactiveElasticsearchConfiguration; @Configuration public class MyClientConfig extends ReactiveElasticsearchConfiguration { @Override public ClientConfiguration clientConfiguration() { return ClientConfiguration.builder() (1) .connectedTo(""localhost:9200"") .build(); } } 1 for a detailed description of the builder methods see Client Configuration(#elasticsearch.clients.configuration) The ReactiveElasticsearchConfiguration(../api/java/org/springframework/data/elasticsearch/client/elc/ReactiveElasticsearchConfiguration.html) class allows further configuration by overriding for example the jsonpMapper() or transportOptions() methods. The following beans can then be injected in other Spring components: @Autowired ReactiveElasticsearchOperations operations; (1) @Autowired ReactiveElasticsearchClient elasticsearchClient; (2) @Autowired RestClient restClient; (3) @Autowired JsonpMapper jsonpMapper; (4) the following can be injected: 1 an implementation of ReactiveElasticsearchOperations(../api/java/org/springframework/data/elasticsearch/core/ReactiveElasticsearchOperations.html) 2 the org.springframework.data.elasticsearch.client.elc.ReactiveElasticsearchClient that is used. This is a reactive implementation based on the Elasticsearch client implementation. 3 the low level RestClient from the Elasticsearch libraries 4 the JsonpMapper user by the Elasticsearch Transport Basically one should just use the ReactiveElasticsearchOperations(../api/java/org/springframework/data/elasticsearch/core/ReactiveElasticsearchOperations.html) to interact with the Elasticsearch cluster. When using repositories, this instance is used under the hood as well. Client Configuration: Client behaviour can be changed via the ClientConfiguration(../api/java/org/springframework/data/elasticsearch/client/ClientConfiguration.html) that allows to set options for SSL, connect and socket timeouts, headers and other parameters. Example 1. Client Configuration import org.springframework.data.elasticsearch.client.ClientConfiguration; import org.springframework.data.elasticsearch.support.HttpHeaders; import static org.springframework.data.elasticsearch.client.elc.ElasticsearchClients.*; HttpHeaders httpHeaders = new HttpHeaders(); httpHeaders.add(""some-header"", ""on every request"") (1) ClientConfiguration clientConfiguration = ClientConfiguration.builder() .connectedTo(""localhost:9200"", ""localhost:9291"") (2) .usingSsl() (3) .withProxy(""localhost:8888"") (4) .withPathPrefix(""ela"") (5) .withConnectTimeout(Duration.ofSeconds(5)) (6) .withSocketTimeout(Duration.ofSeconds(3)) (7) .withDefaultHeaders(defaultHeaders) (8) .withBasicAuth(username, password) (9) .withHeaders(() -> { (10) HttpHeaders headers = new HttpHeaders(); headers.add(""currentTime"", LocalDateTime.now().format(DateTimeFormatter.ISO_LOCAL_DATE_TIME)); return headers; }) .withClientConfigurer( (11) ElasticsearchHttpClientConfigurationCallback.from(clientBuilder -> { // ... return clientBuilder; })) . // ... other options .build(); 1 Define default headers, if they need to be customized 2 Use the builder to provide cluster addresses, set default HttpHeaders or enable SSL. 3 Optionally enable SSL.There exist overloads of this function that can take a SSLContext or as an alternative the fingerprint of the certificate as it is output by Elasticsearch 8 on startup. 4 Optionally set a proxy. 5 Optionally set a path prefix, mostly used when different clusters a behind some reverse proxy. 6 Set the connection timeout. 7 Set the socket timeout. 8 Optionally set headers. 9 Add basic authentication. 10 A Supplier<HttpHeaders> function can be specified which is called every time before a request is sent to Elasticsearch - here, as an example, the current time is written in a header. 11 a function to configure the created client (see Client configuration callbacks(#elasticsearch.clients.configuration.callbacks) ), can be added multiple times. Adding a Header supplier as shown in above example allows to inject headers that may change over the time, like authentication JWT tokens. If this is used in the reactive setup, the supplier function must not block! Client configuration callbacks: The ClientConfiguration(../api/java/org/springframework/data/elasticsearch/client/ClientConfiguration.html) class offers the most common parameters to configure the client. In the case this is not enough, the user can add callback functions by using the withClientConfigurer(ClientConfigurationCallback<?>) method. The following callbacks are provided: Configuration of the low level Elasticsearch RestClient:: This callback provides a org.elasticsearch.client.RestClientBuilder that can be used to configure the Elasticsearch RestClient : ClientConfiguration.builder() .connectedTo(""localhost:9200"", ""localhost:9291"") .withClientConfigurer(ElasticsearchClients.ElasticsearchRestClientConfigurationCallback.from(restClientBuilder -> { // configure the Elasticsearch RestClient return restClientBuilder; })) .build(); Configuration of the HttpAsyncClient used by the low level Elasticsearch RestClient:: This callback provides a org.apache.http.impl.nio.client.HttpAsyncClientBuilder to configure the HttpCLient that is used by the RestClient . ClientConfiguration.builder() .connectedTo(""localhost:9200"", ""localhost:9291"") .withClientConfigurer(ElasticsearchClients.ElasticsearchHttpClientConfigurationCallback.from(httpAsyncClientBuilder -> { // configure the HttpAsyncClient return httpAsyncClientBuilder; })) .build(); Client Logging: To see what is actually sent to and received from the server Request / Response logging on the transport level needs to be turned on as outlined in the snippet below. This can be enabled in the Elasticsearch client by setting the level of the tracer package to ""trace"" (see www.elastic.co/guide/en/elasticsearch/client/java-api-client/current/java-rest-low-usage-logging.html(https://www.elastic.co/guide/en/elasticsearch/client/java-api-client/current/java-rest-low-usage-logging.html) ) Enable transport layer logging <logger name=""tracer"" level=""trace""/>"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/object-mapping.html","Elasticsearch Object Mapping: Spring Data Elasticsearch Object Mapping is the process that maps a Java object - the domain entity - into the JSON representation that is stored in Elasticsearch and back. The class that is internally used for this mapping is the MappingElasticsearchConverter . Meta Model Object Mapping: The Metamodel based approach uses domain type information for reading/writing from/to Elasticsearch. This allows to register Converter instances for specific domain type mapping. Mapping Annotation Overview: The MappingElasticsearchConverter uses metadata to drive the mapping of objects to documents. The metadata is taken from the entity’s properties which can be annotated. The following annotations are available: @Document : Applied at the class level to indicate this class is a candidate for mapping to the database. The most important attributes are (check the API documentation for the complete list of attributes): indexName : the name of the index to store this entity in. This can contain a SpEL template expression like ""log-#{T(java.time.LocalDate).now().toString()}"" createIndex : flag whether to create an index on repository bootstrapping. Default value is true . See Automatic creation of indices with the corresponding mapping(repositories/elasticsearch-repositories.html#elasticsearch.repositories.autocreation) @Id : Applied at the field level to mark the field used for identity purpose. @Transient , @ReadOnlyProperty , @WriteOnlyProperty : see the following section Controlling which properties are written to and read from Elasticsearch(#elasticsearch.mapping.meta-model.annotations.read-write) for detailed information. @PersistenceConstructor : Marks a given constructor - even a package protected one - to use when instantiating the object from the database. Constructor arguments are mapped by name to the key values in the retrieved Document. @Field : Applied at the field level and defines properties of the field, most of the attributes map to the respective Elasticsearch Mapping(https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html) definitions (the following list is not complete, check the annotation Javadoc for a complete reference): name : The name of the field as it will be represented in the Elasticsearch document, if not set, the Java field name is used. type : The field type, can be one of Text, Keyword, Long, Integer, Short, Byte, Double, Float, Half_Float, Scaled_Float, Date, Date_Nanos, Boolean, Binary, Integer_Range, Float_Range, Long_Range, Double_Range, Date_Range, Ip_Range, Object, Nested, Ip, TokenCount, Percolator, Flattened, Search_As_You_Type . See Elasticsearch Mapping Types(https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html) . If the field type is not specified, it defaults to FieldType.Auto . This means, that no mapping entry is written for the property and that Elasticsearch will add a mapping entry dynamically when the first data for this property is stored (check the Elasticsearch documentation for dynamic mapping rules). format : One or more built-in date formats, see the next section Date format mapping(#elasticsearch.mapping.meta-model.annotations.date-formats) . pattern : One or more custom date formats, see the next section Date format mapping(#elasticsearch.mapping.meta-model.annotations.date-formats) . store : Flag whether the original field value should be store in Elasticsearch, default value is false . analyzer , searchAnalyzer , normalizer for specifying custom analyzers and normalizer. @GeoPoint : Marks a field as geo_point datatype. Can be omitted if the field is an instance of the GeoPoint class. @ValueConverter defines a class to be used to convert the given property. In difference to a registered Spring Converter this only converts the annotated property and not every property of the given type. The mapping metadata infrastructure is defined in a separate spring-data-commons project that is technology agnostic. Controlling which properties are written to and read from Elasticsearch: This section details the annotations that define if the value of a property is written to or read from Elasticsearch. @Transient : A property annotated with this annotation will not be written to the mapping, it’s value will not be sent to Elasticsearch and when documents are returned from Elasticsearch, this property will not be set in the resulting entity. @ReadOnlyProperty : A property with this annotation will not have its value written to Elasticsearch, but when returning data, the property will be filled with the value returned in the document from Elasticsearch. One use case for this are runtime fields defined in the index mapping. @WriteOnlyProperty : A property with this annotation will have its value stored in Elasticsearch but will not be set with any value when reading document. This can be used for example for synthesized fields which should go into the Elasticsearch index but are not used elsewhere. Date format mapping: Properties that derive from TemporalAccessor or are of type java.util.Date must either have a @Field annotation of type FieldType.Date or a custom converter must be registered for this type. This paragraph describes the use of FieldType.Date . There are two attributes of the @Field annotation that define which date format information is written to the mapping (also see Elasticsearch Built In Formats(https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html#built-in-date-formats) and Elasticsearch Custom Date Formats(https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html#custom-date-formats) ) The format attribute is used to define at least one of the predefined formats. If it is not defined, then a default value of _date_optional_time and epoch_millis is used. The pattern attribute can be used to add additional custom format strings. If you want to use only custom date formats, you must set the format property to empty {} . The following table shows the different attributes and the mapping created from their values: annotation format string in Elasticsearch mapping @Field(type=FieldType.Date) ""date_optional_time||epoch_millis"", @Field(type=FieldType.Date, format=DateFormat.basic_date) ""basic_date"" @Field(type=FieldType.Date, format={DateFormat.basic_date, DateFormat.basic_time}) ""basic_date||basic_time"" @Field(type=FieldType.Date, pattern=""dd.MM.uuuu"") ""date_optional_time||epoch_millis||dd.MM.uuuu"", @Field(type=FieldType.Date, format={}, pattern=""dd.MM.uuuu"") ""dd.MM.uuuu"" If you are using a custom date format, you need to use uuuu for the year instead of yyyy . This is due to a change in Elasticsearch 7(https://www.elastic.co/guide/en/elasticsearch/reference/current/migrate-to-java-time.html#java-time-migration-incompatible-date-formats) . Check the code of the org.springframework.data.elasticsearch.annotations.DateFormat enum for a complete list of predefined values and their patterns. Range types: When a field is annotated with a type of one of Integer_Range, Float_Range, Long_Range, Double_Range, Date_Range, or Ip_Range the field must be an instance of a class that will be mapped to an Elasticsearch range, for example: class SomePersonData { @Field(type = FieldType.Integer_Range) private ValidAge validAge; // getter and setter } class ValidAge { @Field(name=""gte"") private Integer from; @Field(name=""lte"") private Integer to; // getter and setter } As an alternative Spring Data Elasticsearch provides a Range<T> class so that the previous example can be written as: class SomePersonData { @Field(type = FieldType.Integer_Range) private Range<Integer> validAge; // getter and setter } Supported classes for the type <T> are Integer , Long , Float , Double , Date and classes that implement the TemporalAccessor interface. Mapped field names: Without further configuration, Spring Data Elasticsearch will use the property name of an object as field name in Elasticsearch. This can be changed for individual field by using the @Field annotation on that property. It is also possible to define a FieldNamingStrategy in the configuration of the client ( Elasticsearch Clients(clients.html) ). If for example a SnakeCaseFieldNamingStrategy is configured, the property sampleProperty of the object would be mapped to sample_property in Elasticsearch. A FieldNamingStrategy applies to all entities; it can be overwritten by setting a specific name with @Field on a property. Non-field-backed properties: Normally the properties used in an entity are fields of the entity class. There might be cases, when a property value is calculated in the entity and should be stored in Elasticsearch. In this case, the getter method ( getProperty() ) can be annotated with the @Field annotation, in addition to that the method must be annotated with @AccessType(AccessType.Type .PROPERTY) . The third annotation that is needed in such a case is @WriteOnlyProperty , as such a value is only written to Elasticsearch. A full example: @Field(type = Keyword) @WriteOnlyProperty @AccessType(AccessType.Type.PROPERTY) public String getProperty() { return ""some value that is calculated here""; } Other property annotations: @IndexedIndexName: This annotation can be set on a String property of an entity. This property will not be written to the mapping, it will not be stored in Elasticsearch and its value will not be read from an Elasticsearch document. After an entity is persisted, for example with a call to ElasticsearchOperations.save(T entity) , the entity returned from that call will contain the name of the index that an entity was saved to in that property. This is useful when the index name is dynamically set by a bean, or when writing to a write alias. Putting some value into such a property does not set the index into which an entity is stored! Mapping Rules: Type Hints: Mapping uses type hints embedded in the document sent to the server to allow generic type mapping. Those type hints are represented as _class attributes within the document and are written for each aggregate root. Example 1. Type Hints public class Person { (1) @Id String id; String firstname; String lastname; } { ""_class"" : ""com.example.Person"", (1) ""id"" : ""cb7bef"", ""firstname"" : ""Sarah"", ""lastname"" : ""Connor"" } 1 By default the domain types class name is used for the type hint. Type hints can be configured to hold custom information. Use the @TypeAlias annotation to do so. Make sure to add types with @TypeAlias to the initial entity set ( AbstractElasticsearchConfiguration#getInitialEntitySet ) to already have entity information available when first reading data from the store. Example 2. Type Hints with Alias @TypeAlias(""human"") (1) public class Person { @Id String id; // ... } { ""_class"" : ""human"", (1) ""id"" : ... } 1 The configured alias is used when writing the entity. Type hints will not be written for nested Objects unless the properties type is Object , an interface or the actual value type does not match the properties declaration. Disabling Type Hints: It may be necessary to disable writing of type hints when the index that should be used already exists without having the type hints defined in its mapping and with the mapping mode set to strict. In this case, writing the type hint will produce an error, as the field cannot be added automatically. Type hints can be disabled for the whole application by overriding the method writeTypeHints() in a configuration class derived from AbstractElasticsearchConfiguration (see Elasticsearch Clients(clients.html) ). As an alternative they can be disabled for a single index with the @Document annotation: @Document(indexName = ""index"", writeTypeHint = WriteTypeHint.FALSE) We strongly advise against disabling Type Hints. Only do this if you are forced to. Disabling type hints can lead to documents not being retrieved correctly from Elasticsearch in case of polymorphic data or document retrieval may fail completely. Geospatial Types: Geospatial types like Point & GeoPoint are converted into lat/lon pairs. Example 3. Geospatial types public class Address { String city, street; Point location; } { ""city"" : ""Los Angeles"", ""street"" : ""2800 East Observatory Road"", ""location"" : { ""lat"" : 34.118347, ""lon"" : -118.3026284 } } GeoJson Types: Spring Data Elasticsearch supports the GeoJson types by providing an interface GeoJson and implementations for the different geometries. They are mapped to Elasticsearch documents according to the GeoJson specification. The corresponding properties of the entity are specified in the index mappings as geo_shape when the index mappings is written. (check the Elasticsearch documentation(https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-shape.html) as well) Example 4. GeoJson types public class Address { String city, street; GeoJsonPoint location; } { ""city"": ""Los Angeles"", ""street"": ""2800 East Observatory Road"", ""location"": { ""type"": ""Point"", ""coordinates"": [-118.3026284, 34.118347] } } The following GeoJson types are implemented: GeoJsonPoint GeoJsonMultiPoint GeoJsonLineString GeoJsonMultiLineString GeoJsonPolygon GeoJsonMultiPolygon GeoJsonGeometryCollection Collections: For values inside Collections apply the same mapping rules as for aggregate roots when it comes to type hints and Custom Conversions(#elasticsearch.mapping.meta-model.conversions) . Example 5. Collections public class Person { // ... List<Person> friends; } { // ... ""friends"" : [ { ""firstname"" : ""Kyle"", ""lastname"" : ""Reese"" } ] } Maps: For values inside Maps apply the same mapping rules as for aggregate roots when it comes to type hints and Custom Conversions(#elasticsearch.mapping.meta-model.conversions) . However the Map key needs to a String to be processed by Elasticsearch. Example 6. Collections public class Person { // ... Map<String, Address> knownLocations; } { // ... ""knownLocations"" : { ""arrivedAt"" : { ""city"" : ""Los Angeles"", ""street"" : ""2800 East Observatory Road"", ""location"" : { ""lat"" : 34.118347, ""lon"" : -118.3026284 } } } } Custom Conversions: Looking at the Configuration from the previous section(#elasticsearch.mapping.meta-model) ElasticsearchCustomConversions allows registering specific rules for mapping domain and simple types. Example 7. Meta Model Object Mapping Configuration @Configuration public class Config extends ElasticsearchConfiguration { @Override public ClientConfiguration clientConfiguration() { return ClientConfiguration.builder() // .connectedTo(""localhost:9200"") // .build(); } @Bean @Override public ElasticsearchCustomConversions elasticsearchCustomConversions() { return new ElasticsearchCustomConversions( Arrays.asList(new AddressToMap(), new MapToAddress())); (1) } @WritingConverter (2) static class AddressToMap implements Converter<Address, Map<String, Object>> { @Override public Map<String, Object> convert(Address source) { LinkedHashMap<String, Object> target = new LinkedHashMap<>(); target.put(""ciudad"", source.getCity()); // ... return target; } } @ReadingConverter (3) static class MapToAddress implements Converter<Map<String, Object>, Address> { @Override public Address convert(Map<String, Object> source) { // ... return address; } } } { ""ciudad"" : ""Los Angeles"", ""calle"" : ""2800 East Observatory Road"", ""localidad"" : { ""lat"" : 34.118347, ""lon"" : -118.3026284 } } 1 Add Converter implementations. 2 Set up the Converter used for writing DomainType to Elasticsearch. 3 Set up the Converter used for reading DomainType from search result."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/template.html","Elasticsearch Operations: Spring Data Elasticsearch uses several interfaces to define the operations that can be called against an Elasticsearch index (for a description of the reactive interfaces see Reactive Elasticsearch Operations(reactive-template.html) ). IndexOperations(../api/java/org/springframework/data/elasticsearch/core/IndexOperations.html) defines actions on index level like creating or deleting an index. DocumentOperations(../api/java/org/springframework/data/elasticsearch/core/DocumentOperations.html) defines actions to store, update and retrieve entities based on their id. SearchOperations(../api/java/org/springframework/data/elasticsearch/core/SearchOperations.html) define the actions to search for multiple entities using queries ElasticsearchOperations(../api/java/org/springframework/data/elasticsearch/core/ElasticsearchOperations.html) combines the DocumentOperations and SearchOperations interfaces. These interfaces correspond to the structuring of the Elasticsearch API(https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html) . The default implementations of the interfaces offer: index management functionality. Read/Write mapping support for domain types. A rich query and criteria api. Resource management and Exception translation. Index management and automatic creation of indices and mappings. The IndexOperations interface and the provided implementation which can be obtained from an ElasticsearchOperations instance - for example with a call to operations.indexOps(clazz) - give the user the ability to create indices, put mappings or store template and alias information in the Elasticsearch cluster. Details of the index that will be created can be set by using the @Setting annotation, refer to Index settings(misc.html#elasticsearc.misc.index.settings) for further information. None of these operations are done automatically by the implementations of IndexOperations or ElasticsearchOperations . It is the user’s responsibility to call the methods. There is support for automatic creation of indices and writing the mappings when using Spring Data Elasticsearch repositories, see Automatic creation of indices with the corresponding mapping(repositories/elasticsearch-repositories.html#elasticsearch.repositories.autocreation) Usage examples: The example shows how to use an injected ElasticsearchOperations instance in a Spring REST controller. The example assumes that Person is a class that is annotated with @Document , @Id etc (see Mapping Annotation Overview(object-mapping.html#elasticsearch.mapping.meta-model.annotations) ). Example 1. ElasticsearchOperations usage @RestController @RequestMapping(""/"") public class TestController { private ElasticsearchOperations elasticsearchOperations; public TestController(ElasticsearchOperations elasticsearchOperations) { (1) this.elasticsearchOperations = elasticsearchOperations; } @PostMapping(""/person"") public String save(@RequestBody Person person) { (2) Person savedEntity = elasticsearchOperations.save(person); return savedEntity.getId(); } @GetMapping(""/person/{id}"") public Person findById(@PathVariable(""id"") Long id) { (3) Person person = elasticsearchOperations.get(id.toString(), Person.class); return person; } } 1 Let Spring inject the provided ElasticsearchOperations bean in the constructor. 2 Store some entity in the Elasticsearch cluster. The id is read from the returned entity, as it might have been null in the person object and been created by Elasticsearch. 3 Retrieve the entity with a get by id. To see the full possibilities of ElasticsearchOperations please refer to the API documentation. Search Result Types: When a document is retrieved with the methods of the DocumentOperations interface, just the found entity will be returned. When searching with the methods of the SearchOperations interface, additional information is available for each entity, for example the score or the sortValues of the found entity. In order to return this information, each entity is wrapped in a SearchHit object that contains this entity-specific additional information. These SearchHit objects themselves are returned within a SearchHits object which additionally contains informations about the whole search like the maxScore or requested aggregations. The following classes and interfaces are now available: SearchHit<T> Contains the following information: Id Score Sort Values Highlight fields Inner hits (this is an embedded SearchHits object containing eventually returned inner hits) The retrieved entity of type <T> SearchHits<T> Contains the following information: Number of total hits Total hits relation Maximum score A list of SearchHit<T> objects Returned aggregations Returned suggest results SearchPage<T> Defines a Spring Data Page that contains a SearchHits<T> element and can be used for paging access using repository methods. SearchScrollHits<T> Returned by the low level scroll API functions in ElasticsearchRestTemplate , it enriches a SearchHits<T> with the Elasticsearch scroll id. SearchHitsIterator<T> An Iterator returned by the streaming functions of the SearchOperations interface. ReactiveSearchHits ReactiveSearchOperations has methods returning a Mono<ReactiveSearchHits<T>> , this contains the same information as a SearchHits<T> object, but will provide the contained SearchHit<T> objects as a Flux<SearchHit<T>> and not as a list. Queries: Almost all of the methods defined in the SearchOperations and ReactiveSearchOperations interface take a Query parameter that defines the query to execute for searching. Query is an interface and Spring Data Elasticsearch provides three implementations: CriteriaQuery , StringQuery and NativeQuery . CriteriaQuery: CriteriaQuery based queries allow the creation of queries to search for data without knowing the syntax or basics of Elasticsearch queries. They allow the user to build queries by simply chaining and combining Criteria objects that specify the criteria the searched documents must fulfill. when talking about AND or OR when combining criteria keep in mind, that in Elasticsearch AND are converted to a must condition and OR to a should Criteria and their usage are best explained by example (let’s assume we have a Book entity with a price property): Example 2. Get books with a given price Criteria criteria = new Criteria(""price"").is(42.0); Query query = new CriteriaQuery(criteria); Conditions for the same field can be chained, they will be combined with a logical AND: Example 3. Get books with a given price Criteria criteria = new Criteria(""price"").greaterThan(42.0).lessThan(34.0); Query query = new CriteriaQuery(criteria); When chaining Criteria , by default a AND logic is used: Example 4. Get all persons with first name James and last name Miller : Criteria criteria = new Criteria(""lastname"").is(""Miller"") (1) .and(""firstname"").is(""James"") (2) Query query = new CriteriaQuery(criteria); 1 the first Criteria 2 the and() creates a new Criteria and chaines it to the first one. If you want to create nested queries, you need to use subqueries for this. Let’s assume we want to find all persons with a last name of Miller and a first name of either Jack or John : Example 5. Nested subqueries Criteria miller = new Criteria(""lastName"").is(""Miller"") (1) .subCriteria( (2) new Criteria().or(""firstName"").is(""John"") (3) .or(""firstName"").is(""Jack"") (4) ); Query query = new CriteriaQuery(criteria); 1 create a first Criteria for the last name 2 this is combined with AND to a subCriteria 3 This sub Criteria is an OR combination for the first name John 4 and the first name Jack Please refer to the API documentation of the Criteria class for a complete overview of the different available operations. StringQuery: This class takes an Elasticsearch query as JSON String. The following code shows a query that searches for persons having the first name ""Jack"": Query query = new StringQuery(""{ \""match\"": { \""firstname\"": { \""query\"": \""Jack\"" } } } ""); SearchHits<Person> searchHits = operations.search(query, Person.class); Using StringQuery may be appropriate if you already have an Elasticsearch query to use. NativeQuery: NativeQuery is the class to use when you have a complex query, or a query that cannot be expressed by using the Criteria API, for example when building queries and using aggregates. It allows to use all the different co.elastic.clients.elasticsearch._types.query_dsl.Query implementations from the Elasticsearch library therefore named ""native"". The following code shows how to search for persons with a given firstName and for the found documents have a terms aggregation that counts the number of occurrences of the lastName for these persons: Query query = NativeQuery.builder() .withAggregation(""lastNames"", Aggregation.of(a -> a .terms(ta -> ta.field(""lastName"").size(10)))) .withQuery(q -> q .match(m -> m .field(""firstName"") .query(firstName) ) ) .withPageable(pageable) .build(); SearchHits<Person> searchHits = operations.search(query, Person.class); SearchTemplateQuery: This is a special implementation of the Query interface to be used in combination with a stored search template. See Search Template support(misc.html#elasticsearch.misc.searchtemplates) for further information."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/reactive-template.html","Reactive Elasticsearch Operations: ReactiveElasticsearchOperations is the gateway to executing high level commands against an Elasticsearch cluster using the ReactiveElasticsearchClient . The ReactiveElasticsearchTemplate is the default implementation of ReactiveElasticsearchOperations . To get started the ReactiveElasticsearchOperations needs to know about the actual client to work with. Please see Reactive Rest Client(clients.html#elasticsearch.clients.reactiverestclient) for details on the client and how to configure it. Reactive Operations Usage: ReactiveElasticsearchOperations lets you save, find and delete your domain objects and map those objects to documents stored in Elasticsearch. Consider the following: Example 1. Use the ReactiveElasticsearchOperations @Document(indexName = ""marvel"") public class Person { private @Id String id; private String name; private int age; // Getter/Setter omitted... } ReactiveElasticsearchOperations operations; // ... operations.save(new Person(""Bruce Banner"", 42)) (1) .doOnNext(System.out::println) .flatMap(person -> operations.get(person.id, Person.class)) (2) .doOnNext(System.out::println) .flatMap(person -> operations.delete(person)) (3) .doOnNext(System.out::println) .flatMap(id -> operations.count(Person.class)) (4) .doOnNext(System.out::println) .subscribe(); (5) The above outputs the following sequence on the console. > Person(id=QjWCWWcBXiLAnp77ksfR, name=Bruce Banner, age=42) > Person(id=QjWCWWcBXiLAnp77ksfR, name=Bruce Banner, age=42) > QjWCWWcBXiLAnp77ksfR > 0 1 Insert a new Person document into the marvel index . The id is generated on server side and set into the instance returned. 2 Lookup the Person with matching id in the marvel index. 3 Delete the Person with matching id , extracted from the given instance, in the marvel index. 4 Count the total number of documents in the marvel index. 5 Don’t forget to subscribe() ."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/entity-callbacks.html","Entity Callbacks: The Spring Data infrastructure provides hooks for modifying an entity before and after certain methods are invoked. Those so called EntityCallback instances provide a convenient way to check and potentially modify an entity in a callback fashioned style. An EntityCallback looks pretty much like a specialized ApplicationListener . Some Spring Data modules publish store specific events (such as BeforeSaveEvent ) that allow modifying the given entity. In some cases, such as when working with immutable types, these events can cause trouble. Also, event publishing relies on ApplicationEventMulticaster . If configuring that with an asynchronous TaskExecutor it can lead to unpredictable outcomes, as event processing can be forked onto a Thread. Entity callbacks provide integration points with both synchronous and reactive APIs to guarantee in-order execution at well-defined checkpoints within the processing chain, returning a potentially modified entity or an reactive wrapper type. Entity callbacks are typically separated by API type. This separation means that a synchronous API considers only synchronous entity callbacks and a reactive implementation considers only reactive entity callbacks. The Entity Callback API has been introduced with Spring Data Commons 2.2. It is the recommended way of applying entity modifications. Existing store specific ApplicationEvents are still published before the invoking potentially registered EntityCallback instances. Implementing Entity Callbacks: An EntityCallback is directly associated with its domain type through its generic type argument. Each Spring Data module typically ships with a set of predefined EntityCallback interfaces covering the entity lifecycle. Anatomy of an EntityCallback @FunctionalInterface public interface BeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked before a domain object is saved. * Can return either the same or a modified instance. * * @return the domain object to be persisted. */ (1) T onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called before an entity is saved. Returns a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Anatomy of a reactive EntityCallback @FunctionalInterface public interface ReactiveBeforeSaveCallback<T> extends EntityCallback<T> { /** * Entity callback method invoked on subscription, before a domain object is saved. * The returned Publisher can emit either the same or a modified instance. * * @return Publisher emitting the domain object to be persisted. */ (1) Publisher<T> onBeforeSave(T entity, (2) String collection); (3) } 1 BeforeSaveCallback specific method to be called on subscription, before an entity is saved. Emits a potentially modifed instance. 2 The entity right before persisting. 3 A number of store specific arguments like the collection the entity is persisted to. Optional entity callback parameters are defined by the implementing Spring Data module and inferred from call site of EntityCallback.callback() . Implement the interface suiting your application needs like shown in the example below: Example BeforeSaveCallback class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { (1) if(collection == ""user"") { return // ... } return // ... } @Override public int getOrder() { return 100; (2) } } 1 Callback implementation according to your requirements. 2 Potentially order the entity callback if multiple ones for the same domain type exist. Ordering follows lowest precedence. Registering Entity Callbacks: EntityCallback beans are picked up by the store specific implementations in case they are registered in the ApplicationContext . Most template APIs already implement ApplicationContextAware and therefore have access to the ApplicationContext The following example explains a collection of valid entity callback registrations: Example EntityCallback Bean registration @Order(1) (1) @Component class First implements BeforeSaveCallback<Person> { @Override public Person onBeforeSave(Person person) { return // ... } } @Component class DefaultingEntityCallback implements BeforeSaveCallback<Person>, Ordered { (2) @Override public Object onBeforeSave(Person entity, String collection) { // ... } @Override public int getOrder() { return 100; (2) } } @Configuration public class EntityCallbackConfiguration { @Bean BeforeSaveCallback<Person> unorderedLambdaReceiverCallback() { (3) return (BeforeSaveCallback<Person>) it -> // ... } } @Component class UserCallbacks implements BeforeConvertCallback<User>, BeforeSaveCallback<User> { (4) @Override public Person onBeforeConvert(User user) { return // ... } @Override public Person onBeforeSave(User user) { return // ... } } 1 BeforeSaveCallback receiving its order from the @Order annotation. 2 BeforeSaveCallback receiving its order via the Ordered interface implementation. 3 BeforeSaveCallback using a lambda expression. Unordered by default and invoked last. Note that callbacks implemented by a lambda expression do not expose typing information hence invoking these with a non-assignable entity affects the callback throughput. Use a class or enum to enable type filtering for the callback bean. 4 Combine multiple entity callback interfaces in a single implementation class. Store specific EntityCallbacks: Spring Data Elasticsearch uses the EntityCallback API internally for its auditing support and reacts on the following callbacks: Table 1. Supported Entity Callbacks Callback Method Description Order Reactive/BeforeConvertCallback onBeforeConvert(T entity, IndexCoordinates index) Invoked before a domain object is converted to org.springframework.data.elasticsearch.core.document.Document . Can return the entity or a modified entity which then will be converted. Ordered.LOWEST_PRECEDENCE Reactive/AfterLoadCallback onAfterLoad(Document document, Class<T> type, IndexCoordinates indexCoordinates) Invoked after the result from Elasticsearch has been read into a org.springframework.data.elasticsearch.core.document.Document . Ordered.LOWEST_PRECEDENCE Reactive/AfterConvertCallback onAfterConvert(T entity, Document document, IndexCoordinates indexCoordinates) Invoked after a domain object is converted from org.springframework.data.elasticsearch.core.document.Document on reading result data from Elasticsearch. Ordered.LOWEST_PRECEDENCE Reactive/AuditingEntityCallback onBeforeConvert(Object entity, IndexCoordinates index) Marks an auditable entity created or modified 100 Reactive/AfterSaveCallback T onAfterSave(T entity, IndexCoordinates index) Invoked after a domain object is saved. Ordered.LOWEST_PRECEDENCE"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/auditing.html","Elasticsearch Auditing: Preparing entities: In order for the auditing code to be able to decide whether an entity instance is new, the entity must implement the Persistable<ID> interface which is defined as follows: package org.springframework.data.domain; import org.springframework.lang.Nullable; public interface Persistable<ID> { @Nullable ID getId(); boolean isNew(); } As the existence of an Id is not a sufficient criterion to determine if an enitity is new in Elasticsearch, additional information is necessary. One way is to use the creation-relevant auditing fields for this decision: A Person entity might look as follows - omitting getter and setter methods for brevity: @Document(indexName = ""person"") public class Person implements Persistable<Long> { @Id private Long id; private String lastName; private String firstName; @CreatedDate @Field(type = FieldType.Date, format = DateFormat.basic_date_time) private Instant createdDate; @CreatedBy private String createdBy @Field(type = FieldType.Date, format = DateFormat.basic_date_time) @LastModifiedDate private Instant lastModifiedDate; @LastModifiedBy private String lastModifiedBy; public Long getId() { (1) return id; } @Override public boolean isNew() { return id == null || (createdDate == null && createdBy == null); (2) } } 1 the getter is the required implementation from the interface 2 an object is new if it either has no id or none of fields containing creation attributes are set. Activating auditing: After the entities have been set up and providing the AuditorAware - or ReactiveAuditorAware - the Auditing must be activated by setting the @EnableElasticsearchAuditing on a configuration class: @Configuration @EnableElasticsearchRepositories @EnableElasticsearchAuditing class MyConfiguration { // configuration code } When using the reactive stack this must be: @Configuration @EnableReactiveElasticsearchRepositories @EnableReactiveElasticsearchAuditing class MyConfiguration { // configuration code } If your code contains more than one AuditorAware bean for different types, you must provide the name of the bean to use as an argument to the auditorAwareRef parameter of the @EnableElasticsearchAuditing annotation."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/join-types.html","Join-Type implementation: Spring Data Elasticsearch supports the Join data type(https://www.elastic.co/guide/en/elasticsearch/reference/current/parent-join.html) for creating the corresponding index mappings and for storing the relevant information. Setting up the data: For an entity to be used in a parent child join relationship, it must have a property of type JoinField which must be annotated. Let’s assume a Statement entity where a statement may be a question , an answer , a comment or a vote (a Builder is also shown in this example, it’s not necessary, but later used in the sample code): @Document(indexName = ""statements"") @Routing(""routing"") (1) public class Statement { @Id private String id; @Field(type = FieldType.Text) private String text; @Field(type = FieldType.Keyword) private String routing; @JoinTypeRelations( relations = { @JoinTypeRelation(parent = ""question"", children = {""answer"", ""comment""}), (2) @JoinTypeRelation(parent = ""answer"", children = ""vote"") (3) } ) private JoinField<String> relation; (4) private Statement() { } public static StatementBuilder builder() { return new StatementBuilder(); } public String getId() { return id; } public void setId(String id) { this.id = id; } public String getRouting() { return routing; } public void setRouting(String routing) { this.routing = routing; } public String getText() { return text; } public void setText(String text) { this.text = text; } public JoinField<String> getRelation() { return relation; } public void setRelation(JoinField<String> relation) { this.relation = relation; } public static final class StatementBuilder { private String id; private String text; private String routing; private JoinField<String> relation; private StatementBuilder() { } public StatementBuilder withId(String id) { this.id = id; return this; } public StatementBuilder withRouting(String routing) { this.routing = routing; return this; } public StatementBuilder withText(String text) { this.text = text; return this; } public StatementBuilder withRelation(JoinField<String> relation) { this.relation = relation; return this; } public Statement build() { Statement statement = new Statement(); statement.setId(id); statement.setRouting(routing); statement.setText(text); statement.setRelation(relation); return statement; } } } 1 for routing related info see Routing values(routing.html) 2 a question can have answers and comments 3 an answer can have votes 4 the JoinField property is used to combine the name ( question , answer , comment or vote ) of the relation with the parent id. The generic type must be the same as the @Id annotated property. Spring Data Elasticsearch will build the following mapping for this class: { ""statements"": { ""mappings"": { ""properties"": { ""_class"": { ""type"": ""text"", ""fields"": { ""keyword"": { ""type"": ""keyword"", ""ignore_above"": 256 } } }, ""routing"": { ""type"": ""keyword"" }, ""relation"": { ""type"": ""join"", ""eager_global_ordinals"": true, ""relations"": { ""question"": [ ""answer"", ""comment"" ], ""answer"": ""vote"" } }, ""text"": { ""type"": ""text"" } } } } } Storing data: Given a repository for this class the following code inserts a question, two answers, a comment and a vote: void init() { repository.deleteAll(); Statement savedWeather = repository.save( Statement.builder() .withText(""How is the weather?"") .withRelation(new JoinField<>(""question"")) (1) .build()); Statement sunnyAnswer = repository.save( Statement.builder() .withText(""sunny"") .withRelation(new JoinField<>(""answer"", savedWeather.getId())) (2) .build()); repository.save( Statement.builder() .withText(""rainy"") .withRelation(new JoinField<>(""answer"", savedWeather.getId())) (3) .build()); repository.save( Statement.builder() .withText(""I don't like the rain"") .withRelation(new JoinField<>(""comment"", savedWeather.getId())) (4) .build()); repository.save( Statement.builder() .withText(""+1 for the sun"") .withRouting(savedWeather.getId()) .withRelation(new JoinField<>(""vote"", sunnyAnswer.getId())) (5) .build()); } 1 create a question statement 2 the first answer to the question 3 the second answer 4 a comment to the question 5 a vote for the first answer, this needs to have the routing set to the weather document, see Routing values(routing.html) . Retrieving data: Currently native queries must be used to query the data, so there is no support from standard repository methods. Custom Repository Implementations(../repositories/custom-implementations.html) can be used instead. The following code shows as an example how to retrieve all entries that have a vote (which must be answers , because only answers can have a vote) using an ElasticsearchOperations instance: SearchHits<Statement> hasVotes() { Query query = NativeQuery.builder() .withQuery(co.elastic.clients.elasticsearch._types.query_dsl.Query.of(qb -> qb .hasChild(hc -> hc .type(""answer"") .queryName(""vote"") .query(matchAllQueryAsQuery()) .scoreMode(ChildScoreMode.None) ))) .build(); return operations.search(query, Statement.class); }"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/routing.html","Routing values: When Elasticsearch stores a document in an index that has multiple shards, it determines the shard to you use based on the id of the document. Sometimes it is necessary to predefine that multiple documents should be indexed on the same shard (join-types, faster search for related data). For this Elasticsearch offers the possibility to define a routing, which is the value that should be used to calculate the shard from instead of the id . Spring Data Elasticsearch supports routing definitions on storing and retrieving data in the following ways: Routing on join-types: When using join-types (see Join-Type implementation(join-types.html) ), Spring Data Elasticsearch will automatically use the parent property of the entity’s JoinField property as the value for the routing. This is correct for all the use-cases where the parent-child relationship has just one level. If it is deeper, like a child-parent-grandparent relationship - like in the above example from vote → answer → question - then the routing needs to explicitly specified by using the techniques described in the next section (the vote needs the question.id as routing value). Custom routing values: To define a custom routing for an entity, Spring Data Elasticsearch provides a @Routing annotation (reusing the Statement class from above): @Document(indexName = ""statements"") @Routing(""routing"") (1) public class Statement { @Id private String id; @Field(type = FieldType.Text) private String text; @JoinTypeRelations( relations = { @JoinTypeRelation(parent = ""question"", children = {""answer"", ""comment""}), @JoinTypeRelation(parent = ""answer"", children = ""vote"") } ) private JoinField<String> relation; @Nullable @Field(type = FieldType.Keyword) private String routing; (2) // getter/setter... } 1 This defines ""routing"" as routing specification 2 a property with the name routing If the routing specification of the annotation is a plain string and not a SpEL expression, it is interpreted as the name of a property of the entity, in the example it’s the routing property. The value of this property will then be used as the routing value for all requests that use the entity. It is also possible to us a SpEL expression in the @Document annotation like this: @Document(indexName = ""statements"") @Routing(""@myBean.getRouting(#entity)"") public class Statement{ // all the needed stuff } In this case the user needs to provide a bean with the name myBean that has a method String getRouting(Object) . To reference the entity ""#entity"" must be used in the SpEL expression, and the return value must be null or the routing value as a String. If plain property’s names and SpEL expressions are not enough to customize the routing definitions, it is possible to define provide an implementation of the RoutingResolver interface. This can then be set on the ElasticOperations instance: RoutingResolver resolver = ...; ElasticsearchOperations customOperations= operations.withRouting(resolver); The withRouting() functions return a copy of the original ElasticsearchOperations instance with the customized routing set. When a routing has been defined on an entity when it is stored in Elasticsearch, the same value must be provided when doing a get or delete operation. For methods that do not use an entity - like get(ID) or delete(ID) - the ElasticsearchOperations.withRouting(RoutingResolver) method can be used like this: String id = ""someId""; String routing = ""theRoutingValue""; // get an entity Statement s = operations .withRouting(RoutingResolver.just(routing)) (1) .get(id, Statement.class); // delete an entity operations.withRouting(RoutingResolver.just(routing)).delete(id); 1 RoutingResolver.just(s) returns a resolver that will just return the given String."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/misc.html","Miscellaneous Elasticsearch Operation Support: This chapter covers additional support for Elasticsearch operations that cannot be directly accessed via the repository interface. It is recommended to add those operations as custom implementation as described in Custom Repository Implementations(../repositories/custom-implementations.html) . Index settings: When creating Elasticsearch indices with Spring Data Elasticsearch different index settings can be defined by using the @Setting annotation. The following arguments are available: useServerConfiguration does not send any settings parameters, so the Elasticsearch server configuration determines them. settingPath refers to a JSON file defining the settings that must be resolvable in the classpath shards the number of shards to use, defaults to 1 replicas the number of replicas, defaults to 1 refreshIntervall , defaults to ""1s"" indexStoreType , defaults to ""fs"" It is as well possible to define index sorting(https://www.elastic.co/guide/en/elasticsearch/reference/7.11/index-modules-index-sorting.html) (check the linked Elasticsearch documentation for the possible field types and values): @Document(indexName = ""entities"") @Setting( sortFields = { ""secondField"", ""firstField"" }, (1) sortModes = { Setting.SortMode.max, Setting.SortMode.min }, (2) sortOrders = { Setting.SortOrder.desc, Setting.SortOrder.asc }, sortMissingValues = { Setting.SortMissing._last, Setting.SortMissing._first }) class Entity { @Nullable @Id private String id; @Nullable @Field(name = ""first_field"", type = FieldType.Keyword) private String firstField; @Nullable @Field(name = ""second_field"", type = FieldType.Keyword) private String secondField; // getter and setter... } 1 when defining sort fields, use the name of the Java property ( firstField ), not the name that might be defined for Elasticsearch ( first_field ) 2 sortModes , sortOrders and sortMissingValues are optional, but if they are set, the number of entries must match the number of sortFields elements Index Mapping: When Spring Data Elasticsearch creates the index mapping with the IndexOperations.createMapping() methods, it uses the annotations described in Mapping Annotation Overview(object-mapping.html#elasticsearch.mapping.meta-model.annotations) , especially the @Field annotation. In addition to that it is possible to add the @Mapping annotation to a class. This annotation has the following properties: mappingPath a classpath resource in JSON format; if this is not empty it is used as the mapping, no other mapping processing is done. enabled when set to false, this flag is written to the mapping and no further processing is done. dateDetection and numericDetection set the corresponding properties in the mapping when not set to DEFAULT . dynamicDateFormats when this String array is not empty, it defines the date formats used for automatic date detection. runtimeFieldsPath a classpath resource in JSON format containing the definition of runtime fields which is written to the index mappings, for example: { ""day_of_week"": { ""type"": ""keyword"", ""script"": { ""source"": ""emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))"" } } } Filter Builder: Filter Builder improves query speed. private ElasticsearchOperations operations; IndexCoordinates index = IndexCoordinates.of(""sample-index""); Query query = NativeQuery.builder() .withQuery(q -> q .matchAll(ma -> ma)) .withFilter( q -> q .bool(b -> b .must(m -> m .term(t -> t .field(""id"") .value(documentId)) ))) .build(); SearchHits<SampleEntity> sampleEntities = operations.search(query, SampleEntity.class, index); Using Scroll For Big Result Set: Elasticsearch has a scroll API for getting big result set in chunks. This is internally used by Spring Data Elasticsearch to provide the implementations of the <T> SearchHitsIterator<T> SearchOperations.searchForStream(Query query, Class<T> clazz, IndexCoordinates index) method. IndexCoordinates index = IndexCoordinates.of(""sample-index""); Query searchQuery = NativeQuery.builder() .withQuery(q -> q .matchAll(ma -> ma)) .withFields(""message"") .withPageable(PageRequest.of(0, 10)) .build(); SearchHitsIterator<SampleEntity> stream = elasticsearchOperations.searchForStream(searchQuery, SampleEntity.class, index); List<SampleEntity> sampleEntities = new ArrayList<>(); while (stream.hasNext()) { sampleEntities.add(stream.next()); } stream.close(); There are no methods in the SearchOperations API to access the scroll id, if it should be necessary to access this, the following methods of the AbstractElasticsearchTemplate can be used (this is the base implementation for the different ElasticsearchOperations implementations): @Autowired ElasticsearchOperations operations; AbstractElasticsearchTemplate template = (AbstractElasticsearchTemplate)operations; IndexCoordinates index = IndexCoordinates.of(""sample-index""); Query query = NativeQuery.builder() .withQuery(q -> q .matchAll(ma -> ma)) .withFields(""message"") .withPageable(PageRequest.of(0, 10)) .build(); SearchScrollHits<SampleEntity> scroll = template.searchScrollStart(1000, query, SampleEntity.class, index); String scrollId = scroll.getScrollId(); List<SampleEntity> sampleEntities = new ArrayList<>(); while (scroll.hasSearchHits()) { sampleEntities.addAll(scroll.getSearchHits()); scrollId = scroll.getScrollId(); scroll = template.searchScrollContinue(scrollId, 1000, SampleEntity.class); } template.searchScrollClear(scrollId); To use the Scroll API with repository methods, the return type must defined as Stream in the Elasticsearch Repository. The implementation of the method will then use the scroll methods from the ElasticsearchTemplate. interface SampleEntityRepository extends Repository<SampleEntity, String> { Stream<SampleEntity> findBy(); } Sort options: In addition to the default sort options described in Paging and Sorting(../repositories/query-methods-details.html#repositories.paging-and-sorting) , Spring Data Elasticsearch provides the class org.springframework.data.elasticsearch.core.query.Order which derives from org.springframework.data.domain.Sort.Order . It offers additional parameters that can be sent to Elasticsearch when specifying the sorting of the result (see www.elastic.co/guide/en/elasticsearch/reference/7.15/sort-search-results.html(https://www.elastic.co/guide/en/elasticsearch/reference/7.15/sort-search-results.html) ). There also is the org.springframework.data.elasticsearch.core.query.GeoDistanceOrder class which can be used to have the result of a search operation ordered by geographical distance. If the class to be retrieved has a GeoPoint property named location , the following Sort would sort the results by distance to the given point: Sort.by(new GeoDistanceOrder(""location"", new GeoPoint(48.137154, 11.5761247))) Runtime Fields: From version 7.12 on Elasticsearch has added the feature of runtime fields ( www.elastic.co/guide/en/elasticsearch/reference/7.12/runtime.html(https://www.elastic.co/guide/en/elasticsearch/reference/7.12/runtime.html) ). Spring Data Elasticsearch supports this in two ways: Runtime field definitions in the index mappings: The first way to define runtime fields is by adding the definitions to the index mappings (see www.elastic.co/guide/en/elasticsearch/reference/7.12/runtime-mapping-fields.html(https://www.elastic.co/guide/en/elasticsearch/reference/7.12/runtime-mapping-fields.html) ). To use this approach in Spring Data Elasticsearch the user must provide a JSON file that contains the corresponding definition, for example: Example 1. runtime-fields.json { ""day_of_week"": { ""type"": ""keyword"", ""script"": { ""source"": ""emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))"" } } } The path to this JSON file, which must be present on the classpath, must then be set in the @Mapping annotation of the entity: @Document(indexName = ""runtime-fields"") @Mapping(runtimeFieldsPath = ""/runtime-fields.json"") public class RuntimeFieldEntity { // properties, getter, setter,... } Runtime fields definitions set on a Query: The second way to define runtime fields is by adding the definitions to a search query (see www.elastic.co/guide/en/elasticsearch/reference/7.12/runtime-search-request.html(https://www.elastic.co/guide/en/elasticsearch/reference/7.12/runtime-search-request.html) ). The following code example shows how to do this with Spring Data Elasticsearch : The entity used is a simple object that has a price property: @Document(indexName = ""some_index_name"") public class SomethingToBuy { private @Id @Nullable String id; @Nullable @Field(type = FieldType.Text) private String description; @Nullable @Field(type = FieldType.Double) private Double price; // getter and setter } The following query uses a runtime field that calculates a priceWithTax value by adding 19% to the price and uses this value in the search query to find all entities where priceWithTax is higher or equal than a given value: RuntimeField runtimeField = new RuntimeField(""priceWithTax"", ""double"", ""emit(doc['price'].value * 1.19)""); Query query = new CriteriaQuery(new Criteria(""priceWithTax"").greaterThanEqual(16.5)); query.addRuntimeField(runtimeField); SearchHits<SomethingToBuy> searchHits = operations.search(query, SomethingToBuy.class); This works with every implementation of the Query interface. Point In Time (PIT) API: ElasticsearchOperations supports the point in time API of Elasticsearch (see www.elastic.co/guide/en/elasticsearch/reference/8.3/point-in-time-api.html(https://www.elastic.co/guide/en/elasticsearch/reference/8.3/point-in-time-api.html) ). The following code snippet shows how to use this feature with a fictional Person class: ElasticsearchOperations operations; // autowired Duration tenSeconds = Duration.ofSeconds(10); String pit = operations.openPointInTime(IndexCoordinates.of(""person""), tenSeconds); (1) // create query for the pit Query query1 = new CriteriaQueryBuilder(Criteria.where(""lastName"").is(""Smith"")) .withPointInTime(new Query.PointInTime(pit, tenSeconds)) (2) .build(); SearchHits<Person> searchHits1 = operations.search(query1, Person.class); // do something with the data // create 2nd query for the pit, use the id returned in the previous result Query query2 = new CriteriaQueryBuilder(Criteria.where(""lastName"").is(""Miller"")) .withPointInTime( new Query.PointInTime(searchHits1.getPointInTimeId(), tenSeconds)) (3) .build(); SearchHits<Person> searchHits2 = operations.search(query2, Person.class); // do something with the data operations.closePointInTime(searchHits2.getPointInTimeId()); (4) 1 create a point in time for an index (can be multiple names) and a keep-alive duration and retrieve its id 2 pass that id into the query to search together with the next keep-alive value 3 for the next query, use the id returned from the previous search 4 when done, close the point in time using the last returned id Search Template support: Use of the search template API is supported. To use this, it first is necessary to create a stored script. The ElasticsearchOperations interface extends ScriptOperations which provides the necessary functions. The example used here assumes that we have Person entity with a property named firstName . A search template script can be saved like this: import org.springframework.data.elasticsearch.core.ElasticsearchOperations; import org.springframework.data.elasticsearch.core.script.Script; operations.putScript( (1) Script.builder() .withId(""person-firstname"") (2) .withLanguage(""mustache"") (3) .withSource("""""" (4) { ""query"": { ""bool"": { ""must"": [ { ""match"": { ""firstName"": ""{{firstName}}"" (5) } } ] } }, ""from"": ""{{from}}"", (6) ""size"": ""{{size}}"" (7) } """""") .build() ); 1 Use the putScript() method to store a search template script 2 The name / id of the script 3 Scripts that are used in search templates must be in the mustache language. 4 The script source 5 The search parameter in the script 6 Paging request offset 7 Paging request size To use a search template in a search query, Spring Data Elasticsearch provides the SearchTemplateQuery , an implementation of the org.springframework.data.elasticsearch.core.query.Query interface. In the following code, we will add a call using a search template query to a custom repository implementation (see Custom Repository Implementations(../repositories/custom-implementations.html) ) as an example how this can be integrated into a repository call. We first define the custom repository fragment interface: interface PersonCustomRepository { SearchPage<Person> findByFirstNameWithSearchTemplate(String firstName, Pageable pageable); } The implementation of this repository fragment looks like this: public class PersonCustomRepositoryImpl implements PersonCustomRepository { private final ElasticsearchOperations operations; public PersonCustomRepositoryImpl(ElasticsearchOperations operations) { this.operations = operations; } @Override public SearchPage<Person> findByFirstNameWithSearchTemplate(String firstName, Pageable pageable) { var query = SearchTemplateQuery.builder() (1) .withId(""person-firstname"") (2) .withParams( Map.of( (3) ""firstName"", firstName, ""from"", pageable.getOffset(), ""size"", pageable.getPageSize() ) ) .build(); SearchHits<Person> searchHits = operations.search(query, Person.class); (4) return SearchHitSupport.searchPageFor(searchHits, pageable); } } 1 Create a SearchTemplateQuery 2 Provide the id of the search template 3 The parameters are passed in a Map<String,Object> 4 Do the search in the same way as with the other query types. Nested sort: Spring Data Elasticsearch supports sorting within nested objects ( www.elastic.co/guide/en/elasticsearch/reference/8.9/sort-search-results.html#nested-sorting(https://www.elastic.co/guide/en/elasticsearch/reference/8.9/sort-search-results.html#nested-sorting) ) The following example, taken from the org.springframework.data.elasticsearch.core.query.sort.NestedSortIntegrationTests class, shows how to define the nested sort. var filter = StringQuery.builder("""""" { ""term"": {""movies.actors.sex"": ""m""} } """""").build(); var order = new org.springframework.data.elasticsearch.core.query.Order(Sort.Direction.DESC, ""movies.actors.yearOfBirth"") .withNested( Nested.builder(""movies"") .withNested( Nested.builder(""movies.actors"") .withFilter(filter) .build()) .build()); var query = Query.findAll().addSort(Sort.by(order)); About the filter query: It is not possible to use a CriteriaQuery here, as this query would be converted into a Elasticsearch nested query which does not work in the filter context. So only StringQuery or NativeQuery can be used here. When using one of these, like the term query above, the Elasticsearch field names must be used, so take care, when these are redefined with the @Field(name=""…​"") definition. For the definition of the order path and the nested paths, the Java entity property names should be used."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/scripted-and-runtime-fields.html","Scripted and runtime fields: Spring Data Elasticsearch supports scripted fields and runtime fields. Please refer to the Elasticsearch documentation about scripting ( www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html(https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html) ) and runtime fields ( www.elastic.co/guide/en/elasticsearch/reference/8.9/runtime.html(https://www.elastic.co/guide/en/elasticsearch/reference/8.9/runtime.html) ) for detailed information about this. In the context of Spring Data Elasticsearch you can use scripted fields that are used to return fields that are calculated on the result documents and added to the returned document. runtime fields that are calculated on the stored documents and can be used in a query and/or be returned in the search result. The following code snippets will show what you can do (these show imperative code, but the reactive implementation works similar). The person entity: The enity that is used in these examples is a Person entity. This entity has a birthDate and an age property. Whereas the birthdate is fix, the age depends on the time when a query is issued and needs to be calculated dynamically. import org.springframework.data.annotation.Id; import org.springframework.data.elasticsearch.annotations.DateFormat; import org.springframework.data.elasticsearch.annotations.Document; import org.springframework.data.elasticsearch.annotations.Field; import org.springframework.data.elasticsearch.annotations.ScriptedField; import org.springframework.lang.Nullable; import java.time.LocalDate; import java.time.format.DateTimeFormatter; import static org.springframework.data.elasticsearch.annotations.FieldType.*; import java.lang.Integer; @Document(indexName = ""persons"") public record Person( @Id @Nullable String id, @Field(type = Text) String lastName, @Field(type = Text) String firstName, @Field(type = Keyword) String gender, @Field(type = Date, format = DateFormat.basic_date) LocalDate birthDate, @Nullable @ScriptedField Integer age (1) ) { public Person(String id,String lastName, String firstName, String gender, String birthDate) { this(id, (2) lastName, firstName, LocalDate.parse(birthDate, DateTimeFormatter.ISO_LOCAL_DATE), gender, null); } } 1 the age property will be calculated and filled in search results. 2 a convenience constructor to set up the test data. Note that the age property is annotated with @ScriptedField . This inhibits the writing of a corresponding entry in the index mapping and marks the property as a target to put a calculated field from a search response. The repository interface: The repository used in this example: public interface PersonRepository extends ElasticsearchRepository<Person, String> { SearchHits<Person> findAllBy(ScriptedField scriptedField); SearchHits<Person> findByGenderAndAgeLessThanEqual(String gender, Integer age, RuntimeField runtimeField); } The service class: The service class has a repository injected and an ElasticsearchOperations instance to show several ways of populating and using the age property. We show the code split up in different pieces to put the explanations in import org.springframework.data.elasticsearch.core.ElasticsearchOperations; import org.springframework.data.elasticsearch.core.SearchHits; import org.springframework.data.elasticsearch.core.query.Criteria; import org.springframework.data.elasticsearch.core.query.CriteriaQuery; import org.springframework.data.elasticsearch.core.query.FetchSourceFilter; import org.springframework.data.elasticsearch.core.query.RuntimeField; import org.springframework.data.elasticsearch.core.query.ScriptData; import org.springframework.data.elasticsearch.core.query.ScriptType; import org.springframework.data.elasticsearch.core.query.ScriptedField; import org.springframework.data.elasticsearch.core.query.StringQuery; import org.springframework.stereotype.Service; import java.util.List; @Service public class PersonService { private final ElasticsearchOperations operations; private final PersonRepository repository; public PersonService(ElasticsearchOperations operations, SaRPersonRepository repository) { this.operations = operations; this.repository = repository; } public void save() { (1) List<Person> persons = List.of( new Person(""1"", ""Smith"", ""Mary"", ""f"", ""1987-05-03""), new Person(""2"", ""Smith"", ""Joshua"", ""m"", ""1982-11-17""), new Person(""3"", ""Smith"", ""Joanna"", ""f"", ""2018-03-27""), new Person(""4"", ""Smith"", ""Alex"", ""m"", ""2020-08-01""), new Person(""5"", ""McNeill"", ""Fiona"", ""f"", ""1989-04-07""), new Person(""6"", ""McNeill"", ""Michael"", ""m"", ""1984-10-20""), new Person(""7"", ""McNeill"", ""Geraldine"", ""f"", ""2020-03-02""), new Person(""8"", ""McNeill"", ""Patrick"", ""m"", ""2022-07-04"")); repository.saveAll(persons); } 1 a utility method to store some data in Elasticsearch. Scripted fields: The next piece shows how to use a scripted field to calculate and return the age of the persons. Scripted fields can only add something to the returned data, the age cannot be used in the query (see runtime fields for that). public SearchHits<Person> findAllWithAge() { var scriptedField = ScriptedField.of(""age"", (1) ScriptData.of(b -> b .withType(ScriptType.INLINE) .withScript("""""" Instant currentDate = Instant.ofEpochMilli(new Date().getTime()); Instant startDate = doc['birth-date'].value.toInstant(); return (ChronoUnit.DAYS.between(startDate, currentDate) / 365); """"""))); // version 1: use a direct query var query = new StringQuery("""""" { ""match_all"": {} } """"""); query.addScriptedField(scriptedField); (2) query.addSourceFilter(FetchSourceFilter.of(b -> b.withIncludes(""*""))); (3) var result1 = operations.search(query, Person.class); (4) // version 2: use the repository var result2 = repository.findAllBy(scriptedField); (5) return result1; } 1 define the ScriptedField that calculates the age of a person. 2 when using a Query , add the scripted field to the query. 3 when adding a scripted field to a Query , an additional source filter is needed to also retrieve the normal fields from the document source. 4 get the data where the Person entities now have the values set in their age property. 5 when using the repository, all that needs to be done is adding the scripted field as method parameter. Runtime fields: When using runtime fields, the calculated value can be used in the query itself. In the following code this is used to run a query for a given gender and maximum age of persons: public SearchHits<Person> findWithGenderAndMaxAge(String gender, Integer maxAge) { var runtimeField = new RuntimeField(""age"", ""long"", """""" (1) Instant currentDate = Instant.ofEpochMilli(new Date().getTime()); Instant startDate = doc['birthDate'].value.toInstant(); emit (ChronoUnit.DAYS.between(startDate, currentDate) / 365); """"""); // variant 1 : use a direct query var query = CriteriaQuery.builder(Criteria .where(""gender"").is(gender) .and(""age"").lessThanEqual(maxAge)) .withRuntimeFields(List.of(runtimeField)) (2) .withFields(""age"") (3) .withSourceFilter(FetchSourceFilter.of(b -> b.withIncludes(""*""))) (4) .build(); var result1 = operations.search(query, Person.class); (5) // variant 2: use the repository (6) var result2 = repository.findByGenderAndAgeLessThanEqual(gender, maxAge, runtimeField); return result1; } } 1 define the runtime field that calculates the age of a person. // see asciidoctor.org/docs/user-manual/#builtin-attributes(https://asciidoctor.org/docs/user-manual/#builtin-attributes) for builtin attributes. 2 when using Query , add the runtime field. 3 when adding a scripted field to a Query , an additional field parameter is needed to have the calculated value returned. 4 when adding a scripted field to a Query , an additional source filter is needed to also retrieve the normal fields from the document source. 5 get the data filtered with the query and where the returned entites have the age property set. 6 when using the repository, all that needs to be done is adding the runtime field as method parameter. In addition to define a runtime fields on a query, they can also be defined in the index by setting the runtimeFieldsPath property of the @Mapping annotation to point to a JSON file that contains the runtime field definitions."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories.html","Repositories: This chapter explains the basic foundations of Spring Data repositories and Elasticsearch specifics. Before continuing to the Elasticsearch specifics, make sure you have a sound understanding of the basic concepts. The goal of the Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. Section Summary: Core concepts(repositories/core-concepts.html) Defining Repository Interfaces(repositories/definition.html) Elasticsearch Repositories(elasticsearch/repositories/elasticsearch-repositories.html) Reactive Elasticsearch Repositories(elasticsearch/repositories/reactive-elasticsearch-repositories.html) Creating Repository Instances(repositories/create-instances.html) Defining Query Methods(repositories/query-methods-details.html) Query methods(elasticsearch/repositories/elasticsearch-repository-queries.html) Projections(repositories/projections.html) Custom Repository Implementations(repositories/custom-implementations.html) Publishing Events from Aggregate Roots(repositories/core-domain-events.html) Null Handling of Repository Methods(repositories/null-handling.html) CDI Integration(elasticsearch/repositories/cdi-integration.html) Repository query keywords(repositories/query-keywords-reference.html) Repository query return types(repositories/query-return-types-reference.html)"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); } Entity State Detection Strategies: The following table describes the strategies that Spring Data offers for detecting whether an entity is new: Table 1. Options for detection whether an entity is new in Spring Data @Id -Property inspection (the default) By default, Spring Data inspects the identifier property of the given entity. If the identifier property is null or 0 in case of primitive types, then the entity is assumed to be new. Otherwise, it is assumed to not be new. @Version -Property inspection If a property annotated with @Version is present and null , or in case of a version property of primitive type 0 the entity is considered new. If the version property is present but has a different value, the entity is considered to not be new. If no version property is present Spring Data falls back to inspection of the identifier property. Implementing Persistable If an entity implements Persistable , Spring Data delegates the new detection to the isNew(…) method of the entity. See the Javadoc(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//index.html?org/springframework/data/domain/Persistable.html) for details. Note: Properties of Persistable will get detected and persisted if you use AccessType.PROPERTY . To avoid that, use @Transient . Providing a custom EntityInformation implementation You can customize the EntityInformation abstraction used in the repository base implementation by creating a subclass of the module specific repository factory and overriding the getEntityInformation(…) method. You then have to register the custom implementation of module specific repository factory as a Spring bean. Note that this should rarely be necessary."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/repositories/elasticsearch-repositories.html","Elasticsearch Repositories: This chapter includes details of the Elasticsearch repository implementation. Example 1. The sample Book entity @Document(indexName=""books"") class Book { @Id private String id; @Field(type = FieldType.Text) private String name; @Field(type = FieldType.Text) private String summary; @Field(type = FieldType.Integer) private Integer price; // getter/setter ... } Automatic creation of indices with the corresponding mapping: The @Document annotation has an argument createIndex . If this argument is set to true - which is the default value - Spring Data Elasticsearch will during bootstrapping the repository support on application startup check if the index defined by the @Document annotation exists. If it does not exist, the index will be created and the mappings derived from the entity’s annotations (see Elasticsearch Object Mapping(../object-mapping.html) ) will be written to the newly created index. Details of the index that will be created can be set by using the @Setting annotation, refer to Index settings(../misc.html#elasticsearc.misc.index.settings) for further information. Annotations for repository methods: @Highlight: The @Highlight annotation on a repository method defines for which fields of the returned entity highlighting should be included.To search for some text in a Book 's name or summary and have the found data highlighted, the following repository method can be used: interface BookRepository extends Repository<Book, String> { @Highlight(fields = { @HighlightField(name = ""name""), @HighlightField(name = ""summary"") }) SearchHits<Book> findByNameOrSummary(String text, String summary); } It is possible to define multiple fields to be highlighted like above, and both the @Highlight and the @HighlightField annotation can further be customized with a @HighlightParameters annotation. Check the Javadocs for the possible configuration options. In the search results the highlight data can be retrieved from the SearchHit class. @SourceFilters: Sometimes the user does not need to have all the properties of an entity returned from a search but only a subset. Elasticsearch provides source filtering to reduce the amount of data that is transferred across the network to the application. When working with Query implementations and the ElasticsearchOperations this is easily possible by setting a source filter on the query. When using repository methods there is the @SourceFilters annotation: interface BookRepository extends Repository<Book, String> { @SourceFilters(includes = ""name"") SearchHits<Book> findByName(String text); } In this example, all the properties of the returned Book objects would be null except the name. Annotation based configuration: The Spring Data Elasticsearch repositories support can be activated using an annotation through JavaConfig. Example 2. Spring Data Elasticsearch repositories using JavaConfig @Configuration @EnableElasticsearchRepositories( (1) basePackages = ""org.springframework.data.elasticsearch.repositories"" ) static class Config { @Bean public ElasticsearchOperations elasticsearchTemplate() { (2) // ... } } class ProductService { private ProductRepository repository; (3) public ProductService(ProductRepository repository) { this.repository = repository; } public Page<Product> findAvailableBookByName(String name, Pageable pageable) { return repository.findByAvailableTrueAndNameStartingWith(name, pageable); } } 1 The EnableElasticsearchRepositories annotation activates the Repository support. If no base package is configured, it will use the one of the configuration class it is put on. 2 Provide a Bean named elasticsearchTemplate of type ElasticsearchOperations by using one of the configurations shown in the Elasticsearch Operations(../template.html) chapter. 3 Let Spring inject the Repository bean into your class. Spring Namespace: The Spring Data Elasticsearch module contains a custom namespace allowing definition of repository beans as well as elements for instantiating a ElasticsearchServer . Using the repositories element looks up Spring Data repositories as described in Creating Repository Instances(../../repositories/create-instances.html) . Example 3. Setting up Elasticsearch repositories using Namespace <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:elasticsearch=""http://www.springframework.org/schema/data/elasticsearch"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/data/elasticsearch https://www.springframework.org/schema/data/elasticsearch/spring-elasticsearch-1.0.xsd""> <elasticsearch:repositories base-package=""com.acme.repositories"" /> </beans> Using the Transport Client or Rest Client element registers an instance of Elasticsearch Server in the context. Example 4. Transport Client using Namespace <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:elasticsearch=""http://www.springframework.org/schema/data/elasticsearch"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/data/elasticsearch https://www.springframework.org/schema/data/elasticsearch/spring-elasticsearch-1.0.xsd""> <elasticsearch:transport-client id=""client"" cluster-nodes=""localhost:9300,someip:9300"" /> </beans> Example 5. Rest Client using Namespace <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:elasticsearch=""http://www.springframework.org/schema/data/elasticsearch"" xsi:schemaLocation=""http://www.springframework.org/schema/data/elasticsearch https://www.springframework.org/schema/data/elasticsearch/spring-elasticsearch.xsd http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd""> <elasticsearch:rest-client id=""restClient"" hosts=""http://localhost:9200""> </beans>"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/repositories/reactive-elasticsearch-repositories.html","Reactive Elasticsearch Repositories: Reactive Elasticsearch repository support builds on the core repository support explained in Repositories(../../repositories.html) utilizing operations provided via Reactive Elasticsearch Operations(../reactive-template.html) executed by a Reactive REST Client(../clients.html#elasticsearch.clients.reactiverestclient) . Spring Data Elasticsearch reactive repository support uses Project Reactor(https://projectreactor.io/) as its reactive composition library of choice. There are 3 main interfaces to be used: ReactiveRepository ReactiveCrudRepository ReactiveSortingRepository Usage: To access domain objects stored in a Elasticsearch using a Repository , just create an interface for it. Before you can actually go on and do that you will need an entity. Example 1. Sample Person entity public class Person { @Id private String id; private String firstname; private String lastname; private Address address; // … getters and setters omitted } Please note that the id property needs to be of type String . Example 2. Basic repository interface to persist Person entities interface ReactivePersonRepository extends ReactiveSortingRepository<Person, String> { Flux<Person> findByFirstname(String firstname); (1) Flux<Person> findByFirstname(Publisher<String> firstname); (2) Flux<Person> findByFirstnameOrderByLastname(String firstname); (3) Flux<Person> findByFirstname(String firstname, Sort sort); (4) Flux<Person> findByFirstname(String firstname, Pageable page); (5) Mono<Person> findByFirstnameAndLastname(String firstname, String lastname); (6) Mono<Person> findFirstByLastname(String lastname); (7) @Query(""{ \""bool\"" : { \""must\"" : { \""term\"" : { \""lastname\"" : \""?0\"" } } } }"") Flux<Person> findByLastname(String lastname); (8) Mono<Long> countByFirstname(String firstname) (9) Mono<Boolean> existsByFirstname(String firstname) (10) Mono<Long> deleteByFirstname(String firstname) (11) } 1 The method shows a query for all people with the given lastname . 2 Finder method awaiting input from Publisher to bind parameter value for firstname . 3 Finder method ordering matching documents by lastname . 4 Finder method ordering matching documents by the expression defined via the Sort parameter. 5 Use Pageable to pass offset and sorting parameters to the database. 6 Finder method concating criteria using And / Or keywords. 7 Find the first matching entity. 8 The method shows a query for all people with the given lastname looked up by running the annotated @Query with given parameters. 9 Count all entities with matching firstname . 10 Check if at least one entity with matching firstname exists. 11 Delete all entities with matching firstname . Configuration: For Java configuration, use the @EnableReactiveElasticsearchRepositories annotation. If no base package is configured, the infrastructure scans the package of the annotated configuration class. The following listing shows how to use Java configuration for a repository: Example 3. Java configuration for repositories @Configuration @EnableReactiveElasticsearchRepositories public class Config extends AbstractReactiveElasticsearchConfiguration { @Override public ReactiveElasticsearchClient reactiveElasticsearchClient() { return ReactiveRestClients.create(ClientConfiguration.localhost()); } } Because the repository from the previous example extends ReactiveSortingRepository , all CRUD operations are available as well as methods for sorted access to the entities. Working with the repository instance is a matter of dependency injecting it into a client, as the following example shows: Example 4. Sorted access to Person entities public class PersonRepositoryTests { @Autowired ReactivePersonRepository repository; @Test public void sortsElementsCorrectly() { Flux<Person> persons = repository.findAll(Sort.by(new Order(ASC, ""lastname""))); // ... } }"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/create-instances.html","Creating Repository Instances: This section covers how to create instances and bean definitions for the defined repository interfaces. Java Configuration: Use the store-specific @EnableElasticsearchRepositories annotation on a Java configuration class to define a configuration for repository activation. For an introduction to Java-based configuration of the Spring container, see JavaConfig in the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/java.html) . A sample configuration to enable Spring Data repositories resembles the following: Sample annotation-based repository configuration @Configuration @EnableJpaRepositories(""com.acme.repositories"") class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } The preceding example uses the JPA-specific annotation, which you would change according to the store module you actually use. The same applies to the definition of the EntityManagerFactory bean. See the sections covering the store-specific configuration. XML Configuration: Each Spring Data module includes a repositories element that lets you define a base package that Spring scans for you, as shown in the following example: Enabling Spring Data repositories via XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans:beans xmlns:beans=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans:beans> In the preceding example, Spring is instructed to scan com.acme.repositories and all its sub-packages for interfaces extending Repository or one of its sub-interfaces. For each interface found, the infrastructure registers the persistence technology-specific FactoryBean to create the appropriate proxies that handle invocations of the query methods. Each bean is registered under a bean name that is derived from the interface name, so an interface of UserRepository would be registered under userRepository . Bean names for nested repository interfaces are prefixed with their enclosing type name. The base package attribute allows wildcards so that you can define a pattern of scanned packages. Using Filters: By default, the infrastructure picks up every interface that extends the persistence technology-specific Repository sub-interface located under the configured base package and creates a bean instance for it. However, you might want more fine-grained control over which interfaces have bean instances created for them. To do so, use filter elements inside the repository declaration. The semantics are exactly equivalent to the elements in Spring’s component filters. For details, see the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/classpath-scanning.html) for these elements. For example, to exclude certain interfaces from instantiation as repository beans, you could use the following configuration: Using filters Java XML @Configuration @EnableElasticsearchRepositories(basePackages = ""com.acme.repositories"", includeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeRepository"") }, excludeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeOtherRepository"") }) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } <repositories base-package=""com.acme.repositories""> <context:include-filter type=""regex"" expression="".*SomeRepository"" /> <context:exclude-filter type=""regex"" expression="".*SomeOtherRepository"" /> </repositories> The preceding example includes all interfaces ending with SomeRepository and excludes those ending with SomeOtherRepository from being instantiated. Standalone Usage: You can also use the repository infrastructure outside of a Spring container — for example, in CDI environments.You still need some Spring libraries in your classpath, but, generally, you can set up repositories programmatically as well.The Spring Data modules that provide repository support ship with a persistence technology-specific RepositoryFactory that you can use, as follows: Standalone usage of the repository factory RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class);"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/query-methods-details.html","Defining Query Methods: The repository proxy has two ways to derive a store-specific query from the method name: By deriving the query from the method name directly. By using a manually defined query. Available options depend on the actual store. However, there must be a strategy that decides what actual query is created. The next section describes the available options. Query Lookup Strategies: The following strategies are available for the repository infrastructure to resolve the query. With XML configuration, you can configure the strategy at the namespace through the query-lookup-strategy attribute. For Java configuration, you can use the queryLookupStrategy attribute of the EnableElasticsearchRepositories annotation. Some strategies may not be supported for particular datastores. CREATE attempts to construct a store-specific query from the query method name. The general approach is to remove a given set of well known prefixes from the method name and parse the rest of the method. You can read more about query construction in “ Query Creation(#repositories.query-methods.query-creation) ”. USE_DECLARED_QUERY tries to find a declared query and throws an exception if it cannot find one. The query can be defined by an annotation somewhere or declared by other means. See the documentation of the specific store to find available options for that store. If the repository infrastructure does not find a declared query for the method at bootstrap time, it fails. CREATE_IF_NOT_FOUND (the default) combines CREATE and USE_DECLARED_QUERY . It looks up a declared query first, and, if no declared query is found, it creates a custom method name-based query. This is the default lookup strategy and, thus, is used if you do not configure anything explicitly. It allows quick query definition by method names but also custom-tuning of these queries by introducing declared queries as needed. Query Creation: The query builder mechanism built into the Spring Data repository infrastructure is useful for building constraining queries over entities of the repository. The following example shows how to create a number of queries: Query creation from method names interface PersonRepository extends Repository<Person, Long> { List<Person> findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // Enables the distinct flag for the query List<Person> findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List<Person> findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); // Enabling ignoring case for an individual property List<Person> findByLastnameIgnoreCase(String lastname); // Enabling ignoring case for all suitable properties List<Person> findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // Enabling static ORDER BY for a query List<Person> findByLastnameOrderByFirstnameAsc(String lastname); List<Person> findByLastnameOrderByFirstnameDesc(String lastname); } Parsing query method names is divided into subject and predicate. The first part ( find…By , exists…By ) defines the subject of the query, the second part forms the predicate. The introducing clause (subject) can contain further expressions. Any text between find (or other introducing keywords) and By is considered to be descriptive unless using one of the result-limiting keywords such as a Distinct to set a distinct flag on the query to be created or Top/First to limit query results(#repositories.limit-query-result) . The appendix contains the full list of query method subject keywords(query-keywords-reference.html#appendix.query.method.subject) and query method predicate keywords including sorting and letter-casing modifiers(query-keywords-reference.html#appendix.query.method.predicate) . However, the first By acts as a delimiter to indicate the start of the actual criteria predicate. At a very basic level, you can define conditions on entity properties and concatenate them with And and Or . The actual result of parsing the method depends on the persistence store for which you create the query. However, there are some general things to notice: The expressions are usually property traversals combined with operators that can be concatenated. You can combine property expressions with AND and OR . You also get support for operators such as Between , LessThan , GreaterThan , and Like for the property expressions. The supported operators can vary by datastore, so consult the appropriate part of your reference documentation. The method parser supports setting an IgnoreCase flag for individual properties (for example, findByLastnameIgnoreCase(…) ) or for all properties of a type that supports ignoring case (usually String instances — for example, findByLastnameAndFirstnameAllIgnoreCase(…) ). Whether ignoring cases is supported may vary by store, so consult the relevant sections in the reference documentation for the store-specific query method. You can apply static ordering by appending an OrderBy clause to the query method that references a property and by providing a sorting direction ( Asc or Desc ). To create a query method that supports dynamic sorting, see “ Paging, Iterating Large Results, Sorting & Limiting(#repositories.special-parameters) ”. Property Expressions: Property expressions can refer only to a direct property of the managed entity, as shown in the preceding example. At query creation time, you already make sure that the parsed property is a property of the managed domain class. However, you can also define constraints by traversing nested properties. Consider the following method signature: List<Person> findByAddressZipCode(ZipCode zipCode); Assume a Person has an Address with a ZipCode . In that case, the method creates the x.address.zipCode property traversal. The resolution algorithm starts by interpreting the entire part ( AddressZipCode ) as the property and checks the domain class for a property with that name (uncapitalized). If the algorithm succeeds, it uses that property. If not, the algorithm splits up the source at the camel-case parts from the right side into a head and a tail and tries to find the corresponding property — in our example, AddressZip and Code . If the algorithm finds a property with that head, it takes the tail and continues building the tree down from there, splitting the tail up in the way just described. If the first split does not match, the algorithm moves the split point to the left ( Address , ZipCode ) and continues. Although this should work for most cases, it is possible for the algorithm to select the wrong property. Suppose the Person class has an addressZip property as well. The algorithm would match in the first split round already, choose the wrong property, and fail (as the type of addressZip probably has no code property). To resolve this ambiguity you can use _ inside your method name to manually define traversal points. So our method name would be as follows: List<Person> findByAddress_ZipCode(ZipCode zipCode); Because we treat underscores ( _ ) as a reserved character, we strongly advise to follow standard Java naming conventions (that is, not using underscores in property names but applying camel case instead). Field Names starting with underscore: Field names may start with underscores like String _name . Make sure to preserve the _ as in _name and use double _ to split nested paths like user__name . Upper Case Field Names: Field names that are all uppercase can be used as such. Nested paths if applicable require splitting via _ as in USER_name . Field Names with 2nd uppercase letter: Field names that consist of a starting lower case letter followed by an uppercase one like String qCode can be resolved by starting with two upper case letters as in QCode . Please be aware of potential path ambiguities. Path Ambiguities: In the following sample the arrangement of properties qCode and q , with q containing a property called code , creates an ambiguity for the path QCode . record Container(String qCode, Code q) {} record Code(String code) {} Since a direct match on a property is considered first, any potential nested paths will not be considered and the algorithm picks the qCode field. In order to select the code field in q the underscore notation Q_Code is required. Repository Methods Returning Collections or Iterables: Query methods that return multiple results can use standard Java Iterable , List , and Set . Beyond that, we support returning Spring Data’s Streamable , a custom extension of Iterable , as well as collection types provided by Vavr(https://www.vavr.io/) . Refer to the appendix explaining all possible query method return types(query-return-types-reference.html#appendix.query.return.types) . Using Streamable as Query Method Return Type: You can use Streamable as alternative to Iterable or any collection type. It provides convenience methods to access a non-parallel Stream (missing from Iterable ) and the ability to directly ….filter(…) and ….map(…) over the elements and concatenate the Streamable to others: Using Streamable to combine query method results interface PersonRepository extends Repository<Person, Long> { Streamable<Person> findByFirstnameContaining(String firstname); Streamable<Person> findByLastnameContaining(String lastname); } Streamable<Person> result = repository.findByFirstnameContaining(""av"") .and(repository.findByLastnameContaining(""ea"")); Returning Custom Streamable Wrapper Types: Providing dedicated wrapper types for collections is a commonly used pattern to provide an API for a query result that returns multiple elements. Usually, these types are used by invoking a repository method returning a collection-like type and creating an instance of the wrapper type manually. You can avoid that additional step as Spring Data lets you use these wrapper types as query method return types if they meet the following criteria: The type implements Streamable . The type exposes either a constructor or a static factory method named of(…) or valueOf(…) that takes Streamable as an argument. The following listing shows an example: class Product { (1) MonetaryAmount getPrice() { … } } @RequiredArgsConstructor(staticName = ""of"") class Products implements Streamable<Product> { (2) private final Streamable<Product> streamable; public MonetaryAmount getTotal() { (3) return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); } @Override public Iterator<Product> iterator() { (4) return streamable.iterator(); } } interface ProductRepository implements Repository<Product, Long> { Products findAllByDescriptionContaining(String text); (5) } 1 A Product entity that exposes API to access the product’s price. 2 A wrapper type for a Streamable<Product> that can be constructed by using Products.of(…) (factory method created with the Lombok annotation). A standard constructor taking the Streamable<Product> will do as well. 3 The wrapper type exposes an additional API, calculating new values on the Streamable<Product> . 4 Implement the Streamable interface and delegate to the actual result. 5 That wrapper type Products can be used directly as a query method return type. You do not need to return Streamable<Product> and manually wrap it after the query in the repository client. Support for Vavr Collections: Vavr(https://www.vavr.io/) is a library that embraces functional programming concepts in Java. It ships with a custom set of collection types that you can use as query method return types, as the following table shows: Vavr collection type Used Vavr implementation type Valid Java source types io.vavr.collection.Seq io.vavr.collection.List java.util.Iterable io.vavr.collection.Set io.vavr.collection.LinkedHashSet java.util.Iterable io.vavr.collection.Map io.vavr.collection.LinkedHashMap java.util.Map You can use the types in the first column (or subtypes thereof) as query method return types and get the types in the second column used as implementation type, depending on the Java type of the actual query result (third column). Alternatively, you can declare Traversable (the Vavr Iterable equivalent), and we then derive the implementation class from the actual return value. That is, a java.util.List is turned into a Vavr List or Seq , a java.util.Set becomes a Vavr LinkedHashSet Set , and so on. Streaming Query Results: You can process the results of query methods incrementally by using a Java 8 Stream<T> as the return type. Instead of wrapping the query results in a Stream , data store-specific methods are used to perform the streaming, as shown in the following example: Stream the result of a query with Java 8 Stream<T> @Query(""select u from User u"") Stream<User> findAllByCustomQueryAndStream(); Stream<User> readAllByFirstnameNotNull(); @Query(""select u from User u"") Stream<User> streamAllPaged(Pageable pageable); A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example: Working with a Stream<T> result in a try-with-resources block try (Stream<User> stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } Not all Spring Data modules currently support Stream<T> as a return type. Asynchronous Query Results: You can run repository queries asynchronously by using Spring’s asynchronous method running capability(https://docs.spring.io/spring-framework/reference/6.1/integration/scheduling.html) . This means the method returns immediately upon invocation while the actual query occurs in a task that has been submitted to a Spring TaskExecutor . Asynchronous queries differ from reactive queries and should not be mixed. See the store-specific documentation for more details on reactive support. The following example shows a number of asynchronous queries: @Async Future<User> findByFirstname(String firstname); (1) @Async CompletableFuture<User> findOneByFirstname(String firstname); (2) 1 Use java.util.concurrent.Future as the return type. 2 Use a Java 8 java.util.concurrent.CompletableFuture as the return type. Paging, Iterating Large Results, Sorting & Limiting: To handle parameters in your query, define method parameters as already seen in the preceding examples. Besides that, the infrastructure recognizes certain specific types like Pageable , Sort and Limit , to apply pagination, sorting and limiting to your queries dynamically. The following example demonstrates these features: Using Pageable , Slice , Sort and Limit in query methods Page<User> findByLastname(String lastname, Pageable pageable); Slice<User> findByLastname(String lastname, Pageable pageable); List<User> findByLastname(String lastname, Sort sort); List<User> findByLastname(String lastname, Sort sort, Limit limit); List<User> findByLastname(String lastname, Pageable pageable); APIs taking Sort , Pageable and Limit expect non- null values to be handed into methods. If you do not want to apply any sorting or pagination, use Sort.unsorted() , Pageable.unpaged() and Limit.unlimited() . The first method lets you pass an org.springframework.data.domain.Pageable instance to the query method to dynamically add paging to your statically defined query. A Page knows about the total number of elements and pages available. It does so by the infrastructure triggering a count query to calculate the overall number. As this might be expensive (depending on the store used), you can instead return a Slice . A Slice knows only about whether a next Slice is available, which might be sufficient when walking through a larger result set. Sorting options are handled through the Pageable instance, too. If you need only sorting, add an org.springframework.data.domain.Sort parameter to your method. As you can see, returning a List is also possible. In this case, the additional metadata required to build the actual Page instance is not created (which, in turn, means that the additional count query that would have been necessary is not issued). Rather, it restricts the query to look up only the given range of entities. To find out how many pages you get for an entire query, you have to trigger an additional count query. By default, this query is derived from the query you actually trigger. Special parameters may only be used once within a query method. Some special parameters described above are mutually exclusive. Please consider the following list of invalid parameter combinations. Parameters Example Reason Pageable and Sort findBy…​(Pageable page, Sort sort) Pageable already defines Sort Pageable and Limit findBy…​(Pageable page, Limit limit) Pageable already defines a limit. The Top keyword used to limit results can be used to along with Pageable whereas Top defines the total maximum of results, whereas the Pageable parameter may reduce this number. Which Method is Appropriate?: The value provided by the Spring Data abstractions is perhaps best shown by the possible query method return types outlined in the following table below. The table shows which types you can return from a query method Table 1. Consuming Large Query Results Method Amount of Data Fetched Query Structure Constraints List<T>(#repositories.collections-and-iterables) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Streamable<T>(#repositories.collections-and-iterables.streamable) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Stream<T>(#repositories.query-streaming) Chunked (one-by-one or in batches) depending on Stream consumption. Single query using typically cursors. Streams must be closed after usage to avoid resource leaks. Flux<T> Chunked (one-by-one or in batches) depending on Flux consumption. Single query using typically cursors. Store module must provide reactive infrastructure. Slice<T> Pageable.getPageSize() + 1 at Pageable.getOffset() One to many queries fetching data starting at Pageable.getOffset() applying limiting. A Slice can only navigate to the next Slice . Slice provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Window provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Page<T> Pageable.getPageSize() at Pageable.getOffset() One to many queries starting at Pageable.getOffset() applying limiting. Additionally, COUNT(…) query to determine the total number of elements can be required. Often times, COUNT(…) queries are required that are costly. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Paging and Sorting: You can define simple sorting expressions by using property names. You can concatenate expressions to collect multiple criteria into one expression. Defining sort expressions Sort sort = Sort.by(""firstname"").ascending() .and(Sort.by(""lastname"").descending()); For a more type-safe way to define sort expressions, start with the type for which to define the sort expression and use method references to define the properties on which to sort. Defining sort expressions by using the type-safe API TypedSort<Person> person = Sort.sort(Person.class); Sort sort = person.by(Person::getFirstname).ascending() .and(person.by(Person::getLastname).descending()); TypedSort.by(…) makes use of runtime proxies by (typically) using CGlib, which may interfere with native image compilation when using tools such as Graal VM Native. If your store implementation supports Querydsl, you can also use the generated metamodel types to define sort expressions: Defining sort expressions by using the Querydsl API QSort sort = QSort.by(QPerson.firstname.asc()) .and(QSort.by(QPerson.lastname.desc())); Limiting Query Results: In addition to paging it is possible to limit the result size using a dedicated Limit parameter. You can also limit the results of query methods by using the First or Top keywords, which you can use interchangeably but may not be mixed with a Limit parameter. You can append an optional numeric value to Top or First to specify the maximum result size to be returned. If the number is left out, a result size of 1 is assumed. The following example shows how to limit the query size: Limiting the result size of a query with Top and First List<User> findByLastname(Limit limit); User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page<User> queryFirst10ByLastname(String lastname, Pageable pageable); Slice<User> findTop3ByLastname(String lastname, Pageable pageable); List<User> findFirst10ByLastname(String lastname, Sort sort); List<User> findTop10ByLastname(String lastname, Pageable pageable); The limiting expressions also support the Distinct keyword for datastores that support distinct queries. Also, for the queries that limit the result set to one instance, wrapping the result into with the Optional keyword is supported. If pagination or slicing is applied to a limiting query pagination (and the calculation of the number of available pages), it is applied within the limited result. Limiting the results in combination with dynamic sorting by using a Sort parameter lets you express query methods for the 'K' smallest as well as for the 'K' biggest elements."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/repositories/elasticsearch-repository-queries.html","Query methods: Query lookup strategies: The Elasticsearch module supports all basic query building feature as string queries, native search queries, criteria based queries or have it being derived from the method name. Declared queries: Deriving the query from the method name is not always sufficient and/or may result in unreadable method names. In this case one might make use of the @Query annotation (see Using @Query Annotation(#elasticsearch.query-methods.at-query) ). Query creation: Generally the query creation mechanism for Elasticsearch works as described in Defining Query Methods(../../repositories/query-methods-details.html) . Here’s a short example of what a Elasticsearch query method translates into: Example 1. Query creation from method names interface BookRepository extends Repository<Book, String> { List<Book> findByNameAndPrice(String name, Integer price); } The method name above will be translated into the following Elasticsearch json query { ""query"": { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""?"", ""fields"" : [ ""name"" ] } }, { ""query_string"" : { ""query"" : ""?"", ""fields"" : [ ""price"" ] } } ] } } } A list of supported keywords for Elasticsearch is shown below. Table 1. Supported keywords inside method names Keyword Sample Elasticsearch Query String And findByNameAndPrice { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""?"", ""fields"" : [ ""name"" ] } }, { ""query_string"" : { ""query"" : ""?"", ""fields"" : [ ""price"" ] } } ] } }} Or findByNameOrPrice { ""query"" : { ""bool"" : { ""should"" : [ { ""query_string"" : { ""query"" : ""?"", ""fields"" : [ ""name"" ] } }, { ""query_string"" : { ""query"" : ""?"", ""fields"" : [ ""price"" ] } } ] } }} Is findByName { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""?"", ""fields"" : [ ""name"" ] } } ] } }} Not findByNameNot { ""query"" : { ""bool"" : { ""must_not"" : [ { ""query_string"" : { ""query"" : ""?"", ""fields"" : [ ""name"" ] } } ] } }} Between findByPriceBetween { ""query"" : { ""bool"" : { ""must"" : [ {""range"" : {""price"" : {""from"" : ?, ""to"" : ?, ""include_lower"" : true, ""include_upper"" : true } } } ] } }} LessThan findByPriceLessThan { ""query"" : { ""bool"" : { ""must"" : [ {""range"" : {""price"" : {""from"" : null, ""to"" : ?, ""include_lower"" : true, ""include_upper"" : false } } } ] } }} LessThanEqual findByPriceLessThanEqual { ""query"" : { ""bool"" : { ""must"" : [ {""range"" : {""price"" : {""from"" : null, ""to"" : ?, ""include_lower"" : true, ""include_upper"" : true } } } ] } }} GreaterThan findByPriceGreaterThan { ""query"" : { ""bool"" : { ""must"" : [ {""range"" : {""price"" : {""from"" : ?, ""to"" : null, ""include_lower"" : false, ""include_upper"" : true } } } ] } }} GreaterThanEqual findByPriceGreaterThanEqual { ""query"" : { ""bool"" : { ""must"" : [ {""range"" : {""price"" : {""from"" : ?, ""to"" : null, ""include_lower"" : true, ""include_upper"" : true } } } ] } }} Before findByPriceBefore { ""query"" : { ""bool"" : { ""must"" : [ {""range"" : {""price"" : {""from"" : null, ""to"" : ?, ""include_lower"" : true, ""include_upper"" : true } } } ] } }} After findByPriceAfter { ""query"" : { ""bool"" : { ""must"" : [ {""range"" : {""price"" : {""from"" : ?, ""to"" : null, ""include_lower"" : true, ""include_upper"" : true } } } ] } }} Like findByNameLike { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""?*"", ""fields"" : [ ""name"" ] }, ""analyze_wildcard"": true } ] } }} StartingWith findByNameStartingWith { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""?*"", ""fields"" : [ ""name"" ] }, ""analyze_wildcard"": true } ] } }} EndingWith findByNameEndingWith { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""*?"", ""fields"" : [ ""name"" ] }, ""analyze_wildcard"": true } ] } }} Contains/Containing findByNameContaining { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""*?*"", ""fields"" : [ ""name"" ] }, ""analyze_wildcard"": true } ] } }} In (when annotated as FieldType.Keyword) findByNameIn(Collection<String>names) { ""query"" : { ""bool"" : { ""must"" : [ {""bool"" : {""must"" : [ {""terms"" : {""name"" : [""?"",""?""]}} ] } } ] } }} In findByNameIn(Collection<String>names) { ""query"": {""bool"": {""must"": [{""query_string"":{""query"": ""\""?\"" \""?\"""", ""fields"": [""name""]}}]}}} NotIn (when annotated as FieldType.Keyword) findByNameNotIn(Collection<String>names) { ""query"" : { ""bool"" : { ""must"" : [ {""bool"" : {""must_not"" : [ {""terms"" : {""name"" : [""?"",""?""]}} ] } } ] } }} NotIn findByNameNotIn(Collection<String>names) {""query"": {""bool"": {""must"": [{""query_string"": {""query"": ""NOT(\""?\"" \""?\"")"", ""fields"": [""name""]}}]}}} True findByAvailableTrue { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""true"", ""fields"" : [ ""available"" ] } } ] } }} False findByAvailableFalse { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""false"", ""fields"" : [ ""available"" ] } } ] } }} OrderBy findByAvailableTrueOrderByNameDesc { ""query"" : { ""bool"" : { ""must"" : [ { ""query_string"" : { ""query"" : ""true"", ""fields"" : [ ""available"" ] } } ] } }, ""sort"":[{""name"":{""order"":""desc""}}] } Exists findByNameExists {""query"":{""bool"":{""must"":[{""exists"":{""field"":""name""}}]}}} IsNull findByNameIsNull {""query"":{""bool"":{""must_not"":[{""exists"":{""field"":""name""}}]}}} IsNotNull findByNameIsNotNull {""query"":{""bool"":{""must"":[{""exists"":{""field"":""name""}}]}}} IsEmpty findByNameIsEmpty {""query"":{""bool"":{""must"":[{""bool"":{""must"":[{""exists"":{""field"":""name""}}],""must_not"":[{""wildcard"":{""name"":{""wildcard"":""*""}}}]}}]}}} IsNotEmpty findByNameIsNotEmpty {""query"":{""bool"":{""must"":[{""wildcard"":{""name"":{""wildcard"":""*""}}}]}}} Methods names to build Geo-shape queries taking GeoJson parameters are not supported. Use ElasticsearchOperations with CriteriaQuery in a custom repository implementation if you need to have such a function in a repository. Method return types: Repository methods can be defined to have the following return types for returning multiple Elements: List<T> Stream<T> SearchHits<T> List<SearchHit<T>> Stream<SearchHit<T>> SearchPage<T> Using @Query Annotation: Example 2. Declare query on the method using the @Query annotation. The arguments passed to the method can be inserted into placeholders in the query string. The placeholders are of the form ?0 , ?1 , ?2 etc. for the first, second, third parameter and so on. interface BookRepository extends ElasticsearchRepository<Book, String> { @Query(""{\""match\"": {\""name\"": {\""query\"": \""?0\""}}}"") Page<Book> findByName(String name,Pageable pageable); } The String that is set as the annotation argument must be a valid Elasticsearch JSON query. It will be sent to Easticsearch as value of the query element; if for example the function is called with the parameter John , it would produce the following query body: { ""query"": { ""match"": { ""name"": { ""query"": ""John"" } } } } Example 3. @Query annotation on a method taking a Collection argument A repository method such as @Query(""{\""ids\"": {\""values\"": ?0 }}"") List<SampleEntity> getByIds(Collection<String> ids); would make an IDs query(https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-ids-query.html) to return all the matching documents. So calling the method with a List of [""id1"", ""id2"", ""id3""] would produce the query body { ""query"": { ""ids"": { ""values"": [""id1"", ""id2"", ""id3""] } } } Using SpEL Expressions: Example 4. Declare query on the method using the @Query annotation with SpEL expression. SpEL expression(https://docs.spring.io/spring-framework/reference/core/expressions.html) is also supported when defining query in @Query . interface BookRepository extends ElasticsearchRepository<Book, String> { @Query("""""" { ""bool"":{ ""must"":[ { ""term"":{ ""name"": ""#{#name}"" } } ] } } """""") Page<Book> findByName(String name, Pageable pageable); } If for example the function is called with the parameter John , it would produce the following query body: { ""bool"":{ ""must"":[ { ""term"":{ ""name"": ""John"" } } ] } } Example 5. accessing parameter property. Supposing that we have the following class as query parameter type: public record QueryParameter(String value) { } It’s easy to access the parameter by # symbol, then reference the property value with a simple . : interface BookRepository extends ElasticsearchRepository<Book, String> { @Query("""""" { ""bool"":{ ""must"":[ { ""term"":{ ""name"": ""#{#parameter.value}"" } } ] } } """""") Page<Book> findByName(QueryParameter parameter, Pageable pageable); } We can pass new QueryParameter(""John"") as the parameter now, and it will produce the same query string as above. Example 6. accessing bean property. Bean property(https://docs.spring.io/spring-framework/reference/core/expressions/language-ref/bean-references.html) is also supported to access. Given that there is a bean named queryParameter of type QueryParameter , we can access the bean with symbol @ rather than # , and there is no need to declare a parameter of type QueryParameter in the query method: interface BookRepository extends ElasticsearchRepository<Book, String> { @Query("""""" { ""bool"":{ ""must"":[ { ""term"":{ ""name"": ""#{@queryParameter.value}"" } } ] } } """""") Page<Book> findByName(Pageable pageable); } Example 7. SpEL and Collection param. Collection parameter is also supported and is as easy to use as normal String , such as the following terms query: interface BookRepository extends ElasticsearchRepository<Book, String> { @Query("""""" { ""bool"":{ ""must"":[ { ""terms"":{ ""name"": #{#names} } } ] } } """""") Page<Book> findByName(Collection<String> names, Pageable pageable); } collection values should not be quoted when declaring the elasticsearch json query. A collection of names like List.of(""name1"", ""name2"") will produce the following terms query: { ""bool"":{ ""must"":[ { ""terms"":{ ""name"": [""name1"", ""name2""] } } ] } } Example 8. access property in the Collection param. SpEL Collection Projection(https://docs.spring.io/spring-framework/reference/core/expressions/language-ref/collection-projection.html) is convenient to use when values in the Collection parameter is not plain String : interface BookRepository extends ElasticsearchRepository<Book, String> { @Query("""""" { ""bool"":{ ""must"":[ { ""terms"":{ ""name"": #{#parameters.![value]} } } ] } } """""") Page<Book> findByName(Collection<QueryParameter> parameters, Pageable pageable); } This will extract all the value property values as a new Collection from QueryParameter collection, thus takes the same effect as above. Example 9. alter parameter name by using @Param When accessing the parameter by SpEL, it’s also useful to alter the parameter name to another one by @Param annotation in Sping Data: interface BookRepository extends ElasticsearchRepository<Book, String> { @Query("""""" { ""bool"":{ ""must"":[ { ""terms"":{ ""name"": #{#another.![value]} } } ] } } """""") Page<Book> findByName(@Param(""another"") Collection<QueryParameter> parameters, Pageable pageable); }"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/projections.html","Projections: Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> ."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableElasticsearchRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableElasticsearchRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" />"
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/null-handling.html","Null Handling of Repository Methods: As of Spring Data 2.0, repository CRUD methods that return an individual aggregate instance use Java 8’s Optional to indicate the potential absence of a value. Besides that, Spring Data supports returning the following wrapper types on query methods: com.google.common.base.Optional scala.Option io.vavr.control.Option Alternatively, query methods can choose not to use a wrapper type at all. The absence of a query result is then indicated by returning null . Repository methods returning collections, collection alternatives, wrappers, and streams are guaranteed never to return null but rather the corresponding empty representation. See “ Repository query return types(query-return-types-reference.html) ” for details. Nullability Annotations: You can express nullability constraints for repository methods by using Spring Framework’s nullability annotations(https://docs.spring.io/spring-framework/reference/6.1/core/null-safety.html) . They provide a tooling-friendly approach and opt-in null checks during runtime, as follows: @NonNullApi(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNullApi.html) : Used on the package level to declare that the default behavior for parameters and return values is, respectively, neither to accept nor to produce null values. @NonNull(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/NonNull.html) : Used on a parameter or return value that must not be null (not needed on a parameter and return value where @NonNullApi applies). @Nullable(https://docs.spring.io/spring-framework/docs/6.1.13/javadoc-api/org/springframework/lang/Nullable.html) : Used on a parameter or return value that can be null . Spring annotations are meta-annotated with JSR 305(https://jcp.org/en/jsr/detail?id=305) annotations (a dormant but widely used JSR). JSR 305 meta-annotations let tooling vendors (such as IDEA(https://www.jetbrains.com/help/idea/nullable-and-notnull-annotations.html) , Eclipse(https://help.eclipse.org/latest/index.jsp?topic=/org.eclipse.jdt.doc.user/tasks/task-using_external_null_annotations.htm) , and Kotlin(https://kotlinlang.org/docs/reference/java-interop.html#null-safety-and-platform-types) ) provide null-safety support in a generic way, without having to hard-code support for Spring annotations. To enable runtime checking of nullability constraints for query methods, you need to activate non-nullability on the package level by using Spring’s @NonNullApi in package-info.java , as shown in the following example: Declaring Non-nullability in package-info.java @org.springframework.lang.NonNullApi package com.acme; Once non-null defaulting is in place, repository query method invocations get validated at runtime for nullability constraints. If a query result violates the defined constraint, an exception is thrown. This happens when the method would return null but is declared as non-nullable (the default with the annotation defined on the package in which the repository resides). If you want to opt-in to nullable results again, selectively use @Nullable on individual methods. Using the result wrapper types mentioned at the start of this section continues to work as expected: an empty result is translated into the value that represents absence. The following example shows a number of the techniques just described: Using different nullability constraints package com.acme; (1) import org.springframework.lang.Nullable; interface UserRepository extends Repository<User, Long> { User getByEmailAddress(EmailAddress emailAddress); (2) @Nullable User findByEmailAddress(@Nullable EmailAddress emailAdress); (3) Optional<User> findOptionalByEmailAddress(EmailAddress emailAddress); (4) } 1 The repository resides in a package (or sub-package) for which we have defined non-null behavior. 2 Throws an EmptyResultDataAccessException when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . 3 Returns null when the query does not produce a result. Also accepts null as the value for emailAddress . 4 Returns Optional.empty() when the query does not produce a result. Throws an IllegalArgumentException when the emailAddress handed to the method is null . Nullability in Kotlin-based Repositories: Kotlin has the definition of nullability constraints(https://kotlinlang.org/docs/reference/null-safety.html) baked into the language. Kotlin code compiles to bytecode, which does not express nullability constraints through method signatures but rather through compiled-in metadata. Make sure to include the kotlin-reflect JAR in your project to enable introspection of Kotlin’s nullability constraints. Spring Data repositories use the language mechanism to define those constraints to apply the same runtime checks, as follows: Using nullability constraints on Kotlin repositories interface UserRepository : Repository<User, String> { fun findByUsername(username: String): User (1) fun findByFirstname(firstname: String?): User? (2) } 1 The method defines both the parameter and the result as non-nullable (the Kotlin default). The Kotlin compiler rejects method invocations that pass null to the method. If the query yields an empty result, an EmptyResultDataAccessException is thrown. 2 This method accepts null for the firstname parameter and returns null if the query does not produce a result."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/elasticsearch/repositories/cdi-integration.html","CDI Integration: The Spring Data Elasticsearch repositories can also be set up using CDI functionality. Example 1. Spring Data Elasticsearch repositories using CDI class ElasticsearchTemplateProducer { @Produces @ApplicationScoped public ElasticsearchOperations createElasticsearchTemplate() { // ... (1) } } class ProductService { private ProductRepository repository; (2) public Page<Product> findAvailableBookByName(String name, Pageable pageable) { return repository.findByAvailableTrueAndNameStartingWith(name, pageable); } @Inject public void setRepository(ProductRepository repository) { this.repository = repository; } } 1 Create a component by using the same calls as are used in the Elasticsearch Operations(../template.html) chapter. 2 Let the CDI framework inject the Repository into your class."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/elasticsearch/reference/5.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/introduction-and-preface/index.html","Your way through this document: This documentation tries to bridge between a broad spectrum of possible users: People new to all the Spring ecosystem, including Spring Framework, Spring Data, the concrete module (in this case Spring Data Neo4j) and Neo4j. Experienced Neo4j developers that are new to Spring Data and want to make best use of their Neo4j knowledge but are unfamiliar with declarative transactions for example and how to incorporate the latter with Neo4j cluster requirements. Experienced Spring Data developers who are new to this specific module and Neo4j and need to learn how the building blocks interact together. While the programming paradigm of this module is very much in line with Spring Data JDBC, Mongo and others, the query language (Cypher), transactional and clustering behaviour is different and can’t be abstracted away. Here’s how we address those different needs: A lot of Neo4j specific questions can be found in the Frequently Asked Questions(../faq.html#faq) . These questions are particular relevant for people who well aware of Neo4j specific requirements and want to know how to address them with Spring Data Neo4j. If you are already familiar with the core concepts of Spring Data, head straight to getting-started(../getting-started.html#getting-started) . This chapter will walk you through different options of configuring an application to connect to a Neo4j instance and how to model your domain. In most cases, you will need a domain. Go to mapping(../object-mapping/metadata-based-mapping.html#mapping.annotations) to learn about how to map nodes and relationships to your domain model. After that, you will need some means to query the domain. Choices are Neo4j repositories, the Neo4j Template or on a lower level, the Neo4j Client. All of them are available in a reactive fashion as well. Apart from the paging mechanism, all the features of standard repositories are available in the reactive variant. If you come from older versions of Spring Data Neo4j - which are usually abbreviated SDN+OGM or SDN5 - you will most likely be interested in the introduction to SDN(preface-sdn.html#preface.sdn) and especially in the relationship between SDN+OGM and the current SDN(../faq.html#faq.sdn-related-to-ogm) . In the same chapter, you will find out about the building blocks(building-blocks.html#building-blocks) of SDN. To learn more about the general concepts of repositories, head over to repositories(../repositories.html#repositories) . You can of course read on, continuing with the preface, and a gentle getting started guide. Section Summary: Introducing Neo4j(preface-neo4j.html) Introducing Spring Data(preface-sd.html) Introducing Spring Data Neo4j(preface-sdn.html) Building blocks of Spring Data Neo4j(building-blocks.html) New & Noteworthy(new-and-noteworthy.html) Dependencies(../dependencies.html)"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/introduction-and-preface/preface-neo4j.html","Introducing Neo4j: A graph database is a storage engine that specializes in storing and retrieving vast networks of information. It efficiently stores data as nodes with relationships to other or even the same nodes, thus allowing high-performance retrieval and querying of those structures. Properties can be added to both nodes and relationships. Nodes can be labelled by zero or more labels, relationships are always directed and named. Graph databases are well suited for storing most kinds of domain models. In almost all domains, there are certain things connected to other things. In most other modeling approaches, the relationships between things are reduced to a single link without identity and attributes. Graph databases allow to keep the rich relationships that originate from the domain equally well-represented in the database without resorting to also modeling the relationships as ""things"". There is very little ""impedance mismatch"" when putting real-life domains into a graph database. Neo4j(https://neo4j.com/) is an open source NoSQL graph database. It is a fully transactional database (ACID) that stores data structured as graphs consisting of nodes, connected by relationships. Inspired by the structure of the real world, it allows for high query performance on complex data, while remaining intuitive and simple for the developer. The starting point for learning about Neo4j is neo4j.com(https://neo4j.com/) . Here is a list of useful resources: The Neo4j documentation(https://neo4j.com/docs/) introduces Neo4j and contains links to getting started guides, reference documentation and tutorials. The online sandbox(https://neo4j.com/sandbox/) provides a convenient way to interact with a Neo4j instance in combination with the online tutorial(https://neo4j.com/developer/get-started/) . Neo4j Java Bolt Driver(https://neo4j.com/developer/java/) Several books(https://neo4j.com/books/) available for purchase and videos(https://www.youtube.com/neo4j) to watch."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/introduction-and-preface/preface-sd.html","Introducing Spring Data: Spring Data uses Spring Framework’s core(https://docs.spring.io/spring-framework/reference/6.1/core.html) functionality, such as the IoC(https://docs.spring.io/spring-framework/reference/6.1/core.html#beans) container, type conversion system(https://docs.spring.io/spring-framework/reference/6.1/core.html#core-convert) , expression language(https://docs.spring.io/spring-framework/reference/6.1/core.html#expressions) , JMX integration(https://docs.spring.io/spring-framework/reference/6.1/integration.html#jmx) , and portable DAO exception hierarchy(https://docs.spring.io/spring-framework/reference/6.1/data-access.html#dao-exceptions) . While it is not necessary to know all the Spring APIs, understanding the concepts behind them is. At a minimum, the idea behind IoC should be familiar. To learn more about Spring, you can refer to the comprehensive documentation that explains in detail the Spring Framework. There are a lot of articles, blog entries and books on the matter - take a look at the Spring Framework home page(https://spring.io/docs) for more information. The beauty of Spring Data is that it applies the same programming model to a variety of different stores, such as JPA, JDBC Mongo and others. For that reason, parts of the general Spring Data documentations are included in this document, especially the general chapter about working with Spring Data repositories(../repositories.html#repositories) . Make sure to have a look at that if you haven’t worked with a Spring Data module in the past."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/introduction-and-preface/preface-sdn.html","Introducing Spring Data Neo4j: Spring Data Neo4j or in short SDN is the next-generation Spring Data(https://spring.io/projects/spring-data) module, created and maintained by Neo4j, Inc.(https://neo4j.com) in close collaboration with VMware’s(https://www.vmware.com/) Spring Data Team. It supports all officially supported releases of Neo4j, including Neo4j AuraDB. The Spring Data Neo4j project applies aforementioned Spring Data concepts to the development of solutions using the Neo4j graph data store. SDN relies completely on the Neo4j Java Driver(https://github.com/neo4j/neo4j-java-driver) , without introducing another ""driver"" or ""transport"" layer between the mapping framework and the driver. The Neo4j Java Driver - sometimes dubbed Bolt or the Bolt driver - is used as a protocol much like JDBC is with relational databases. SDN is an Object-Graph-Mapping (OGM) library. An OGM maps nodes and relationships in the graph to objects and references in a domain model. Object instances are mapped to nodes while object references are mapped using relationships, or serialized to properties (e.g. references to a Date). JVM primitives are mapped to node or relationship properties. An OGM abstracts the database and provides a convenient way to persist your domain model in the graph and query it without having to use low level drivers directly. It also provides the flexibility to the developer to supply custom queries where the queries generated by SDN are insufficient. SDN is the official successor to prior SDN version 5, to which this documentation refers as SDN+OGM. SDN version 5 used a separate object mapping framework, much in the way Spring Data JPA relates to JPA. That separate layer aka Neo4j-OGM (Neo4j Object Graph Mapper) is now contained in this module itself. Spring Data Neo4j itself is an object mapper, dedicated to be used in Spring and Spring Boot applications and in some supported Jakarta EE environments. It does not require or support a separate implementation of an object mapper. Noteworthy features that differentiate the current SDN version from prior SDN+OGM are SDN is a complete OGM on its own Full support for immutable entities and thus full support for Kotlin’s data classes Full support for the reactive programming model in the Spring Framework itself and Spring Data Neo4j client and reactive client feature, resurrecting the idea of a template over the plain driver, easing database access We provide repositories as a high-level abstraction for storing and querying documents as well as templates and clients for generic domain access or generic query execution. All of them are integrated with Spring’s application transactions. The core functionality of the Neo4j support can be used directly, through either the Neo4jClient or the Neo4jTemplate or the reactive variants thereof. All of them provide integration with Spring’s application level transactions. On a lower level, you can grab the Bolt driver instance, but than you have to manage your own transactions in these cases. You still can use Neo4j-OGM, even in modern Spring Boot applications. But you cannot use it with SDN 6+. If you tried you would have two different sets of entities in two different - and unrelated - persistence context. Hence, if you want to stick to Neo4j-OGM 3.2.x, you would use the Java driver instantiated by Spring Boot and pass it onto a Neo4j-OGM session. Neo4j-OGM 3.2.x is still supported, and we recommend its use in frameworks such as Quarkus. In a Spring Boot application however your primary choice should be SDN. Please make sure you read the Frequently Asked Questions(../faq.html#faq) where we address many reoccurring questions about our mapping decisions but also how interaction with Neo4j cluster instances such as Neo4j AuraDB(https://neo4j.com/cloud/platform/aura-graph-database/) and on-premise cluster deployments can be significantly improved. Concepts that are important to understand are Neo4j Bookmarks, the potential need(https://medium.com/neo4j/try-and-then-retry-there-can-be-failure-30bf336383da) for incorporating a proper retry mechanism such as Spring Retry(https://github.com/spring-projects/spring-retry) or Resilience4j(https://github.com/resilience4j/resilience4j) (we recommend the latter, as this knowledge is applicable outside Spring, too) and the importance of read-only vs write queries in the context of Neo4j cluster."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/introduction-and-preface/building-blocks.html","Building blocks of Spring Data Neo4j: Overview: SDN consists of composable building blocks. It builds on top of the Neo4j Java Driver(https://github.com/neo4j/neo4j-java-driver) . The instance of the Java driver is provided through Spring Boot’s automatic configuration itself. All configuration options of the driver are accessible in the namespace spring.neo4j . The driver bean provides imperative, asynchronous and reactive methods to interact with Neo4j. You can use all transaction methods the driver provides on that bean such as auto-commit transactions(https://neo4j.com/docs/driver-manual/4.0/terminology/#term-auto-commit) , transaction functions(https://neo4j.com/docs/driver-manual/4.0/terminology/#term-transaction-function) and unmanaged transactions. Be aware that those transactions are not tight to an ongoing Spring transaction. Integration with Spring Data and Spring’s platform or reactive transaction manager starts at the Neo4j Client(../appendix/neo4j-client.html#neo4j-client) . The client is part of SDN is configured through a separate starter, spring-boot-starter-data-neo4j . The configuration namespace of that starter is spring.data.neo4j . The client is mapping agnostic. It doesn’t know about your domain classes, and you are responsible for mapping a result to an object suiting your needs. The next higher level of abstraction is the Neo4j Template. It is aware of your domain, and you can use it to query arbitrary domain objects. The template comes in handy in scenarios with a large number of domain classes or custom queries for which you don’t want to create an additional repository abstraction each. The highest level of abstraction is a Spring Data repository. All abstractions of SDN come in both imperative and reactive fashions. It is not recommended mixing both programming styles in the same application. The reactive infrastructure requires a Neo4j 4.0+ database. Figure 1. SDN building blocks The template mechanism is similar to the templates of others stores. Find some more information about it in our FAQ(../faq.html#template-support) . The Neo4j Client as such is unique to SDN. You will find its documentation in the appendix(../appendix/neo4j-client.html#neo4j-client) . On the package level: Package Description org.springframework.data.neo4j.config This package contains configuration related support classes that can be used for application specific, annotated configuration classes. The abstract base classes are helpful if you don’t rely on Spring Boot’s autoconfiguration. The package provides some additional annotations that enable auditing. org.springframework.data.neo4j.core This package contains the core infrastructure for creating an imperative or reactive client that can execute queries. Packages marked as @API(status = API.Status.STABLE) are safe to be used. The core package provides access to both the imperative and reactive variants of the client and the template. org.springframework.data.neo4j.core.convert Provides a set of simples types that SDN supports. The Neo4jConversions allows bringing in additional, custom converters. org.springframework.data.neo4j.core.support This package provides a couple of support classes that might be helpful in your domain, for example a predicate indicating that some transaction may be retried and additional converters and id generators. org.springframework.data.neo4j.core.transaction Contains the core infrastructure for translating unmanaged Neo4j transaction into Spring managed transactions. Exposes both the imperative and reactive TransactionManager as Neo4jTransactionManager and ReactiveNeo4jTransactionManager . org.springframework.data.neo4j.repository This package provides the Neo4j imperative and reactive repository API. org.springframework.data.neo4j.repository.config Configuration infrastructure for Neo4j specific repositories, especially dedicated annotations to enable imperative and reactive Spring Data Neo4j repositories. org.springframework.data.neo4j.repository.support This package provides a couple of public support classes for building custom imperative and reactive Spring Data Neo4j repository base classes. The support classes are the same classes used by SDN itself."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/introduction-and-preface/new-and-noteworthy.html","New & Noteworthy: Release notes for Spring Data Neo4j version 7.2(https://github.com/spring-projects/spring-data-commons/wiki/Spring-Data-2023.1-(Vaughan)-Release-Notes#spring-data-neo4j---72) Release notes for Spring Data Neo4j version 7.1(https://github.com/spring-projects/spring-data-commons/wiki/Spring-Data-2023.0-(Ullman)-Release-Notes#spring-data-neo4j---71) Release notes for Spring Data Neo4j version 7.0(https://github.com/spring-projects/spring-data-commons/wiki/Spring-Data-2022.0-(Turing)-Release-Notes#spring-data-neo4j—​70) Release notes for Spring Data Neo4j version 6.3(https://github.com/spring-projects/spring-data-commons/wiki/Spring-Data-2021.2-(Raj)-Release-Notes#spring-data-neo4j---63) Release notes for Spring Data Neo4j version 6.2(https://github.com/spring-projects/spring-data-commons/wiki/Spring-Data-2021.1-(Q)-Release-Notes#spring-data-neo4j---62) For more detailed and technical information, please refer to the change log(https://github.com/spring-projects/spring-data-neo4j/releases) ."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/dependencies.html","Dependencies: Due to the different inception dates of individual Spring Data modules, most of them carry different major and minor version numbers. The easiest way to find compatible ones is to rely on the Spring Data Release Train BOM that we ship with the compatible versions defined. In a Maven project, you would declare this dependency in the <dependencyManagement /> section of your POM as follows: Using the Spring Data release train BOM <dependencyManagement> <dependencies> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-bom</artifactId> <version>2024.0.4</version> <scope>import</scope> <type>pom</type> </dependency> </dependencies> </dependencyManagement> The current release train version is 2024.0.4 . The train version uses calver(https://calver.org/) with the pattern YYYY.MINOR.MICRO . The version name follows ${calver} for GA releases and service releases and the following pattern for all other versions: ${calver}-${modifier} , where modifier can be one of the following: SNAPSHOT : Current snapshots M1 , M2 , and so on: Milestones RC1 , RC2 , and so on: Release candidates You can find a working example of using the BOMs in our Spring Data examples repository(https://github.com/spring-projects/spring-data-examples/tree/main/bom) . With that in place, you can declare the Spring Data modules you would like to use without a version in the <dependencies /> block, as follows: Declaring a dependency to a Spring Data module such as JPA <dependencies> <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-jpa</artifactId> </dependency> <dependencies> Dependency Management with Spring Boot: Spring Boot selects a recent version of the Spring Data modules for you. If you still want to upgrade to a newer version, set the spring-data-bom.version property to the train version and iteration(#dependencies.train-version) you would like to use. See Spring Boot’s documentation(https://docs.spring.io/spring-boot/docs/current/reference/html/dependency-versions.html#appendix.dependency-versions.properties) (search for ""Spring Data Bom"") for more details. Spring Framework: The current version of Spring Data modules require Spring Framework 6.1.13 or better. The modules might also work with an older bugfix version of that minor version. However, using the most recent version within that generation is highly recommended."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/getting-started.html","Getting started: We provide a Spring Boot starter for SDN. Please include the starter module via your dependency management and configure the bolt URL to use, for example spring.neo4j.uri=bolt://localhost:7687 . The starter assumes that the server has disabled authentication. As the SDN starter depends on the starter for the Java Driver, all things regarding configuration said there, apply here as well. For a reference of the available properties, use your IDEs autocompletion in the spring.neo4j namespace. SDN supports The well known and understood imperative programming model (much like Spring Data JDBC or JPA) Reactive programming based on Reactive Streams(https://www.reactive-streams.org) , including full support for reactive transactions(https://spring.io/blog/2019/05/16/reactive-transactions-with-spring) . Those are all included in the same binary. The reactive programming model requires a 4+ Neo4j server on the database side and reactive Spring on the other hand. Prepare the database: For this example, we stay within the movie graph(https://neo4j.com/developer/movie-database/) , as it comes for free with every Neo4j instance. If you don’t have a running database but Docker installed, please run: Start a local Neo4j instance inside Docker. docker run --publish=7474:7474 --publish=7687:7687 -e 'NEO4J_AUTH=neo4j/secret' neo4j:5 You can now access http://localhost:7474(http://localhost:7474/browser/?cmd=play&arg=movies) . The above command sets the password of the server to secret . Note the command ready to run in the prompt ( :play movies ). Execute it to fill your database with some test data. Create a new Spring Boot project: The easiest way to set up a Spring Boot project is start.spring.io(https://start.spring.io#!type=maven-project&dependencies=webflux,data-neo4j) (which is integrated in the major IDEs as well, in case you don’t want to use the website). Select the ""Spring Web Starter"" to get all the dependencies needed for creating a Spring based web application. The Spring Initializr will take care of creating a valid project structure for you, with all the files and settings in place for the selected build tool. Using Maven: You can issue a curl request against the Spring Initializer to create a basic Maven project: Create a basic Maven project with the Spring Initializr curl https://start.spring.io/starter.tgz \ -d dependencies=webflux,data-neo4j \ -d bootVersion=3.2.0 \ -d baseDir=Neo4jSpringBootExample \ -d name=Neo4j%20SpringBoot%20Example | tar -xzvf - This will create a new folder Neo4jSpringBootExample . As this starter is not yet on the initializer, you will have to add the following dependency manually to your pom.xml : Inclusion of the spring-data-neo4j-spring-boot-starter in a Maven project <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-neo4j</artifactId> </dependency> You would also add the dependency manually in case of an existing project. Using Gradle: The idea is the same, just generate a Gradle project: Create a basic Gradle project with the Spring Initializr curl https://start.spring.io/starter.tgz \ -d dependencies=webflux,data-neo4j \ -d type=gradle-project \ -d bootVersion=3.2.0 \ -d baseDir=Neo4jSpringBootExampleGradle \ -d name=Neo4j%20SpringBoot%20Example | tar -xzvf - The dependency for Gradle looks like this and must be added to build.gradle : Inclusion of the spring-data-neo4j-spring-boot-starter in a Gradle project dependencies { implementation 'org.springframework.boot:spring-boot-starter-data-neo4j' } You would also add the dependency manually in case of an existing project. Configure the project: Now open any of those projects in your favorite IDE. Find application.properties and configure your Neo4j credentials: spring.neo4j.uri=bolt://localhost:7687 spring.neo4j.authentication.username=neo4j spring.neo4j.authentication.password=verysecret This is the bare minimum of what you need to connect to a Neo4j instance. It is not necessary to add any programmatic configuration of the driver when you use this starter. SDN repositories will be automatically enabled by this starter. Configure Neo4j Cypher-DSL: Depending on the Neo4j version you are running your application with, it is advised to configure the dialect Neo4j Cypher-DSL runs with. The default dialect that is used is targeting Neo4j 4.4. as the LTS version of Neo4j. This can be changed by defining a Cypher-DSL Configuration bean. Make Cypher-DSL use the Neo4j 5 dialect @Bean Configuration cypherDslConfiguration() { return Configuration.newConfig() .withDialect(Dialect.NEO4J_5).build(); } Although Spring Data Neo4j tries it best to be compatible with also the combination of Neo4j 5 and a default dialect, it is always recommend to explicitly define the dialect. E.g. it will lead to more optimized queries and make use of elementId() for newer Neo4j versions. Running on the Module-Path: Spring Data Neo4j can run on the module path. It’s automatic module name is spring.data.neo4j . It does not provide a module itself due to restrictions in the current Spring Data build setup. Hence, it uses an automatic but stable module name. However, it does depend on a modularized library (the Cypher-DSL(https://github.com/neo4j-contrib/cypher-dsl) ). Without a module-info.java due to the restriction mentioned above, we cannot express the requirement for that library on your behalf. Therefore, the minimal required module-info.java in your project for running Spring Data Neo4j 6.1+ on the module path is the following: A module-info.java in a project supposed to use Spring Data Neo4j on the module path module your.module { requires org.neo4j.cypherdsl.core; requires spring.data.commons; requires spring.data.neo4j; opens your.domain to spring.core; (1) exports your.domain; (2) } 1 Spring Data Neo4j uses Spring Data Commons and its reflective capabilities, so you would need to open up your domain packages to spring.core at least. 2 We assume here that your.domain contains also repositories: Those must be exported to be accessible by spring.beans , spring.context and spring.data.commons . If you don’t want to export them to the world, you can restrict them to those modules. Create your domain: Our domain layer should accomplish two things: Map your graph to objects Provide access to those Example Node-Entity: SDN fully supports unmodifiable entities, for both Java and data classes in Kotlin. Therefore, we will focus on immutable entities here, MovieEntity.java(#movie-entity) shows a such an entity. SDN supports all data types the Neo4j Java Driver supports, see Map Neo4j types to native language types(https://neo4j.com/docs/driver-manual/current/cypher-workflow/#driver-type-mapping) inside the chapter ""The Cypher type system"". Future versions will support additional converters. MovieEntity.java import java.util.ArrayList; import java.util.List; import org.springframework.data.neo4j.core.schema.Id; import org.springframework.data.neo4j.core.schema.Node; import org.springframework.data.neo4j.core.schema.Property; import org.springframework.data.neo4j.core.schema.Relationship; import org.springframework.data.neo4j.core.schema.Relationship.Direction; @Node(""Movie"") (1) public class MovieEntity { @Id (2) private final String title; @Property(""tagline"") (3) private final String description; @Relationship(type = ""ACTED_IN"", direction = Direction.INCOMING) (4) private List<Roles> actorsAndRoles = new ArrayList<>(); @Relationship(type = ""DIRECTED"", direction = Direction.INCOMING) private List<PersonEntity> directors = new ArrayList<>(); public MovieEntity(String title, String description) { (5) this.title = title; this.description = description; } // Getters omitted for brevity } 1 @Node is used to mark this class as a managed entity. It also is used to configure the Neo4j label. The label defaults to the name of the class, if you’re just using plain @Node . 2 Each entity has to have an id. The movie class shown here uses the attribute title as a unique business key. If you don’t have such a unique key, you can use the combination of @Id and @GeneratedValue to configure SDN to use Neo4j’s internal id. We also provide generators for UUIDs. 3 This shows @Property as a way to use a different name for the field than for the graph property. 4 This defines a relationship to a class of type PersonEntity and the relationship type ACTED_IN 5 This is the constructor to be used by your application code. As a general remark: immutable entities using internally generated ids are a bit contradictory, as SDN needs a way to set the field with the value generated by the database. If you don’t find a good business key or don’t want to use a generator for IDs, here’s the same entity using the internally generated id together with a regular constructor and a so called wither -Method, that is used by SDN: MovieEntity.java import org.springframework.data.neo4j.core.schema.GeneratedValue; import org.springframework.data.neo4j.core.schema.Id; import org.springframework.data.neo4j.core.schema.Node; import org.springframework.data.neo4j.core.schema.Property; import org.springframework.data.annotation.PersistenceConstructor; @Node(""Movie"") public class MovieEntity { @Id @GeneratedValue private Long id; private final String title; @Property(""tagline"") private final String description; public MovieEntity(String title, String description) { (1) this.id = null; this.title = title; this.description = description; } public MovieEntity withId(Long id) { (2) if (this.id.equals(id)) { return this; } else { MovieEntity newObject = new MovieEntity(this.title, this.description); newObject.id = id; return newObject; } } } 1 This is the constructor to be used by your application code. It sets the id to null, as the field containing the internal id should never be manipulated. 2 This is a so-called wither for the id -attribute. It creates a new entity and sets the field accordingly, without modifying the original entity, thus making it immutable. You can of course use SDN with Kotlin(https://kotlinlang.org/) and model your domain with Kotlin’s data classes. Project Lombok(https://projectlombok.org/) is an alternative if you want or need to stay purely within Java. Declaring Spring Data repositories: You basically have two options here: you can work in a store-agnostic fashion with SDN and make your domain specific extend one of org.springframework.data.repository.Repository org.springframework.data.repository.CrudRepository org.springframework.data.repository.reactive.ReactiveCrudRepository org.springframework.data.repository.reactive.ReactiveSortingRepository Choose imperative and reactive accordingly. While technically not prohibited, it is not recommended mixing imperative and reactive database access in the same application. We won’t support you with scenarios like this. The other option is to settle on a store specific implementation and gain all the methods we support out of the box. The advantage of this approach is also its biggest disadvantage: once out, all those methods will be part of your API. Most of the time it’s harder to take something away, than to add stuff afterwards. Furthermore, using store specifics leaks your store into your domain. From a performance point of view, there is no penalty. A reactive repository fitting to any of the movie entities above looks like this: MovieRepository.java import reactor.core.publisher.Mono; import org.springframework.data.neo4j.repository.ReactiveNeo4jRepository; public interface MovieRepository extends ReactiveNeo4jRepository<MovieEntity, String> { Mono<MovieEntity> findOneByTitle(String title); } Testing reactive code is done with a reactor.test.StepVerifier . Have a look at the corresponding documentation of Project Reactor(https://projectreactor.io/docs/core/release/reference/#testing) or see our example code."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/object-mapping.html","Object Mapping: The following sections will explain the process of mapping between your graph and your domain. It is split into three parts. The first part explains the actual mapping and the available tools for you to describe how to map nodes, relationships and properties to objects. The second part shows the options and implications of using a specific identifier style for your entities. The third part will have a look at Spring Data’s object mapping fundamentals. It gives valuable tips on general mapping, why you should prefer immutable domain objects and how you can model them with Java or Kotlin. Section Summary: Metadata-based Mapping(object-mapping/metadata-based-mapping.html) Handling and provisioning of unique IDs(object-mapping/mapping-ids.html) Spring Data Object Mapping Fundamentals(object-mapping/sdc-object-mapping.html)"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/object-mapping/metadata-based-mapping.html","Metadata-based Mapping: To take full advantage of the object mapping functionality inside SDN, you should annotate your mapped objects with the @Node annotation. Although it is not necessary for the mapping framework to have this annotation (your POJOs are mapped correctly, even without any annotations), it lets the classpath scanner find and pre-process your domain objects to extract the necessary metadata. If you do not use this annotation, your application takes a slight performance hit the first time you store a domain object, because the mapping framework needs to build up its internal metadata model so that it knows about the properties of your domain object and how to persist them. Mapping Annotation Overview: From SDN: @Node : Applied at the class level to indicate this class is a candidate for mapping to the database. @Id : Applied at the field level to mark the field used for identity purpose. @GeneratedValue : Applied at the field level together with @Id to specify how unique identifiers should be generated. @Property : Applied at the field level to modify the mapping from attributes to properties. @CompositeProperty : Applied at the field level on attributes of type Map that shall be read back as a composite. See Composite properties(../appendix/conversions.html#custom.conversions.composite-properties) . @Relationship : Applied at the field level to specify the details of a relationship. @DynamicLabels : Applied at the field level to specify the source of dynamic labels. @RelationshipProperties : Applied at the class level to indicate this class as the target for properties of a relationship. @TargetNode : Applied on a field of a class annotated with @RelationshipProperties to mark the target of that relationship from the perspective of the other end. The following annotations are used to specify conversions and ensure backwards compatibility with OGM. @DateLong @DateString @ConvertWith See Conversions(../appendix/conversions.html#custom.conversions.attribute.specific) for more information on that. From Spring Data commons: @org.springframework.data.annotation.Id same as @Id from SDN, in fact, @Id is annotated with Spring Data Common’s Id-annotation. @CreatedBy : Applied at the field level to indicate the creator of a node. @CreatedDate : Applied at the field level to indicate the creation date of a node. @LastModifiedBy : Applied at the field level to indicate the author of the last change to a node. @LastModifiedDate : Applied at the field level to indicate the last modification date of a node. @PersistenceCreator : Applied at one constructor to mark it as the preferred constructor when reading entities. @Persistent : Applied at the class level to indicate this class is a candidate for mapping to the database. @Version : Applied at field level it is used for optimistic locking and checked for modification on save operations. The initial value is zero which is bumped automatically on every update. @ReadOnlyProperty : Applied at field level to mark a property as read only. The property will be hydrated during database reads, but not be subject to writes. When used on relationships be aware that no related entity in that collection will be persisted if not related otherwise. Have a look at Auditing(../auditing.html) for all annotations regarding auditing support. The basic building block: @Node: The @Node annotation is used to mark a class as a managed domain class, subject to the classpath scanning by the mapping context. To map an Object to nodes in the graph and vice versa, we need a label to identify the class to map to and from. @Node has an attribute labels that allows you to configure one or more labels to be used when reading and writing instances of the annotated class. The value attribute is an alias for labels . If you don’t specify a label, then the simple class name will be used as the primary label. In case you want to provide multiple labels, you could either: Supply an array to the labels property. The first element in the array will be considered as the primary label. Supply a value for primaryLabel and put the additional labels in labels . The primary label should always be the most concrete label that reflects your domain class. For each instance of an annotated class that is written through a repository or through the Neo4j template, one node in the graph with at least the primary label will be written. Vice versa, all nodes with the primary label will be mapped to the instances of the annotated class. A note on class hierarchies: The @Node annotation is not inherited from super-types and interfaces. You can however annotate your domain classes individually at every inheritance level. This allows polymorphic queries: You can pass in base or intermediate classes and retrieve the correct, concrete instance for your nodes. This is only supported for abstract bases annotated with @Node . The labels defined on such a class will be used as additional labels together with the labels of the concrete implementations. We also support interfaces in domain-class-hierarchies for some scenarios: Domain model in a separate module, same primary label like the interface name public interface SomeInterface { (1) String getName(); SomeInterface getRelated(); } @Node(""SomeInterface"") (2) public static class SomeInterfaceEntity implements SomeInterface { @Id @GeneratedValue private Long id; private final String name; private SomeInterface related; public SomeInterfaceEntity(String name) { this.name = name; } @Override public String getName() { return name; } @Override public SomeInterface getRelated() { return related; } } 1 Just the plain interface name, as you would name your domain 2 As we need to synchronize the primary labels, we put @Node on the implementing class, which is probably in another module. Note that the value is exactly the same as the name of the interface implemented. Renaming is not possible. Using a different primary label instead of the interface name is possible, too: Different primary label @Node(""PrimaryLabelWN"") (1) public interface SomeInterface2 { String getName(); SomeInterface2 getRelated(); } public static class SomeInterfaceEntity2 implements SomeInterface2 { // Overrides omitted for brevity } 1 Put the @Node annotation on the interface It’s also possible to use different implementations of an interface and have a polymorph domain model. When doing so, at least two labels are required: A label determining the interface and one determining the concrete class: Multiple implementations @Node(""SomeInterface3"") (1) public interface SomeInterface3 { String getName(); SomeInterface3 getRelated(); } @Node(""SomeInterface3a"") (2) public static class SomeInterfaceImpl3a implements SomeInterface3 { // Overrides omitted for brevity } @Node(""SomeInterface3b"") (3) public static class SomeInterfaceImpl3b implements SomeInterface3 { // Overrides omitted for brevity } @Node public static class ParentModel { (4) @Id @GeneratedValue private Long id; private SomeInterface3 related1; (5) private SomeInterface3 related2; } 1 Explicitly specifying the label that identifies the interface is required in this scenario 2 Which applies for the first… 3 and second implementation as well 4 This is a client or parent model, using SomeInterface3 transparently for two relationships 5 No concrete type is specified The data structure needed is shown in the following test. The same would be written by the OGM: Data structure needed for using multiple, different interface implementations Long id; try (Session session = driver.session(bookmarkCapture.createSessionConfig()); Transaction transaction = session.beginTransaction()) { id = transaction.run("""" + ""CREATE (s:ParentModel{name:'s'}) "" + ""CREATE (s)-[:RELATED_1]-> (:SomeInterface3:SomeInterface3b {name:'3b'}) "" + ""CREATE (s)-[:RELATED_2]-> (:SomeInterface3:SomeInterface3a {name:'3a'}) "" + ""RETURN id(s)"") .single().get(0).asLong(); transaction.commit(); } Optional<Inheritance.ParentModel> optionalParentModel = transactionTemplate.execute(tx -> template.findById(id, Inheritance.ParentModel.class)); assertThat(optionalParentModel).hasValueSatisfying(v -> { assertThat(v.getName()).isEqualTo(""s""); assertThat(v).extracting(Inheritance.ParentModel::getRelated1) .isInstanceOf(Inheritance.SomeInterfaceImpl3b.class) .extracting(Inheritance.SomeInterface3::getName) .isEqualTo(""3b""); assertThat(v).extracting(Inheritance.ParentModel::getRelated2) .isInstanceOf(Inheritance.SomeInterfaceImpl3a.class) .extracting(Inheritance.SomeInterface3::getName) .isEqualTo(""3a""); }); Interfaces cannot define an identifier field. As a consequence they are not a valid entity type for repositories. Dynamic or ""runtime"" managed labels: All labels implicitly defined through the simple class name or explicitly via the @Node annotation are static. They cannot be changed during runtime. If you need additional labels that can be manipulated during runtime, you can use @DynamicLabels . @DynamicLabels is an annotation on field level and marks an attribute of type java.util.Collection<String> (a List or Set ) for example) as source of dynamic labels. If this annotation is present, all labels present on a node and not statically mapped via @Node and the class names, will be collected into that collection during load. During writes, all labels of the node will be replaced with the statically defined labels plus the contents of the collection. If you have other applications add additional labels to nodes, don’t use @DynamicLabels . If @DynamicLabels is present on a managed entity, the resulting set of labels will be ""the truth"" written to the database. Identifying instances: @Id: While @Node creates a mapping between a class and nodes having a specific label, we also need to make the connection between individual instances of that class (objects) and instances of the node. This is where @Id comes into play. @Id marks an attribute of the class to be the unique identifier of the object. That unique identifier is in an optimal world a unique business key or in other words, a natural key. @Id can be used on all attributes with a supported simple type. Natural keys are however pretty hard to find. Peoples names for example are seldom unique, change over time or worse, not everyone has a first and last name. We therefore support two different kind of surrogate keys . On an attribute of type String , long or Long , @Id can be used with @GeneratedValue . Long and long maps to the Neo4j internal id. String maps to the elementId that is available since Neo4j 5. Both are not a property on a node or relationship and usually not visible, to the attribute and allows SDN to retrieve individual instances of the class. @GeneratedValue provides the attribute generatorClass . generatorClass can be used to specify a class implementing IdGenerator . An IdGenerator is a functional interface and its generateId takes the primary label and the instance to generate an Id for. We support UUIDStringGenerator as one implementation out of the box. You can also specify a Spring Bean from the application context on @GeneratedValue via generatorRef . That bean also needs to implement IdGenerator , but can make use of everything in the context, including the Neo4j client or template to interact with the database. Don’t skip the important notes about ID handling in Handling and provisioning of unique IDs(mapping-ids.html#mapping.id-handling) Optimistic locking: @Version: Spring Data Neo4j supports optimistic locking by using the @Version annotation on a Long typed field. This attribute will get incremented automatically during updates and must not be manually modified. If, e.g., two transactions in different threads want to modify the same object with version x , the first operation will get successfully persisted to the database. At this moment, the version field will get incremented, so it is x+1 . The second operation will fail with a OptimisticLockingFailureException because it wants to modify the object with the version x that does not exist anymore in the database. In such cases the operation needs to get retried, beginning with a fresh fetch of the object with the current version from the database. The @Version attribute is also mandatory if business ids(mapping-ids.html#mapping.id-handling.business-key) are used. Spring Data Neo4j will check this field to determine if the entity is new or has already been persisted before. Mapping properties: @Property: All attributes of a @Node -annotated class will be persisted as properties of Neo4j nodes and relationships. Without further configuration, the name of the attribute in the Java or Kotlin class will be used as Neo4j property. If you are working with an existing Neo4j schema or just like to adapt the mapping to your needs, you will need to use @Property . The name is used to specify the name of the property inside the database. Connecting nodes: @Relationship: The @Relationship annotation can be used on all attributes that are not a simple type. It is applicable on attributes of other types annotated with @Node or collections and maps thereof. The type or the value attribute allow configuration of the relationship’s type, direction allows specifying the direction. The default direction in SDN is Relationship.Direction#OUTGOING . We support dynamic relationships. Dynamic relationships are represented as a Map<String, AnnotatedDomainClass> or Map<Enum, AnnotatedDomainClass> . In such a case, the type of the relationship to the other domain class is given by the maps key and must not be configured through the @Relationship . Map relationship properties: Neo4j supports defining properties not only on nodes but also on relationships. To express those properties in the model SDN provides @RelationshipProperties to be applied on a simple Java class. Within the properties class there have to be exactly one field marked as @TargetNode to define the entity the relationship points towards. Or, in an INCOMING relationship context, is coming from. A relationship property class and its usage may look like this: Relationship properties Roles @RelationshipProperties public class Roles { @RelationshipId private Long id; private final List<String> roles; @TargetNode private final PersonEntity person; public Roles(PersonEntity person, List<String> roles) { this.person = person; this.roles = roles; } public List<String> getRoles() { return roles; } @Override public String toString() { return ""Roles{"" + ""id="" + id + '}' + this.hashCode(); } } You must define a property for the generated, internal ID ( @RelationshipId ) so that SDN can determine during save which relationships can be safely overwritten without losing properties. If SDN does not find a field for storing the internal node id, it will fail during startup. Defining relationship properties for an entity @Relationship(type = ""ACTED_IN"", direction = Direction.INCOMING) (1) private List<Roles> actorsAndRoles = new ArrayList<>(); Relationship query remarks: In general there is no limitation of relationships / hops for creating the queries. SDN parses the whole reachable graph from your modelled nodes. This said, when there is the idea of mapping a relationship bidirectional, meaning you define the relationship on both ends of your entity, you might get more than what you are expecting. Consider an example where a movie has actors , and you want to fetch a certain movie with all its actors. This won’t be problematical if the relationship from movie to actor were just unidirectional. In a bidirectional scenario SDN would fetch the particular movie , its actors but also the other movies defined for this actor per definition of the relationship. In the worst case, this will cascade to fetching the whole graph for a single entity. A complete example: Putting all those together, we can create a simple domain. We use movies and people with different roles: Example 1. The MovieEntity import java.util.ArrayList; import java.util.List; import org.springframework.data.neo4j.core.schema.Id; import org.springframework.data.neo4j.core.schema.Node; import org.springframework.data.neo4j.core.schema.Property; import org.springframework.data.neo4j.core.schema.Relationship; import org.springframework.data.neo4j.core.schema.Relationship.Direction; @Node(""Movie"") (1) public class MovieEntity { @Id (2) private final String title; @Property(""tagline"") (3) private final String description; @Relationship(type = ""ACTED_IN"", direction = Direction.INCOMING) (4) private List<Roles> actorsAndRoles = new ArrayList<>(); @Relationship(type = ""DIRECTED"", direction = Direction.INCOMING) private List<PersonEntity> directors = new ArrayList<>(); public MovieEntity(String title, String description) { (5) this.title = title; this.description = description; } // Getters omitted for brevity } 1 @Node is used to mark this class as a managed entity. It also is used to configure the Neo4j label. The label defaults to the name of the class, if you’re just using plain @Node . 2 Each entity has to have an id. We use the movie’s name as unique identifier. 3 This shows @Property as a way to use a different name for the field than for the graph property. 4 This configures an incoming relationship to a person. 5 This is the constructor to be used by your application code as well as by SDN. People are mapped in two roles here, actors and directors . The domain class is the same: Example 2. The PersonEntity import org.springframework.data.neo4j.core.schema.Id; import org.springframework.data.neo4j.core.schema.Node; @Node(""Person"") public class PersonEntity { @Id private final String name; private final Integer born; public PersonEntity(Integer born, String name) { this.born = born; this.name = name; } public Integer getBorn() { return born; } public String getName() { return name; } } We haven’t modelled the relationship between movies and people in both direction. Why is that? We see the MovieEntity as the aggregate root, owning the relationships. On the other hand, we want to be able to pull all people from the database without selecting all the movies associated with them. Please consider your application’s use case before you try to map every relationship in your database in every direction. While you can do this, you may end up rebuilding a graph database inside your object graph and this is not the intention of a mapping framework. If you have to model your circular or bidirectional domain and don’t want to fetch the whole graph, you can define a fine-grained description of the data that you want to fetch by using projections(../repositories/projections.html) ."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/object-mapping/mapping-ids.html","Handling and provisioning of unique IDs: Using the internal Neo4j id: The easiest way to give your domain classes a unique identifier is the combination of @Id and @GeneratedValue on a field of type String or Long (preferable the object, not the scalar long , as literal null is the better indicator whether an instance is new or not): Example 1. Mutable MovieEntity with internal Neo4j id @Node(""Movie"") public class MovieEntity { @Id @GeneratedValue private Long id; private String name; public MovieEntity(String name) { this.name = name; } } You don’t need to provide a setter for the field, SDN will use reflection to assign the field, but use a setter if there is one. If you want to create an immutable entity with an internally generated id, you have to provide a wither . Example 2. Immutable MovieEntity with internal Neo4j id @Node(""Movie"") public class MovieEntity { @Id @GeneratedValue private final Long id; (1) private String name; public MovieEntity(String name) { (2) this(null, name); } private MovieEntity(Long id, String name) { (3) this.id = id; this.name = name; } public MovieEntity withId(Long id) { (4) if (this.id.equals(id)) { return this; } else { return new MovieEntity(id, this.title); } } } 1 Immutable final id field indicating a generated value 2 Public constructor, used by the application and Spring Data 3 Internally used constructor 4 This is a so-called wither for the id -attribute. It creates a new entity and set’s the field accordingly, without modifying the original entity, thus making it immutable. You either have to provide a setter for the id attribute or something like a wither , if you want to have Advantages: It is pretty clear that the id attribute is the surrogate business key, it takes no further effort or configuration to use it. Disadvantage: It is tied to Neo4js internal database id, which is not unique to our application entity only over a database lifetime. Disadvantage: It takes more effort to create an immutable entity Use externally provided surrogate keys: The @GeneratedValue annotation can take a class implementing org.springframework.data.neo4j.core.schema.IdGenerator as parameter. SDN provides InternalIdGenerator (the default) and UUIDStringGenerator out of the box. The latter generates new UUIDs for each entity and returns them as java.lang.String . An application entity using that would look like this: Example 3. Mutable MovieEntity with externally generated surrogate key @Node(""Movie"") public class MovieEntity { @Id @GeneratedValue(UUIDStringGenerator.class) private String id; private String name; } We have to discuss two separate things regarding advantages and disadvantages. The assignment itself and the UUID-Strategy. A universally unique identifier(https://en.wikipedia.org/wiki/Universally_unique_identifier) is meant to be unique for practical purposes. To quote Wikipedia: “Thus, anyone can create a UUID and use it to identify something with near certainty that the identifier does not duplicate one that has already been, or will be, created to identify something else.” Our strategy uses Java internal UUID mechanism, employing a cryptographically strong pseudo random number generator. In most cases that should work fine, but your mileage might vary. That leaves the assignment itself: Advantage: The application is in full control and can generate a unique key that is just unique enough for the purpose of the application. The generated value will be stable and there won’t be a need to change it later on. Disadvantage: The generated strategy is applied on the application side of things. In those days most applications will be deployed in more than one instance to scale nicely. If your strategy is prone to generate duplicates then inserts will fail as the uniqueness property of the primary key will be violated. So while you don’t have to think about a unique business key in this scenario, you have to think more what to generate. You have several options to roll out your own ID generator. One is a POJO implementing a generator: Example 4. Naive sequence generator import java.util.concurrent.atomic.AtomicInteger; import org.springframework.data.neo4j.core.schema.IdGenerator; import org.springframework.util.StringUtils; public class TestSequenceGenerator implements IdGenerator<String> { private final AtomicInteger sequence = new AtomicInteger(0); @Override public String generateId(String primaryLabel, Object entity) { return StringUtils.uncapitalize(primaryLabel) + ""-"" + sequence.incrementAndGet(); } } Another option is to provide an additional Spring Bean like this: Example 5. Neo4jClient based ID generator @Component class MyIdGenerator implements IdGenerator<String> { private final Neo4jClient neo4jClient; public MyIdGenerator(Neo4jClient neo4jClient) { this.neo4jClient = neo4jClient; } @Override public String generateId(String primaryLabel, Object entity) { return neo4jClient.query(""YOUR CYPHER QUERY FOR THE NEXT ID"") (1) .fetchAs(String.class).one().get(); } } 1 Use exactly the query or logic your need. The generator above would be configured as a bean reference like this: Example 6. Mutable MovieEntity using a Spring Bean as Id generator @Node(""Movie"") public class MovieEntity { @Id @GeneratedValue(generatorRef = ""myIdGenerator"") private String id; private String name; } Using a business key: We have been using a business key in the complete example’s MovieEntity and PersonEntity(metadata-based-mapping.html#mapping.complete-example.person) . The name of the person is assigned at construction time, both by your application and while being loaded through Spring Data. This is only possible, if you find a stable, unique business key, but makes great immutable domain objects. Advantages: Using a business or natural key as primary key is natural. The entity in question is clearly identified, and it feels most of the time just right in the further modelling of your domain. Disadvantages: Business keys as primary keys will be hard to update once you realise that the key you found is not as stable as you thought. Often it turns out that it can change, even when promised otherwise. Apart from that, finding identifier that are truly unique for a thing is hard. Please keep in mind that a business key is always set on the domain entity before Spring Data Neo4j processes it. This means that it cannot determine if the entity was new or not (it always assumes that the entity is new), unless also a @Version field(metadata-based-mapping.html#mapping.annotations.version) is provided."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/object-mapping/sdc-object-mapping.html","Spring Data Object Mapping Fundamentals: This section covers the fundamentals of Spring Data object mapping, object creation, field and property access, mutability and immutability. Core responsibility of the Spring Data object mapping is to create instances of domain objects and map the store-native data structures onto those. This means we need two fundamental steps: Instance creation by using one of the constructors exposed. Instance population to materialize all exposed properties. Object creation: Spring Data automatically tries to detect a persistent entity’s constructor to be used to materialize objects of that type. The resolution algorithm works as follows: If there is a no-argument constructor, it will be used. Other constructors will be ignored. If there is a single constructor taking arguments, it will be used. If there are multiple constructors taking arguments, the one to be used by Spring Data will have to be annotated with @PersistenceCreator . The value resolution assumes constructor argument names to match the property names of the entity, i.e. the resolution will be performed as if the property was to be populated, including all customizations in mapping (different datastore column or field name etc.). This also requires either parameter names information available in the class file or an @ConstructorProperties annotation being present on the constructor. Object creation internals To avoid the overhead of reflection, Spring Data object creation uses a factory class generated at runtime by default, which will call the domain classes constructor directly. I.e. for this example type: class Person { Person(String firstname, String lastname) { … } } we will create a factory class semantically equivalent to this one at runtime: class PersonObjectInstantiator implements ObjectInstantiator { Object newInstance(Object... args) { return new Person((String) args[0], (String) args[1]); } } This gives us a roundabout 10% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: it must not be a private class it must not be a non-static inner class it must not be a CGLib proxy class the constructor to be used by Spring Data must not be private If any of these criteria match, Spring Data will fall back to entity instantiation via reflection. Property population: Once an instance of the entity has been created, Spring Data populates all remaining persistent properties of that class. Unless already populated by the entity’s constructor (i.e. consumed through its constructor argument list), the identifier property will be populated first to allow the resolution of cyclic object references. After that, all non-transient properties that have not already been populated by the constructor are set on the entity instance. For that we use the following algorithm: If the property is immutable but exposes a wither method (see below), we use the wither to create a new entity instance with the new property value. If property access (i.e. access through getters and setters) is defined, we are invoking the setter method. By default, we set the field value directly. Property population internals Similarly to our optimizations in object construction(#mapping.fundamentals.object-creation.details) we also use Spring Data runtime generated accessor classes to interact with the entity instance. class Person { private final Long id; private String firstname; private @AccessType(Type.PROPERTY) String lastname; Person() { this.id = null; } Person(Long id, String firstname, String lastname) { // Field assignments } Person withId(Long id) { return new Person(id, this.firstname, this.lastame); } void setLastname(String lastname) { this.lastname = lastname; } } Example 1. A generated Property Accessor class PersonPropertyAccessor implements PersistentPropertyAccessor { private static final MethodHandle firstname; (2) private Person person; (1) public void setProperty(PersistentProperty property, Object value) { String name = property.getName(); if (""firstname"".equals(name)) { firstname.invoke(person, (String) value); (2) } else if (""id"".equals(name)) { this.person = person.withId((Long) value); (3) } else if (""lastname"".equals(name)) { this.person.setLastname((String) value); (4) } } } 1 PropertyAccessor’s hold a mutable instance of the underlying object. This is, to enable mutations of otherwise immutable properties. 2 By default, Spring Data uses field-access to read and write property values. As per visibility rules of private fields, MethodHandles are used to interact with fields. 3 The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. Calling withId(…) creates a new Vertex object. All subsequent mutations will take place in the new instance leaving the previous untouched. 4 Using property-access allows direct method invocations without using MethodHandles . This gives us a roundabout 25% performance boost over reflection. For the domain class to be eligible for such optimization, it needs to adhere to a set of constraints: Types must not reside in the default or under the java package. Types and their constructors must be public Types that are inner classes must be static . The used Java Runtime must allow for declaring classes in the originating ClassLoader . Java 9 and newer impose certain limitations. By default, Spring Data attempts to use generated property accessors and falls back to reflection-based ones if a limitation is detected. Let’s have a look at the following entity: Example 2. A sample entity class Person { private final @Id Long id; (1) private final String firstname, lastname; (2) private final LocalDate birthday; private final int age; (3) private String comment; (4) private @AccessType(Type.PROPERTY) String remarks; (5) static Person of(String firstname, String lastname, LocalDate birthday) { (6) return new Person(null, firstname, lastname, birthday, Period.between(birthday, LocalDate.now()).getYears()); } Person(Long id, String firstname, String lastname, LocalDate birthday, int age) { (6) this.id = id; this.firstname = firstname; this.lastname = lastname; this.birthday = birthday; this.age = age; } Person withId(Long id) { (1) return new Person(id, this.firstname, this.lastname, this.birthday); } void setRemarks(String remarks) { (5) this.remarks = remarks; } } 1 The identifier property is final but set to null in the constructor. The class exposes a withId(…) method that’s used to set the identifier, e.g. when an instance is inserted into the datastore and an identifier has been generated. The original Vertex instance stays unchanged as a new one is created. The same pattern is usually applied for other properties that are store managed but might have to be changed for persistence operations. 2 The firstname and lastname properties are ordinary immutable properties potentially exposed through getters. 3 The age property is an immutable but derived one from the birthday property. With the design shown, the database value will trump the defaulting as Spring Data uses the only declared constructor. Even if the intent is that the calculation should be preferred, it’s important that this constructor also takes age as parameter (to potentially ignore it) as otherwise the property population step will attempt to set the age field and fail due to it being immutable and no wither being present. 4 The comment property is mutable is populated by setting its field directly. 5 The remarks properties are mutable and populated by setting the comment field directly or by invoking the setter method for 6 The class exposes a factory method and a constructor for object creation. The core idea here is to use factory methods instead of additional constructors to avoid the need for constructor disambiguation through @PersistenceCreator . Instead, defaulting of properties is handled within the factory method. General recommendations: Try to stick to immutable objects — Immutable objects are straightforward to create as materializing an object is then a matter of calling its constructor only. Also, this prevents your domain objects from being littered with setter methods that allow client code to manipulate the objects state. If you need those, prefer to make them package protected so that they can only be invoked by a limited amount of co-located types. Constructor-only materialization is up to 30% faster than properties population. Provide an all-args constructor — Even if you cannot or don’t want to model your entities as immutable values, there’s still value in providing a constructor that takes all properties of the entity as arguments, including the mutable ones, as this allows the object mapping to skip the property population for optimal performance. Use factory methods instead of overloaded constructors to avoid @PersistenceCreator — With an all-argument constructor needed for optimal performance, we usually want to expose more application use case specific constructors that omit things like auto-generated identifiers etc. It’s an established pattern to rather use static factory methods to expose these variants of the all-args constructor. Make sure you adhere to the constraints that allow the generated instantiator and property accessor classes to be used For identifiers to be generated, still use a final field in combination with a wither method Use Lombok to avoid boilerplate code — As persistence operations usually require a constructor taking all arguments, their declaration becomes a tedious repetition of boilerplate parameter to field assignments that can best be avoided by using Lombok’s @AllArgsConstructor . A note on immutable mapping: Although we recommend to use immutable mapping and constructs wherever possible, there are some limitations when it comes to mapping. Given a bidirectional relationship where A has a constructor reference to B and B has a reference to A , or a more complex scenario. This hen/egg situation is not solvable for Spring Data Neo4j. During the instantiation of A it eagerly needs to have a fully instantiated B , which on the other hand requires an instance (to be precise, the same instance) of A . SDN allows such models in general, but will throw a MappingException at runtime if the data that gets returned from the database contains such constellation as described above. In such cases or scenarios, where you cannot foresee what the data that gets returned looks like, you are better suited with a mutable field for the relationships. Kotlin support: Spring Data adapts specifics of Kotlin to allow object creation and mutation. Kotlin object creation: Kotlin classes are supported to be instantiated , all classes are immutable by default and require explicit property declarations to define mutable properties. Consider the following data class Vertex : data class Person(val id: String, val name: String) The class above compiles to a typical class with an explicit constructor. We can customize this class by adding another constructor and annotate it with @PersistenceCreator to indicate a constructor preference: data class Person(var id: String, val name: String) { @PersistenceCreator constructor(id: String) : this(id, ""unknown"") } Kotlin supports parameter optionality by allowing default values to be used if a parameter is not provided. When Spring Data detects a constructor with parameter defaulting, then it leaves these parameters absent if the data store does not provide a value (or simply returns null ) so Kotlin can apply parameter defaulting. Consider the following class that applies parameter defaulting for name data class Person(var id: String, val name: String = ""unknown"") Every time the name parameter is either not part of the result or its value is null , then the name defaults to unknown . Property population of Kotlin data classes: In Kotlin, all classes are immutable by default and require explicit property declarations to define mutable properties. Consider the following data class Vertex : data class Person(val id: String, val name: String) This class is effectively immutable. It allows creating new instances as Kotlin generates a copy(…) method that creates new object instances copying all property values from the existing object and applying property values provided as arguments to the method."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories.html","Repositories: This chapter explains the basic foundations of Spring Data repositories and Neo4j specifics. Before continuing to the Neo4j specifics, make sure you have a sound understanding of the basic concepts. The goal of the Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. Section Summary: Core concepts(repositories/core-concepts.html) Query Methods(repositories/query-methods.html) Defining Repository Interfaces(repositories/definition.html) Defining Query Methods(repositories/query-methods-details.html) Creating Repository Instances(repositories/create-instances.html) Custom Repository Implementations(repositories/custom-implementations.html) Publishing Events from Aggregate Roots(repositories/core-domain-events.html) Spring Data Extensions(repositories/core-extensions.html) Query by Example(query-by-example.html) Scrolling(repositories/scrolling.html) Spring Data Neo4j Extensions(repositories/sdn-extension.html) Repository query keywords(repositories/query-keywords-reference.html) Repository query return types(repositories/query-return-types-reference.html)"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/core-concepts.html","Core concepts: The central interface in the Spring Data repository abstraction is Repository . It takes the domain class to manage as well as the identifier type of the domain class as type arguments. This interface acts primarily as a marker interface to capture the types to work with and to help you to discover interfaces that extend this one. The CrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/CrudRepository.html) and ListCrudRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListCrudRepository.html) interfaces provide sophisticated CRUD functionality for the entity class that is being managed. CrudRepository Interface public interface CrudRepository<T, ID> extends Repository<T, ID> { <S extends T> S save(S entity); (1) Optional<T> findById(ID primaryKey); (2) Iterable<T> findAll(); (3) long count(); (4) void delete(T entity); (5) boolean existsById(ID primaryKey); (6) // … more functionality omitted. } 1 Saves the given entity. 2 Returns the entity identified by the given ID. 3 Returns all entities. 4 Returns the number of entities. 5 Deletes the given entity. 6 Indicates whether an entity with the given ID exists. The methods declared in this interface are commonly referred to as CRUD methods. ListCrudRepository offers equivalent methods, but they return List where the CrudRepository methods return an Iterable . We also provide persistence technology-specific abstractions, such as JpaRepository or MongoRepository . Those interfaces extend CrudRepository and expose the capabilities of the underlying persistence technology in addition to the rather generic persistence technology-agnostic interfaces such as CrudRepository . Additional to the CrudRepository , there are PagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/PagingAndSortingRepository.html) and ListPagingAndSortingRepository(https://docs.spring.io/spring-data/commons/docs/3.3.4/api//org/springframework/data/repository/ListPagingAndSortingRepository.html) which add additional methods to ease paginated access to entities: PagingAndSortingRepository interface public interface PagingAndSortingRepository<T, ID> { Iterable<T> findAll(Sort sort); Page<T> findAll(Pageable pageable); } Extension interfaces are subject to be supported by the actual store module. While this documentation explains the general scheme, make sure that your store module supports the interfaces that you want to use. To access the second page of User by a page size of 20, you could do something like the following: PagingAndSortingRepository<User, Long> repository = // … get access to a bean Page<User> users = repository.findAll(PageRequest.of(1, 20)); ListPagingAndSortingRepository offers equivalent methods, but returns a List where the PagingAndSortingRepository methods return an Iterable . In addition to query methods, query derivation for both count and delete queries is available. The following list shows the interface definition for a derived count query: Derived Count Query interface UserRepository extends CrudRepository<User, Long> { long countByLastname(String lastname); } The following listing shows the interface definition for a derived delete query: Derived Delete Query interface UserRepository extends CrudRepository<User, Long> { long deleteByLastname(String lastname); List<User> removeByLastname(String lastname); }"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/query-methods.html","Query Methods: Standard CRUD functionality repositories usually have queries on the underlying datastore. With Spring Data, declaring those queries becomes a four-step process: Declare an interface extending Repository or one of its subinterfaces and type it to the domain class and ID type that it should handle, as shown in the following example: interface PersonRepository extends Repository<Person, Long> { … } Declare query methods on the interface. interface PersonRepository extends Repository<Person, Long> { List<Person> findByLastname(String lastname); } Set up Spring to create proxy instances for those interfaces, either with JavaConfig(create-instances.html#repositories.create-instances.java-config) or with XML configuration(create-instances.html) . Java XML import org.springframework.data.….repository.config.EnableNeo4jRepositories; @EnableNeo4jRepositories class Config { … } <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:jpa=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <repositories base-package=""com.acme.repositories""/> </beans> The JPA namespace is used in this example. If you use the repository abstraction for any other store, you need to change this to the appropriate namespace declaration of your store module. In other words, you should exchange jpa in favor of, for example, mongodb . Note that the JavaConfig variant does not configure a package explicitly, because the package of the annotated class is used by default. To customize the package to scan, use one of the basePackage… attributes of the data-store-specific repository’s @EnableNeo4jRepositories -annotation. Inject the repository instance and use it, as shown in the following example: class SomeClient { private final PersonRepository repository; SomeClient(PersonRepository repository) { this.repository = repository; } void doSomething() { List<Person> persons = repository.findByLastname(""Matthews""); } } The sections that follow explain each step in detail: Defining Repository Interfaces(definition.html) Defining Query Methods(query-methods-details.html) Creating Repository Instances(create-instances.html) Custom Implementations for Spring Data Repositories(custom-implementations.html)"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/definition.html","Defining Repository Interfaces: To define a repository interface, you first need to define a domain class-specific repository interface. The interface must extend Repository and be typed to the domain class and an ID type. If you want to expose CRUD methods for that domain type, you may extend CrudRepository , or one of its variants instead of Repository . Fine-tuning Repository Definition: There are a few variants how you can get started with your repository interface. The typical approach is to extend CrudRepository , which gives you methods for CRUD functionality. CRUD stands for Create, Read, Update, Delete. With version 3.0 we also introduced ListCrudRepository which is very similar to the CrudRepository but for those methods that return multiple entities it returns a List instead of an Iterable which you might find easier to use. If you are using a reactive store you might choose ReactiveCrudRepository , or RxJava3CrudRepository depending on which reactive framework you are using. If you are using Kotlin you might pick CoroutineCrudRepository which utilizes Kotlin’s coroutines. Additional you can extend PagingAndSortingRepository , ReactiveSortingRepository , RxJava3SortingRepository , or CoroutineSortingRepository if you need methods that allow to specify a Sort abstraction or in the first case a Pageable abstraction. Note that the various sorting repositories no longer extended their respective CRUD repository as they did in Spring Data Versions pre 3.0. Therefore, you need to extend both interfaces if you want functionality of both. If you do not want to extend Spring Data interfaces, you can also annotate your repository interface with @RepositoryDefinition . Extending one of the CRUD repository interfaces exposes a complete set of methods to manipulate your entities. If you prefer to be selective about the methods being exposed, copy the methods you want to expose from the CRUD repository into your domain repository. When doing so, you may change the return type of methods. Spring Data will honor the return type if possible. For example, for methods returning multiple entities you may choose Iterable<T> , List<T> , Collection<T> or a VAVR list. If many repositories in your application should have the same set of methods you can define your own base interface to inherit from. Such an interface must be annotated with @NoRepositoryBean . This prevents Spring Data to try to create an instance of it directly and failing because it can’t determine the entity for that repository, since it still contains a generic type variable. The following example shows how to selectively expose CRUD methods ( findById and save , in this case): Selectively exposing CRUD methods @NoRepositoryBean interface MyBaseRepository<T, ID> extends Repository<T, ID> { Optional<T> findById(ID id); <S extends T> S save(S entity); } interface UserRepository extends MyBaseRepository<User, Long> { User findByEmailAddress(EmailAddress emailAddress); } In the prior example, you defined a common base interface for all your domain repositories and exposed findById(…) as well as save(…) .These methods are routed into the base repository implementation of the store of your choice provided by Spring Data (for example, if you use JPA, the implementation is SimpleJpaRepository ), because they match the method signatures in CrudRepository . So the UserRepository can now save users, find individual users by ID, and trigger a query to find Users by email address. The intermediate repository interface is annotated with @NoRepositoryBean . Make sure you add that annotation to all repository interfaces for which Spring Data should not create instances at runtime. Using Repositories with Multiple Spring Data Modules: Using a unique Spring Data module in your application makes things simple, because all repository interfaces in the defined scope are bound to the Spring Data module. Sometimes, applications require using more than one Spring Data module. In such cases, a repository definition must distinguish between persistence technologies. When it detects multiple repository factories on the class path, Spring Data enters strict repository configuration mode. Strict configuration uses details on the repository or the domain class to decide about Spring Data module binding for a repository definition: If the repository definition extends the module-specific repository(#repositories.multiple-modules.types) , it is a valid candidate for the particular Spring Data module. If the domain class is annotated with the module-specific type annotation(#repositories.multiple-modules.annotations) , it is a valid candidate for the particular Spring Data module. Spring Data modules accept either third-party annotations (such as JPA’s @Entity ) or provide their own annotations (such as @Document for Spring Data MongoDB and Spring Data Elasticsearch). The following example shows a repository that uses module-specific interfaces (JPA in this case): Example 1. Repository definitions using module-specific interfaces interface MyRepository extends JpaRepository<User, Long> { } @NoRepositoryBean interface MyBaseRepository<T, ID> extends JpaRepository<T, ID> { … } interface UserRepository extends MyBaseRepository<User, Long> { … } MyRepository and UserRepository extend JpaRepository in their type hierarchy. They are valid candidates for the Spring Data JPA module. The following example shows a repository that uses generic interfaces: Example 2. Repository definitions using generic interfaces interface AmbiguousRepository extends Repository<User, Long> { … } @NoRepositoryBean interface MyBaseRepository<T, ID> extends CrudRepository<T, ID> { … } interface AmbiguousUserRepository extends MyBaseRepository<User, Long> { … } AmbiguousRepository and AmbiguousUserRepository extend only Repository and CrudRepository in their type hierarchy. While this is fine when using a unique Spring Data module, multiple modules cannot distinguish to which particular Spring Data these repositories should be bound. The following example shows a repository that uses domain classes with annotations: Example 3. Repository definitions using domain classes with annotations interface PersonRepository extends Repository<Person, Long> { … } @Entity class Person { … } interface UserRepository extends Repository<User, Long> { … } @Document class User { … } PersonRepository references Person , which is annotated with the JPA @Entity annotation, so this repository clearly belongs to Spring Data JPA. UserRepository references User , which is annotated with Spring Data MongoDB’s @Document annotation. The following bad example shows a repository that uses domain classes with mixed annotations: Example 4. Repository definitions using domain classes with mixed annotations interface JpaPersonRepository extends Repository<Person, Long> { … } interface MongoDBPersonRepository extends Repository<Person, Long> { … } @Entity @Document class Person { … } This example shows a domain class using both JPA and Spring Data MongoDB annotations. It defines two repositories, JpaPersonRepository and MongoDBPersonRepository . One is intended for JPA and the other for MongoDB usage. Spring Data is no longer able to tell the repositories apart, which leads to undefined behavior. Repository type details(#repositories.multiple-modules.types) and distinguishing domain class annotations(#repositories.multiple-modules.annotations) are used for strict repository configuration to identify repository candidates for a particular Spring Data module. Using multiple persistence technology-specific annotations on the same domain type is possible and enables reuse of domain types across multiple persistence technologies. However, Spring Data can then no longer determine a unique module with which to bind the repository. The last way to distinguish repositories is by scoping repository base packages. Base packages define the starting points for scanning for repository interface definitions, which implies having repository definitions located in the appropriate packages. By default, annotation-driven configuration uses the package of the configuration class. The base package in XML-based configuration(create-instances.html#repositories.create-instances.xml) is mandatory. The following example shows annotation-driven configuration of base packages: Annotation-driven configuration of base packages @EnableJpaRepositories(basePackages = ""com.acme.repositories.jpa"") @EnableMongoRepositories(basePackages = ""com.acme.repositories.mongo"") class Configuration { … }"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/query-methods-details.html","Defining Query Methods: The repository proxy has two ways to derive a store-specific query from the method name: By deriving the query from the method name directly. By using a manually defined query. Available options depend on the actual store. However, there must be a strategy that decides what actual query is created. The next section describes the available options. Query Lookup Strategies: The following strategies are available for the repository infrastructure to resolve the query. With XML configuration, you can configure the strategy at the namespace through the query-lookup-strategy attribute. For Java configuration, you can use the queryLookupStrategy attribute of the EnableNeo4jRepositories annotation. Some strategies may not be supported for particular datastores. CREATE attempts to construct a store-specific query from the query method name. The general approach is to remove a given set of well known prefixes from the method name and parse the rest of the method. You can read more about query construction in “ Query Creation(#repositories.query-methods.query-creation) ”. USE_DECLARED_QUERY tries to find a declared query and throws an exception if it cannot find one. The query can be defined by an annotation somewhere or declared by other means. See the documentation of the specific store to find available options for that store. If the repository infrastructure does not find a declared query for the method at bootstrap time, it fails. CREATE_IF_NOT_FOUND (the default) combines CREATE and USE_DECLARED_QUERY . It looks up a declared query first, and, if no declared query is found, it creates a custom method name-based query. This is the default lookup strategy and, thus, is used if you do not configure anything explicitly. It allows quick query definition by method names but also custom-tuning of these queries by introducing declared queries as needed. Query Creation: The query builder mechanism built into the Spring Data repository infrastructure is useful for building constraining queries over entities of the repository. The following example shows how to create a number of queries: Query creation from method names interface PersonRepository extends Repository<Person, Long> { List<Person> findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // Enables the distinct flag for the query List<Person> findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); List<Person> findPeopleDistinctByLastnameOrFirstname(String lastname, String firstname); // Enabling ignoring case for an individual property List<Person> findByLastnameIgnoreCase(String lastname); // Enabling ignoring case for all suitable properties List<Person> findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // Enabling static ORDER BY for a query List<Person> findByLastnameOrderByFirstnameAsc(String lastname); List<Person> findByLastnameOrderByFirstnameDesc(String lastname); } Parsing query method names is divided into subject and predicate. The first part ( find…By , exists…By ) defines the subject of the query, the second part forms the predicate. The introducing clause (subject) can contain further expressions. Any text between find (or other introducing keywords) and By is considered to be descriptive unless using one of the result-limiting keywords such as a Distinct to set a distinct flag on the query to be created or Top/First to limit query results(#repositories.limit-query-result) . The appendix contains the full list of query method subject keywords(query-keywords-reference.html#appendix.query.method.subject) and query method predicate keywords including sorting and letter-casing modifiers(query-keywords-reference.html#appendix.query.method.predicate) . However, the first By acts as a delimiter to indicate the start of the actual criteria predicate. At a very basic level, you can define conditions on entity properties and concatenate them with And and Or . The actual result of parsing the method depends on the persistence store for which you create the query. However, there are some general things to notice: The expressions are usually property traversals combined with operators that can be concatenated. You can combine property expressions with AND and OR . You also get support for operators such as Between , LessThan , GreaterThan , and Like for the property expressions. The supported operators can vary by datastore, so consult the appropriate part of your reference documentation. The method parser supports setting an IgnoreCase flag for individual properties (for example, findByLastnameIgnoreCase(…) ) or for all properties of a type that supports ignoring case (usually String instances — for example, findByLastnameAndFirstnameAllIgnoreCase(…) ). Whether ignoring cases is supported may vary by store, so consult the relevant sections in the reference documentation for the store-specific query method. You can apply static ordering by appending an OrderBy clause to the query method that references a property and by providing a sorting direction ( Asc or Desc ). To create a query method that supports dynamic sorting, see “ Paging, Iterating Large Results, Sorting & Limiting(#repositories.special-parameters) ”. Property Expressions: Property expressions can refer only to a direct property of the managed entity, as shown in the preceding example. At query creation time, you already make sure that the parsed property is a property of the managed domain class. However, you can also define constraints by traversing nested properties. Consider the following method signature: List<Person> findByAddressZipCode(ZipCode zipCode); Assume a Person has an Address with a ZipCode . In that case, the method creates the x.address.zipCode property traversal. The resolution algorithm starts by interpreting the entire part ( AddressZipCode ) as the property and checks the domain class for a property with that name (uncapitalized). If the algorithm succeeds, it uses that property. If not, the algorithm splits up the source at the camel-case parts from the right side into a head and a tail and tries to find the corresponding property — in our example, AddressZip and Code . If the algorithm finds a property with that head, it takes the tail and continues building the tree down from there, splitting the tail up in the way just described. If the first split does not match, the algorithm moves the split point to the left ( Address , ZipCode ) and continues. Although this should work for most cases, it is possible for the algorithm to select the wrong property. Suppose the Person class has an addressZip property as well. The algorithm would match in the first split round already, choose the wrong property, and fail (as the type of addressZip probably has no code property). To resolve this ambiguity you can use _ inside your method name to manually define traversal points. So our method name would be as follows: List<Person> findByAddress_ZipCode(ZipCode zipCode); Because we treat underscores ( _ ) as a reserved character, we strongly advise to follow standard Java naming conventions (that is, not using underscores in property names but applying camel case instead). Field Names starting with underscore: Field names may start with underscores like String _name . Make sure to preserve the _ as in _name and use double _ to split nested paths like user__name . Upper Case Field Names: Field names that are all uppercase can be used as such. Nested paths if applicable require splitting via _ as in USER_name . Field Names with 2nd uppercase letter: Field names that consist of a starting lower case letter followed by an uppercase one like String qCode can be resolved by starting with two upper case letters as in QCode . Please be aware of potential path ambiguities. Path Ambiguities: In the following sample the arrangement of properties qCode and q , with q containing a property called code , creates an ambiguity for the path QCode . record Container(String qCode, Code q) {} record Code(String code) {} Since a direct match on a property is considered first, any potential nested paths will not be considered and the algorithm picks the qCode field. In order to select the code field in q the underscore notation Q_Code is required. Repository Methods Returning Collections or Iterables: Query methods that return multiple results can use standard Java Iterable , List , and Set . Beyond that, we support returning Spring Data’s Streamable , a custom extension of Iterable , as well as collection types provided by Vavr(https://www.vavr.io/) . Refer to the appendix explaining all possible query method return types(query-return-types-reference.html#appendix.query.return.types) . Using Streamable as Query Method Return Type: You can use Streamable as alternative to Iterable or any collection type. It provides convenience methods to access a non-parallel Stream (missing from Iterable ) and the ability to directly ….filter(…) and ….map(…) over the elements and concatenate the Streamable to others: Using Streamable to combine query method results interface PersonRepository extends Repository<Person, Long> { Streamable<Person> findByFirstnameContaining(String firstname); Streamable<Person> findByLastnameContaining(String lastname); } Streamable<Person> result = repository.findByFirstnameContaining(""av"") .and(repository.findByLastnameContaining(""ea"")); Returning Custom Streamable Wrapper Types: Providing dedicated wrapper types for collections is a commonly used pattern to provide an API for a query result that returns multiple elements. Usually, these types are used by invoking a repository method returning a collection-like type and creating an instance of the wrapper type manually. You can avoid that additional step as Spring Data lets you use these wrapper types as query method return types if they meet the following criteria: The type implements Streamable . The type exposes either a constructor or a static factory method named of(…) or valueOf(…) that takes Streamable as an argument. The following listing shows an example: class Product { (1) MonetaryAmount getPrice() { … } } @RequiredArgsConstructor(staticName = ""of"") class Products implements Streamable<Product> { (2) private final Streamable<Product> streamable; public MonetaryAmount getTotal() { (3) return streamable.stream() .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); } @Override public Iterator<Product> iterator() { (4) return streamable.iterator(); } } interface ProductRepository implements Repository<Product, Long> { Products findAllByDescriptionContaining(String text); (5) } 1 A Product entity that exposes API to access the product’s price. 2 A wrapper type for a Streamable<Product> that can be constructed by using Products.of(…) (factory method created with the Lombok annotation). A standard constructor taking the Streamable<Product> will do as well. 3 The wrapper type exposes an additional API, calculating new values on the Streamable<Product> . 4 Implement the Streamable interface and delegate to the actual result. 5 That wrapper type Products can be used directly as a query method return type. You do not need to return Streamable<Product> and manually wrap it after the query in the repository client. Support for Vavr Collections: Vavr(https://www.vavr.io/) is a library that embraces functional programming concepts in Java. It ships with a custom set of collection types that you can use as query method return types, as the following table shows: Vavr collection type Used Vavr implementation type Valid Java source types io.vavr.collection.Seq io.vavr.collection.List java.util.Iterable io.vavr.collection.Set io.vavr.collection.LinkedHashSet java.util.Iterable io.vavr.collection.Map io.vavr.collection.LinkedHashMap java.util.Map You can use the types in the first column (or subtypes thereof) as query method return types and get the types in the second column used as implementation type, depending on the Java type of the actual query result (third column). Alternatively, you can declare Traversable (the Vavr Iterable equivalent), and we then derive the implementation class from the actual return value. That is, a java.util.List is turned into a Vavr List or Seq , a java.util.Set becomes a Vavr LinkedHashSet Set , and so on. Streaming Query Results: You can process the results of query methods incrementally by using a Java 8 Stream<T> as the return type. Instead of wrapping the query results in a Stream , data store-specific methods are used to perform the streaming, as shown in the following example: Stream the result of a query with Java 8 Stream<T> @Query(""select u from User u"") Stream<User> findAllByCustomQueryAndStream(); Stream<User> readAllByFirstnameNotNull(); @Query(""select u from User u"") Stream<User> streamAllPaged(Pageable pageable); A Stream potentially wraps underlying data store-specific resources and must, therefore, be closed after usage. You can either manually close the Stream by using the close() method or by using a Java 7 try-with-resources block, as shown in the following example: Working with a Stream<T> result in a try-with-resources block try (Stream<User> stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } Not all Spring Data modules currently support Stream<T> as a return type. Asynchronous Query Results: You can run repository queries asynchronously by using Spring’s asynchronous method running capability(https://docs.spring.io/spring-framework/reference/6.1/integration/scheduling.html) . This means the method returns immediately upon invocation while the actual query occurs in a task that has been submitted to a Spring TaskExecutor . Asynchronous queries differ from reactive queries and should not be mixed. See the store-specific documentation for more details on reactive support. The following example shows a number of asynchronous queries: @Async Future<User> findByFirstname(String firstname); (1) @Async CompletableFuture<User> findOneByFirstname(String firstname); (2) 1 Use java.util.concurrent.Future as the return type. 2 Use a Java 8 java.util.concurrent.CompletableFuture as the return type. Paging, Iterating Large Results, Sorting & Limiting: To handle parameters in your query, define method parameters as already seen in the preceding examples. Besides that, the infrastructure recognizes certain specific types like Pageable , Sort and Limit , to apply pagination, sorting and limiting to your queries dynamically. The following example demonstrates these features: Using Pageable , Slice , Sort and Limit in query methods Page<User> findByLastname(String lastname, Pageable pageable); Slice<User> findByLastname(String lastname, Pageable pageable); List<User> findByLastname(String lastname, Sort sort); List<User> findByLastname(String lastname, Sort sort, Limit limit); List<User> findByLastname(String lastname, Pageable pageable); APIs taking Sort , Pageable and Limit expect non- null values to be handed into methods. If you do not want to apply any sorting or pagination, use Sort.unsorted() , Pageable.unpaged() and Limit.unlimited() . The first method lets you pass an org.springframework.data.domain.Pageable instance to the query method to dynamically add paging to your statically defined query. A Page knows about the total number of elements and pages available. It does so by the infrastructure triggering a count query to calculate the overall number. As this might be expensive (depending on the store used), you can instead return a Slice . A Slice knows only about whether a next Slice is available, which might be sufficient when walking through a larger result set. Sorting options are handled through the Pageable instance, too. If you need only sorting, add an org.springframework.data.domain.Sort parameter to your method. As you can see, returning a List is also possible. In this case, the additional metadata required to build the actual Page instance is not created (which, in turn, means that the additional count query that would have been necessary is not issued). Rather, it restricts the query to look up only the given range of entities. To find out how many pages you get for an entire query, you have to trigger an additional count query. By default, this query is derived from the query you actually trigger. Special parameters may only be used once within a query method. Some special parameters described above are mutually exclusive. Please consider the following list of invalid parameter combinations. Parameters Example Reason Pageable and Sort findBy…​(Pageable page, Sort sort) Pageable already defines Sort Pageable and Limit findBy…​(Pageable page, Limit limit) Pageable already defines a limit. The Top keyword used to limit results can be used to along with Pageable whereas Top defines the total maximum of results, whereas the Pageable parameter may reduce this number. Which Method is Appropriate?: The value provided by the Spring Data abstractions is perhaps best shown by the possible query method return types outlined in the following table below. The table shows which types you can return from a query method Table 1. Consuming Large Query Results Method Amount of Data Fetched Query Structure Constraints List<T>(#repositories.collections-and-iterables) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Streamable<T>(#repositories.collections-and-iterables.streamable) All results. Single query. Query results can exhaust all memory. Fetching all data can be time-intensive. Stream<T>(#repositories.query-streaming) Chunked (one-by-one or in batches) depending on Stream consumption. Single query using typically cursors. Streams must be closed after usage to avoid resource leaks. Flux<T> Chunked (one-by-one or in batches) depending on Flux consumption. Single query using typically cursors. Store module must provide reactive infrastructure. Slice<T> Pageable.getPageSize() + 1 at Pageable.getOffset() One to many queries fetching data starting at Pageable.getOffset() applying limiting. A Slice can only navigate to the next Slice . Slice provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Window provides details whether there is more data to fetch. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Page<T> Pageable.getPageSize() at Pageable.getOffset() One to many queries starting at Pageable.getOffset() applying limiting. Additionally, COUNT(…) query to determine the total number of elements can be required. Often times, COUNT(…) queries are required that are costly. Offset-based queries becomes inefficient when the offset is too large because the database still has to materialize the full result. Paging and Sorting: You can define simple sorting expressions by using property names. You can concatenate expressions to collect multiple criteria into one expression. Defining sort expressions Sort sort = Sort.by(""firstname"").ascending() .and(Sort.by(""lastname"").descending()); For a more type-safe way to define sort expressions, start with the type for which to define the sort expression and use method references to define the properties on which to sort. Defining sort expressions by using the type-safe API TypedSort<Person> person = Sort.sort(Person.class); Sort sort = person.by(Person::getFirstname).ascending() .and(person.by(Person::getLastname).descending()); TypedSort.by(…) makes use of runtime proxies by (typically) using CGlib, which may interfere with native image compilation when using tools such as Graal VM Native. If your store implementation supports Querydsl, you can also use the generated metamodel types to define sort expressions: Defining sort expressions by using the Querydsl API QSort sort = QSort.by(QPerson.firstname.asc()) .and(QSort.by(QPerson.lastname.desc())); Limiting Query Results: In addition to paging it is possible to limit the result size using a dedicated Limit parameter. You can also limit the results of query methods by using the First or Top keywords, which you can use interchangeably but may not be mixed with a Limit parameter. You can append an optional numeric value to Top or First to specify the maximum result size to be returned. If the number is left out, a result size of 1 is assumed. The following example shows how to limit the query size: Limiting the result size of a query with Top and First List<User> findByLastname(Limit limit); User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page<User> queryFirst10ByLastname(String lastname, Pageable pageable); Slice<User> findTop3ByLastname(String lastname, Pageable pageable); List<User> findFirst10ByLastname(String lastname, Sort sort); List<User> findTop10ByLastname(String lastname, Pageable pageable); The limiting expressions also support the Distinct keyword for datastores that support distinct queries. Also, for the queries that limit the result set to one instance, wrapping the result into with the Optional keyword is supported. If pagination or slicing is applied to a limiting query pagination (and the calculation of the number of available pages), it is applied within the limited result. Limiting the results in combination with dynamic sorting by using a Sort parameter lets you express query methods for the 'K' smallest as well as for the 'K' biggest elements."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/create-instances.html","Creating Repository Instances: This section covers how to create instances and bean definitions for the defined repository interfaces. Java Configuration: Use the store-specific @EnableNeo4jRepositories annotation on a Java configuration class to define a configuration for repository activation. For an introduction to Java-based configuration of the Spring container, see JavaConfig in the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/java.html) . A sample configuration to enable Spring Data repositories resembles the following: Sample annotation-based repository configuration @Configuration @EnableJpaRepositories(""com.acme.repositories"") class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } The preceding example uses the JPA-specific annotation, which you would change according to the store module you actually use. The same applies to the definition of the EntityManagerFactory bean. See the sections covering the store-specific configuration. XML Configuration: Each Spring Data module includes a repositories element that lets you define a base package that Spring scans for you, as shown in the following example: Enabling Spring Data repositories via XML <?xml version=""1.0"" encoding=""UTF-8""?> <beans:beans xmlns:beans=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""http://www.springframework.org/schema/data/jpa"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa https://www.springframework.org/schema/data/jpa/spring-jpa.xsd""> <jpa:repositories base-package=""com.acme.repositories"" /> </beans:beans> In the preceding example, Spring is instructed to scan com.acme.repositories and all its sub-packages for interfaces extending Repository or one of its sub-interfaces. For each interface found, the infrastructure registers the persistence technology-specific FactoryBean to create the appropriate proxies that handle invocations of the query methods. Each bean is registered under a bean name that is derived from the interface name, so an interface of UserRepository would be registered under userRepository . Bean names for nested repository interfaces are prefixed with their enclosing type name. The base package attribute allows wildcards so that you can define a pattern of scanned packages. Using Filters: By default, the infrastructure picks up every interface that extends the persistence technology-specific Repository sub-interface located under the configured base package and creates a bean instance for it. However, you might want more fine-grained control over which interfaces have bean instances created for them. To do so, use filter elements inside the repository declaration. The semantics are exactly equivalent to the elements in Spring’s component filters. For details, see the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/core/beans/classpath-scanning.html) for these elements. For example, to exclude certain interfaces from instantiation as repository beans, you could use the following configuration: Using filters Java XML @Configuration @EnableNeo4jRepositories(basePackages = ""com.acme.repositories"", includeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeRepository"") }, excludeFilters = { @Filter(type = FilterType.REGEX, pattern = "".*SomeOtherRepository"") }) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } <repositories base-package=""com.acme.repositories""> <context:include-filter type=""regex"" expression="".*SomeRepository"" /> <context:exclude-filter type=""regex"" expression="".*SomeOtherRepository"" /> </repositories> The preceding example includes all interfaces ending with SomeRepository and excludes those ending with SomeOtherRepository from being instantiated. Standalone Usage: You can also use the repository infrastructure outside of a Spring container — for example, in CDI environments.You still need some Spring libraries in your classpath, but, generally, you can set up repositories programmatically as well.The Spring Data modules that provide repository support ship with a persistence technology-specific RepositoryFactory that you can use, as follows: Standalone usage of the repository factory RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class);"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/custom-implementations.html","Custom Repository Implementations: Spring Data provides various options to create query methods with little coding. But when those options don’t fit your needs you can also provide your own custom implementation for repository methods. This section describes how to do that. Customizing Individual Repositories: To enrich a repository with custom functionality, you must first define a fragment interface and an implementation for the custom functionality, as follows: Interface for custom repository functionality interface CustomizedUserRepository { void someCustomMethod(User user); } Implementation of custom repository functionality class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } } The most important part of the class name that corresponds to the fragment interface is the Impl postfix. The implementation itself does not depend on Spring Data and can be a regular Spring bean. Consequently, you can use standard dependency injection behavior to inject references to other beans (such as a JdbcTemplate ), take part in aspects, and so on. Then you can let your repository interface extend the fragment interface, as follows: Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, CustomizedUserRepository { // Declare query methods here } Extending the fragment interface with your repository interface combines the CRUD and custom functionality and makes it available to clients. Spring Data repositories are implemented by using fragments that form a repository composition. Fragments are the base repository, functional aspects (such as QueryDsl(core-extensions.html#core.extensions.querydsl) ), and custom interfaces along with their implementations. Each time you add an interface to your repository interface, you enhance the composition by adding a fragment. The base repository and repository aspect implementations are provided by each Spring Data module. The following example shows custom interfaces and their implementations: Fragments with their implementations interface HumanRepository { void someHumanMethod(User user); } class HumanRepositoryImpl implements HumanRepository { public void someHumanMethod(User user) { // Your custom implementation } } interface ContactRepository { void someContactMethod(User user); User anotherContactMethod(User user); } class ContactRepositoryImpl implements ContactRepository { public void someContactMethod(User user) { // Your custom implementation } public User anotherContactMethod(User user) { // Your custom implementation } } The following example shows the interface for a custom repository that extends CrudRepository : Changes to your repository interface interface UserRepository extends CrudRepository<User, Long>, HumanRepository, ContactRepository { // Declare query methods here } Repositories may be composed of multiple custom implementations that are imported in the order of their declaration. Custom implementations have a higher priority than the base implementation and repository aspects. This ordering lets you override base repository and aspect methods and resolves ambiguity if two fragments contribute the same method signature. Repository fragments are not limited to use in a single repository interface. Multiple repositories may use a fragment interface, letting you reuse customizations across different repositories. The following example shows a repository fragment and its implementation: Fragments overriding save(…) interface CustomizedSave<T> { <S extends T> S save(S entity); } class CustomizedSaveImpl<T> implements CustomizedSave<T> { public <S extends T> S save(S entity) { // Your custom implementation } } The following example shows a repository that uses the preceding repository fragment: Customized repository interfaces interface UserRepository extends CrudRepository<User, Long>, CustomizedSave<User> { } interface PersonRepository extends CrudRepository<Person, Long>, CustomizedSave<Person> { } Configuration: The repository infrastructure tries to autodetect custom implementation fragments by scanning for classes below the package in which it found a repository. These classes need to follow the naming convention of appending a postfix defaulting to Impl . The following example shows a repository that uses the default postfix and a repository that sets a custom value for the postfix: Example 1. Configuration example Java XML @EnableNeo4jRepositories(repositoryImplementationPostfix = ""MyPostfix"") class Configuration { … } <repositories base-package=""com.acme.repository"" /> <repositories base-package=""com.acme.repository"" repository-impl-postfix=""MyPostfix"" /> The first configuration in the preceding example tries to look up a class called com.acme.repository.CustomizedUserRepositoryImpl to act as a custom repository implementation. The second example tries to look up com.acme.repository.CustomizedUserRepositoryMyPostfix . Resolution of Ambiguity: If multiple implementations with matching class names are found in different packages, Spring Data uses the bean names to identify which one to use. Given the following two custom implementations for the CustomizedUserRepository shown earlier, the first implementation is used. Its bean name is customizedUserRepositoryImpl , which matches that of the fragment interface ( CustomizedUserRepository ) plus the postfix Impl . Example 2. Resolution of ambiguous implementations package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(""specialCustomImpl"") class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } If you annotate the UserRepository interface with @Component(""specialCustom"") , the bean name plus Impl then matches the one defined for the repository implementation in com.acme.impl.two , and it is used instead of the first one. Manual Wiring: If your custom implementation uses annotation-based configuration and autowiring only, the preceding approach shown works well, because it is treated as any other Spring bean. If your implementation fragment bean needs special wiring, you can declare the bean and name it according to the conventions described in the preceding section(#repositories.single-repository-behaviour.ambiguity) . The infrastructure then refers to the manually defined bean definition by name instead of creating one itself. The following example shows how to manually wire a custom implementation: Example 3. Manual wiring of custom implementations Java XML class MyClass { MyClass(@Qualifier(""userRepositoryImpl"") UserRepository userRepository) { … } } <repositories base-package=""com.acme.repository"" /> <beans:bean id=""userRepositoryImpl"" class=""…""> <!-- further configuration --> </beans:bean> Customize the Base Repository: The approach described in the preceding section(#repositories.manual-wiring) requires customization of each repository interfaces when you want to customize the base repository behavior so that all repositories are affected. To instead change behavior for all repositories, you can create an implementation that extends the persistence technology-specific repository base class. This class then acts as a custom base class for the repository proxies, as shown in the following example: Custom repository base class class MyRepositoryImpl<T, ID> extends SimpleJpaRepository<T, ID> { private final EntityManager entityManager; MyRepositoryImpl(JpaEntityInformation entityInformation, EntityManager entityManager) { super(entityInformation, entityManager); // Keep the EntityManager around to used from the newly introduced methods. this.entityManager = entityManager; } @Transactional public <S extends T> S save(S entity) { // implementation goes here } } The class needs to have a constructor of the super class which the store-specific repository factory implementation uses. If the repository base class has multiple constructors, override the one taking an EntityInformation plus a store specific infrastructure object (such as an EntityManager or a template class). The final step is to make the Spring Data infrastructure aware of the customized repository base class. In configuration, you can do so by using the repositoryBaseClass , as shown in the following example: Example 4. Configuring a custom repository base class Java XML @Configuration @EnableNeo4jRepositories(repositoryBaseClass = MyRepositoryImpl.class) class ApplicationConfiguration { … } <repositories base-package=""com.acme.repository"" base-class=""….MyRepositoryImpl"" />"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/core-domain-events.html","Publishing Events from Aggregate Roots: Entities managed by repositories are aggregate roots. In a Domain-Driven Design application, these aggregate roots usually publish domain events. Spring Data provides an annotation called @DomainEvents that you can use on a method of your aggregate root to make that publication as easy as possible, as shown in the following example: Exposing domain events from an aggregate root class AnAggregateRoot { @DomainEvents (1) Collection<Object> domainEvents() { // … return events you want to get published here } @AfterDomainEventPublication (2) void callbackMethod() { // … potentially clean up domain events list } } 1 The method that uses @DomainEvents can return either a single event instance or a collection of events. It must not take any arguments. 2 After all events have been published, we have a method annotated with @AfterDomainEventPublication . You can use it to potentially clean the list of events to be published (among other uses). The methods are called every time one of the following a Spring Data repository methods are called: save(…) , saveAll(…) delete(…) , deleteAll(…) , deleteAllInBatch(…) , deleteInBatch(…) Note, that these methods take the aggregate root instances as arguments. This is why deleteById(…) is notably absent, as the implementations might choose to issue a query deleting the instance and thus we would never have access to the aggregate instance in the first place."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/core-extensions.html","Spring Data Extensions: This section documents a set of Spring Data extensions that enable Spring Data usage in a variety of contexts. Currently, most of the integration is targeted towards Spring MVC. Querydsl Extension: Querydsl(http://www.querydsl.com/) is a framework that enables the construction of statically typed SQL-like queries through its fluent API. Several Spring Data modules offer integration with Querydsl through QuerydslPredicateExecutor , as the following example shows: QuerydslPredicateExecutor interface public interface QuerydslPredicateExecutor<T> { Optional<T> findById(Predicate predicate); (1) Iterable<T> findAll(Predicate predicate); (2) long count(Predicate predicate); (3) boolean exists(Predicate predicate); (4) // … more functionality omitted. } 1 Finds and returns a single entity matching the Predicate . 2 Finds and returns all entities matching the Predicate . 3 Returns the number of entities matching the Predicate . 4 Returns whether an entity that matches the Predicate exists. To use the Querydsl support, extend QuerydslPredicateExecutor on your repository interface, as the following example shows: Querydsl integration on repositories interface UserRepository extends CrudRepository<User, Long>, QuerydslPredicateExecutor<User> { } The preceding example lets you write type-safe queries by using Querydsl Predicate instances, as the following example shows: Predicate predicate = user.firstname.equalsIgnoreCase(""dave"") .and(user.lastname.startsWithIgnoreCase(""mathews"")); userRepository.findAll(predicate); Web support: Spring Data modules that support the repository programming model ship with a variety of web support. The web related components require Spring MVC JARs to be on the classpath. Some of them even provide integration with Spring HATEOAS(https://github.com/spring-projects/spring-hateoas) . In general, the integration support is enabled by using the @EnableSpringDataWebSupport annotation in your JavaConfig configuration class, as the following example shows: Enabling Spring Data web support Java XML @Configuration @EnableWebMvc @EnableSpringDataWebSupport class WebConfiguration {} <bean class=""org.springframework.data.web.config.SpringDataWebConfiguration"" /> <!-- If you use Spring HATEOAS, register this one *instead* of the former --> <bean class=""org.springframework.data.web.config.HateoasAwareSpringDataWebConfiguration"" /> The @EnableSpringDataWebSupport annotation registers a few components. We discuss those later in this section. It also detects Spring HATEOAS on the classpath and registers integration components (if present) for it as well. Basic Web Support: Enabling Spring Data web support in XML The configuration shown in the previous section(#core.web) registers a few basic components: A Using the DomainClassConverter Class(#core.web.basic.domain-class-converter) to let Spring MVC resolve instances of repository-managed domain classes from request parameters or path variables. HandlerMethodArgumentResolver(#core.web.basic.paging-and-sorting) implementations to let Spring MVC resolve Pageable and Sort instances from request parameters. Jackson Modules(#core.web.basic.jackson-mappers) to de-/serialize types like Point and Distance , or store specific ones, depending on the Spring Data Module used. Using the DomainClassConverter Class: The DomainClassConverter class lets you use domain types in your Spring MVC controller method signatures directly so that you need not manually lookup the instances through the repository, as the following example shows: A Spring MVC controller using domain types in method signatures @Controller @RequestMapping(""/users"") class UserController { @RequestMapping(""/{id}"") String showUserForm(@PathVariable(""id"") User user, Model model) { model.addAttribute(""user"", user); return ""userForm""; } } The method receives a User instance directly, and no further lookup is necessary. The instance can be resolved by letting Spring MVC convert the path variable into the id type of the domain class first and eventually access the instance through calling findById(…) on the repository instance registered for the domain type. Currently, the repository has to implement CrudRepository to be eligible to be discovered for conversion. HandlerMethodArgumentResolvers for Pageable and Sort: The configuration snippet shown in the previous section(#core.web.basic.domain-class-converter) also registers a PageableHandlerMethodArgumentResolver as well as an instance of SortHandlerMethodArgumentResolver . The registration enables Pageable and Sort as valid controller method arguments, as the following example shows: Using Pageable as a controller method argument @Controller @RequestMapping(""/users"") class UserController { private final UserRepository repository; UserController(UserRepository repository) { this.repository = repository; } @RequestMapping String showUsers(Model model, Pageable pageable) { model.addAttribute(""users"", repository.findAll(pageable)); return ""users""; } } The preceding method signature causes Spring MVC try to derive a Pageable instance from the request parameters by using the following default configuration: Table 1. Request parameters evaluated for Pageable instances page Page you want to retrieve. 0-indexed and defaults to 0. size Size of the page you want to retrieve. Defaults to 20. sort Properties that should be sorted by in the format property,property(,ASC|DESC)(,IgnoreCase) . The default sort direction is case-sensitive ascending. Use multiple sort parameters if you want to switch direction or case sensitivity — for example, ?sort=firstname&sort=lastname,asc&sort=city,ignorecase . To customize this behavior, register a bean that implements the PageableHandlerMethodArgumentResolverCustomizer interface or the SortHandlerMethodArgumentResolverCustomizer interface, respectively. Its customize() method gets called, letting you change settings, as the following example shows: @Bean SortHandlerMethodArgumentResolverCustomizer sortCustomizer() { return s -> s.setPropertyDelimiter(""<-->""); } If setting the properties of an existing MethodArgumentResolver is not sufficient for your purpose, extend either SpringDataWebConfiguration or the HATEOAS-enabled equivalent, override the pageableResolver() or sortResolver() methods, and import your customized configuration file instead of using the @Enable annotation. If you need multiple Pageable or Sort instances to be resolved from the request (for multiple tables, for example), you can use Spring’s @Qualifier annotation to distinguish one from another. The request parameters then have to be prefixed with ${qualifier}_ . The following example shows the resulting method signature: String showUsers(Model model, @Qualifier(""thing1"") Pageable first, @Qualifier(""thing2"") Pageable second) { … } You have to populate thing1_page , thing2_page , and so on. The default Pageable passed into the method is equivalent to a PageRequest.of(0, 20) , but you can customize it by using the @PageableDefault annotation on the Pageable parameter. Creating JSON representations for Page: It’s common for Spring MVC controllers to try to ultimately render a representation of a Spring Data page to clients. While one could simply return Page instances from handler methods to let Jackson render them as is, we strongly recommend against this as the underlying implementation class PageImpl is a domain type. This means we might want or have to change its API for unrelated reasons, and such changes might alter the resulting JSON representation in a breaking way. With Spring Data 3.1, we started hinting at the problem by issuing a warning log describing the problem. We still ultimately recommend to leverage the integration with Spring HATEOAS(#core.web.pageables) for a fully stable and hypermedia-enabled way of rendering pages that easily allow clients to navigate them. But as of version 3.3 Spring Data ships a page rendering mechanism that is convenient to use but does not require the inclusion of Spring HATEOAS. Using Spring Data' PagedModel: At its core, the support consists of a simplified version of Spring HATEOAS' PagedModel (the Spring Data one located in the org.springframework.data.web package). It can be used to wrap Page instances and result in a simplified representation that reflects the structure established by Spring HATEOAS but omits the navigation links. import org.springframework.data.web.PagedModel; @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") PagedModel<?> page(Pageable pageable) { return new PagedModel<>(repository.findAll(pageable)); (1) } } 1 Wraps the Page instance into a PagedModel . This will result in a JSON structure looking like this: { ""content"" : [ … // Page content rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } Note how the document contains a page field exposing the essential pagination metadata. Globally enabling simplified Page rendering: If you don’t want to change all your existing controllers to add the mapping step to return PagedModel instead of Page you can enable the automatic translation of PageImpl instances into PagedModel by tweaking @EnableSpringDataWebSupport as follows: @EnableSpringDataWebSupport(pageSerializationMode = VIA_DTO) class MyConfiguration { } This will allow your controller to still return Page instances and they will automatically be rendered into the simplified representation: @Controller class MyController { private final MyRepository repository; // Constructor ommitted @GetMapping(""/page"") Page<?> page(Pageable pageable) { return repository.findAll(pageable); } } Hypermedia Support for Page and Slice: Spring HATEOAS ships with a representation model class ( PagedModel / SlicedModel ) that allows enriching the content of a Page or Slice instance with the necessary Page / Slice metadata as well as links to let the clients easily navigate the pages. The conversion of a Page to a PagedModel is done by an implementation of the Spring HATEOAS RepresentationModelAssembler interface, called the PagedResourcesAssembler . Similarly Slice instances can be converted to a SlicedModel using a SlicedResourcesAssembler . The following example shows how to use a PagedResourcesAssembler as a controller method argument, as the SlicedResourcesAssembler works exactly the same: Using a PagedResourcesAssembler as controller method argument @Controller class PersonController { private final PersonRepository repository; // Constructor omitted @GetMapping(""/people"") HttpEntity<PagedModel<Person>> people(Pageable pageable, PagedResourcesAssembler assembler) { Page<Person> people = repository.findAll(pageable); return ResponseEntity.ok(assembler.toModel(people)); } } Enabling the configuration, as shown in the preceding example, lets the PagedResourcesAssembler be used as a controller method argument. Calling toModel(…) on it has the following effects: The content of the Page becomes the content of the PagedModel instance. The PagedModel object gets a PageMetadata instance attached, and it is populated with information from the Page and the underlying Pageable . The PagedModel may get prev and next links attached, depending on the page’s state. The links point to the URI to which the method maps. The pagination parameters added to the method match the setup of the PageableHandlerMethodArgumentResolver to make sure the links can be resolved later. Assume we have 30 Person instances in the database. You can now trigger a request ( GET localhost:8080/people(http://localhost:8080/people) ) and see output similar to the following: { ""links"" : [ { ""rel"" : ""next"", ""href"" : ""http://localhost:8080/persons?page=1&size=20"" } ], ""content"" : [ … // 20 Person instances rendered here ], ""page"" : { ""size"" : 20, ""totalElements"" : 30, ""totalPages"" : 2, ""number"" : 0 } } The JSON envelope format shown here doesn’t follow any formally specified structure and it’s not guaranteed stable and we might change it at any time. It’s highly recommended to enable the rendering as a hypermedia-enabled, official media type, supported by Spring HATEOAS, like HAL(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#mediatypes.hal) . Those can be activated by using its @EnableHypermediaSupport annotation. Find more information in the Spring HATEOAS reference documentation(https://docs.spring.io/spring-hateoas/docs/2.3.3/reference/html/#configuration.at-enable) . The assembler produced the correct URI and also picked up the default configuration to resolve the parameters into a Pageable for an upcoming request. This means that, if you change that configuration, the links automatically adhere to the change. By default, the assembler points to the controller method it was invoked in, but you can customize that by passing a custom Link to be used as base to build the pagination links, which overloads the PagedResourcesAssembler.toModel(…) method. Spring Data Jackson Modules: The core module, and some of the store specific ones, ship with a set of Jackson Modules for types, like org.springframework.data.geo.Distance and org.springframework.data.geo.Point , used by the Spring Data domain. Those Modules are imported once web support(#core.web) is enabled and com.fasterxml.jackson.databind.ObjectMapper is available. During initialization SpringDataJacksonModules , like the SpringDataJacksonConfiguration , get picked up by the infrastructure, so that the declared com.fasterxml.jackson.databind.Module s are made available to the Jackson ObjectMapper . Data binding mixins for the following domain types are registered by the common infrastructure. org.springframework.data.geo.Distance org.springframework.data.geo.Point org.springframework.data.geo.Box org.springframework.data.geo.Circle org.springframework.data.geo.Polygon The individual module may provide additional SpringDataJacksonModules . Please refer to the store specific section for more details. Web Databinding Support: You can use Spring Data projections (described in Projections(projections.html) ) to bind incoming request payloads by using either JSONPath(https://goessner.net/articles/JsonPath/) expressions (requires Jayway JsonPath(https://github.com/json-path/JsonPath) ) or XPath(https://www.w3.org/TR/xpath-31/) expressions (requires XmlBeam(https://xmlbeam.org/) ), as the following example shows: HTTP payload binding using JSONPath or XPath expressions @ProjectedPayload public interface UserPayload { @XBRead(""//firstname"") @JsonPath(""$..firstname"") String getFirstname(); @XBRead(""/lastname"") @JsonPath({ ""$.lastname"", ""$.user.lastname"" }) String getLastname(); } You can use the type shown in the preceding example as a Spring MVC handler method argument or by using ParameterizedTypeReference on one of methods of the RestTemplate . The preceding method declarations would try to find firstname anywhere in the given document. The lastname XML lookup is performed on the top-level of the incoming document. The JSON variant of that tries a top-level lastname first but also tries lastname nested in a user sub-document if the former does not return a value. That way, changes in the structure of the source document can be mitigated easily without having clients calling the exposed methods (usually a drawback of class-based payload binding). Nested projections are supported as described in Projections(projections.html) . If the method returns a complex, non-interface type, a Jackson ObjectMapper is used to map the final value. For Spring MVC, the necessary converters are registered automatically as soon as @EnableSpringDataWebSupport is active and the required dependencies are available on the classpath. For usage with RestTemplate , register a ProjectingJackson2HttpMessageConverter (JSON) or XmlBeamHttpMessageConverter manually. For more information, see the web projection example(https://github.com/spring-projects/spring-data-examples/tree/main/web/projection) in the canonical Spring Data Examples repository(https://github.com/spring-projects/spring-data-examples) . Querydsl Web Support: For those stores that have QueryDSL(http://www.querydsl.com/) integration, you can derive queries from the attributes contained in a Request query string. Consider the following query string: ?firstname=Dave&lastname=Matthews Given the User object from the previous examples, you can resolve a query string to the following value by using the QuerydslPredicateArgumentResolver , as follows: QUser.user.firstname.eq(""Dave"").and(QUser.user.lastname.eq(""Matthews"")) The feature is automatically enabled, along with @EnableSpringDataWebSupport , when Querydsl is found on the classpath. Adding a @QuerydslPredicate to the method signature provides a ready-to-use Predicate , which you can run by using the QuerydslPredicateExecutor . Type information is typically resolved from the method’s return type. Since that information does not necessarily match the domain type, it might be a good idea to use the root attribute of QuerydslPredicate . The following example shows how to use @QuerydslPredicate in a method signature: @Controller class UserController { @Autowired UserRepository repository; @RequestMapping(value = ""/"", method = RequestMethod.GET) String index(Model model, @QuerydslPredicate(root = User.class) Predicate predicate, (1) Pageable pageable, @RequestParam MultiValueMap<String, String> parameters) { model.addAttribute(""users"", repository.findAll(predicate, pageable)); return ""index""; } } 1 Resolve query string arguments to matching Predicate for User . The default binding is as follows: Object on simple properties as eq . Object on collection like properties as contains . Collection on simple properties as in . You can customize those bindings through the bindings attribute of @QuerydslPredicate or by making use of Java 8 default methods and adding the QuerydslBinderCustomizer method to the repository interface, as follows: interface UserRepository extends CrudRepository<User, String>, QuerydslPredicateExecutor<User>, (1) QuerydslBinderCustomizer<QUser> { (2) @Override default void customize(QuerydslBindings bindings, QUser user) { bindings.bind(user.username).first((path, value) -> path.contains(value)) (3) bindings.bind(String.class) .first((StringPath path, String value) -> path.containsIgnoreCase(value)); (4) bindings.excluding(user.password); (5) } } 1 QuerydslPredicateExecutor provides access to specific finder methods for Predicate . 2 QuerydslBinderCustomizer defined on the repository interface is automatically picked up and shortcuts @QuerydslPredicate(bindings=…​) . 3 Define the binding for the username property to be a simple contains binding. 4 Define the default binding for String properties to be a case-insensitive contains match. 5 Exclude the password property from Predicate resolution. You can register a QuerydslBinderCustomizerDefaults bean holding default Querydsl bindings before applying specific bindings from the repository or @QuerydslPredicate . Repository Populators: If you work with the Spring JDBC module, you are probably familiar with the support for populating a DataSource with SQL scripts. A similar abstraction is available on the repositories level, although it does not use SQL as the data definition language because it must be store-independent. Thus, the populators support XML (through Spring’s OXM abstraction) and JSON (through Jackson) to define data with which to populate the repositories. Assume you have a file called data.json with the following content: Data defined in JSON [ { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Dave"", ""lastname"" : ""Matthews"" }, { ""_class"" : ""com.acme.Person"", ""firstname"" : ""Carter"", ""lastname"" : ""Beauford"" } ] You can populate your repositories by using the populator elements of the repository namespace provided in Spring Data Commons. To populate the preceding data to your PersonRepository , declare a populator similar to the following: Declaring a Jackson repository populator <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd""> <repository:jackson2-populator locations=""classpath:data.json"" /> </beans> The preceding declaration causes the data.json file to be read and deserialized by a Jackson ObjectMapper . The type to which the JSON object is unmarshalled is determined by inspecting the _class attribute of the JSON document. The infrastructure eventually selects the appropriate repository to handle the object that was deserialized. To instead use XML to define the data the repositories should be populated with, you can use the unmarshaller-populator element. You configure it to use one of the XML marshaller options available in Spring OXM. See the Spring reference documentation(https://docs.spring.io/spring-framework/reference/6.1/data-access/oxm.html) for details. The following example shows how to unmarshall a repository populator with JAXB: Declaring an unmarshalling repository populator (using JAXB) <?xml version=""1.0"" encoding=""UTF-8""?> <beans xmlns=""http://www.springframework.org/schema/beans"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:repository=""http://www.springframework.org/schema/data/repository"" xmlns:oxm=""http://www.springframework.org/schema/oxm"" xsi:schemaLocation=""http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/repository https://www.springframework.org/schema/data/repository/spring-repository.xsd http://www.springframework.org/schema/oxm https://www.springframework.org/schema/oxm/spring-oxm.xsd""> <repository:unmarshaller-populator locations=""classpath:data.json"" unmarshaller-ref=""unmarshaller"" /> <oxm:jaxb2-marshaller contextPath=""com.acme"" /> </beans>"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/query-by-example.html","Query by Example: Introduction: This chapter provides an introduction to Query by Example and explains how to use it. Query by Example (QBE) is a user-friendly querying technique with a simple interface. It allows dynamic query creation and does not require you to write queries that contain field names. In fact, Query by Example does not require you to write queries by using store-specific query languages at all. This chapter explains the core concepts of Query by Example. The information is pulled from the Spring Data Commons module. Depending on your database, String matching support can be limited. Usage: The Query by Example API consists of four parts: Probe: The actual example of a domain object with populated fields. ExampleMatcher : The ExampleMatcher carries details on how to match particular fields. It can be reused across multiple Examples. Example : An Example consists of the probe and the ExampleMatcher . It is used to create the query. FetchableFluentQuery : A FetchableFluentQuery offers a fluent API, that allows further customization of a query derived from an Example . Using the fluent API lets you specify ordering projection and result processing for your query. Query by Example is well suited for several use cases: Querying your data store with a set of static or dynamic constraints. Frequent refactoring of the domain objects without worrying about breaking existing queries. Working independently of the underlying data store API. Query by Example also has several limitations: No support for nested or grouped property constraints, such as firstname = ?0 or (firstname = ?1 and lastname = ?2) . Store-specific support on string matching. Depending on your databases, String matching can support starts/contains/ends/regex for strings. Exact matching for other property types. Before getting started with Query by Example, you need to have a domain object. To get started, create an interface for your repository, as shown in the following example: Sample Person object public class Person { @Id private String id; private String firstname; private String lastname; private Address address; // … getters and setters omitted } The preceding example shows a simple domain object. You can use it to create an Example . By default, fields having null values are ignored, and strings are matched by using the store specific defaults. Inclusion of properties into a Query by Example criteria is based on nullability. Properties using primitive types ( int , double , …) are always included unless the ExampleMatcher ignores the property path(#query-by-example.matchers) . Examples can be built by either using the of factory method or by using ExampleMatcher(#query-by-example.matchers) . Example is immutable. The following listing shows a simple Example: Example 1. Simple Example Person person = new Person(); (1) person.setFirstname(""Dave""); (2) Example<Person> example = Example.of(person); (3) 1 Create a new instance of the domain object. 2 Set the properties to query. 3 Create the Example . You can run the example queries by using repositories. To do so, let your repository interface extend QueryByExampleExecutor<T> . The following listing shows an excerpt from the QueryByExampleExecutor interface: The QueryByExampleExecutor public interface QueryByExampleExecutor<T> { <S extends T> S findOne(Example<S> example); <S extends T> Iterable<S> findAll(Example<S> example); // … more functionality omitted. } Example Matchers: Examples are not limited to default settings. You can specify your own defaults for string matching, null handling, and property-specific settings by using the ExampleMatcher , as shown in the following example: Example 2. Example matcher with customized matching Person person = new Person(); (1) person.setFirstname(""Dave""); (2) ExampleMatcher matcher = ExampleMatcher.matching() (3) .withIgnorePaths(""lastname"") (4) .withIncludeNullValues() (5) .withStringMatcher(StringMatcher.ENDING); (6) Example<Person> example = Example.of(person, matcher); (7) 1 Create a new instance of the domain object. 2 Set properties. 3 Create an ExampleMatcher to expect all values to match. It is usable at this stage even without further configuration. 4 Construct a new ExampleMatcher to ignore the lastname property path. 5 Construct a new ExampleMatcher to ignore the lastname property path and to include null values. 6 Construct a new ExampleMatcher to ignore the lastname property path, to include null values, and to perform suffix string matching. 7 Create a new Example based on the domain object and the configured ExampleMatcher . By default, the ExampleMatcher expects all values set on the probe to match. If you want to get results matching any of the predicates defined implicitly, use ExampleMatcher.matchingAny() . You can specify behavior for individual properties (such as ""firstname"" and ""lastname"" or, for nested properties, ""address.city""). You can tune it with matching options and case sensitivity, as shown in the following example: Configuring matcher options ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", endsWith()) .withMatcher(""lastname"", startsWith().ignoreCase()); } Another way to configure matcher options is to use lambdas (introduced in Java 8). This approach creates a callback that asks the implementor to modify the matcher. You need not return the matcher, because configuration options are held within the matcher instance. The following example shows a matcher that uses lambdas: Configuring matcher options with lambdas ExampleMatcher matcher = ExampleMatcher.matching() .withMatcher(""firstname"", match -> match.endsWith()) .withMatcher(""firstname"", match -> match.startsWith()); } Queries created by Example use a merged view of the configuration. Default matching settings can be set at the ExampleMatcher level, while individual settings can be applied to particular property paths. Settings that are set on ExampleMatcher are inherited by property path settings unless they are defined explicitly. Settings on a property patch have higher precedence than default settings. The following table describes the scope of the various ExampleMatcher settings: Table 1. Scope of ExampleMatcher settings Setting Scope Null-handling ExampleMatcher String matching ExampleMatcher and property path Ignoring properties Property path Case sensitivity ExampleMatcher and property path Value transformation Property path Fluent API: QueryByExampleExecutor offers one more method, which we did not mention so far: <S extends T, R> R findBy(Example<S> example, Function<FluentQuery.FetchableFluentQuery<S>, R> queryFunction) . As with other methods, it executes a query derived from an Example . However, with the second argument, you can control aspects of that execution that you cannot dynamically control otherwise. You do so by invoking the various methods of the FetchableFluentQuery in the second argument. sortBy lets you specify an ordering for your result. as lets you specify the type to which you want the result to be transformed. project limits the queried attributes. first , firstValue , one , oneValue , all , page , stream , count , and exists define what kind of result you get and how the query behaves when more than the expected number of results are available. Use the fluent API to get the last of potentially many results, ordered by lastname. Optional<Person> match = repository.findBy(example, q -> q .sortBy(Sort.by(""lastname"").descending()) .first() );"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/scrolling.html","Scrolling: Scrolling is a more fine-grained approach to iterate through larger results set chunks. Scrolling consists of a stable sort, a scroll type (Offset- or Keyset-based scrolling) and result limiting. You can define simple sorting expressions by using property names and define static result limiting using the Top or First keyword(query-methods-details.html#repositories.limit-query-result) through query derivation. You can concatenate expressions to collect multiple criteria into one expression. Scroll queries return a Window<T> that allows obtaining the element’s scroll position to fetch the next Window<T> until your application has consumed the entire query result. Similar to consuming a Java Iterator<List<…>> by obtaining the next batch of results, query result scrolling lets you access the a ScrollPosition through Window.positionAt(…​) . Window<User> users = repository.findFirst10ByLastnameOrderByFirstname(""Doe"", ScrollPosition.offset()); do { for (User u : users) { // consume the user } // obtain the next Scroll users = repository.findFirst10ByLastnameOrderByFirstname(""Doe"", users.positionAt(users.size() - 1)); } while (!users.isEmpty() && users.hasNext()); The ScrollPosition identifies the exact position of an element with the entire query result. Query execution treats the position parameter exclusive , results will start after the given position. ScrollPosition#offset() and ScrollPosition#keyset() as special incarnations of a ScrollPosition indicating the start of a scroll operation. WindowIterator provides a utility to simplify scrolling across Window s by removing the need to check for the presence of a next Window and applying the ScrollPosition . WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(ScrollPosition.offset()); while (users.hasNext()) { User u = users.next(); // consume the user } Scrolling using Offset: Offset scrolling uses similar to pagination, an Offset counter to skip a number of results and let the data source only return results beginning at the given Offset. This simple mechanism avoids large results being sent to the client application. However, most databases require materializing the full query result before your server can return the results. Example 1. Using OffsetScrollPosition with Repository Query Methods interface UserRepository extends Repository<User, Long> { Window<User> findFirst10ByLastnameOrderByFirstname(String lastname, OffsetScrollPosition position); } WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(OffsetScrollPosition.initial()); (1) 1 Start with no offset to include the element at position 0 . There is a difference between ScollPosition.offset() and ScollPosition.offset(0L) . The former indicates the start of scroll operation, pointing to no specific offset whereas the latter identifies the first element (at position 0 ) of the result. Given the exclusive nature of scrolling, using ScollPosition.offset(0) skips the first element and translate to an offset of 1 . Scrolling using Keyset-Filtering: Offset-based requires most databases require materializing the entire result before your server can return the results. So while the client only sees the portion of the requested results, your server needs to build the full result, which causes additional load. Keyset-Filtering approaches result subset retrieval by leveraging built-in capabilities of your database aiming to reduce the computation and I/O requirements for individual queries. This approach maintains a set of keys to resume scrolling by passing keys into the query, effectively amending your filter criteria. The core idea of Keyset-Filtering is to start retrieving results using a stable sorting order. Once you want to scroll to the next chunk, you obtain a ScrollPosition that is used to reconstruct the position within the sorted result. The ScrollPosition captures the keyset of the last entity within the current Window . To run the query, reconstruction rewrites the criteria clause to include all sort fields and the primary key so that the database can leverage potential indexes to run the query. The database needs only constructing a much smaller result from the given keyset position without the need to fully materialize a large result and then skipping results until reaching a particular offset. Keyset-Filtering requires the keyset properties (those used for sorting) to be non-nullable. This limitation applies due to the store specific null value handling of comparison operators as well as the need to run queries against an indexed source. Keyset-Filtering on nullable properties will lead to unexpected results. Using KeysetScrollPosition with Repository Query Methods interface UserRepository extends Repository<User, Long> { Window<User> findFirst10ByLastnameOrderByFirstname(String lastname, KeysetScrollPosition position); } WindowIterator<User> users = WindowIterator.of(position -> repository.findFirst10ByLastnameOrderByFirstname(""Doe"", position)) .startingAt(ScrollPosition.keyset()); (1) 1 Start at the very beginning and do not apply additional filtering. Keyset-Filtering works best when your database contains an index that matches the sort fields, hence a static sort works well. Scroll queries applying Keyset-Filtering require to the properties used in the sort order to be returned by the query, and these must be mapped in the returned entity. You can use interface and DTO projections, however make sure to include all properties that you’ve sorted by to avoid keyset extraction failures. When specifying your Sort order, it is sufficient to include sort properties relevant to your query; You do not need to ensure unique query results if you do not want to. The keyset query mechanism amends your sort order by including the primary key (or any remainder of composite primary keys) to ensure each query result is unique."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/sdn-extension.html","Spring Data Neo4j Extensions: Available extensions for Spring Data Neo4j repositories: Spring Data Neo4j offers a couple of extensions or ""mixins"" that can be added to repositories. What is a mixin? According to Wikipedia(https://en.wikipedia.org/wiki/Mixin) mixins are a language concept that allows a programmer to inject some code into a class. Mixin programming is a style of software development, in which units of functionality are created in a class and then mixed in with other classes. Java does not support that concept on the language level, but we do emulate it via a couple of interfaces and a runtime that adds appropriate implementations and interceptors for. Mixins added by default are QueryByExampleExecutor and ReactiveQueryByExampleExecutor respectively. Those interfaces are explained in detail in Query by Example(../query-by-example.html#query-by-example) . Additional mixins provided are: QuerydslPredicateExecutor CypherdslConditionExecutor CypherdslStatementExecutor ReactiveQuerydslPredicateExecutor ReactiveCypherdslConditionExecutor ReactiveCypherdslStatementExecutor Add dynamic conditions to generated queries: Both the QuerydslPredicateExecutor and CypherdslConditionExecutor provide the same concept: SDN generates a query, you provide ""predicates"" (Query DSL) or ""conditions"" (Cypher DSL) that will be added. We recommend the Cypher DSL, as this is what SDN uses natively. You might even want to consider using the annotation processor(http://neo4j-contrib.github.io/cypher-dsl/2021.1.1/#thespringdataneo4j6annotationprocessor) that generates a static meta model for you. How does that work? Declare your repository as described above and add one of the following interfaces: interface QueryDSLPersonRepository extends Neo4jRepository<Person, Long>, (1) QuerydslPredicateExecutor<Person> { (2) } 1 Standard repository declaration 2 The Query DSL mixin OR import org.springframework.data.neo4j.repository.Neo4jRepository; import org.springframework.data.neo4j.repository.support.CypherdslConditionExecutor; interface PersonRepository extends Neo4jRepository<Person, Long>, (1) CypherdslConditionExecutor<Person> { (2) } 1 Standard repository declaration 2 The Cypher DSL mixin Exemplary usage is shown with the Cypher DSL condition executor: Node person = Cypher.node(""Person"").named(""person""); (1) Property firstName = person.property(""firstName""); (2) Property lastName = person.property(""lastName""); assertThat( repository.findAll( firstName.eq(Cypher.anonParameter(""Helge"")) .or(lastName.eq(Cypher.parameter(""someName"", ""B.""))), (3) lastName.descending() (4) )) .extracting(Person::getFirstName) .containsExactly(""Helge"", ""Bela""); 1 Define a named Node object, targeting the root of the query 2 Derive some properties from it 3 Create an or condition. An anonymous parameter is used for the first name, a named parameter for the last name. This is how you define parameters in those fragments and one of the advantages over the Query-DSL mixin which can’t do that. Literals can be expressed with Cypher.literalOf . 4 Define a SortItem from one of the properties The code looks pretty similar for the Query-DSL mixin. Reasons for the Query-DSL mixin can be familiarity of the API and that it works with other stores, too. Reasons against it are the fact that you need an additional library on the class path, it’s missing support for traversing relationships and the above-mentioned fact that it doesn’t support parameters in its predicates (it technically does, but there are no API methods to actually pass them to the query being executed). Using (dynamic) Cypher-DSL statements for entities and projections: Adding the corresponding mixin is not different from using the condition excecutor(#sdn-mixins.dynamic-conditions) : interface PersonRepository extends Neo4jRepository<Person, Long>, CypherdslStatementExecutor<Person> { } Please use the ReactiveCypherdslStatementExecutor when extending the ReactiveNeo4jRepository . The CypherdslStatementExecutor comes with several overloads for findOne and findAll . They all take a Cypher-DSL statement respectively an ongoing definition of that as a first parameter and in case of the projecting methods, a type. If a query requires parameters, they must be defined via the Cypher-DSL itself and also populated by it, as the following listing shows: static Statement whoHasFirstNameWithAddress(String name) { (1) Node p = Cypher.node(""Person"").named(""p""); (2) Node a = Cypher.anyNode(""a""); Relationship r = p.relationshipTo(a, ""LIVES_AT""); return Cypher.match(r) .where(p.property(""firstName"").isEqualTo(Cypher.anonParameter(name))) (3) .returning( p.getRequiredSymbolicName(), Cypher.collect(r), Cypher.collect(a) ) .build(); } @Test void fineOneShouldWork(@Autowired PersonRepository repository) { Optional<Person> result = repository.findOne(whoHasFirstNameWithAddress(""Helge"")); (4) assertThat(result).hasValueSatisfying(namesOnly -> { assertThat(namesOnly.getFirstName()).isEqualTo(""Helge""); assertThat(namesOnly.getLastName()).isEqualTo(""Schneider""); assertThat(namesOnly.getAddress()).extracting(Person.Address::getCity) .isEqualTo(""Mülheim an der Ruhr""); }); } @Test void fineOneProjectedShouldWork(@Autowired PersonRepository repository) { Optional<NamesOnly> result = repository.findOne( whoHasFirstNameWithAddress(""Helge""), NamesOnly.class (5) ); assertThat(result).hasValueSatisfying(namesOnly -> { assertThat(namesOnly.getFirstName()).isEqualTo(""Helge""); assertThat(namesOnly.getLastName()).isEqualTo(""Schneider""); assertThat(namesOnly.getFullName()).isEqualTo(""Helge Schneider""); }); } 1 The dynamic query is build in a type safe way in a helper method 2 We already saw this in here(#sdn-mixins.dynamic-conditions) , where we also defined some variables holding the model 3 We define an anonymous parameter, filled by the actual value of name , which was passed to the method 4 The statement returned from the helper method is used to find an entity 5 Or a projection. The findAll methods works similar. The imperative Cypher-DSL statement executor also provides an overload returning paged results."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/query-keywords-reference.html","Repository query keywords: Supported query method subject keywords: The following table lists the subject keywords generally supported by the Spring Data repository query derivation mechanism to express the predicate. Consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 1. Query subject keywords Keyword Description find…By , read…By , get…By , query…By , search…By , stream…By General query method returning typically the repository type, a Collection or Streamable subtype or a result wrapper such as Page , GeoResults or any other store-specific result wrapper. Can be used as findBy… , findMyDomainTypeBy… or in combination with additional keywords. exists…By Exists projection, returning typically a boolean result. count…By Count projection returning a numeric result. delete…By , remove…By Delete query method returning either no result ( void ) or the delete count. …First<number>… , …Top<number>… Limit the query results to the first <number> of results. This keyword can occur in any place of the subject between find (and the other keywords) and by . …Distinct… Use a distinct query to return only unique results. Consult the store-specific documentation whether that feature is supported. This keyword can occur in any place of the subject between find (and the other keywords) and by . Supported query method predicate keywords and modifiers: The following table lists the predicate keywords generally supported by the Spring Data repository query derivation mechanism. However, consult the store-specific documentation for the exact list of supported keywords, because some keywords listed here might not be supported in a particular store. Table 2. Query predicate keywords Logical keyword Keyword expressions AND And OR Or AFTER After , IsAfter BEFORE Before , IsBefore CONTAINING Containing , IsContaining , Contains BETWEEN Between , IsBetween ENDING_WITH EndingWith , IsEndingWith , EndsWith EXISTS Exists FALSE False , IsFalse GREATER_THAN GreaterThan , IsGreaterThan GREATER_THAN_EQUALS GreaterThanEqual , IsGreaterThanEqual IN In , IsIn IS Is , Equals , (or no keyword) IS_EMPTY IsEmpty , Empty IS_NOT_EMPTY IsNotEmpty , NotEmpty IS_NOT_NULL NotNull , IsNotNull IS_NULL Null , IsNull LESS_THAN LessThan , IsLessThan LESS_THAN_EQUAL LessThanEqual , IsLessThanEqual LIKE Like , IsLike NEAR Near , IsNear NOT Not , IsNot NOT_IN NotIn , IsNotIn NOT_LIKE NotLike , IsNotLike REGEX Regex , MatchesRegex , Matches STARTING_WITH StartingWith , IsStartingWith , StartsWith TRUE True , IsTrue WITHIN Within , IsWithin In addition to filter predicates, the following list of modifiers is supported: Table 3. Query predicate modifier keywords Keyword Description IgnoreCase , IgnoringCase Used with a predicate keyword for case-insensitive comparison. AllIgnoreCase , AllIgnoringCase Ignore case for all suitable properties. Used somewhere in the query method predicate. OrderBy… Specify a static sorting order followed by the property path and direction (e. g. OrderByFirstnameAscLastnameDesc )."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/query-return-types-reference.html","Repository query return types: Supported Query Return Types: The following table lists the return types generally supported by Spring Data repositories. However, consult the store-specific documentation for the exact list of supported return types, because some types listed here might not be supported in a particular store. Geospatial types (such as GeoResult , GeoResults , and GeoPage ) are available only for data stores that support geospatial queries. Some store modules may define their own result wrapper types. Table 1. Query return types Return type Description void Denotes no return value. Primitives Java primitives. Wrapper types Java wrapper types. T A unique entity. Expects the query method to return one result at most. If no result is found, null is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Iterator<T> An Iterator . Collection<T> A Collection . List<T> A List . Optional<T> A Java 8 or Guava Optional . Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Option<T> Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional , described earlier. Stream<T> A Java 8 Stream . Streamable<T> A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…) / ….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types(query-methods-details.html#repositories.collections-and-iterables.streamable-wrapper) for details. Vavr Seq , List , Map , Set Vavr collection types. See Support for Vavr Collections(query-methods-details.html#repositories.collections-and-iterables.vavr) for details. Future<T> A Future . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture<T> A Java 8 CompletableFuture . Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice<T> A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page<T> A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. Window<T> A Window of results obtained from a scroll query. Provides ScrollPosition to issue the next scroll query. Requires a ScrollPosition method parameter. GeoResult<T> A result entry with additional information, such as the distance to a reference location. GeoResults<T> A list of GeoResult<T> with additional information, such as the average distance to a reference location. GeoPage<T> A Page with GeoResult<T> , such as the average distance to a reference location. Mono<T> A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flux<T> A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single<T> A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Maybe<T> A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException . Flowable<T> A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/repositories/projections.html","Projections: Spring Data query methods usually return one or multiple instances of the aggregate root managed by the repository. However, it might sometimes be desirable to create projections based on certain attributes of those types. Spring Data allows modeling dedicated return types, to more selectively retrieve partial views of the managed aggregates. Imagine a repository and aggregate root type such as the following example: A sample aggregate and repository class Person { @Id UUID id; String firstname, lastname; Address address; static class Address { String zipCode, city, street; } } interface PersonRepository extends Repository<Person, UUID> { Collection<Person> findByLastname(String lastname); } Now imagine that we want to retrieve the person’s name attributes only. What means does Spring Data offer to achieve this? The rest of this chapter answers that question. Projection types are types residing outside the entity’s type hierarchy. Superclasses and interfaces implemented by the entity are inside the type hierarchy hence returning a supertype (or implemented interface) returns an instance of the fully materialized entity. Interface-based Projections: The easiest way to limit the result of the queries to only the name attributes is by declaring an interface that exposes accessor methods for the properties to be read, as shown in the following example: A projection interface to retrieve a subset of attributes interface NamesOnly { String getFirstname(); String getLastname(); } The important bit here is that the properties defined here exactly match properties in the aggregate root. Doing so lets a query method be added as follows: A repository using an interface based projection with a query method interface PersonRepository extends Repository<Person, UUID> { Collection<NamesOnly> findByLastname(String lastname); } The query execution engine creates proxy instances of that interface at runtime for each element returned and forwards calls to the exposed methods to the target object. Declaring a method in your Repository that overrides a base method (e.g. declared in CrudRepository , a store-specific repository interface, or the Simple…Repository ) results in a call to the base method regardless of the declared return type. Make sure to use a compatible return type as base methods cannot be used for projections. Some store modules support @Query annotations to turn an overridden base method into a query method that then can be used to return projections. Projections can be used recursively. If you want to include some of the Address information as well, create a projection interface for that and return that interface from the declaration of getAddress() , as shown in the following example: A projection interface to retrieve a subset of attributes interface PersonSummary { String getFirstname(); String getLastname(); AddressSummary getAddress(); interface AddressSummary { String getCity(); } } On method invocation, the address property of the target instance is obtained and wrapped into a projecting proxy in turn. Closed Projections: A projection interface whose accessor methods all match properties of the target aggregate is considered to be a closed projection. The following example (which we used earlier in this chapter, too) is a closed projection: A closed projection interface NamesOnly { String getFirstname(); String getLastname(); } If you use a closed projection, Spring Data can optimize the query execution, because we know about all the attributes that are needed to back the projection proxy. For more details on that, see the module-specific part of the reference documentation. Open Projections: Accessor methods in projection interfaces can also be used to compute new values by using the @Value annotation, as shown in the following example: An Open Projection interface NamesOnly { @Value(""#{target.firstname + ' ' + target.lastname}"") String getFullName(); … } The aggregate root backing the projection is available in the target variable. A projection interface using @Value is an open projection. Spring Data cannot apply query execution optimizations in this case, because the SpEL expression could use any attribute of the aggregate root. The expressions used in @Value should not be too complex — you want to avoid programming in String variables. For very simple expressions, one option might be to resort to default methods (introduced in Java 8), as shown in the following example: A projection interface using a default method for custom logic interface NamesOnly { String getFirstname(); String getLastname(); default String getFullName() { return getFirstname().concat("" "").concat(getLastname()); } } This approach requires you to be able to implement logic purely based on the other accessor methods exposed on the projection interface. A second, more flexible, option is to implement the custom logic in a Spring bean and then invoke that from the SpEL expression, as shown in the following example: Sample Person object @Component class MyBean { String getFullName(Person person) { … } } interface NamesOnly { @Value(""#{@myBean.getFullName(target)}"") String getFullName(); … } Notice how the SpEL expression refers to myBean and invokes the getFullName(…) method and forwards the projection target as a method parameter. Methods backed by SpEL expression evaluation can also use method parameters, which can then be referred to from the expression. The method parameters are available through an Object array named args . The following example shows how to get a method parameter from the args array: Sample Person object interface NamesOnly { @Value(""#{args[0] + ' ' + target.firstname + '!'}"") String getSalutation(String prefix); } Again, for more complex expressions, you should use a Spring bean and let the expression invoke a method, as described earlier(#projections.interfaces.open.bean-reference) . Nullable Wrappers: Getters in projection interfaces can make use of nullable wrappers for improved null-safety. Currently supported wrapper types are: java.util.Optional com.google.common.base.Optional scala.Option io.vavr.control.Option A projection interface using nullable wrappers interface NamesOnly { Optional<String> getFirstname(); } If the underlying projection value is not null , then values are returned using the present-representation of the wrapper type. In case the backing value is null , then the getter method returns the empty representation of the used wrapper type. Class-based Projections (DTOs): Another way of defining projections is by using value type DTOs (Data Transfer Objects) that hold properties for the fields that are supposed to be retrieved. These DTO types can be used in exactly the same way projection interfaces are used, except that no proxying happens and no nested projections can be applied. If the store optimizes the query execution by limiting the fields to be loaded, the fields to be loaded are determined from the parameter names of the constructor that is exposed. The following example shows a projecting DTO: A projecting DTO record NamesOnly(String firstname, String lastname) { } Java Records are ideal to define DTO types since they adhere to value semantics: All fields are private final and equals(…) / hashCode() / toString() methods are created automatically. Alternatively, you can use any class that defines the properties you want to project. Dynamic Projections: So far, we have used the projection type as the return type or element type of a collection. However, you might want to select the type to be used at invocation time (which makes it dynamic). To apply dynamic projections, use a query method such as the one shown in the following example: A repository using a dynamic projection parameter interface PersonRepository extends Repository<Person, UUID> { <T> Collection<T> findByLastname(String lastname, Class<T> type); } This way, the method can be used to obtain the aggregates as is or with a projection applied, as shown in the following example: Using a repository with dynamic projections void someMethod(PersonRepository people) { Collection<Person> aggregates = people.findByLastname(""Matthews"", Person.class); Collection<NamesOnly> aggregates = people.findByLastname(""Matthews"", NamesOnly.class); } Query parameters of type Class are inspected whether they qualify as dynamic projection parameter. If the actual return type of the query equals the generic parameter type of the Class parameter, then the matching Class parameter is not available for usage within the query or SpEL expressions. If you want to use a Class parameter as query argument then make sure to use a different generic parameter, for example Class<?> . Section Summary: Spring Data Neo4j Projections(../projections/sdn-projections.html)"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/projections/sdn-projections.html","Spring Data Neo4j Projections: As stated above, projections come in two flavors: Interface and DTO based projections. In Spring Data Neo4j both types of projections have a direct influence which properties and relationships are transferred over the wire. Therefore, both approaches can reduce the load on your database in case you are dealing with nodes and entities containing lots of properties which might not be needed in all usage scenarios in your application. For both interface and DTO based projections, Spring Data Neo4j will use the repository’s domain type for building the query. All annotations on all attributes that might change the query will be taken in consideration. The domain type is the type that has been defined through the repository declaration (Given a declaration like interface TestRepository extends CrudRepository<TestEntity, Long> the domain type would be TestEntity ). Interface based projections will always be dynamic proxies to the underlying domain type. The names of the accessors defined on such interfaces (like getName ) must resolve to properties (here: name ) that are present on the projected entity. Whether those properties have accessors or not on the domain type is not relevant, as long as they can be accessed through the common Spring Data infrastructure. The latter is already ensured, as the domain type wouldn’t be a persistent entity in the first place. DTO based projections are somewhat more flexible when used with custom queries. While the standard query is derived from the original domain type and therefore only the properties and relationship being defined there can be used, custom queries can add additional properties. The rules are as follows: first, the properties of the domain type are used to populate the DTO. In case the DTO declares additional properties - via accessors or fields - Spring Data Neo4j looks in the resulting record for matching properties. Properties must match exactly by name and can be of simple types (as defined in org.springframework.data.neo4j.core.convert.Neo4jSimpleTypes ) or of known persistent entities. Collections of those are supported, but maps are not. Multi-level projections: Spring Data Neo4j also supports multi-level projections. Example of multi-level projection interface ProjectionWithNestedProjection { String getName(); List<Subprojection1> getLevel1(); interface Subprojection1 { String getName(); List<Subprojection2> getLevel2(); } interface Subprojection2 { String getName(); } } Even though it is possible to model cyclic projections or point towards entities that will create a cycle, the projection logic will not follow those cycles but only create cycle-free queries. Multi-level projections are bounded to the entities they should project. RelationshipProperties fall into the category of entities in this case and needs to get respected if projections get applied. Data manipulation of projections: If you have fetched the projection as a DTO, you can modify its values. But in case you are using the interface-based projection, you cannot just update the interface. A typical pattern that can be used is to provide a method in your domain entity class that consumes the interface and creates a domain entity with the copied values from the interface. This way, you can then update the entity and persist it again with the projection blueprint/mask as described in the next section. Persistence of projections: Analogue to the retrieval of data via projections, they can also be used as a blueprint for persistence. The Neo4jTemplate offers a fluent API to apply those projections to a save operation. You could either save a projection for a given domain class Save projection for a given domain class Projection projection = neo4jTemplate.save(DomainClass.class).one(projectionValue); or you could save a domain object but only respect the fields defined in the projection. Save domain object with a given projection blueprint Projection projection = neo4jTemplate.saveAs(domainObject, Projection.class); In both cases, that also are available for collection based operations, only the fields and relationships defined in the projection will get updated. To prevent deletion of data (e.g. removal of relationships), you should always load at least all the data that should get persisted later. A full example: Given the following entities, projections and the corresponding repository: A simple entity @Node class TestEntity { @Id @GeneratedValue private Long id; private String name; @Property(""a_property"") (1) private String aProperty; } 1 This property has a different name in the Graph A derived entity, inheriting from TestEntity @Node class ExtendedTestEntity extends TestEntity { private String otherAttribute; } Interface projection of TestEntity interface TestEntityInterfaceProjection { String getName(); } DTO projection of TestEntity , including one additional attribute class TestEntityDTOProjection { private String name; private Long numberOfRelations; (1) public String getName() { return name; } public void setName(String name) { this.name = name; } public Long getNumberOfRelations() { return numberOfRelations; } public void setNumberOfRelations(Long numberOfRelations) { this.numberOfRelations = numberOfRelations; } } 1 This attribute doesn’t exist on the projected entity A repository for TestEntity is shown below and it will behave as explained with the listing. A repository for the TestEntity interface TestRepository extends CrudRepository<TestEntity, Long> { (1) List<TestEntity> findAll(); (2) List<ExtendedTestEntity> findAllExtendedEntities(); (3) List<TestEntityInterfaceProjection> findAllInterfaceProjectionsBy(); (4) List<TestEntityDTOProjection> findAllDTOProjectionsBy(); (5) @Query(""MATCH (t:TestEntity) - [r:RELATED_TO] -> () RETURN t, COUNT(r) AS numberOfRelations"") (6) List<TestEntityDTOProjection> findAllDTOProjectionsWithCustomQuery(); } 1 The domain type of the repository is TestEntity 2 Methods returning one or more TestEntity will just return instances of it, as it matches the domain type 3 Methods returning one or more instances of classes that extend the domain type will just return instances of the extending class. The domain type of the method in question will be the extended class, which still satisfies the domain type of the repository itself 4 This method returns an interface projection, the return type of the method is therefore different from the repository’s domain type. The interface can only access properties defined in the domain type. The suffix By is needed to make SDN not look for a property called InterfaceProjections in the TestEntity 5 This method returns a DTO projection. Executing it will cause SDN to issue a warning, as the DTO defines numberOfRelations as additional attribute, which is not in the contract of the domain type. The annotated attribute aProperty in TestEntity will be correctly translated to a_property in the query. As above, the return type is different from the repositories' domain type. The suffix By is needed to make SDN not look for a property called DTOProjections in the TestEntity 6 This method also returns a DTO projection. However, no warning will be issued, as the query contains a fitting value for the additional attributes defined in the projection While the repository in the listing above(#projections.simple-entity-repository) uses a concrete return type to define the projection, another variant is the use of dynamic projections(../repositories/projections.html#projection.dynamic) as explained in the parts of the documentation Spring Data Neo4j shares with other Spring Data Projects. A dynamic projection can be applied to both closed and open interface projections as well as to class based DTO projections: The key to a dynamic projection is to specify the desired projection type as the last parameter to a query method in a repository like this: <T> Collection<T> findByName(String name, Class<T> type) . This is a declaration that could be added to the TestRepository above and allow for different projections retrieved by the same method, without to repeat a possible @Query annotation on several methods."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/testing.html","Testing: Section Summary: Without Spring Boot(testing/testing-without-spring-boot.html) With Spring Boot and @DataNeo4jTest(testing/testing-with-spring-boot.html)"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/testing/testing-without-spring-boot.html","Without Spring Boot: We work a lot with our abstract base classes for configuration in our own integration tests. They can be used like this: One possible test setup without Spring Boot import org.junit.jupiter.api.Test; import org.junit.jupiter.api.extension.ExtendWith; import org.neo4j.driver.AuthTokens; import org.neo4j.driver.Driver; import org.neo4j.driver.GraphDatabase; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.neo4j.config.AbstractNeo4jConfig; import org.springframework.data.neo4j.core.Neo4jTemplate; import org.springframework.data.neo4j.repository.config.EnableNeo4jRepositories; import org.springframework.test.context.junit.jupiter.SpringExtension; import org.springframework.transaction.annotation.EnableTransactionManagement; @ExtendWith(SpringExtension.class) class YourIntegrationTest { @Test void thingsShouldWork(@Autowired Neo4jTemplate neo4jTemplate) { // Add your test } @Configuration @EnableNeo4jRepositories(considerNestedRepositories = true) @EnableTransactionManagement static class Config extends AbstractNeo4jConfig { @Bean public Driver driver() { return GraphDatabase.driver(""bolt://yourtestserver:7687"", AuthTokens.none()); (1) } } } Here you should provide a connection to your test server or container. Similar classes are provided for reactive tests."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/testing/testing-with-spring-boot.html","With Spring Boot and @DataNeo4jTest: Spring Boot offers @DataNeo4jTest through org.springframework.boot:spring-boot-starter-test . The latter brings in org.springframework.boot:spring-boot-test-autoconfigure which contains the annotation and the required infrastructure code. Include Spring Boot Starter Test in a Maven build <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-test</artifactId> <scope>test</scope> </dependency> Include Spring Boot Starter Test in a Gradle build dependencies { testImplementation 'org.springframework.boot:spring-boot-starter-test' } @DataNeo4jTest is a Spring Boot test slice(https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-testing) . The test slice provides all the necessary infrastructure for tests using Neo4j: a transaction manager, a client, a template and declared repositories, in their imperative or reactive variants, depending on reactive dependencies present or not. The test slice already includes @ExtendWith(SpringExtension.class) so that it runs automatically with JUnit 5 (JUnit Jupiter). @DataNeo4jTest provides both imperative and reactive infrastructure by default and also adds an implicit @Transactional as well. @Transactional in Spring tests however always means imperative transactional, as declarative transactions needs the return type of a method to decide whether the imperative PlatformTransactionManager or the reactive ReactiveTransactionManager is needed. To assert the correct transactional behaviour for reactive repositories or services, you will need to inject a TransactionalOperator into the test or wrap your domain logic in services that use annotated methods exposing a return type that makes it possible for the infrastructure to select the correct transaction manager. The test slice does not bring in an embedded database or any other connection setting. It is up to you to use an appropriate connection. We recommend one of two options: either use the Neo4j Testcontainers module(https://www.testcontainers.org/modules/databases/neo4j/) or the Neo4j test harness. While Testcontainers is a known project with modules for a lot of different services, Neo4j test harness is rather unknown. It is an embedded instance that is especially useful when testing stored procedures as described in Testing your Neo4j-based Java application(https://medium.com/neo4j/testing-your-neo4j-based-java-application-34bef487cc3c) . The test harness can however be used to test an application as well. As it brings up a database inside the same JVM as your application, performance and timings may not resemble your production setup. For your convenience we provide three possible scenarios, Neo4j test harness 3.5 and 4.x/5.x as well as Testcontainers Neo4j. We provide different examples for 3.5 and 4.x/5.x as the test harness changed between those versions. Also, 4.0 requires JDK 11. @DataNeo4jTest with Neo4j test harness 3.5: You need the following dependencies to run Using Neo4j 3.5 test harness(#dataneo4jtest-harness35-example) : Neo4j 3.5 test harness dependencies <dependency> <groupId>org.neo4j.test</groupId> <artifactId>neo4j-harness</artifactId> <version>3.5.33</version> <scope>test</scope> </dependency> The dependencies for the enterprise version of Neo4j 3.5 are available under the com.neo4j.test:neo4j-harness-enterprise and an appropriate repository configuration. Using Neo4j 3.5 test harness import static org.assertj.core.api.Assertions.assertThat; import java.util.Optional; import org.junit.jupiter.api.AfterAll; import org.junit.jupiter.api.BeforeAll; import org.junit.jupiter.api.Test; import org.neo4j.harness.ServerControls; import org.neo4j.harness.TestServerBuilders; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.data.neo4j.DataNeo4jTest; import org.springframework.data.neo4j.core.Neo4jClient; import org.springframework.test.context.DynamicPropertyRegistry; import org.springframework.test.context.DynamicPropertySource; @DataNeo4jTest class MovieRepositoryTest { private static ServerControls embeddedDatabaseServer; @BeforeAll static void initializeNeo4j() { embeddedDatabaseServer = TestServerBuilders.newInProcessBuilder() (1) .newServer(); } @AfterAll static void stopNeo4j() { embeddedDatabaseServer.close(); (2) } @DynamicPropertySource (3) static void neo4jProperties(DynamicPropertyRegistry registry) { registry.add(""spring.neo4j.uri"", embeddedDatabaseServer::boltURI); registry.add(""spring.neo4j.authentication.username"", () -> ""neo4j""); registry.add(""spring.neo4j.authentication.password"", () -> null); } @Test public void findSomethingShouldWork(@Autowired Neo4jClient client) { Optional<Long> result = client.query(""MATCH (n) RETURN COUNT(n)"") .fetchAs(Long.class) .one(); assertThat(result).hasValue(0L); } } 1 Entrypoint to create an embedded Neo4j 2 This is a Spring Boot annotation that allows for dynamically registered application properties. We overwrite the corresponding Neo4j settings. 3 Shutdown Neo4j after all tests. @DataNeo4jTest with Neo4j test harness 4.x/5.x: You need the following dependencies to run Using Neo4j 4.x/5.x test harness(#dataneo4jtest-harness40-example) : Neo4j 4.x test harness dependencies <dependency> <groupId>org.neo4j.test</groupId> <artifactId>neo4j-harness</artifactId> <version>4.4.25</version> <scope>test</scope> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-nop</artifactId> </exclusion> </exclusions> </dependency> The dependencies for the enterprise version of Neo4j 4.x/5.x are available under the com.neo4j.test:neo4j-harness-enterprise and an appropriate repository configuration. Using Neo4j 4.x/5.x test harness import static org.assertj.core.api.Assertions.assertThat; import java.util.Optional; import org.junit.jupiter.api.AfterAll; import org.junit.jupiter.api.BeforeAll; import org.junit.jupiter.api.Test; import org.neo4j.harness.Neo4j; import org.neo4j.harness.Neo4jBuilders; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.data.neo4j.DataNeo4jTest; import org.springframework.data.neo4j.core.Neo4jClient; import org.springframework.test.context.DynamicPropertyRegistry; import org.springframework.test.context.DynamicPropertySource; @DataNeo4jTest class MovieRepositoryTest { private static Neo4j embeddedDatabaseServer; @BeforeAll static void initializeNeo4j() { embeddedDatabaseServer = Neo4jBuilders.newInProcessBuilder() (1) .withDisabledServer() (2) .build(); } @DynamicPropertySource (3) static void neo4jProperties(DynamicPropertyRegistry registry) { registry.add(""spring.neo4j.uri"", embeddedDatabaseServer::boltURI); registry.add(""spring.neo4j.authentication.username"", () -> ""neo4j""); registry.add(""spring.neo4j.authentication.password"", () -> null); } @AfterAll static void stopNeo4j() { embeddedDatabaseServer.close(); (4) } @Test public void findSomethingShouldWork(@Autowired Neo4jClient client) { Optional<Long> result = client.query(""MATCH (n) RETURN COUNT(n)"") .fetchAs(Long.class) .one(); assertThat(result).hasValue(0L); } } 1 Entrypoint to create an embedded Neo4j 2 Disable the unneeded Neo4j HTTP server 3 This is a Spring Boot annotation that allows for dynamically registered application properties. We overwrite the corresponding Neo4j settings. 4 Shut down Neo4j after all tests. @DataNeo4jTest with Testcontainers Neo4j: The principal of configuring the connection is of course still the same with Testcontainers as shown in Using Test containers(#dataneo4jtest-testcontainers-example) . You need the following dependencies: <dependency> <groupId>org.testcontainers</groupId> <artifactId>neo4j</artifactId> <version>1.17.6</version> <scope>test</scope> </dependency> And a complete test: Using Test containers import static org.assertj.core.api.Assertions.assertThat; import java.util.Optional; import org.junit.jupiter.api.AfterAll; import org.junit.jupiter.api.BeforeAll; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.data.neo4j.DataNeo4jTest; import org.springframework.data.neo4j.core.Neo4jClient; import org.springframework.test.context.DynamicPropertyRegistry; import org.springframework.test.context.DynamicPropertySource; import org.testcontainers.containers.Neo4jContainer; @DataNeo4jTest class MovieRepositoryTCTest { private static Neo4jContainer<?> neo4jContainer; @BeforeAll static void initializeNeo4j() { neo4jContainer = new Neo4jContainer<>() .withAdminPassword(""somePassword""); neo4jContainer.start(); } @AfterAll static void stopNeo4j() { neo4jContainer.close(); } @DynamicPropertySource static void neo4jProperties(DynamicPropertyRegistry registry) { registry.add(""spring.neo4j.uri"", neo4jContainer::getBoltUrl); registry.add(""spring.neo4j.authentication.username"", () -> ""neo4j""); registry.add(""spring.neo4j.authentication.password"", neo4jContainer::getAdminPassword); } @Test public void findSomethingShouldWork(@Autowired Neo4jClient client) { Optional<Long> result = client.query(""MATCH (n) RETURN COUNT(n)"") .fetchAs(Long.class) .one(); assertThat(result).hasValue(0L); } } Alternatives to a @DynamicPropertySource: There are some scenarios in which the above annotation does not fit your use case. One of those might be that you want to have 100% control over how the driver is initialized. With a test container running, you could do this with a nested, static configuration class like this: @TestConfiguration(proxyBeanMethods = false) static class TestNeo4jConfig { @Bean Driver driver() { return GraphDatabase.driver( neo4jContainer.getBoltUrl(), AuthTokens.basic(""neo4j"", neo4jContainer.getAdminPassword()) ); } } If you want to use the properties but cannot use a @DynamicPropertySource , you would use an initializer: Alternative injection of dynamic properties @ContextConfiguration(initializers = PriorToBoot226Test.Initializer.class) @DataNeo4jTest class PriorToBoot226Test { private static Neo4jContainer<?> neo4jContainer; @BeforeAll static void initializeNeo4j() { neo4jContainer = new Neo4jContainer<>() .withAdminPassword(""somePassword""); neo4jContainer.start(); } @AfterAll static void stopNeo4j() { neo4jContainer.close(); } static class Initializer implements ApplicationContextInitializer<ConfigurableApplicationContext> { public void initialize(ConfigurableApplicationContext configurableApplicationContext) { TestPropertyValues.of( ""spring.neo4j.uri="" + neo4jContainer.getBoltUrl(), ""spring.neo4j.authentication.username=neo4j"", ""spring.neo4j.authentication.password="" + neo4jContainer.getAdminPassword() ).applyTo(configurableApplicationContext.getEnvironment()); } } }"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/auditing.html","Auditing: Basics: Spring Data provides sophisticated support to transparently keep track of who created or changed an entity and when the change happened.To benefit from that functionality, you have to equip your entity classes with auditing metadata that can be defined either using annotations or by implementing an interface. Additionally, auditing has to be enabled either through Annotation configuration or XML configuration to register the required infrastructure components. Please refer to the store-specific section for configuration samples. Applications that only track creation and modification dates are not required do make their entities implement AuditorAware(#auditing.auditor-aware) . Annotation-based Auditing Metadata: We provide @CreatedBy and @LastModifiedBy to capture the user who created or modified the entity as well as @CreatedDate and @LastModifiedDate to capture when the change happened. An audited entity class Customer { @CreatedBy private User user; @CreatedDate private Instant createdDate; // … further properties omitted } As you can see, the annotations can be applied selectively, depending on which information you want to capture. The annotations, indicating to capture when changes are made, can be used on properties of type JDK8 date and time types, long , Long , and legacy Java Date and Calendar . Auditing metadata does not necessarily need to live in the root level entity but can be added to an embedded one (depending on the actual store in use), as shown in the snippet below. Audit metadata in embedded entity class Customer { private AuditMetadata auditingMetadata; // … further properties omitted } class AuditMetadata { @CreatedBy private User user; @CreatedDate private Instant createdDate; } Interface-based Auditing Metadata: In case you do not want to use annotations to define auditing metadata, you can let your domain class implement the Auditable interface. It exposes setter methods for all of the auditing properties. AuditorAware: In case you use either @CreatedBy or @LastModifiedBy , the auditing infrastructure somehow needs to become aware of the current principal. To do so, we provide an AuditorAware<T> SPI interface that you have to implement to tell the infrastructure who the current user or system interacting with the application is. The generic type T defines what type the properties annotated with @CreatedBy or @LastModifiedBy have to be. The following example shows an implementation of the interface that uses Spring Security’s Authentication object: Implementation of AuditorAware based on Spring Security class SpringSecurityAuditorAware implements AuditorAware<User> { @Override public Optional<User> getCurrentAuditor() { return Optional.ofNullable(SecurityContextHolder.getContext()) .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getPrincipal) .map(User.class::cast); } } The implementation accesses the Authentication object provided by Spring Security and looks up the custom UserDetails instance that you have created in your UserDetailsService implementation. We assume here that you are exposing the domain user through the UserDetails implementation but that, based on the Authentication found, you could also look it up from anywhere. ReactiveAuditorAware: When using reactive infrastructure you might want to make use of contextual information to provide @CreatedBy or @LastModifiedBy information. We provide an ReactiveAuditorAware<T> SPI interface that you have to implement to tell the infrastructure who the current user or system interacting with the application is. The generic type T defines what type the properties annotated with @CreatedBy or @LastModifiedBy have to be. The following example shows an implementation of the interface that uses reactive Spring Security’s Authentication object: Implementation of ReactiveAuditorAware based on Spring Security class SpringSecurityAuditorAware implements ReactiveAuditorAware<User> { @Override public Mono<User> getCurrentAuditor() { return ReactiveSecurityContextHolder.getContext() .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getPrincipal) .map(User.class::cast); } } The implementation accesses the Authentication object provided by Spring Security and looks up the custom UserDetails instance that you have created in your UserDetailsService implementation. We assume here that you are exposing the domain user through the UserDetails implementation but that, based on the Authentication found, you could also look it up from anywhere."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/faq.html","FAQ: How does SDN relate to Neo4j-OGM?: Neo4j-OGM(https://neo4j.com/docs/ogm-manual/current/) is an Object Graph Mapping library, which is mainly used by previous versions of Spring Data Neo4j as its backend for the heavy lifting of mapping nodes and relationships into domain object. The current SDN does not need and does not support Neo4j-OGM. SDN uses Spring Data’s mapping context exclusively for scanning classes and building the meta model. While this pins SDN to the Spring ecosystem, it has several advantages, among them the smaller footprint regarding CPU and memory usage and especially, all the features of Spring’s mapping context. Why should I use SDN in favor of SDN+OGM: SDN has several features not present in SDN+OGM, notably Full support for Springs reactive story, including reactive transaction Full support for Query By Example(https://docs.spring.io/spring-data/jpa/docs/current/reference/html/#query-by-example) Full support for fully immutable entities Support for all modifiers and variations of derived finder methods, including spatial queries Does SDN support connections over HTTP to Neo4j?: No. Does SDN support embedded Neo4j?: Embedded Neo4j has multiple facets to it: Does SDN provide an embedded instance for your application?: No. Does SDN interact directly with an embedded instance?: No. An embedded database is usually represented by an instance of org.neo4j.graphdb.GraphDatabaseService and has no Bolt connector out of the box. SDN can however work very much with Neo4j’s test harness, the test harness is specially meant to be a drop-in replacement for the real database. Support for Neo4j 3.5, 4.x and 5.x test harness is implemented via the Spring Boot starter for the driver(https://github.com/neo4j/neo4j-java-driver-spring-boot-starter) . Have a look at the corresponding module org.neo4j.driver:neo4j-java-driver-test-harness-spring-boot-autoconfigure . Which Neo4j Java Driver can be used and how?: SDN relies on the Neo4j Java Driver. Each SDN release uses a Neo4j Java Driver version compatible with the latest Neo4j available at the time of its release. While patch versions of the Neo4j Java Driver are usually drop-in replacements, SDN makes sure that even minor versions are interchangeable as it checks for the presence or absence of methods or interface changes if necessary. Therefore, you are able to use any 4.x Neo4j Java Driver with any SDN 6.x version, and any 5.x Neo4j Driver with any SDN 7.x version. With Spring Boot: These days, a Spring boot deployment is the most likely deployment of a Spring Data based applications. Please use Spring Boots dependency management to change the driver version like this: Change the driver version from Maven (pom.xml) <properties> <neo4j-java-driver.version>5.4.0</neo4j-java-driver.version> </properties> Or Change the driver version from Gradle (gradle.properties) neo4j-java-driver.version = 5.4.0 Without Spring Boot: Without Spring Boot, you would just manually declare the dependency. For Maven, we recommend using the <dependencyManagement /> section like this: Change the driver version without Spring Boot from Maven (pom.xml) <dependencyManagement> <dependency> <groupId>org.neo4j.driver</groupId> <artifactId>neo4j-java-driver</artifactId> <version>5.4.0</version> </dependency> </dependencyManagement> Neo4j 4 supports multiple databases - How can I use them?: You can either statically configure the database name or run your own database name provider. Bear in mind that SDN will not create the databases for you. You can do this with the help of a migrations tool(https://github.com/michael-simons/neo4j-migrations) or of course with a simple script upfront. Statically configured: Configure the database name to use in your Spring Boot configuration like this (the same property applies of course for YML or environment based configuration, with Spring Boot’s conventions applied): spring.data.neo4j.database = yourDatabase With that configuration in place, all queries generated by all instances of SDN repositories (both reactive and imperative) and by the ReactiveNeo4jTemplate respectively Neo4jTemplate will be executed against the database yourDatabase . Dynamically configured: Provide a bean with the type Neo4jDatabaseNameProvider or ReactiveDatabaseSelectionProvider depending on the type of your Spring application. That bean could use for example Spring’s security context to retrieve a tenant. Here is a working example for an imperative application secured with Spring Security: Neo4jConfig.java import org.neo4j.springframework.data.core.DatabaseSelection; import org.neo4j.springframework.data.core.DatabaseSelectionProvider; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.security.core.Authentication; import org.springframework.security.core.context.SecurityContext; import org.springframework.security.core.context.SecurityContextHolder; import org.springframework.security.core.userdetails.User; @Configuration public class Neo4jConfig { @Bean DatabaseSelectionProvider databaseSelectionProvider() { return () -> Optional.ofNullable(SecurityContextHolder.getContext()).map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated).map(Authentication::getPrincipal).map(User.class::cast) .map(User::getUsername).map(DatabaseSelection::byName).orElseGet(DatabaseSelection::undecided); } } Be careful that you don’t mix up entities retrieved from one database with another database. The database name is requested for each new transaction, so you might end up with less or more entities than expected when changing the database name in between calls. Or worse, you could inevitably store the wrong entities in the wrong database. The Spring Boot Neo4j health indicator targets the default database, how can I change that?: Spring Boot comes with both imperative and reactive Neo4j health indicators.(https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-features.html#production-ready-health) Both variants are able to detect multiple beans of org.neo4j.driver.Driver inside the application context and provide a contribution to the overall health for each instance. The Neo4j driver however does connect to a server and not to a specific database inside that server. Spring Boot is able to configure the driver without Spring Data Neo4j and as the information which database is to be used is tied to Spring Data Neo4j, this information is not available to the built-in health indicator. This is most likely not a problem in many deployment scenarios. However, if configured database user does not have at least access rights to the default database, the health checks will fail. This can be mitigated by custom Neo4j health contributors that are aware of the database selection. Imperative variant: import java.util.Optional; import org.neo4j.driver.Driver; import org.neo4j.driver.Result; import org.neo4j.driver.SessionConfig; import org.neo4j.driver.summary.DatabaseInfo; import org.neo4j.driver.summary.ResultSummary; import org.neo4j.driver.summary.ServerInfo; import org.springframework.boot.actuate.health.AbstractHealthIndicator; import org.springframework.boot.actuate.health.Health; import org.springframework.data.neo4j.core.DatabaseSelection; import org.springframework.data.neo4j.core.DatabaseSelectionProvider; import org.springframework.util.StringUtils; public class DatabaseSelectionAwareNeo4jHealthIndicator extends AbstractHealthIndicator { private final Driver driver; private final DatabaseSelectionProvider databaseSelectionProvider; public DatabaseSelectionAwareNeo4jHealthIndicator( Driver driver, DatabaseSelectionProvider databaseSelectionProvider ) { this.driver = driver; this.databaseSelectionProvider = databaseSelectionProvider; } @Override protected void doHealthCheck(Health.Builder builder) { try { SessionConfig sessionConfig = Optional .ofNullable(databaseSelectionProvider.getDatabaseSelection()) .filter(databaseSelection -> databaseSelection != DatabaseSelection.undecided()) .map(DatabaseSelection::getValue) .map(v -> SessionConfig.builder().withDatabase(v).build()) .orElseGet(SessionConfig::defaultConfig); class Tuple { String edition; ResultSummary resultSummary; Tuple(String edition, ResultSummary resultSummary) { this.edition = edition; this.resultSummary = resultSummary; } } String query = ""CALL dbms.components() YIELD name, edition WHERE name = 'Neo4j Kernel' RETURN edition""; Tuple health = driver.session(sessionConfig) .writeTransaction(tx -> { Result result = tx.run(query); String edition = result.single().get(""edition"").asString(); return new Tuple(edition, result.consume()); }); addHealthDetails(builder, health.edition, health.resultSummary); } catch (Exception ex) { builder.down().withException(ex); } } static void addHealthDetails(Health.Builder builder, String edition, ResultSummary resultSummary) { ServerInfo serverInfo = resultSummary.server(); builder.up() .withDetail( ""server"", serverInfo.version() + ""@"" + serverInfo.address()) .withDetail(""edition"", edition); DatabaseInfo databaseInfo = resultSummary.database(); if (StringUtils.hasText(databaseInfo.name())) { builder.withDetail(""database"", databaseInfo.name()); } } } This uses the available database selection to run the same query that Boot runs to check whether a connection is healthy or not. Use the following configuration to apply it: import java.util.Map; import org.neo4j.driver.Driver; import org.springframework.beans.factory.InitializingBean; import org.springframework.boot.actuate.health.CompositeHealthContributor; import org.springframework.boot.actuate.health.HealthContributor; import org.springframework.boot.actuate.health.HealthContributorRegistry; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.neo4j.core.DatabaseSelectionProvider; @Configuration(proxyBeanMethods = false) public class Neo4jHealthConfig { @Bean (1) DatabaseSelectionAwareNeo4jHealthIndicator databaseSelectionAwareNeo4jHealthIndicator( Driver driver, DatabaseSelectionProvider databaseSelectionProvider ) { return new DatabaseSelectionAwareNeo4jHealthIndicator(driver, databaseSelectionProvider); } @Bean (2) HealthContributor neo4jHealthIndicator( Map<String, DatabaseSelectionAwareNeo4jHealthIndicator> customNeo4jHealthIndicators) { return CompositeHealthContributor.fromMap(customNeo4jHealthIndicators); } @Bean (3) InitializingBean healthContributorRegistryCleaner( HealthContributorRegistry healthContributorRegistry, Map<String, DatabaseSelectionAwareNeo4jHealthIndicator> customNeo4jHealthIndicators ) { return () -> customNeo4jHealthIndicators.keySet() .stream() .map(HealthContributorNameFactory.INSTANCE) .forEach(healthContributorRegistry::unregisterContributor); } } 1 If you have multiple drivers and database selection providers, you would need to create one indicator per combination 2 This makes sure that all of those indicators are grouped under Neo4j, replacing the default Neo4j health indicator 3 This prevents the individual contributors showing up in the health endpoint directly Reactive variant: The reactive variant is basically the same, using reactive types and the corresponding reactive infrastructure classes: import reactor.core.publisher.Mono; import reactor.util.function.Tuple2; import org.neo4j.driver.Driver; import org.neo4j.driver.SessionConfig; import org.neo4j.driver.reactivestreams.RxResult; import org.neo4j.driver.reactivestreams.RxSession; import org.neo4j.driver.summary.DatabaseInfo; import org.neo4j.driver.summary.ResultSummary; import org.neo4j.driver.summary.ServerInfo; import org.reactivestreams.Publisher; import org.springframework.boot.actuate.health.AbstractReactiveHealthIndicator; import org.springframework.boot.actuate.health.Health; import org.springframework.data.neo4j.core.DatabaseSelection; import org.springframework.data.neo4j.core.ReactiveDatabaseSelectionProvider; import org.springframework.util.StringUtils; public final class DatabaseSelectionAwareNeo4jReactiveHealthIndicator extends AbstractReactiveHealthIndicator { private final Driver driver; private final ReactiveDatabaseSelectionProvider databaseSelectionProvider; public DatabaseSelectionAwareNeo4jReactiveHealthIndicator( Driver driver, ReactiveDatabaseSelectionProvider databaseSelectionProvider ) { this.driver = driver; this.databaseSelectionProvider = databaseSelectionProvider; } @Override protected Mono<Health> doHealthCheck(Health.Builder builder) { String query = ""CALL dbms.components() YIELD name, edition WHERE name = 'Neo4j Kernel' RETURN edition""; return databaseSelectionProvider.getDatabaseSelection() .map(databaseSelection -> databaseSelection == DatabaseSelection.undecided() ? SessionConfig.defaultConfig() : SessionConfig.builder().withDatabase(databaseSelection.getValue()).build() ) .flatMap(sessionConfig -> Mono.usingWhen( Mono.fromSupplier(() -> driver.rxSession(sessionConfig)), s -> { Publisher<Tuple2<String, ResultSummary>> f = s.readTransaction(tx -> { RxResult result = tx.run(query); return Mono.from(result.records()) .map((record) -> record.get(""edition"").asString()) .zipWhen((edition) -> Mono.from(result.consume())); }); return Mono.fromDirect(f); }, RxSession::close ) ).map((result) -> { addHealthDetails(builder, result.getT1(), result.getT2()); return builder.build(); }); } static void addHealthDetails(Health.Builder builder, String edition, ResultSummary resultSummary) { ServerInfo serverInfo = resultSummary.server(); builder.up() .withDetail( ""server"", serverInfo.version() + ""@"" + serverInfo.address()) .withDetail(""edition"", edition); DatabaseInfo databaseInfo = resultSummary.database(); if (StringUtils.hasText(databaseInfo.name())) { builder.withDetail(""database"", databaseInfo.name()); } } } And of course, the reactive variant of the configuration. It needs two different registry cleaners, as Spring Boot will wrap existing reactive indicators to be used with the non-reactive actuator endpoint, too. import java.util.Map; import org.springframework.beans.factory.InitializingBean; import org.springframework.boot.actuate.health.CompositeReactiveHealthContributor; import org.springframework.boot.actuate.health.HealthContributorNameFactory; import org.springframework.boot.actuate.health.HealthContributorRegistry; import org.springframework.boot.actuate.health.ReactiveHealthContributor; import org.springframework.boot.actuate.health.ReactiveHealthContributorRegistry; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration(proxyBeanMethods = false) public class Neo4jHealthConfig { @Bean ReactiveHealthContributor neo4jHealthIndicator( Map<String, DatabaseSelectionAwareNeo4jReactiveHealthIndicator> customNeo4jHealthIndicators) { return CompositeReactiveHealthContributor.fromMap(customNeo4jHealthIndicators); } @Bean InitializingBean healthContributorRegistryCleaner(HealthContributorRegistry healthContributorRegistry, Map<String, DatabaseSelectionAwareNeo4jReactiveHealthIndicator> customNeo4jHealthIndicators) { return () -> customNeo4jHealthIndicators.keySet() .stream() .map(HealthContributorNameFactory.INSTANCE) .forEach(healthContributorRegistry::unregisterContributor); } @Bean InitializingBean reactiveHealthContributorRegistryCleaner( ReactiveHealthContributorRegistry healthContributorRegistry, Map<String, DatabaseSelectionAwareNeo4jReactiveHealthIndicator> customNeo4jHealthIndicators) { return () -> customNeo4jHealthIndicators.keySet() .stream() .map(HealthContributorNameFactory.INSTANCE) .forEach(healthContributorRegistry::unregisterContributor); } } Neo4j 4.4+ supports impersonation of different users - How can I use them?: User impersonation is especially interesting in big multi-tenant settings, in which one physically connected (or technical) user can impersonate many tenants. Depending on your setup this will drastically reduce the number of physical driver instances needed. The feature requires Neo4j Enterprise 4.4+ on the server side and a 4.4+ driver on the client side ( org.neo4j.driver:neo4j-java-driver:4.4.0 or higher). For both imperative and reactive versions you need to provide a UserSelectionProvider respectively a ReactiveUserSelectionProvider . The same instance needs to be passed along to the Neo4Client and Neo4jTransactionManager respectively their reactive variants. In Bootless imperative(#bootless-imperative-configuration) and reactive(#bootless-reactive-configuration) configurations you just need to provide a bean of the type in question: User selection provider bean import org.springframework.data.neo4j.core.UserSelection; import org.springframework.data.neo4j.core.UserSelectionProvider; public class CustomConfig { @Bean public UserSelectionProvider getUserSelectionProvider() { return () -> UserSelection.impersonate(""someUser""); } } In a typical Spring Boot scenario this feature requires a bit more work, as Boot supports also SDN versions without that feature. So given the bean in User selection provider bean(#faq.impersonation.userselectionbean) you would need fully customize the client and transaction manager: Necessary customization for Spring Boot import org.neo4j.driver.Driver; import org.springframework.data.neo4j.core.DatabaseSelectionProvider; import org.springframework.data.neo4j.core.Neo4jClient; import org.springframework.data.neo4j.core.UserSelectionProvider; import org.springframework.data.neo4j.core.transaction.Neo4jTransactionManager; import org.springframework.transaction.PlatformTransactionManager; public class CustomConfig { @Bean public Neo4jClient neo4jClient( Driver driver, DatabaseSelectionProvider databaseSelectionProvider, UserSelectionProvider userSelectionProvider ) { return Neo4jClient.with(driver) .withDatabaseSelectionProvider(databaseSelectionProvider) .withUserSelectionProvider(userSelectionProvider) .build(); } @Bean public PlatformTransactionManager transactionManager( Driver driver, DatabaseSelectionProvider databaseSelectionProvider, UserSelectionProvider userSelectionProvider ) { return Neo4jTransactionManager .with(driver) .withDatabaseSelectionProvider(databaseSelectionProvider) .withUserSelectionProvider(userSelectionProvider) .build(); } } Using a Neo4j cluster instance from Spring Data Neo4j: The following questions apply to Neo4j AuraDB as well as to Neo4j on-premise cluster instances. Do I need specific configuration so that transactions work seamless with a Neo4j Causal Cluster?: No, you don’t. SDN uses Neo4j Causal Cluster bookmarks internally without any configuration on your side required. Transactions in the same thread or the same reactive stream following each other will be able to read their previously changed values as you would expect. Is it important to use read-only transactions for Neo4j cluster?: Yes, it is. The Neo4j cluster architecture is a causal clustering architecture, and it distinguishes between primary and secondary servers. Primary server either are single instances or core instances. Both of them can answer to read and write operations. Write operations are propagated from the core instances to read replicas or more generally, followers, inside the cluster. Those followers are secondary servers. Secondary servers don’t answer to write operations. In a standard deployment scenario you’ll have some core instances and many read replicas inside a cluster. Therefore, it is important to mark operations or queries as read-only to scale your cluster in such a way that leaders are never overwhelmed and queries are propagated as much as possible to read replicas. Neither Spring Data Neo4j nor the underlying Java driver do Cypher parsing and both building blocks assume write operations by default. This decision has been made to support all operations out of the box. If something in the stack would assume read-only by default, the stack might end up sending write queries to read replicas and fail on executing them. All findById , findAllById , findAll and predefined existential methods are marked as read-only by default. Some options are described below: Making a whole repository read-only import org.springframework.data.neo4j.repository.Neo4jRepository; import org.springframework.transaction.annotation.Transactional; @Transactional(readOnly = true) interface PersonRepository extends Neo4jRepository<Person, Long> { } Making selected repository methods read-only import org.springframework.data.neo4j.repository.Neo4jRepository; import org.springframework.data.neo4j.repository.query.Query; import org.springframework.transaction.annotation.Transactional; interface PersonRepository extends Neo4jRepository<Person, Long> { @Transactional(readOnly = true) Person findOneByName(String name); (1) @Transactional(readOnly = true) @Query("""""" CALL apoc.search.nodeAll('{Person: ""name"",Movie: [""title"",""tagline""]}','contains','her') YIELD node AS n RETURN n"""""") Person findByCustomQuery(); (2) } 1 Why isn’t this read-only be default? While it would work for the derived finder above (which we actually know to be read-only), we often have seen cases in which user add a custom @Query and implement it via a MERGE construct, which of course is a write operation. 2 Custom procedures can do all kinds of things, there’s no way at the moment to check for read-only vs write here for us. Orchestrate calls to a repository from a service import java.util.Optional; import org.springframework.data.neo4j.repository.Neo4jRepository; import org.springframework.transaction.annotation.Transactional; interface PersonRepository extends Neo4jRepository<Person, Long> { } interface MovieRepository extends Neo4jRepository<Movie, Long> { List<Movie> findByLikedByPersonName(String name); } public class PersonService { private final PersonRepository personRepository; private final MovieRepository movieRepository; public PersonService(PersonRepository personRepository, MovieRepository movieRepository) { this.personRepository = personRepository; this.movieRepository = movieRepository; } @Transactional(readOnly = true) public Optional<PersonDetails> getPerson(Long id) { (1) return this.repository.findById(id) .map(person -> { var movies = this.movieRepository .findByLikedByPersonName(person.getName()); return new PersonDetails(person, movies); }); } } 1 Here, several calls to multiple repositories are wrapped in one single, read-only transaction. Using Springs TransactionTemplate inside private service methods and / or with the Neo4j client import java.util.Collection; import org.neo4j.driver.types.Node; import org.springframework.data.neo4j.core.Neo4jClient; import org.springframework.transaction.PlatformTransactionManager; import org.springframework.transaction.TransactionDefinition; import org.springframework.transaction.support.TransactionTemplate; public class PersonService { private final TransactionTemplate readOnlyTx; private final Neo4jClient neo4jClient; public PersonService(PlatformTransactionManager transactionManager, Neo4jClient neo4jClient) { this.readOnlyTx = new TransactionTemplate(transactionManager, (1) new TransactionDefinition() { @Override public boolean isReadOnly() { return true; } } ); this.neo4jClient = neo4jClient; } void internalOperation() { (2) Collection<Node> nodes = this.readOnlyTx.execute(state -> { return neo4jClient.query(""MATCH (n) RETURN n"").fetchAs(Node.class) (3) .mappedBy((types, record) -> record.get(0).asNode()) .all(); }); } } 1 Create an instance of the TransactionTemplate with the characteristics you need. Of course, this can be a global bean, too. 2 Reason number one for using the transaction template: Declarative transactions don’t work in package private or private methods and also not in inner method calls (imagine another method in this service calling internalOperation ) due to their nature being implemented with Aspects and proxies. 3 The Neo4jClient is a fixed utility provided by SDN. It cannot be annotated, but it integrates with Spring. So it gives you everything you would do with the pure driver and without automatic mapping and with transactions. It also obeys declarative transactions. Can I retrieve the latest Bookmarks or seed the transaction manager?: As mentioned briefly in Bookmark Management(appendix/migrating.html#migrating.bookmarks) , there is no need to configure anything with regard to bookmarks. It may however be useful to retrieve the latest bookmark the SDN transaction system received from a database. You can add a @Bean like BookmarkCapture to do this: BookmarkCapture.java import java.util.Set; import org.neo4j.driver.Bookmark; import org.springframework.context.ApplicationListener; public final class BookmarkCapture implements ApplicationListener<Neo4jBookmarksUpdatedEvent> { @Override public void onApplicationEvent(Neo4jBookmarksUpdatedEvent event) { // We make sure that this event is called only once, // the thread safe application of those bookmarks is up to your system. Set<Bookmark> latestBookmarks = event.getBookmarks(); } } For seeding the transaction system, a customized transaction manager like the following is required: BookmarkSeedingConfig.java import java.util.Set; import java.util.function.Supplier; import org.neo4j.driver.Bookmark; import org.neo4j.driver.Driver; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.neo4j.core.DatabaseSelectionProvider; import org.springframework.data.neo4j.core.transaction.Neo4jBookmarkManager; import org.springframework.data.neo4j.core.transaction.Neo4jTransactionManager; import org.springframework.transaction.PlatformTransactionManager; @Configuration public class BookmarkSeedingConfig { @Bean public PlatformTransactionManager transactionManager( Driver driver, DatabaseSelectionProvider databaseNameProvider) { (1) Supplier<Set<Bookmark>> bookmarkSupplier = () -> { (2) Bookmark a = null; Bookmark b = null; return Set.of(a, b); }; Neo4jBookmarkManager bookmarkManager = Neo4jBookmarkManager.create(bookmarkSupplier); (3) return new Neo4jTransactionManager( driver, databaseNameProvider, bookmarkManager); (4) } } 1 Let Spring inject those 2 This supplier can be anything that holds the latest bookmarks you want to bring into the system 3 Create the bookmark manager with it 4 Pass it on to the customized transaction manager There is no need to do any of these things above, unless your application has the need to access or provide this data. If in doubt, don’t do either. Can I disable bookmark management?: We provide a Noop bookmark manager that effectively disables bookmark management. Use this bookmark manager at your own risk, it will effectively disable any bookmark management by dropping all bookmarks and never supplying any. In a cluster you will be at a high risk of experiencing stale reads. In a single instance it will most likely not make any difference. + In a cluster this can be a sensible approach only and if only you can tolerate stale reads and are not in danger of overwriting old data. The following configuration creates a ""noop"" variant of the bookmark manager that will be picked up from relevant classes. BookmarksDisabledConfig.java import org.neo4j.driver.Driver; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.neo4j.core.transaction.Neo4jBookmarkManager; @Configuration public class BookmarksDisabledConfig { @Bean public Neo4jBookmarkManager neo4jBookmarkManager() { return Neo4jBookmarkManager.noop(); } } You can configure the pairs of Neo4jTransactionManager/Neo4jClient and ReactiveNeo4jTransactionManager/ReactiveNeo4jClient individually, but we recommend in doing so only when you already configuring them for specific database selection needs. Do I need to use Neo4j specific annotations?: No. You are free to use the following, equivalent Spring Data annotations: SDN specific annotation Spring Data common annotation Purpose Difference org.springframework.data.neo4j.core.schema.Id org.springframework.data.annotation.Id Marks the annotated attribute as the unique id. Specific annotation has no additional features. org.springframework.data.neo4j.core.schema.Node org.springframework.data.annotation.Persistent Marks the class as persistent entity. @Node allows customizing the labels How do I use assigned ids?: Just use @Id without @GeneratedValue and fill your id attribute via a constructor parameter or a setter or wither . See this blog post(https://medium.com/neo4j/neo4j-ogm-and-spring-data-neo4j-a55a866df68c) for some general remarks about finding good ids. How do I use externally generated ids?: We provide the interface org.springframework.data.neo4j.core.schema.IdGenerator . Implement it in any way you want and configure your implementation like this: ThingWithGeneratedId.java @Node public class ThingWithGeneratedId { @Id @GeneratedValue(TestSequenceGenerator.class) private String theId; } If you pass in the name of a class to @GeneratedValue , this class must have a no-args default constructor. You can however use a string as well: ThingWithIdGeneratedByBean.java @Node public class ThingWithIdGeneratedByBean { @Id @GeneratedValue(generatorRef = ""idGeneratingBean"") private String theId; } With that, idGeneratingBean refers to a bean in the Spring context. This might be useful for sequence generating. Setters are not required on non-final fields for the id. Do I have to create repositories for each domain class?: No. Have a look at the SDN building blocks(introduction-and-preface/building-blocks.html#sdn-building-blocks) and find the Neo4jTemplate or the ReactiveNeo4jTemplate . Those templates know your domain and provide all necessary basic CRUD methods for retrieving, writing and counting entities. This is our canonical movie example with the imperative template: TemplateExampleTest.java import static org.assertj.core.api.Assertions.assertThat; import java.util.Collections; import java.util.Optional; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.neo4j.core.Neo4jTemplate; import org.springframework.data.neo4j.documentation.domain.MovieEntity; import org.springframework.data.neo4j.documentation.domain.PersonEntity; import org.springframework.data.neo4j.documentation.domain.Roles; @DataNeo4jTest public class TemplateExampleTest { @Test void shouldSaveAndReadEntities(@Autowired Neo4jTemplate neo4jTemplate) { MovieEntity movie = new MovieEntity(""The Love Bug"", ""A movie that follows the adventures of Herbie, Herbie's driver, "" + ""Jim Douglas (Dean Jones), and Jim's love interest, "" + ""Carole Bennett (Michele Lee)""); Roles roles1 = new Roles(new PersonEntity(1931, ""Dean Jones""), Collections.singletonList(""Didi"")); Roles roles2 = new Roles(new PersonEntity(1942, ""Michele Lee""), Collections.singletonList(""Michi"")); movie.getActorsAndRoles().add(roles1); movie.getActorsAndRoles().add(roles2); MovieEntity result = neo4jTemplate.save(movie); assertThat(result.getActorsAndRoles()).allSatisfy(relationship -> assertThat(relationship.getId()).isNotNull()); Optional<PersonEntity> person = neo4jTemplate.findById(""Dean Jones"", PersonEntity.class); assertThat(person).map(PersonEntity::getBorn).hasValue(1931); assertThat(neo4jTemplate.count(PersonEntity.class)).isEqualTo(2L); } } And here is the reactive version, omitting the setup for brevity: ReactiveTemplateExampleTest.java import reactor.test.StepVerifier; import java.util.Collections; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.neo4j.core.ReactiveNeo4jTemplate; import org.springframework.data.neo4j.documentation.domain.MovieEntity; import org.springframework.data.neo4j.documentation.domain.PersonEntity; import org.springframework.data.neo4j.documentation.domain.Roles; import org.springframework.test.context.DynamicPropertyRegistry; import org.springframework.test.context.DynamicPropertySource; import org.testcontainers.containers.Neo4jContainer; import org.testcontainers.junit.jupiter.Container; import org.testcontainers.junit.jupiter.Testcontainers; @Testcontainers @DataNeo4jTest class ReactiveTemplateExampleTest { @Container private static Neo4jContainer<?> neo4jContainer = new Neo4jContainer<>(""neo4j:5""); @DynamicPropertySource static void neo4jProperties(DynamicPropertyRegistry registry) { registry.add(""org.neo4j.driver.uri"", neo4jContainer::getBoltUrl); registry.add(""org.neo4j.driver.authentication.username"", () -> ""neo4j""); registry.add(""org.neo4j.driver.authentication.password"", neo4jContainer::getAdminPassword); } @Test void shouldSaveAndReadEntities(@Autowired ReactiveNeo4jTemplate neo4jTemplate) { MovieEntity movie = new MovieEntity(""The Love Bug"", ""A movie that follows the adventures of Herbie, Herbie's driver, Jim Douglas (Dean Jones), and Jim's love interest, Carole Bennett (Michele Lee)""); Roles role1 = new Roles(new PersonEntity(1931, ""Dean Jones""), Collections.singletonList(""Didi"")); Roles role2 = new Roles(new PersonEntity(1942, ""Michele Lee""), Collections.singletonList(""Michi"")); movie.getActorsAndRoles().add(role1); movie.getActorsAndRoles().add(role2); StepVerifier.create(neo4jTemplate.save(movie)).expectNextCount(1L).verifyComplete(); StepVerifier.create(neo4jTemplate.findById(""Dean Jones"", PersonEntity.class).map(PersonEntity::getBorn)) .expectNext(1931).verifyComplete(); StepVerifier.create(neo4jTemplate.count(PersonEntity.class)).expectNext(2L).verifyComplete(); } } Please note that both examples use @DataNeo4jTest from Spring Boot. How do I use custom queries with repository methods returning Page<T> or Slice<T>?: While you don’t have to provide anything else apart a Pageable as a parameter on derived finder methods that return a Page<T> or a Slice<T> , you must prepare your custom query to handle the pageable. Pages and Slices(#custom-queries-with-page-and-slice-examples) gives you an overview about what’s needed. Pages and Slices import org.springframework.data.domain.Pageable; import org.springframework.data.neo4j.repository.Neo4jRepository; import org.springframework.data.neo4j.repository.query.Query; public interface MyPersonRepository extends Neo4jRepository<Person, Long> { Page<Person> findByName(String name, Pageable pageable); (1) @Query("""" + ""MATCH (n:Person) WHERE n.name = $name RETURN n "" + ""ORDER BY n.name ASC SKIP $skip LIMIT $limit"" ) Slice<Person> findSliceByName(String name, Pageable pageable); (2) @Query( value = """" + ""MATCH (n:Person) WHERE n.name = $name RETURN n "" + ""ORDER BY n.name ASC SKIP $skip LIMIT $limit"", countQuery = """" + ""MATCH (n:Person) WHERE n.name = $name RETURN count(n)"" ) Page<Person> findPageByName(String name, Pageable pageable); (3) } 1 A derived finder method that creates a query for you. It handles the Pageable for you. You should use a sorted pageable. 2 This method uses @Query to define a custom query. It returns a Slice<Person> . A slice does not know about the total number of pages, so the custom query doesn’t need a dedicated count query. SDN will notify you that it estimates the next slice. The Cypher template must spot both $skip and $limit Cypher parameter. If you omit them, SDN will issue a warning. The will probably not match your expectations. Also, the Pageable should be unsorted and you should provide a stable order. We won’t use the sorting information from the pageable. 3 This method returns a page. A page knows about the exact number of total pages. Therefore, you must specify an additional count query. All other restrictions from the second method apply. Can I map named paths?: A series of connected nodes and relationships is called a ""path"" in Neo4j. Cypher allows paths to be named using an identifier, as exemplified by: p = (a)-[*3..5]->(b) or as in the infamous Movie graph, that includes the following path (in that case, one of the shortest path between two actors): The ""Bacon"" distance MATCH p=shortestPath((bacon:Person {name:""Kevin Bacon""})-[*]-(meg:Person {name:""Meg Ryan""})) RETURN p Which looks like this: We find 3 nodes labeled Vertex and 2 nodes labeled Movie . Both can be mapped with a custom query. Assume there’s a node entity for both Vertex and Movie as well as Actor taking care of the relationship: ""Standard"" movie graph domain model @Node public final class Person { @Id @GeneratedValue private final Long id; private final String name; private Integer born; @Relationship(""REVIEWED"") private List<Movie> reviewed = new ArrayList<>(); } @RelationshipProperties public final class Actor { @RelationshipId private final Long id; @TargetNode private final Person person; private final List<String> roles; } @Node public final class Movie { @Id private final String title; @Property(""tagline"") private final String description; @Relationship(value = ""ACTED_IN"", direction = Direction.INCOMING) private final List<Actor> actors; } When using a query as shown in The ""Bacon"" distance(#bacon-distance) for a domain class of type Vertex like this interface PeopleRepository extends Neo4jRepository<Person, Long> { @Query("""" + ""MATCH p=shortestPath((bacon:Person {name: $person1})-[*]-(meg:Person {name: $person2}))\n"" + ""RETURN p"" ) List<Person> findAllOnShortestPathBetween(@Param(""person1"") String person1, @Param(""person2"") String person2); } it will retrieve all people from the path and map them. If there are relationship types on the path like REVIEWED that are also present on the domain, these will be filled accordingly from the path. Take special care when you use nodes hydrated from a path based query to save data. If not all relationships are hydrated, data will be lost. The other way round works as well. The same query can be used with the Movie entity. It then will only populate movies. The following listing shows how todo this as well as how the query can be enriched with additional data not found on the path. That data is used to correctly populate the missing relationships (in that case, all the actors) interface MovieRepository extends Neo4jRepository<Movie, String> { @Query("""" + ""MATCH p=shortestPath(\n"" + ""(bacon:Person {name: $person1})-[*]-(meg:Person {name: $person2}))\n"" + ""WITH p, [n IN nodes(p) WHERE n:Movie] AS x\n"" + ""UNWIND x AS m\n"" + ""MATCH (m) <-[r:DIRECTED]-(d:Person)\n"" + ""RETURN p, collect(r), collect(d)"" ) List<Movie> findAllOnShortestPathBetween(@Param(""person1"") String person1, @Param(""person2"") String person2); } The query returns the path plus all relationships and related nodes collected so that the movie entities are fully hydrated. The path mapping works for single paths as well for multiple records of paths (which are returned by the allShortestPath function.) Named paths can be used efficiently to populate and return more than just a root node, see appendix/custom-queries.adoc#custom-query.paths(appendix/custom-queries.html#custom-query.paths) . Is @Query the only way to use custom queries?: No, @Query is not the only way to run custom queries. The annotation is comfortable in situations in which your custom query fills your domain completely. Please remember that SDN assumes your mapped domain model to be the truth. That means if you use a custom query via @Query that only fills a model partially, you are in danger of using the same object to write the data back which will eventually erase or overwrite data you didn’t consider in your query. So, please use repositories and declarative methods with @Query in all cases where the result is shaped like your domain model or you are sure you don’t use a partially mapped model for write commands. What are the alternatives? Projections(projections/sdn-projections.html#projections.sdn.general-remarks) might be already enough to shape your view on the graph: They can be used to define the depth of fetching properties and related entities in an explicit way: By modelling them. If your goal is to make only the conditions of your queries dynamic , then have a look at the QuerydslPredicateExecutor(repositories/core-extensions.html#core.extensions.querydsl) but especially our own variant of it, the CypherdslConditionExecutor . Both mixins(repositories/sdn-extension.html#sdn-mixins) allow adding conditions to the full queries we create for you. Thus, you will have the domain fully populated together with custom conditions. Of course, your conditions must work with what we generate. Find the names of the root node, the related nodes and more here(appendix/custom-queries.html#custom-queries) . Use the Cypher-DSL(http://neo4j-contrib.github.io/cypher-dsl/current/) via the CypherdslStatementExecutor or the ReactiveCypherdslStatementExecutor . The Cypher-DSL is predestined to create dynamic queries. In the end, it’s what SDN uses under the hood anyway. The corresponding mixins work both with the domain type of repository itself and with projections (something that the mixins for adding conditions don’t). If you think that you can solve your problem with a partially dynamic query or a full dynamic query together with a projection, please jump back now to the chapter about Spring Data Neo4j Mixins(repositories/sdn-extension.html#sdn-mixins) . Otherwise, please read up on two things: custom repository fragments(repositories/custom-implementations.html#repositories.custom-implementations) the levels of abstractions(introduction-and-preface/building-blocks.html#sdn-building-blocks) we offer in SDN. Why speaking about custom repository fragments now? You might have more complex situation in which more than one dynamic query is required, but the queries still belong conceptually in a repository and not in the service layer Your custom queries return a graph shaped result that fits not quite to your domain model and therefore the custom query should be accompanied by a custom mapping as well You have the need for interacting with the driver, i.e. for bulk loads that should not go through object mapping. Assume the following repository declaration that basically aggregates one base repository plus 3 fragments: A repository composed of several fragments import org.springframework.data.neo4j.repository.Neo4jRepository; public interface MovieRepository extends Neo4jRepository<MovieEntity, String>, DomainResults, NonDomainResults, LowlevelInteractions { } The repository contains Movies(getting-started.html#movie-entity) as shown in the getting started section(getting-started.html#example-node-spring-boot-project) . The additional interface from which the repository extends ( DomainResults , NonDomainResults and LowlevelInteractions ) are the fragments that addresses all the concerns above. Using complex, dynamic custom queries but still returning domain types: The fragment DomainResults declares one additional method findMoviesAlongShortestPath : DomainResults fragment interface DomainResults { @Transactional(readOnly = true) List<MovieEntity> findMoviesAlongShortestPath(PersonEntity from, PersonEntity to); } This method is annotated with @Transactional(readOnly = true) to indicate that readers can answer it. It cannot be derived by SDN but would need a custom query. This custom query is provided by the one implementation of that interface. The implementation has the same name with the suffix Impl : A fragment implementation using the Neo4jTemplate import static org.neo4j.cypherdsl.core.Cypher.anyNode; import static org.neo4j.cypherdsl.core.Cypher.listWith; import static org.neo4j.cypherdsl.core.Cypher.name; import static org.neo4j.cypherdsl.core.Cypher.node; import static org.neo4j.cypherdsl.core.Cypher.parameter; import static org.neo4j.cypherdsl.core.Cypher.shortestPath; import org.neo4j.cypherdsl.core.Cypher; class DomainResultsImpl implements DomainResults { private final Neo4jTemplate neo4jTemplate; (1) DomainResultsImpl(Neo4jTemplate neo4jTemplate) { this.neo4jTemplate = neo4jTemplate; } @Override public List<MovieEntity> findMoviesAlongShortestPath(PersonEntity from, PersonEntity to) { var p1 = node(""Person"").withProperties(""name"", parameter(""person1"")); var p2 = node(""Person"").withProperties(""name"", parameter(""person2"")); var shortestPath = shortestPath(""p"").definedBy( p1.relationshipBetween(p2).unbounded() ); var p = shortestPath.getRequiredSymbolicName(); var statement = Cypher.match(shortestPath) .with(p, listWith(name(""n"")) .in(Cypher.nodes(shortestPath)) .where(anyNode().named(""n"").hasLabels(""Movie"")).returning().as(""mn"") ) .unwind(name(""mn"")).as(""m"") .with(p, name(""m"")) .match(node(""Person"").named(""d"") .relationshipTo(anyNode(""m""), ""DIRECTED"").named(""r"") ) .returning(p, Cypher.collect(name(""r"")), Cypher.collect(name(""d""))) .build(); Map<String, Object> parameters = new HashMap<>(); parameters.put(""person1"", from.getName()); parameters.put(""person2"", to.getName()); return neo4jTemplate.findAll(statement, parameters, MovieEntity.class); (2) } } 1 The Neo4jTemplate is injected by the runtime through the constructor of DomainResultsImpl . No need for @Autowired . 2 The Cypher-DSL is used to build a complex statement (pretty much the same as shown in path mapping(#faq.path-mapping) .) The statement can be passed directly to the template. The template has overloads for String-based queries as well, so you could write down the query as String as well. The important takeaway here is: The template ""knows"" your domain objects and maps them accordingly @Query is not the only option to define custom queries They can be generated in various ways The @Transactional annotation is respected Using custom queries and custom mappings: Often times a custom query indicates custom results. Should all of those results be mapped as @Node ? Of course not! Many times those objects represents read commands and are not meant to be used as write commands. It is also not unlikely that SDN cannot or want not map everything that is possible with Cypher. It does however offer several hooks to run your own mapping: On the Neo4jClient . The benefit of using the SDN Neo4jClient over the driver: The Neo4jClient is integrated with Springs transaction management It has a fluent API for binding parameters It has a fluent API exposing both the records and the Neo4j type system so that you can access everything in your result to execute the mapping Declaring the fragment is exactly the same as before: A fragment declaring non-domain-type results interface NonDomainResults { class Result { (1) public final String name; public final String typeOfRelation; Result(String name, String typeOfRelation) { this.name = name; this.typeOfRelation = typeOfRelation; } } @Transactional(readOnly = true) Collection<Result> findRelationsToMovie(MovieEntity movie); (2) } 1 This is a made up non-domain result. A real world query result would probably look more complex. 2 The method this fragment adds. Again, the method is annotated with Spring’s @Transactional Without an implementation for that fragment, startup would fail, so here it is: A fragment implementation using the Neo4jClient class NonDomainResultsImpl implements NonDomainResults { private final Neo4jClient neo4jClient; (1) NonDomainResultsImpl(Neo4jClient neo4jClient) { this.neo4jClient = neo4jClient; } @Override public Collection<Result> findRelationsToMovie(MovieEntity movie) { return this.neo4jClient .query("""" + ""MATCH (people:Person)-[relatedTo]-(:Movie {title: $title}) "" + ""RETURN people.name AS name, "" + "" Type(relatedTo) as typeOfRelation"" ) (2) .bind(movie.getTitle()).to(""title"") (3) .fetchAs(Result.class) (4) .mappedBy((typeSystem, record) -> new Result(record.get(""name"").asString(), record.get(""typeOfRelation"").asString())) (5) .all(); (6) } } 1 Here we use the Neo4jClient , as provided by the infrastructure. 2 The client takes only in Strings, but the Cypher-DSL can still be used when rendering into a String 3 Bind one single value to a named parameter. There’s also an overload to bind a whole map of parameters 4 This is the type of the result you want 5 And finally, the mappedBy method, exposing one Record for each entry in the result plus the drivers type system if needed. This is the API in which you hook in for your custom mappings The whole query runs in the context of a Spring transaction, in this case, a read-only one. Low level interactions: Sometimes you might want to do bulk loadings from a repository or delete whole subgraphs or interact in very specific ways with the Neo4j Java-Driver. This is possible as well. The following example shows how: Fragments using the plain driver interface LowlevelInteractions { int deleteGraph(); } class LowlevelInteractionsImpl implements LowlevelInteractions { private final Driver driver; (1) LowlevelInteractionsImpl(Driver driver) { this.driver = driver; } @Override public int deleteGraph() { try (Session session = driver.session()) { SummaryCounters counters = session .executeWrite(tx -> tx.run(""MATCH (n) DETACH DELETE n"").consume()) (2) .counters(); return counters.nodesDeleted() + counters.relationshipsDeleted(); } } } 1 Work with the driver directly. As with all the examples: There is no need for @Autowired magic. All the fragments are actually testable on their own. 2 The use case is made up. Here we use a driver managed transaction deleting the whole graph and return the number of deleted nodes and relationships This interaction does of course not run in a Spring transaction, as the driver does not know about Spring. Putting it all together, this test succeeds: Testing the composed repository @Test void customRepositoryFragmentsShouldWork( @Autowired PersonRepository people, @Autowired MovieRepository movies ) { PersonEntity meg = people.findById(""Meg Ryan"").get(); PersonEntity kevin = people.findById(""Kevin Bacon"").get(); List<MovieEntity> moviesBetweenMegAndKevin = movies. findMoviesAlongShortestPath(meg, kevin); assertThat(moviesBetweenMegAndKevin).isNotEmpty(); Collection<NonDomainResults.Result> relatedPeople = movies .findRelationsToMovie(moviesBetweenMegAndKevin.get(0)); assertThat(relatedPeople).isNotEmpty(); assertThat(movies.deleteGraph()).isGreaterThan(0); assertThat(movies.findAll()).isEmpty(); assertThat(people.findAll()).isEmpty(); } As a final word: All three interfaces and implementations are picked up by Spring Data Neo4j automatically. There is no need for further configuration. Also, the same overall repository could have been created with only one additional fragment (the interface defining all three methods) and one implementation. The implementation would than have had all three abstractions injected (template, client and driver). All of this applies of course to reactive repositories as well. They would work with the ReactiveNeo4jTemplate and ReactiveNeo4jClient and the reactive session provided by the driver. If you have recurring methods for all repositories, you could swap out the default repository implementation. How do I use custom Spring Data Neo4j base repositories?: Basically the same ways as the shared Spring Data Commons documentation shows for Spring Data JPA in Customize the Base Repository(repositories/custom-implementations.html#repositories.customize-base-repository) . Only that in our case you would extend from Custom base repository public class MyRepositoryImpl<T, ID> extends SimpleNeo4jRepository<T, ID> { MyRepositoryImpl( Neo4jOperations neo4jOperations, Neo4jEntityInformation<T, ID> entityInformation ) { super(neo4jOperations, entityInformation); (1) } @Override public List<T> findAll() { throw new UnsupportedOperationException(""This implementation does not support `findAll`""); } } 1 This signature is required by the base class. Take the Neo4jOperations (the actual specification of the Neo4jTemplate ) and the entity information and store them on an attribute if needed. In this example we forbid the use of the findAll method. You could add methods taking in a fetch depth and run custom queries based on that depth. One way to do this is shown in DomainResults fragment(#domain-results) . To enable this base repository for all declared repositories enable Neo4j repositories with: @EnableNeo4jRepositories(repositoryBaseClass = MyRepositoryImpl.class) . How do I audit entities?: All Spring Data annotations are supported. Those are org.springframework.data.annotation.CreatedBy org.springframework.data.annotation.CreatedDate org.springframework.data.annotation.LastModifiedBy org.springframework.data.annotation.LastModifiedDate Auditing(auditing.html) gives you a general view how to use auditing in the bigger context of Spring Data Commons. The following listing presents every configuration option provided by Spring Data Neo4j: Enabling and configuring Neo4j auditing import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Import; import org.springframework.data.auditing.DateTimeProvider; import org.springframework.data.domain.AuditorAware; @Configuration @EnableNeo4jAuditing( modifyOnCreate = false, (1) auditorAwareRef = ""auditorProvider"", (2) dateTimeProviderRef = ""fixedDateTimeProvider"" (3) ) class AuditingConfig { @Bean public AuditorAware<String> auditorProvider() { return () -> Optional.of(""A user""); } @Bean public DateTimeProvider fixedDateTimeProvider() { return () -> Optional.of(AuditingITBase.DEFAULT_CREATION_AND_MODIFICATION_DATE); } } 1 Set to true if you want the modification data to be written during creating as well 2 Use this attribute to specify the name of the bean that provides the auditor (i.e. a user name) 3 Use this attribute to specify the name of a bean that provides the current date. In this case a fixed date is used as the above configuration is part of our tests The reactive version is basically the same apart from the fact the auditor aware bean is of type ReactiveAuditorAware , so that the retrieval of an auditor is part of the reactive flow. In addition to those auditing mechanism you can add as many beans implementing BeforeBindCallback<T> or ReactiveBeforeBindCallback<T> to the context. These beans will be picked up by Spring Data Neo4j and called in order (in case they implement Ordered or are annotated with @Order ) just before an entity is persisted. They can modify the entity or return a completely new one. The following example adds one callback to the context that changes one attribute before the entity is persisted: Modifying entities before save import java.util.UUID; import java.util.stream.StreamSupport; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.neo4j.core.mapping.callback.AfterConvertCallback; import org.springframework.data.neo4j.core.mapping.callback.BeforeBindCallback; @Configuration class CallbacksConfig { @Bean BeforeBindCallback<ThingWithAssignedId> nameChanger() { return entity -> { ThingWithAssignedId updatedThing = new ThingWithAssignedId( entity.getTheId(), entity.getName() + "" (Edited)""); return updatedThing; }; } @Bean AfterConvertCallback<ThingWithAssignedId> randomValueAssigner() { return (entity, definition, source) -> { entity.setRandomValue(UUID.randomUUID().toString()); return entity; }; } } No additional configuration is required. How do I use ""Find by example""?: ""Find by example"" is a new feature in SDN. You instantiate an entity or use an existing one. With this instance you create an org.springframework.data.domain.Example . If your repository extends org.springframework.data.neo4j.repository.Neo4jRepository or org.springframework.data.neo4j.repository.ReactiveNeo4jRepository , you can immediately use the available findBy methods taking in an example, like shown in findByExample(#find-by-example-example) . findByExample in Action Example<MovieEntity> movieExample = Example.of(new MovieEntity(""The Matrix"", null)); Flux<MovieEntity> movies = this.movieRepository.findAll(movieExample); movieExample = Example.of( new MovieEntity(""Matrix"", null), ExampleMatcher .matchingAny() .withMatcher( ""title"", ExampleMatcher.GenericPropertyMatcher.of(ExampleMatcher.StringMatcher.CONTAINING) ) ); movies = this.movieRepository.findAll(movieExample); You can also negate individual properties. This will add an appropriate NOT operation, thus turning an = into a <> . All scalar datatypes and all string operators are supported: findByExample with negated values Example<MovieEntity> movieExample = Example.of( new MovieEntity(""Matrix"", null), ExampleMatcher .matchingAny() .withMatcher( ""title"", ExampleMatcher.GenericPropertyMatcher.of(ExampleMatcher.StringMatcher.CONTAINING) ) .withTransformer(""title"", Neo4jPropertyValueTransformers.notMatching()) ); Flux<MovieEntity> allMoviesThatNotContainMatrix = this.movieRepository.findAll(movieExample); Do I need Spring Boot to use Spring Data Neo4j?: No, you don’t. While the automatic configuration of many Spring aspects through Spring Boot takes away a lot of manual cruft and is the recommended approach for setting up new Spring projects, you don’t need to have to use this. The following dependency is required for the solutions described above: <dependency> <groupId>org.springframework.data</groupId> <artifactId>spring-data-neo4j</artifactId> <version>7.3.4</version> </dependency> The coordinates for a Gradle setup are the same. To select a different database - either statically or dynamically - you can add a Bean of type DatabaseSelectionProvider as explained in Neo4j 4 supports multiple databases - How can I use them?(#faq.multidatabase) . For a reactive scenario, we provide ReactiveDatabaseSelectionProvider . Using Spring Data Neo4j inside a Spring context without Spring Boot: We provide two abstract configuration classes to support you in bringing in the necessary beans: org.springframework.data.neo4j.config.AbstractNeo4jConfig for imperative database access and org.springframework.data.neo4j.config.AbstractReactiveNeo4jConfig for the reactive version. They are meant to be used with @EnableNeo4jRepositories and @EnableReactiveNeo4jRepositories respectively. See Enabling Spring Data Neo4j infrastructure for imperative database access(#bootless-imperative-configuration) and Enabling Spring Data Neo4j infrastructure for reactive database access(#bootless-reactive-configuration) for an example usage. Both classes require you to override driver() in which you are supposed to create the driver. To get the imperative version of the Neo4j client(appendix/neo4j-client.html#neo4j-client) , the template and support for imperative repositories, use something similar as shown here: Enabling Spring Data Neo4j infrastructure for imperative database access import org.neo4j.driver.Driver; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.transaction.annotation.EnableTransactionManagement; import org.springframework.data.neo4j.config.AbstractNeo4jConfig; import org.springframework.data.neo4j.core.DatabaseSelectionProvider; import org.springframework.data.neo4j.repository.config.EnableNeo4jRepositories; @Configuration @EnableNeo4jRepositories @EnableTransactionManagement class MyConfiguration extends AbstractNeo4jConfig { @Override @Bean public Driver driver() { (1) return GraphDatabase.driver(""bolt://localhost:7687"", AuthTokens.basic(""neo4j"", ""secret"")); } @Override protected Collection<String> getMappingBasePackages() { return Collections.singletonList(Person.class.getPackage().getName()); } @Override @Bean (2) protected DatabaseSelectionProvider databaseSelectionProvider() { return DatabaseSelectionProvider.createStaticDatabaseSelectionProvider(""yourDatabase""); } } 1 The driver bean is required. 2 This statically selects a database named yourDatabase and is optional . The following listing provides the reactive Neo4j client and template, enables reactive transaction management and discovers Neo4j related repositories: Enabling Spring Data Neo4j infrastructure for reactive database access import org.neo4j.driver.Driver; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.neo4j.config.AbstractReactiveNeo4jConfig; import org.springframework.data.neo4j.repository.config.EnableReactiveNeo4jRepositories; import org.springframework.transaction.annotation.EnableTransactionManagement; @Configuration @EnableReactiveNeo4jRepositories @EnableTransactionManagement class MyConfiguration extends AbstractReactiveNeo4jConfig { @Bean @Override public Driver driver() { return GraphDatabase.driver(""bolt://localhost:7687"", AuthTokens.basic(""neo4j"", ""secret"")); } @Override protected Collection<String> getMappingBasePackages() { return Collections.singletonList(Person.class.getPackage().getName()); } } Using Spring Data Neo4j in a CDI 2.0 environment: For your convenience we provide a CDI extension with Neo4jCdiExtension . When run in a compatible CDI 2.0 container, it will be automatically be registered and loaded through Java’s service loader SPI(https://docs.oracle.com/javase/tutorial/ext/basics/spi.html) . The only thing you have to bring into your application is an annotated type that produces the Neo4j Java Driver: A CDI producer for the Neo4j Java Driver import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; import org.neo4j.driver.AuthTokens; import org.neo4j.driver.Driver; import org.neo4j.driver.GraphDatabase; public class Neo4jConfig { @Produces @ApplicationScoped public Driver driver() { (1) return GraphDatabase .driver(""bolt://localhost:7687"", AuthTokens.basic(""neo4j"", ""secret"")); } public void close(@Disposes Driver driver) { driver.close(); } @Produces @Singleton public DatabaseSelectionProvider getDatabaseSelectionProvider() { (2) return DatabaseSelectionProvider.createStaticDatabaseSelectionProvider(""yourDatabase""); } } 1 Same as with plain Spring in Enabling Spring Data Neo4j infrastructure for imperative database access(#bootless-imperative-configuration) , but annotated with the corresponding CDI infrastructure. 2 This is optional . However, if you run a custom database selection provider, you must not qualify this bean. If you are running in a SE Container - like the one Weld(https://weld.cdi-spec.org) provides for example, you can enable the extension like that: Enabling the Neo4j CDI extension in a SE container import javax.enterprise.inject.se.SeContainer; import javax.enterprise.inject.se.SeContainerInitializer; import org.springframework.data.neo4j.config.Neo4jCdiExtension; public class SomeClass { void someMethod() { try (SeContainer container = SeContainerInitializer.newInstance() .disableDiscovery() .addExtensions(Neo4jCdiExtension.class) .addBeanClasses(YourDriverFactory.class) .addPackages(Package.getPackage(""your.domain.package"")) .initialize() ) { SomeRepository someRepository = container.select(SomeRepository.class).get(); } } }"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/index.html","Appendix: Section Summary: Conversions(conversions.html) Neo4jClient(neo4j-client.html) Logging(logging.html) Query creation(query-creation.html) Custom queries(custom-queries.html) Spatial types(spatial-types.html) Migrating from SDN+OGM to SDN(migrating.html) Building Spring Data Neo4j(build.html)"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/conversions.html","Conversions: Convention-based Mapping: The Neo4j Converter has a few conventions for mapping objects when no additional mapping metadata is provided. The conventions are: The short Java class name is mapped to the primary label in the following manner: The class com.bigbank.SavingsAccount maps to the savingsAccount primary label. The converter uses any Spring Converter(#custom.conversions) registered with it to override the default mapping of object properties to node fields and values. The fields of an object are used to convert to and from fields in the graph. Public JavaBean properties are not used. If you have a single non-zero-argument constructor whose constructor argument names match top-level property names of node, that constructor is used. Otherwise, the zero-argument constructor is used. If there is more than one non-zero-argument constructor, an exception will be thrown. We support a broad range of conversions out of the box. Find the list of supported cypher types in the official drivers manual: Type mapping(https://neo4j.com/docs/java-manual/current/cypher-workflow/#java-driver-type-mapping) . Primitive types of wrapper types are equally supported. Domain type Cypher type Maps directly to native type java.lang.Boolean Boolean ✔ boolean[] List of Boolean ✔ java.lang.Long Integer ✔ long[] List of Integer ✔ java.lang.Double Float ✔ double[] List of Float ✔ java.lang.String String ✔ java.lang.String[] List of String ✔ byte[] ByteArray ✔ java.lang.Byte ByteArray with length 1 java.lang.Character String with length 1 char[] List of String with length 1 java.util.Date String formatted as ISO 8601 Date ( yyyy-MM-dd’T’HH:mm:ss.SSSZ ). Notice the Z : SDN will store all java.util.Date instances in UTC . If you require the time zone, use a type that supports it (i.e. ZoneDateTime ) or store the zone as a separate property. java.lang.Float String float[] List of String java.lang.Integer Integer int[] List of Integer java.util.Locale String formatted as BCP 47 language tag java.lang.Short Integer short[] List of Integer java.math.BigDecimal String java.math.BigInteger String java.time.LocalDate Date ✔ java.time.OffsetTime Time ✔ java.time.LocalTime LocalTime ✔ java.time.ZonedDateTime DateTime ✔ java.time.LocalDateTime LocalDateTime ✔ java.time.OffsetDateTime DateTime java.time.Instant DateTime java.util.TimeZone String java.time.ZoneId String java.time.Period Duration java.time.Duration Duration org.neo4j.driver.types.IsoDuration Duration ✔ org.neo4j.driver.types.Point Point ✔ org.springframework.data.neo4j.types.GeographicPoint2d Point with CRS 4326 org.springframework.data.neo4j.types.GeographicPoint3d Point with CRS 4979 org.springframework.data.neo4j.types.CartesianPoint2d Point with CRS 7203 org.springframework.data.neo4j.types.CartesianPoint3d Point with CRS 9157 org.springframework.data.geo.Point Point with CRS 4326 and x/y corresponding to lat/long Instances of Enum String (The name value of the enum) Instances of Enum[] List of String (The name value of the enum) java.net.URL String java.net.URI String java.util.UUID String Custom conversions: For attributes of a given type: If you prefer to work with your own types in the entities or as parameters for @Query annotated methods, you can define and provide a custom converter implementation. First you have to implement a GenericConverter and register the types your converter should handle. For entity property type converters you need to take care of converting your type to and from a Neo4j Java Driver Value . If your converter is supposed to work only with custom query methods in the repositories, it is sufficient to provide the one-way conversion to the Value type. Example of a custom converter implementation public class MyCustomTypeConverter implements GenericConverter { @Override public Set<ConvertiblePair> getConvertibleTypes() { Set<ConvertiblePair> convertiblePairs = new HashSet<>(); convertiblePairs.add(new ConvertiblePair(MyCustomType.class, Value.class)); convertiblePairs.add(new ConvertiblePair(Value.class, MyCustomType.class)); return convertiblePairs; } @Override public Object convert(Object source, TypeDescriptor sourceType, TypeDescriptor targetType) { if (MyCustomType.class.isAssignableFrom(sourceType.getType())) { // convert to Neo4j Driver Value return convertToNeo4jValue(source); } else { // convert to MyCustomType return convertToMyCustomType(source); } } } To make SDN aware of your converter, it has to be registered in the Neo4jConversions . To do this, you have to create a @Bean with the type org.springframework.data.neo4j.core.convert.Neo4jConversions . Otherwise, the Neo4jConversions will get created in the background with the internal default converters only. Example of a custom converter implementation @Bean public Neo4jConversions neo4jConversions() { Set<GenericConverter> additionalConverters = Collections.singleton(new MyCustomTypeConverter()); return new Neo4jConversions(additionalConverters); } If you need multiple converters in your application, you can add as many as you need in the Neo4jConversions constructor. For specific attributes only: If you need conversions only for some specific attributes, we provide @ConvertWith . This is an annotation that can be put on attributes of both entities ( @Node ) and relationship properties ( @RelationshipProperties ) It defines a Neo4jPersistentPropertyConverter via the converter attribute and an optional Neo4jPersistentPropertyConverterFactory to construct the former. With an implementation of Neo4jPersistentPropertyConverter all specific conversions for a given type can be addressed. In addition, @ConvertWith also provides converterRef for referencing any Spring bean in the application context implementing Neo4jPersistentPropertyConverter . The referenced bean will be preferred over constructing a new converter. We provide @DateLong and @DateString as meta-annotated annotations for backward compatibility with Neo4j-OGM schemes not using native types. Those are meta annotated annotations building on the concept above. Composite properties: With @CompositeProperty , attributes of type Map<String, Object> or Map<? extends Enum, Object> can be stored as composite properties. All entries inside the map will be added as properties to the node or relationship containing the property. Either with a configured prefix or prefixed with the name of the property. While we only offer that feature for maps out of the box, you can Neo4jPersistentPropertyToMapConverter and configure it as the converter to use on @CompositeProperty . A Neo4jPersistentPropertyToMapConverter needs to know how a given type can be decomposed to and composed back from a map."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/neo4j-client.html","Neo4jClient: Spring Data Neo4j comes with a Neo4j Client, providing a thin layer on top of Neo4j’s Java driver. While the plain Java driver(https://github.com/neo4j/neo4j-java-driver) is a very versatile tool providing an asynchronous API in addition to the imperative and reactive versions, it doesn’t integrate with Spring application level transactions. SDN uses the driver through the concept of an idiomatic client as directly as possible. The client has the following main goals Integrate into Springs transaction management, for both imperative and reactive scenarios Participate in JTA-Transactions if necessary Provide a consistent API for both imperative and reactive scenarios Don’t add any mapping overhead SDN relies on all those features and uses them to fulfill its entity mapping features. Have a look at the SDN building blocks(../introduction-and-preface/building-blocks.html#sdn-building-blocks) for where both the imperative and reactive Neo4 clients are positioned in our stack. The Neo4j Client comes in two flavors: org.springframework.data.neo4j.core.Neo4jClient org.springframework.data.neo4j.core.ReactiveNeo4jClient While both versions provide an API using the same vocabulary and syntax, they are not API compatible. Both versions feature the same, fluent API to specify queries, bind parameters and extract results. Imperative or reactive?: Interactions with a Neo4j Client usually ends with a call to fetch().one() fetch().first() fetch().all() run() The imperative version will interact at this moment with the database and get the requested results or summary, wrapped in an Optional<> or a Collection . The reactive version will in contrast return a publisher of the requested type. Interaction with the database and retrieval of the results will not happen until the publisher is subscribed to. The publisher can only be subscribed once. Getting an instance of the client: As with most things in SDN, both clients depend on a configured driver instance. Creating an instance of the imperative Neo4j client import org.neo4j.driver.AuthTokens; import org.neo4j.driver.Driver; import org.neo4j.driver.GraphDatabase; import org.springframework.data.neo4j.core.Neo4jClient; public class Demo { public static void main(String...args) { Driver driver = GraphDatabase .driver(""neo4j://localhost:7687"", AuthTokens.basic(""neo4j"", ""secret"")); Neo4jClient client = Neo4jClient.create(driver); } } The driver can only open a reactive session against a 4.0 database and will fail with an exception on any lower version. Creating an instance of the reactive Neo4j client import org.neo4j.driver.AuthTokens; import org.neo4j.driver.Driver; import org.neo4j.driver.GraphDatabase; import org.springframework.data.neo4j.core.ReactiveNeo4jClient; public class Demo { public static void main(String...args) { Driver driver = GraphDatabase .driver(""neo4j://localhost:7687"", AuthTokens.basic(""neo4j"", ""secret"")); ReactiveNeo4jClient client = ReactiveNeo4jClient.create(driver); } } Make sure you use the same driver instance for the client as you used for providing a Neo4jTransactionManager or ReactiveNeo4jTransactionManager in case you have enabled transactions. The client won’t be able to synchronize transactions if you use another instance of a driver. Our Spring Boot starter provide a ready to use bean of the Neo4j Client that fits the environment (imperative or reactive) and you usually don’t have to configure your own instance. Usage: Selecting the target database: The Neo4j client is well prepared to be used with the multidatabase features of Neo4j 4.0. The client uses the default database unless you specify otherwise. The fluent API of the client allows to specify the target database exactly once, after the declaration of the query to execute. Selecting the target database(#neo4j-client-reactive-selecting-the-target-database) demonstrates it with the reactive client: Selecting the target database Flux<Map<String, Object>> allActors = client .query(""MATCH (p:Person) RETURN p"") .in(""neo4j"") (1) .fetch() .all(); 1 Select the target database in which the query is to be executed. Specifying queries: The interaction with the clients starts with a query. A query can be defined by a plain String or a Supplier<String> . The supplier will be evaluated as late as possible and can be provided by any query builder. Specifying a query Mono<Map<String, Object>> firstActor = client .query(() -> ""MATCH (p:Person) RETURN p"") .fetch() .first(); Retrieving results: As the previous listings shows, the interaction with the client always ends with a call to fetch and how many results shall be received. Both reactive and imperative client offer one() Expect exactly one result from the query first() Expect results and return the first record all() Retrieve all records returned The imperative client returns Optional<T> and Collection<T> respectively, while the reactive client returns Mono<T> and Flux<T> , the later one being executed only if subscribed to. If you don’t expect any results from your query, then use run() after specifying the query. Retrieving result summaries in a reactive way Mono<ResultSummary> summary = reactiveClient .query(""MATCH (m:Movie) where m.title = 'Aeon Flux' DETACH DELETE m"") .run(); summary .map(ResultSummary::counters) .subscribe(counters -> System.out.println(counters.nodesDeleted() + "" nodes have been deleted"") ); (1) 1 The actual query is triggered here by subscribing to the publisher. Please take a moment to compare both listings and understand the difference when the actual query is triggered. Retrieving result summaries in an imperative way ResultSummary resultSummary = imperativeClient .query(""MATCH (m:Movie) where m.title = 'Aeon Flux' DETACH DELETE m"") .run(); (1) SummaryCounters counters = resultSummary.counters(); System.out.println(counters.nodesDeleted() + "" nodes have been deleted"") 1 Here the query is immediately triggered. Mapping parameters: Queries can contain named parameters ( $someName ) and the Neo4j client makes it easy to bind values to them. The client doesn’t check whether all parameters are bound or whether there are too many values. That is left to the driver. However, the client prevents you from using a parameter name twice. You can either bind simple types that the Java driver understands without conversion or complex classes. For complex classes you need to provide a binder function as shown in this listing(#neo4j-client-binder) . Please have a look at the drivers manual(https://neo4j.com/docs/driver-manual/current/cypher-workflow/#driver-type-mapping) , to see which simple types are supported. Mapping simple types Map<String, Object> parameters = new HashMap<>(); parameters.put(""name"", ""Li.*""); Flux<Map<String, Object>> directorAndMovies = client .query( ""MATCH (p:Person) - [:DIRECTED] -> (m:Movie {title: $title}), (p) - [:WROTE] -> (om:Movie) "" + ""WHERE p.name =~ $name "" + "" AND p.born < $someDate.year "" + ""RETURN p, om"" ) .bind(""The Matrix"").to(""title"") (1) .bind(LocalDate.of(1979, 9, 21)).to(""someDate"") .bindAll(parameters) (2) .fetch() .all(); 1 There’s a fluent API for binding simple types. 2 Alternatively parameters can be bound via a map of named parameters. SDN does a lot of complex mapping and it uses the same API that you can use from the client. You can provide a Function<T, Map<String, Object>> for any given domain object like an owner of bicycles in Example of a domain type(#neo4j-client-domain-example) to the Neo4j Client to map those domain objects to parameters the driver can understand. Example of a domain type public class Director { private final String name; private final List<Movie> movies; Director(String name, List<Movie> movies) { this.name = name; this.movies = new ArrayList<>(movies); } public String getName() { return name; } public List<Movie> getMovies() { return Collections.unmodifiableList(movies); } } public class Movie { private final String title; public Movie(String title) { this.title = title; } public String getTitle() { return title; } } The mapping function has to fill in all named parameters that might occur in the query like Using a mapping function for binding domain objects(#neo4j-client-binder) shows: Using a mapping function for binding domain objects Director joseph = new Director(""Joseph Kosinski"", Arrays.asList(new Movie(""Tron Legacy""), new Movie(""Top Gun: Maverick""))); Mono<ResultSummary> summary = client .query("""" + ""MERGE (p:Person {name: $name}) "" + ""WITH p UNWIND $movies as movie "" + ""MERGE (m:Movie {title: movie}) "" + ""MERGE (p) - [o:DIRECTED] -> (m) "" ) .bind(joseph).with(director -> { (1) Map<String, Object> mappedValues = new HashMap<>(); List<String> movies = director.getMovies().stream() .map(Movie::getTitle).collect(Collectors.toList()); mappedValues.put(""name"", director.getName()); mappedValues.put(""movies"", movies); return mappedValues; }) .run(); 1 The with method allows for specifying the binder function. Working with result objects: Both clients return collections or publishers of maps ( Map<String, Object> ). Those maps correspond exactly with the records that a query might have produced. In addition, you can plug in your own BiFunction<TypeSystem, Record, T> through fetchAs to reproduce your domain object. Using a mapping function for reading domain objects Mono<Director> lily = client .query("""" + "" MATCH (p:Person {name: $name}) - [:DIRECTED] -> (m:Movie)"" + ""RETURN p, collect(m) as movies"") .bind(""Lilly Wachowski"").to(""name"") .fetchAs(Director.class).mappedBy((TypeSystem t, Record record) -> { List<Movie> movies = record.get(""movies"") .asList(v -> new Movie((v.get(""title"").asString()))); return new Director(record.get(""name"").asString(), movies); }) .one(); TypeSystem gives access to the types the underlying Java driver used to fill the record. Using domain-aware mapping functions: If you know that the result of the query will contain nodes that have entity definitions in your application, you can use the injectable MappingContext to retrieve their mapping functions and apply them during the mapping. Using an existing mapping function BiFunction<TypeSystem, MapAccessor, Movie> mappingFunction = neo4jMappingContext.getRequiredMappingFunctionFor(Movie.class); Mono<Director> lily = client .query("""" + "" MATCH (p:Person {name: $name}) - [:DIRECTED] -> (m:Movie)"" + ""RETURN p, collect(m) as movies"") .bind(""Lilly Wachowski"").to(""name"") .fetchAs(Director.class).mappedBy((TypeSystem t, Record record) -> { List<Movie> movies = record.get(""movies"") .asList(movie -> mappingFunction.apply(t, movie)); return new Director(record.get(""name"").asString(), movies); }) .one(); Interacting directly with the driver while using managed transactions: In case you don’t want or don’t like the opinionated ""client"" approach of the Neo4jClient or the ReactiveNeo4jClient , you can have the client delegate all interactions with the database to your code. The interaction after the delegation is slightly different with the imperative and reactive versions of the client. The imperative version takes in a Function<StatementRunner, Optional<T>> as a callback. Returning an empty optional is ok. Delegate database interaction to an imperative StatementRunner Optional<Long> result = client .delegateTo((StatementRunner runner) -> { // Do as many interactions as you want long numberOfNodes = runner.run(""MATCH (n) RETURN count(n) as cnt"") .single().get(""cnt"").asLong(); return Optional.of(numberOfNodes); }) // .in(""aDatabase"") (1) .run(); 1 The database selection as described in Selecting the target database(#neo4j-client-selecting-the-target-database) is optional. The reactive version receives a RxStatementRunner . Delegate database interaction to a reactive RxStatementRunner Mono<Integer> result = client .delegateTo((RxStatementRunner runner) -> Mono.from(runner.run(""MATCH (n:Unused) DELETE n"").summary()) .map(ResultSummary::counters) .map(SummaryCounters::nodesDeleted)) // .in(""aDatabase"") (1) .run(); 1 Optional selection of the target database. Note that in both Delegate database interaction to an imperative StatementRunner(#neo4j-client-imperative-delegating) and Delegate database interaction to a reactive RxStatementRunner(#neo4j-client-reactive-delegating) the types of the runner have only been stated to provide more clarity to reader of this manual."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/logging.html","Logging: Spring Data Neo4j provides multiple loggers for Cypher notifications(https://neo4j.com/docs/status-codes/current/notifications/all-notifications/) , starting with version 7.1.5. The logger org.springframework.data.neo4j.cypher includes all statements that were invoked by Spring Data Neo4j and all notifications sent from the server. To exclude or elevate some categories, the following loggers are in place: org.springframework.data.neo4j.cypher.performance org.springframework.data.neo4j.cypher.hint org.springframework.data.neo4j.cypher.unrecognized org.springframework.data.neo4j.cypher.unsupported org.springframework.data.neo4j.cypher.deprecation org.springframework.data.neo4j.cypher.generic org.springframework.data.neo4j.cypher.security org.springframework.data.neo4j.cypher.topology"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/query-creation.html","Query creation: This chapter is about the technical creation of queries when using SDN’s abstraction layers. There will be some simplifications because we do not discuss every possible case but stick with the general idea behind it. Save: Beside the find/load operations the save operation is one of the most used when working with data. A save operation call in general issues multiple statements against the database to ensure that the resulting graph model matches the given Java model. A union statement will get created that either creates a node, if the node’s identifier cannot be found, or updates the node’s property if the node itself exists. ( OPTIONAL MATCH (hlp:Person) WHERE id(hlp) = $__id__ WITH hlp WHERE hlp IS NULL CREATE (n:Person) SET n = $__properties__ RETURN id(n) UNION MATCH (n) WHERE id(n) = $__id__ SET n = $__properties__ RETURN id(n) ) If the entity is not new all relationships of the first found type at the domain model will get removed from the database. ( MATCH (startNode)-[rel:Has]→(:Hobby) WHERE id(startNode) = $fromId DELETE rel ) The related entity will get created in the same way as the root entity. ( OPTIONAL MATCH (hlp:Hobby) WHERE id(hlp) = $__id__ WITH hlp WHERE hlp IS NULL CREATE (n:Hobby) SET n = $__properties__ RETURN id(n) UNION MATCH (n) WHERE id(n) = $__id__ SET n = $__properties__ RETURN id(n) ) The relationship itself will get created ( MATCH (startNode) WHERE id(startNode) = $fromId MATCH (endNode) WHERE id(endNode) = 631 MERGE (startNode)-[:Has]→(endNode) ) If the related entity also has relationships to other entities, the same procedure as in 2. will get started. For the next defined relationship on the root entity start with 2. but replace first with next . As you can see SDN does its best to keep your graph model in sync with the Java world. This is one of the reasons why we really advise you to not load, manipulate and save sub-graphs as this might cause relationships to get removed from the database. Multiple entities: The save operation is overloaded with the functionality for accepting multiple entities of the same type. If you are working with generated id values or make use of optimistic locking, every entity will result in a separate CREATE call. In other cases SDN will create a parameter list with the entity information and provide it with a MERGE call. UNWIND $__entities__ AS entity MERGE (n:Person {customId: entity.$__id__}) SET n = entity.__properties__ RETURN collect(n.customId) AS $__ids__ and the parameters look like :params {__entities__: [{__id__: 'aa', __properties__: {name: ""PersonName"", theId: ""aa""}}, {__id__ 'bb', __properties__: {name: ""AnotherPersonName"", theId: ""bb""}}]} Load: The load documentation will not only show you how the MATCH part of the query looks like but also how the data gets returned. The simplest kind of load operation is a findById call. It will match all nodes with the label of the type you queried for and does a filter on the id value. MATCH (n:Person) WHERE id(n) = 1364 If there is a custom id provided SDN will use the property you have defined as the id. MATCH (n:Person) WHERE n.customId = 'anId' The data to return is defined as a map projection(https://neo4j.com/docs/cypher-manual/current/syntax/maps/#cypher-map-projection) . RETURN n{.first_name, .personNumber, __internalNeo4jId__: id(n), __nodeLabels__: labels(n)} As you can see there are two special fields in there: The __internalNeo4jId__ and the __nodeLabels__ . Both are critical when it comes to mapping the data to Java objects. The value of the __internalNeo4jId__ is either id(n) or the provided custom id but in the mapping process one known field to refer to has to exist. The __nodeLabels__ ensures that all defined labels on this node can be found and mapped. This is needed for situations when inheritance is used and you query not for the concrete classes or have relationships defined that only define a super-type. Talking about relationships: If you have defined relationships in your entity, they will get added to the returned map as pattern comprehensions(https://neo4j.com/docs/cypher-manual/4.0/syntax/lists/#cypher-pattern-comprehension) . The above return part will then look like: RETURN n{.first_name, …​, Person_Has_Hobby: [(n)-[:Has]→(n_hobbies:Hobby)|n_hobbies{__internalNeo4jId__: id(n_hobbies), .name, nodeLabels : labels(n_hobbies)}]} The map projection and pattern comprehension used by SDN ensures that only the properties and relationships you have defined are getting queried. In cases where you have self-referencing nodes or creating schemas that potentially lead to cycles in the data that gets returned, SDN falls back to a cascading / data-driven query creation. Starting with an initial query that looks for the specific node and considering the conditions, it steps through the resulting nodes and, if their relationships are also mapped, would create further queries on the fly. This query creation and execution loop will continue until no query finds new relationships or nodes. The way of the creation can be seen analogue to the save/update process."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/custom-queries.html","Custom queries: Spring Data Neo4j, like all the other Spring Data modules, allows you to specify custom queries in you repositories. Those come in handy if you cannot express the finder logic via derived query functions. Because Spring Data Neo4j works heavily record-oriented under the hood, it is important to keep this in mind and not build up a result set with multiple records for the same ""root node"". Please have a look in the FAQ as well to learn about alternative forms of using custom queries from repositories, especially how to use custom queries with custom mappings: Custom queries and custom mappings(../faq.html#faq.custom-queries-and-custom-mappings) . Queries with relationships: Beware of the cartesian product: Assuming you have a query like MATCH (m:Movie{title: 'The Matrix'})←[r:ACTED_IN]-(p:Person) return m,r,p that results into something like this: Multiple records (shortened) +------------------------------------------------------------------------------------------+ | m | r | p | +------------------------------------------------------------------------------------------+ | (:Movie) | [:ACTED_IN {roles: [""Emil""]}] | (:Person {name: ""Emil Eifrem""}) | | (:Movie) | [:ACTED_IN {roles: [""Agent Smith""]}] | (:Person {name: ""Hugo Weaving}) | | (:Movie) | [:ACTED_IN {roles: [""Morpheus""]}] | (:Person {name: ""Laurence Fishburne""}) | | (:Movie) | [:ACTED_IN {roles: [""Trinity""]}] | (:Person {name: ""Carrie-Anne Moss""}) | | (:Movie) | [:ACTED_IN {roles: [""Neo""]}] | (:Person {name: ""Keanu Reeves""}) | +------------------------------------------------------------------------------------------+ The result from the mapping would be most likely unusable. If this would get mapped into a list, it will contain duplicates for the Movie but this movie will only have one relationship. Getting one record per root node: To get the right object(s) back, it is required to collect the relationships and related nodes in the query: MATCH (m:Movie{title: 'The Matrix'})←[r:ACTED_IN]-(p:Person) return m,collect(r),collect(p) Single record (shortened) +------------------------------------------------------------------------+ | m | collect(r) | collect(p) | +------------------------------------------------------------------------+ | (:Movie) | [[:ACTED_IN], [:ACTED_IN], ...]| [(:Person), (:Person),...] | +------------------------------------------------------------------------+ With this result as a single record it is possible for Spring Data Neo4j to add all related nodes correctly to the root node. Reaching deeper into the graph: The example above assumes that you are only trying to fetch the first level of related nodes. This is sometimes not enough and there are maybe nodes deeper in the graph that should also be part of the mapped instance. There are two ways to achieve this: Database-side or client-side reduction. For this the example from above should also contain Movies on the Persons that get returned with the initial Movie . Figure 1. Example for 'The Matrix' and 'Keanu Reeves' Database-side reduction: Keeping in mind that Spring Data Neo4j can only properly process record based, the result for one entity instance needs to be in one record. Using Cypher’s path(https://neo4j.com/docs/cypher-manual/current/syntax/patterns/#cypher-pattern-path-variables) capabilities is a valid option to fetch all branches in the graph. Naive path-based approach MATCH p=(m:Movie{title: 'The Matrix'})<-[:ACTED_IN]-(:Person)-[:ACTED_IN*..0]->(:Movie) RETURN p; This will result in multiple paths that are not merged within one record. It is possible to call collect(p) but Spring Data Neo4j does not understand the concept of paths in the mapping process. Thus, nodes and relationships needs to get extracted for the result. Extracting nodes and relationships MATCH p=(m:Movie{title: 'The Matrix'})<-[:ACTED_IN]-(:Person)-[:ACTED_IN*..0]->(:Movie) RETURN m, nodes(p), relationships(p); Because there are multiple paths that lead from 'The Matrix' to another movie, the result still won’t be a single record. This is where Cypher’s reduce function(https://neo4j.com/docs/cypher-manual/current/functions/list/#functions-reduce) comes into play. Reducing nodes and relationships MATCH p=(m:Movie{title: 'The Matrix'})<-[:ACTED_IN]-(:Person)-[:ACTED_IN*..0]->(:Movie) WITH collect(p) as paths, m WITH m, reduce(a=[], node in reduce(b=[], c in [aa in paths | nodes(aa)] | b + c) | case when node in a then a else a + node end) as nodes, reduce(d=[], relationship in reduce(e=[], f in [dd in paths | relationships(dd)] | e + f) | case when relationship in d then d else d + relationship end) as relationships RETURN m, relationships, nodes; The reduce function allows us to flatten the nodes and relationships from various paths. As a result we will get a tuple similar to Getting one record per root node(#custom-queries.for-relationships.one.record) but with a mixture of relationship types or nodes in the collections. Client-side reduction: If the reduction should happen on the client-side, Spring Data Neo4j enables you to map also lists of lists of relationships or nodes. Still, the requirement applies that the returned record should contain all information to hydrate the resulting entity instance correctly. Collect nodes and relationships from path MATCH p=(m:Movie{title: 'The Matrix'})<-[:ACTED_IN]-(:Person)-[:ACTED_IN*..0]->(:Movie) RETURN m, collect(nodes(p)), collect(relationships(p)); The additional collect statement creates lists in the format: [[rel1, rel2], [rel3, rel4]] Those lists will now get converted during the mapping process into a flat list. Deciding if you want to go with client-side or database-side reduction depends on the amount of data that will get generated. All the paths needs to get created in the database’s memory first when the reduce function is used. On the other hand a large amount of data that needs to get merged on the client-side results in a higher memory usage there. Using paths to populate and return a list of entities: Given are a graph that looks like this: Figure 2. graph with outgoing relationships and a domain model as shown in the mapping(#custom-query.paths.dm) (Constructors and accessors have been omitted for brevity): Domain model for a graph with outgoing relationships(#custom-query.paths.g) . @Node public class SomeEntity { @Id private final Long number; private String name; @Relationship(type = ""SOME_RELATION_TO"", direction = Relationship.Direction.OUTGOING) private Set<SomeRelation> someRelationsOut = new HashSet<>(); } @RelationshipProperties public class SomeRelation { @RelationshipId private Long id; private String someData; @TargetNode private SomeEntity targetPerson; } As you see, the relationships are only outgoing. Generated finder methods (including findById ) will always try to match a root node to be mapped. From there on onwards, all related objects will be mapped. In queries that should return only one object, that root object is returned. In queries that return many objects, all matching objects are returned. Out- and incoming relationships from those objects returned are of course populated. Assume the following Cypher query: MATCH p = (leaf:SomeEntity {number: $a})-[:SOME_RELATION_TO*]-(:SomeEntity) RETURN leaf, collect(nodes(p)), collect(relationships(p)) It follows the recommendation from Getting one record per root node(#custom-queries.for-relationships.one.record) and it works great for the leaf node you want to match here. However: That is only the case in all scenarios that return 0 or 1 mapped objects. While that query will populate all relationships like before, it won’t return all 4 objects. This can be changed by returning the whole path: MATCH p = (leaf:SomeEntity {number: $a})-[:SOME_RELATION_TO*]-(:SomeEntity) RETURN p Here we do want to use the fact that the path p actually returns 3 rows with paths to all 4 nodes. All 4 nodes will be populated, linked together and returned. Parameters in custom queries: You do this exactly the same way as in a standard Cypher query issued in the Neo4j Browser or the Cypher-Shell, with the $ syntax (from Neo4j 4.0 on upwards, the old ${foo} syntax for Cypher parameters has been removed from the database). ARepository.java public interface ARepository extends Neo4jRepository<AnAggregateRoot, String> { @Query(""MATCH (a:AnAggregateRoot {name: $name}) RETURN a"") (1) Optional<AnAggregateRoot> findByCustomQuery(String name); } 1 Here we are referring to the parameter by its name. You can also use $0 etc. instead. You need to compile your Java 8+ project with -parameters to make named parameters work without further annotations. The Spring Boot Maven and Gradle plugins do this automatically for you. If this is not feasible for any reason, you can either add @Param and specify the name explicitly or use the parameters index. Mapped entities (everything with a @Node ) passed as parameter to a function that is annotated with a custom query will be turned into a nested map. The following example represents the structure as Neo4j parameters. Given are a Movie , Vertex and Actor classes annotated as shown in the movie model(#movie-model) : ""Standard"" movies model @Node public final class Movie { @Id private final String title; @Property(""tagline"") private final String description; @Relationship(value = ""ACTED_IN"", direction = Direction.INCOMING) private final List<Actor> actors; @Relationship(value = ""DIRECTED"", direction = Direction.INCOMING) private final List<Person> directors; } @Node public final class Person { @Id @GeneratedValue private final Long id; private final String name; private Integer born; @Relationship(""REVIEWED"") private List<Movie> reviewed = new ArrayList<>(); } @RelationshipProperties public final class Actor { @RelationshipId private final Long id; @TargetNode private final Person person; private final List<String> roles; } interface MovieRepository extends Neo4jRepository<Movie, String> { @Query(""MATCH (m:Movie {title: $movie.__id__})\n"" + ""MATCH (m) <- [r:DIRECTED|REVIEWED|ACTED_IN] - (p:Person)\n"" + ""return m, collect(r), collect(p)"") Movie findByMovie(@Param(""movie"") Movie movie); } Passing an instance of Movie to the repository method above, will generate the following Neo4j map parameter: { ""movie"": { ""__labels__"": [ ""Movie"" ], ""__id__"": ""The Da Vinci Code"", ""__properties__"": { ""ACTED_IN"": [ { ""__properties__"": { ""roles"": [ ""Sophie Neveu"" ] }, ""__target__"": { ""__labels__"": [ ""Person"" ], ""__id__"": 402, ""__properties__"": { ""name"": ""Audrey Tautou"", ""born"": 1976 } } }, { ""__properties__"": { ""roles"": [ ""Sir Leight Teabing"" ] }, ""__target__"": { ""__labels__"": [ ""Person"" ], ""__id__"": 401, ""__properties__"": { ""name"": ""Ian McKellen"", ""born"": 1939 } } }, { ""__properties__"": { ""roles"": [ ""Dr. Robert Langdon"" ] }, ""__target__"": { ""__labels__"": [ ""Person"" ], ""__id__"": 360, ""__properties__"": { ""name"": ""Tom Hanks"", ""born"": 1956 } } }, { ""__properties__"": { ""roles"": [ ""Silas"" ] }, ""__target__"": { ""__labels__"": [ ""Person"" ], ""__id__"": 403, ""__properties__"": { ""name"": ""Paul Bettany"", ""born"": 1971 } } } ], ""DIRECTED"": [ { ""__labels__"": [ ""Person"" ], ""__id__"": 404, ""__properties__"": { ""name"": ""Ron Howard"", ""born"": 1954 } } ], ""tagline"": ""Break The Codes"", ""released"": 2006 } } } A node is represented by a map. The map will always contain id which is the mapped id property. Under labels all labels, static and dynamic, will be available. All properties - and type of relationships - appear in those maps as they would appear in the graph when the entity would have been written by SDN. Values will have the correct Cypher type and won’t need further conversion. All relationships are lists of maps. Dynamic relationships will be resolved accordingly. One-to-one relationships will also be serialized as singleton lists. So to access a one-to-one mapping between people, you would write this das $person.__properties__.BEST_FRIEND[0].__target__.__id__ . If an entity has a relationship with the same type to different types of others nodes, they will all appear in the same list. If you need such a mapping and also have the need to work with those custom parameters, you have to unroll it accordingly. One way to do this are correlated subqueries (Neo4j 4.1+ required). Spring Expression Language in custom queries: Spring Expression Language (SpEL)(https://docs.spring.io/spring-framework/reference/6.1/core/expressions.html) can be used in custom queries inside :#{} . The colon here refers to a parameter and such an expression should be used where parameters make sense. However, when using our literal extension(#literal-extension) you can use SpEL expression in places where standard Cypher won’t allow parameters (such as for labels or relationship types). This is the standard Spring Data way of defining a block of text inside a query that undergoes SpEL evaluation. The following example basically defines the same query as above, but uses a WHERE clause to avoid even more curly braces: ARepository.java public interface ARepository extends Neo4jRepository<AnAggregateRoot, String> { @Query(""MATCH (a:AnAggregateRoot) WHERE a.name = :#{#pt1 + #pt2} RETURN a"") Optional<AnAggregateRoot> findByCustomQueryWithSpEL(String pt1, String pt2); } The SpEL blocked starts with :#{ and then refers to the given String parameters by name ( #pt1 ). Don’t confuse this with the above Cypher syntax! The SpEL expression concatenates both parameters into one single value that is eventually passed on to the appendix/neo4j-client.adoc#neo4j-client(neo4j-client.html#neo4j-client) . The SpEL block ends with } . SpEL also solves two additional problems. We provide two extensions that allow to pass in a Sort object into custom queries. Remember faq.adoc#custom-queries-with-page-and-slice-examples(../faq.html#custom-queries-with-page-and-slice-examples) from custom queries(../faq.html#faq.custom-queries-with-page-and-slice) ? With the orderBy extension you can pass in a Pageable with a dynamic sort to a custom query: orderBy-Extension import org.springframework.data.domain.Pageable; import org.springframework.data.domain.Sort; import org.springframework.data.neo4j.repository.Neo4jRepository; import org.springframework.data.neo4j.repository.query.Query; public interface MyPersonRepository extends Neo4jRepository<Person, Long> { @Query("""" + ""MATCH (n:Person) WHERE n.name = $name RETURN n "" + "":#{orderBy(#pageable)} SKIP $skip LIMIT $limit"" (1) ) Slice<Person> findSliceByName(String name, Pageable pageable); @Query("""" + ""MATCH (n:Person) WHERE n.name = $name RETURN n :#{orderBy(#sort)}"" (2) ) List<Person> findAllByName(String name, Sort sort); } 1 A Pageable has always the name pageable inside the SpEL context. 2 A Sort has always the name sort inside the SpEL context. Spring Expression Language extensions: Literal extension: The literal extension can be used to make things like labels or relationship-types ""dynamic"" in custom queries. Neither labels nor relationship types can be parameterized in Cypher, so they must be given literal. literal-Extension interface BaseClassRepository extends Neo4jRepository<Inheritance.BaseClass, Long> { @Query(""MATCH (n:`:#{literal(#label)}`) RETURN n"") (1) List<Inheritance.BaseClass> findByLabel(String label); } 1 The literal extension will be replaced with the literal value of the evaluated parameter. Here, the literal value has been used to match dynamically on a Label. If you pass in SomeLabel as a parameter to the method, MATCH (n: SomeLabel ) RETURN n will be generated. Ticks have been added to correctly escape values. SDN won’t do this for you as this is probably not what you want in all cases. List extensions: For more than one value there are allOf and anyOf in place that would render either a & or | concatenated list of all values. List extensions interface BaseClassRepository extends Neo4jRepository<Inheritance.BaseClass, Long> { @Query(""MATCH (n:`:#{allOf(#label)}`) RETURN n"") List<Inheritance.BaseClass> findByLabels(List<String> labels); @Query(""MATCH (n:`:#{anyOf(#label)}`) RETURN n"") List<Inheritance.BaseClass> findByLabels(List<String> labels); } Referring to Labels: You already know how to map a Node to a domain object: A Node with many labels @Node(primaryLabel = ""Bike"", labels = {""Gravel"", ""Easy Trail""}) public class BikeNode { @Id String id; String name; } This node has a couple of labels, and it would be rather error prone to repeat them all the time in custom queries: You might forget one or make a typo. We offer the following expression to mitigate this: #{#staticLabels} . Notice that this one does not start with a colon! You use it on repository methods annotated with @Query : #{#staticLabels} in action public interface BikeRepository extends Neo4jRepository<Bike, String> { @Query(""MATCH (n:#{#staticLabels}) WHERE n.id = $nameOrId OR n.name = $nameOrId RETURN n"") Optional<Bike> findByNameOrId(@Param(""nameOrId"") String nameOrId); } This query will resolve to MATCH (n:`Bike`:`Gravel`:`Easy Trail`) WHERE n.id = $nameOrId OR n.name = $nameOrId RETURN n Notice how we used standard parameter for the nameOrId : In most cases there is no need to complicate things here by adding a SpEL expression."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/spatial-types.html","Spatial types: Spring Data Neo4j supports the following spatial types Supported conversions: Spring Data common’s Point ( must be a WGS 84-2D/SRID 4326 point in the database) GeographicPoint2d (WGS84 2D/SRID 4326) GeographicPoint3d (WGS84 3D/SRID 4979) CartesianPoint2d (Cartesian 2D/SRID 7203) CartesianPoint3d (Cartesian 3D/SRID 9157) Derived finder keywords: If you are using the native Neo4j Java driver org.neo4j.driver.types.Point type, you can make use of the following keywords and parameter types in derived finder methods. Query inside an area: findBy[…​]Within(org.springframework.data.geo.Circle circle) findBy[…​]Within(org.springframework.data.geo.Box box) findBy[…​]Within(org.springframework.data.neo4j.repository.query.BoundingBox boundingBox) You could also use a org.springframework.data.geo.Polygon but would need to pass it into a BoundingBox by calling BoundingBox#of . Query near a certain point: findBy[…​]Near(org.neo4j.driver.types.Point point) - returns result sorted by distance to the given point ascending findBy[…​]Near(Point point, org.springframework.data.geo.Distance max) findBy[…​]Near(Point point, org.springframework.data.domain.Range<Distance> between) findBy[…​]Near(Range<Distance> between, Point p)"
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/migrating.html","Migrating from SDN+OGM to SDN: Known issues with past SDN+OGM migrations: SDN+OGM has had quite a history over the years and we understand that migrating big application systems is neither fun nor something that provides immediate profit. The main issues we observed when migrating from older versions of Spring Data Neo4j to newer ones are roughly in order the following: Having skipped more than one major upgrade While Neo4j-OGM can be used stand-alone, Spring Data Neo4j cannot. It depends to large extend on the Spring Data and therefore, on the Spring Framework itself, which eventually affects large parts of your application. Depending on how the application has been structured, that is, how much the any of the framework part leaked into your business code, the more you have to adapt your application. It gets worse when you have more than one Spring Data module in your application, if you accessed a relational database in the same service layer as your graph database. Updating two object mapping frameworks is not fun. Relying on an embedded database configured through Spring Data itself The embedded database in a SDN+OGM project is configured by Neo4j-OGM. Say you want to upgrade from Neo4j 3.0 to 3.5, you can’t without upgrading your whole application. Why is that? As you chose to embed a database into your application, you tied yourself into the modules that configure this embedded database. To have another, embedded database version, you have to upgrade the module that configured it, because the old one does not support the new database. As there is always a Spring Data version corresponding to Neo4j-OGM, you would have to upgrade that as well. Spring Data however depends on Spring Framework and then the arguments from the first bullet apply. Being unsure about which building blocks to include It’s not easy to get the terms right. We wrote the building blocks of an SDN+OGM setting here(https://michael-simons.github.io/neo4j-examples-and-tips/what_are_the_building_blocks_of_sdn_and_ogm.html) . It may be so that all of them have been added by coincidence and you’re dealing with a lot of conflicting dependencies. Backed by those observations, we recommend to make sure you’re using only the Bolt or http transport in your current application before switching from SDN+OGM to SDN. Thus, your application and the access layer of your application is to a large extent independent of the database’s version. From that state, consider moving from SDN+OGM to SDN. Prepare the migration from SDN+OGM Lovelace or SDN+OGM Moore to SDN: The Lovelace release train corresponds to SDN 5.1.x and OGM 3.1.x, while the Moore is SDN 5.2.x and OGM 3.2.x. First, you must make sure that your application runs against Neo4j in server mode over the Bolt protocol, which means work in two of three cases: You’re on embedded: You have added org.neo4j:neo4j-ogm-embedded-driver and org.neo4j:neo4j to you project and starting the database via OGM facilities. This is no longer supported and you have to set up a standard Neo4j server (both standalone and cluster are supported). The above dependencies have to be removed. Migrating from the embedded solution is probably the toughest migration, as you need to set up a server, too. It is however the one that gives you much value in itself: In the future, you will be able to upgrade the database itself without having to consider your application framework, and your data access framework as well. You’re using the HTTP transport: You have added org.neo4j:neo4j-ogm-http-driver and configured an url like user:password@localhost:7474(http://user:password@localhost:7474) . The dependency has to be replaced with org.neo4j:neo4j-ogm-bolt-driver and you need to configure a Bolt url like bolt://localhost:7687 or use the new neo4j:// scheme, which takes care of routing, too. You’re already using Bolt indirectly: A default SDN+OGM project uses org.neo4j:neo4j-ogm-bolt-driver and thus indirectly, the pure Java Driver. You can keep your existing URL. Migrating: Once you have made sure, that your SDN+OGM application works over Bolt as expected, you can start migrating to SDN. Remove all org.neo4j:neo4j-ogm-* dependencies Configuring SDN through a org.neo4j.ogm.config.Configuration bean is not supported, instead of, all configuration of the driver goes through our new Java driver starter. You will especially have to adapt the properties for the url and authentication, see Old and new properties compared(#migrating-auth) You cannot configure SDN through XML. In case you did this with your SDN+OGM application, make sure you learn about annotation-driven or functional configuration of Spring Applications. The easiest choice these days is Spring Boot. With our starter in place, all the necessary bits apart from the connection URL and the authentication is already configured for you. Old and new properties compared # Old spring.data.neo4j.embedded.enabled=false # No longer supported spring.data.neo4j.uri=bolt://localhost:7687 spring.data.neo4j.username=neo4j spring.data.neo4j.password=secret # New spring.neo4j.uri=bolt://localhost:7687 spring.neo4j.authentication.username=neo4j spring.neo4j.authentication.password=secret Those new properties might change in the future again when SDN and the driver eventually fully replace the old setup. And finally, add the new dependency, see Getting started(../getting-started.html) for both Gradle and Maven. You’re then ready to replace annotations: Old New org.neo4j.ogm.annotation.NodeEntity org.springframework.data.neo4j.core.schema.Node org.neo4j.ogm.annotation.GeneratedValue org.springframework.data.neo4j.core.schema.GeneratedValue org.neo4j.ogm.annotation.Id org.springframework.data.neo4j.core.schema.Id org.neo4j.ogm.annotation.Property org.springframework.data.neo4j.core.schema.Property org.neo4j.ogm.annotation.Relationship org.springframework.data.neo4j.core.schema.Relationship org.springframework.data.neo4j.annotation.EnableBookmarkManagement No replacement, not needed org.springframework.data.neo4j.annotation.UseBookmark No replacement, not needed org.springframework.data.neo4j.annotation.QueryResult Use projections(../repositories/projections.html) ; arbitrary result mapping not supported anymore Several Neo4j-OGM annotations have not yet a corresponding annotation in SDN, some will never have. We will add to the list above as we support additional features. Bookmark management: Both @EnableBookmarkManagement and @UseBookmark as well as the org.springframework.data.neo4j.bookmark.BookmarkManager interface and its only implementation org.springframework.data.neo4j.bookmark.CaffeineBookmarkManager are gone and are not needed anymore. SDN uses bookmarks for all transactions, without configuration. You can remove the bean declaration of CaffeineBookmarkManager as well as the dependency to com.github.ben-manes.caffeine:caffeine . If you absolutely must, you can disable the automatic bookmark management by following these instructions(../faq.html#faq.bookmarks.noop) . Automatic creation of constraints and indexes: SDN 5.3 and prior provided the ""Automatic index manager"" from Neo4j-OGM. @Index , @CompositeIndex and @Required have been removed without replacement. Why? We think that creating the schema - even for a schemaless database - is not part of the domain modelling. You could argue that an SDN model is the schema, but than we would answer that we even prefer a Command-query separation(https://en.wikipedia.org/wiki/Command–query_separation) , meaning that we would rather define separate read and write models. Those come in very handy for writing ""boring"" things and reading graph-shaped answers. Apart from that, some of those annotations respectively their values are tied to specific Neo4j editions or versions, which makes them hard to maintain. The best argument however is going to production: While all tools that generate a schema are indeed helpful during development, even more so with databases that enforces a strict scheme, they tend to be not so nice in production: How do you handle different versions of your application running at the same time? Version A asserting the indexes that have been created by a newer version B? We think it’s better to take control about this upfront and recommend using controlled database migrations, based on a tool like Liquigraph(https://www.liquigraph.org) or Neo4j migrations(https://github.com/michael-simons/neo4j-migrations) . The latter has been seen in use with SDN inside the JHipster project. Both projects have in common that they store the current version of the schema within the database and make sure that a schema matches expectations before things are being updated. Migrating off from previous Neo4j-OGM annotations affects @Index , @CompositeIndex and @Required and an example for those is given here in A class making use of Neo4j-OGM automatic index manager(#indexed.class) : A class making use of Neo4j-OGM automatic index manager import org.neo4j.ogm.annotation.CompositeIndex; import org.neo4j.ogm.annotation.GeneratedValue; import org.neo4j.ogm.annotation.Id; import org.neo4j.ogm.annotation.Index; import org.neo4j.ogm.annotation.Required; @CompositeIndex(properties = {""tagline"", ""released""}) public class Movie { @Id @GeneratedValue Long id; @Index(unique = true) private String title; private String description; private String tagline; @Required private Integer released; } It’s annotations are equivalent to the following scheme in Cypher (as of Neo4j 4.2): Example Cypher based migration CREATE CONSTRAINT movies_unique_title ON (m:Movie) ASSERT m.title IS UNIQUE; CREATE CONSTRAINT movies_released_exists ON (m:Movie) ASSERT EXISTS (m.released); CREATE INDEX movies_tagline_released_idx FOR (m:Movie) ON (m.tagline, m.released); Using @Index without unique = true is equivalent to CREATE INDEX movie_title_index FOR (m:Movie) ON (m.title) . Note that a unique index already implies an index."
"https://docs.spring.io/spring-data/neo4j/reference/7.3/appendix/build.html","Building Spring Data Neo4j: Requirements: JDK 17+ (Can be OpenJDK(https://openjdk.java.net) or Oracle JDK(https://www.oracle.com/technetwork/java/index.html) ) Maven 3.8.5 (We provide the Maven wrapper, see mvnw respectively mvnw.cmd in the project root; the wrapper downloads the appropriate Maven version automatically) A Neo4j 5.+ database, either running locally or indirectly via Testcontainers(https://www.testcontainers.org) and Docker(https://www.docker.com) About the JDK version: Choosing JDK 17 is a decision influenced by various aspects SDN is a Spring Data project. Spring Data commons baseline is JDK 17 and so is Spring Framework’s baseline. Thus, it is only natural to keep the JDK 17 baseline. Running the build: The following sections are alternatives and roughly sorted by increased effort. All builds require a local copy of the project: Clone SDN $ git clone [email protected](/cdn-cgi/l/email-protection) :spring-projects/spring-data-neo4j.git Before you proceed, verify your locally installed JDK version. The output should be similar: Verify your JDK $ java -version java version ""18.0.1"" 2022-04-19 Java(TM) SE Runtime Environment (build 18.0.1+10-24) Java HotSpot(TM) 64-Bit Server VM (build 18.0.1+10-24, mixed mode, sharing) With Docker installed: Using the default image: If you don’t have Docker(https://en.wikipedia.org/wiki/Docker_(software)) installed, head over to Docker Desktop(https://www.docker.com/products/docker-desktop) . In short, Docker is a tool that helps you running lightweight software images using OS-level virtualization in so-called containers. Our build uses Testcontainers Neo4j(https://www.testcontainers.org/modules/databases/neo4j/) to bring up a database instance. Build with default settings on Linux / macOS $ ./mvnw clean verify On a Windows machine, use Build with default settings on Windows $ mvnw.cmd clean verify The output should be similar. Using another image: The image version to use can be configured through an environmental variable like this: Build using a different Neo4j Docker image $ SDN_NEO4J_VERSION=5.3.0-enterprise SDN_NEO4J_ACCEPT_COMMERCIAL_EDITION=yes ./mvnw clean verify Here we are using 5.3.0 enterprise and also accept the license agreement. Consult your operating system or shell manual on how to define environment variables if specifying them inline does not work for you. Against a locally running database: Running against a locally running database will erase its complete content. Building against a locally running database is faster, as it does not restart a container each time. We do this a lot during our development. You can get a copy of Neo4j at our download center(https://neo4j.com/download-center/#enterprise) free of charge. Please download the version applicable to your operating system and follow the instructions to start it. A required step is to open a browser and go to localhost:7474(http://localhost:7474) after you started the database and change the default password from neo4j to something of your liking. After that, you can run a complete build by specifying the local bolt URL: Build using a locally running database $ SDN_NEO4J_URL=bolt://localhost:7687 SDN_NEO4J_PASSWORD=verysecret ./mvnw clean verify Summary of environment variables controlling the build: Name Default value Meaning SDN_NEO4J_VERSION 5.3.0 Version of the Neo4j docker image to use, see Neo4j Docker Official Images(https://hub.docker.com/_/neo4j) SDN_NEO4J_ACCEPT_COMMERCIAL_EDITION no Some tests may require the enterprise edition of Neo4j. We build and test against the enterprise edition internally, but we won’t force you to accept the license if you don’t want to. SDN_NEO4J_URL not set Setting this environment allows connecting to a locally running Neo4j instance. We use this a lot during development. SDN_NEO4J_PASSWORD not set Password for the neo4j user of the instance configured with SDN_NEO4J_URL . You need to set both SDN_NEO4J_URL and SDN_NEO4J_PASSWORD to use a local instance. Checkstyle and friends: There is no quality gate in place at the moment to ensure that the code/test ratio stays as is, but please consider adding tests to your contributions. We have some rather mild checkstyle rules in place, enforcing more or less default Java formatting rules. Your build will break on formatting errors or something like unused imports."
