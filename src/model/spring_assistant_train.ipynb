{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30776,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# Install libraries\n\n!pip install -q -U bitsandbytes accelerate transformers datasets trl peft",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-10-03T18:21:50.383942Z",
     "iopub.execute_input": "2024-10-03T18:21:50.384261Z",
     "iopub.status.idle": "2024-10-03T18:22:23.183055Z",
     "shell.execute_reply.started": "2024-10-03T18:21:50.384225Z",
     "shell.execute_reply": "2024-10-03T18:22:23.181610Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Hugging Face Login\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")\n\nimport huggingface_hub\nhuggingface_hub.login(token=hf_token)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T18:22:44.303487Z",
     "iopub.execute_input": "2024-10-03T18:22:44.303906Z",
     "iopub.status.idle": "2024-10-03T18:22:44.997289Z",
     "shell.execute_reply.started": "2024-10-03T18:22:44.303865Z",
     "shell.execute_reply": "2024-10-03T18:22:44.996260Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Import Modules\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport pandas as pd\n\nfrom datasets import Dataset, load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T20:11:52.478964Z",
     "iopub.execute_input": "2024-10-03T20:11:52.479735Z",
     "iopub.status.idle": "2024-10-03T20:11:52.484824Z",
     "shell.execute_reply.started": "2024-10-03T20:11:52.479693Z",
     "shell.execute_reply": "2024-10-03T20:11:52.483918Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"bky373/spring-docs\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T20:11:56.794317Z",
     "iopub.execute_input": "2024-10-03T20:11:56.795081Z",
     "iopub.status.idle": "2024-10-03T20:11:58.622052Z",
     "shell.execute_reply.started": "2024-10-03T20:11:56.795041Z",
     "shell.execute_reply": "2024-10-03T20:11:58.621313Z"
    },
    "trusted": true
   },
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/205 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0633a5871a414d9890455ef90b6c2672"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/6205 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c0b0d0e255e48ebaece8f1f94fcfdf0"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# 데이터셋 탐색 및 예시\n\ndataset",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T20:12:02.556292Z",
     "iopub.execute_input": "2024-10-03T20:12:02.556730Z",
     "iopub.status.idle": "2024-10-03T20:12:02.563257Z",
     "shell.execute_reply.started": "2024-10-03T20:12:02.556687Z",
     "shell.execute_reply": "2024-10-03T20:12:02.562111Z"
    },
    "trusted": true
   },
   "execution_count": 46,
   "outputs": [
    {
     "execution_count": 46,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 6205\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "dataset['train'][750]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T20:12:03.702026Z",
     "iopub.execute_input": "2024-10-03T20:12:03.702738Z",
     "iopub.status.idle": "2024-10-03T20:12:03.709115Z",
     "shell.execute_reply.started": "2024-10-03T20:12:03.702675Z",
     "shell.execute_reply": "2024-10-03T20:12:03.708102Z"
    },
    "trusted": true
   },
   "execution_count": 47,
   "outputs": [
    {
     "execution_count": 47,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'question': '스프링 부트에서 ThreadPoolTaskExecutor의 스레드 풀 크기와 큐 용량을 어떻게 설정할 수 있나요?',\n 'answer': 'ThreadPoolTaskExecutor의 스레드 풀 크기와 큐 용량은 spring.task.execution 네임스페이스를 사용하여 설정할 수 있습니다. 예를 들어, spring.task.execution.pool.max-size=16, spring.task.execution.pool.queue-capacity=100와 같이 설정하면 스레드 풀 크기와 큐 용량을 각각 16개와 100개로 설정할 수 있습니다. (출처: https://docs.spring.io/spring-boot/reference/features/task-execution-and-scheduling.html)'}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Initialize Base Model\n\nbase_model = \"google/gemma-2b-it\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # 4비트 양자화를 활성화, 모델을 메모리 효율적으로 로드\n    bnb_4bit_quant_type=\"nf4\",  # 4비트 양자화에서 사용할 방식(Normal Float 4), 더 나은 표현력 제공\n    bnb_4bit_compute_dtype=torch.float16  # 양자화된 값의 계산에 사용할 데이터 타입, FP16으로 설정하여 메모리 절약\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model, device_map=\"auto\", quantization_config=bnb_config)\n\nmodel.config.use_cache = False\n\n# model.config.pretraining_tp = 1\n\nmax_seq_length = 1024\ntokenizer = AutoTokenizer.from_pretrained(base_model, max_seq_length=max_seq_length)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T20:12:06.074373Z",
     "iopub.execute_input": "2024-10-03T20:12:06.075001Z",
     "iopub.status.idle": "2024-10-03T20:12:13.448199Z",
     "shell.execute_reply.started": "2024-10-03T20:12:06.074960Z",
     "shell.execute_reply": "2024-10-03T20:12:13.447133Z"
    },
    "trusted": true
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec18021e61e940f8bc82b342f5d4fae5"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Build QA Dataset\n\ndef build_qa_pairs(data):\n    pairs = []\n    for i in range(len(data)):\n        pairs.append(f\"question:{data['question'][i]}\\nanswer:{data['answer'][i]}\")\n    return pairs\n\nqa_dataset = build_qa_pairs(dataset['train'])\nqa_dataset[750]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T18:27:43.919908Z",
     "iopub.execute_input": "2024-10-03T18:27:43.920638Z",
     "iopub.status.idle": "2024-10-03T18:30:48.242409Z",
     "shell.execute_reply.started": "2024-10-03T18:27:43.920600Z",
     "shell.execute_reply": "2024-10-03T18:30:48.241302Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'question:스프링 부트에서 ThreadPoolTaskExecutor의 스레드 풀 크기와 큐 용량을 어떻게 설정할 수 있나요?\\nanswer:ThreadPoolTaskExecutor의 스레드 풀 크기와 큐 용량은 spring.task.execution 네임스페이스를 사용하여 설정할 수 있습니다. 예를 들어, spring.task.execution.pool.max-size=16, spring.task.execution.pool.queue-capacity=100와 같이 설정하면 스레드 풀 크기와 큐 용량을 각각 16개와 100개로 설정할 수 있습니다. (출처: https://docs.spring.io/spring-boot/reference/features/task-execution-and-scheduling.html)'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "train_data = (pd.DataFrame(qa_dataset, columns=[\"text\"])\n              .sample(frac=1, random_state=5)\n              .drop_duplicates()\n             )\ntrain_data = Dataset.from_pandas(train_data)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T20:11:44.277732Z",
     "iopub.execute_input": "2024-10-03T20:11:44.278143Z",
     "iopub.status.idle": "2024-10-03T20:11:44.308242Z",
     "shell.execute_reply.started": "2024-10-03T20:11:44.278105Z",
     "shell.execute_reply": "2024-10-03T20:11:44.307451Z"
    },
    "trusted": true
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "output_dir = \"gemma2-2b_spring-assistant-ft\"\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1, # 과적합을 방지하고 모델의 일반화 능력을 향상시키기 위함\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    gradient_checkpointing=True,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=100,\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=1500,\n    warmup_ratio=0.1,\n    group_by_length=False,\n    evaluation_strategy='steps',\n    do_eval=True,\n    eval_steps = 100,\n    eval_accumulation_steps=1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\",\n)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T20:11:46.184219Z",
     "iopub.execute_input": "2024-10-03T20:11:46.184939Z",
     "iopub.status.idle": "2024-10-03T20:11:46.242053Z",
     "shell.execute_reply.started": "2024-10-03T20:11:46.184901Z",
     "shell.execute_reply": "2024-10-03T20:11:46.240693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length,\n    args=training_arguments,\n    packing=False,\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T18:31:27.098366Z",
     "iopub.execute_input": "2024-10-03T18:31:27.099231Z",
     "iopub.status.idle": "2024-10-03T18:31:30.110707Z",
     "shell.execute_reply.started": "2024-10-03T18:31:27.099190Z",
     "shell.execute_reply": "2024-10-03T18:31:30.109848Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/6205 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "107be9fa8e4449759ed6f3f44c5c08f1"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "max_steps is given, it will override any value given in num_train_epochs\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "trainer.train()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T18:31:34.104638Z",
     "iopub.execute_input": "2024-10-03T18:31:34.105104Z",
     "iopub.status.idle": "2024-10-03T19:48:03.195601Z",
     "shell.execute_reply.started": "2024-10-03T18:31:34.105052Z",
     "shell.execute_reply": "2024-10-03T19:48:03.194705Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 1:16:22, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.768400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.447300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.372900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.312400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.271000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.217800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.182400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.104800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.872900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.843100</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "execution_count": 19,
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=1000, training_loss=1.239297004699707, metrics={'train_runtime': 4588.517, 'train_samples_per_second': 1.743, 'train_steps_per_second': 0.218, 'total_flos': 1.3536349998587904e+16, 'train_loss': 1.239297004699707, 'epoch': 1.2892828364222402})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "trainer.save_model()\ntokenizer.save_pretrained(output_dir)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T19:48:44.266290Z",
     "iopub.execute_input": "2024-10-03T19:48:44.267059Z",
     "iopub.status.idle": "2024-10-03T19:48:46.386137Z",
     "shell.execute_reply.started": "2024-10-03T19:48:44.267017Z",
     "shell.execute_reply": "2024-10-03T19:48:46.385080Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "('gemma2-2b_spring-assistant-ft/tokenizer_config.json',\n 'gemma2-2b_spring-assistant-ft/special_tokens_map.json',\n 'gemma2-2b_spring-assistant-ft/tokenizer.model',\n 'gemma2-2b_spring-assistant-ft/added_tokens.json',\n 'gemma2-2b_spring-assistant-ft/tokenizer.json')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import gc\n\ndel [model, tokenizer, peft_config, trainer, train_data, bnb_config, training_arguments]\ndel [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]\n\nfor _ in range(10):\n    torch.cuda.empty_cache()\n    gc.collect()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T19:48:52.132963Z",
     "iopub.execute_input": "2024-10-03T19:48:52.133722Z",
     "iopub.status.idle": "2024-10-03T19:48:55.517698Z",
     "shell.execute_reply.started": "2024-10-03T19:48:52.133678Z",
     "shell.execute_reply": "2024-10-03T19:48:55.516864Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from peft import AutoPeftModelForCausalLM\n \ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n     output_dir,\n     low_cpu_mem_usage=True,\n     device_map=\"auto\",\n     torch_dtype=torch.float16\n)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nmerged_model = model.merge_and_unload()\nmerged_model_name = \"./gemma2_merged_spring-assistant\"\nmerged_model.save_pretrained(merged_model_name, safe_serialization=True, max_shard_size=\"2GB\")\ntokenizer.save_pretrained(merged_model_name)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T19:53:55.274864Z",
     "iopub.execute_input": "2024-10-03T19:53:55.275821Z",
     "iopub.status.idle": "2024-10-03T19:54:27.175757Z",
     "shell.execute_reply.started": "2024-10-03T19:53:55.275778Z",
     "shell.execute_reply": "2024-10-03T19:54:27.174431Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea053ca6ba344fc98752e9a86d83fad6"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 26,
     "output_type": "execute_result",
     "data": {
      "text/plain": "('./gemma2_merged_spring-assistant/tokenizer_config.json',\n './gemma2_merged_spring-assistant/special_tokens_map.json',\n './gemma2_merged_spring-assistant/tokenizer.model',\n './gemma2_merged_spring-assistant/added_tokens.json',\n './gemma2_merged_spring-assistant/tokenizer.json')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import gc\n\ndel [model, tokenizer, merged_model, AutoPeftModelForCausalLM]\n\nfor _ in range(10):\n    torch.cuda.empty_cache()\n    gc.collect()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T19:54:29.114995Z",
     "iopub.execute_input": "2024-10-03T19:54:29.115402Z",
     "iopub.status.idle": "2024-10-03T19:54:33.052185Z",
     "shell.execute_reply.started": "2024-10-03T19:54:29.115362Z",
     "shell.execute_reply": "2024-10-03T19:54:33.051264Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T18:15:12.449607Z",
     "iopub.execute_input": "2024-10-03T18:15:12.450507Z",
     "iopub.status.idle": "2024-10-03T18:15:16.542994Z",
     "shell.execute_reply.started": "2024-10-03T18:15:12.450469Z",
     "shell.execute_reply": "2024-10-03T18:15:16.542112Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    merged_model_name,\n    device_map=\"auto\",\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\n# model.config.pretraining_tp = 1\n\nmax_seq_length = 1024\ntokenizer = AutoTokenizer.from_pretrained(merged_model_name, max_seq_length=max_seq_length)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T19:55:00.783178Z",
     "iopub.execute_input": "2024-10-03T19:55:00.784257Z",
     "iopub.status.idle": "2024-10-03T19:55:05.502072Z",
     "shell.execute_reply.started": "2024-10-03T19:55:00.784187Z",
     "shell.execute_reply": "2024-10-03T19:55:05.500877Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa6dc018b8f2420485b4b6ee4dce140d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "def generate_ai_response(user_input):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": user_input\n        }\n    ]\n    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \n    result = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.8)\n    \n    generated_text = result[0]['generated_text']\n    return generated_text.strip()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T20:06:20.026641Z",
     "iopub.execute_input": "2024-10-03T20:06:20.027159Z",
     "iopub.status.idle": "2024-10-03T20:06:20.033394Z",
     "shell.execute_reply.started": "2024-10-03T20:06:20.027121Z",
     "shell.execute_reply": "2024-10-03T20:06:20.032515Z"
    },
    "trusted": true
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "generate_ai_response(\"Spring AI에서 AWS Bedrock Cohere chat model을 사용하여 요청 특정 런타임 옵션을 어떻게 사용할 수 있나요?\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T18:16:25.465604Z",
     "iopub.execute_input": "2024-10-03T18:16:25.466445Z",
     "iopub.status.idle": "2024-10-03T18:16:49.935528Z",
     "shell.execute_reply.started": "2024-10-03T18:16:25.466408Z",
     "shell.execute_reply": "2024-10-03T18:16:49.934559Z"
    },
    "trusted": true
   },
   "execution_count": 48,
   "outputs": [
    {
     "execution_count": 48,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Spring AI에서 AWS Bedrock Cohere chat model을 사용하여 요청 특정 런타임 옵션을 사용하려면, AWS Bedrock Cohere Chat Model 클라이언트 팩토리를 구성하면 됩니다. Spring AI 문서에서 제공하는 예제를 참조하여 제공되는 옵션을 사용할 수 있습니다. 이 예제에서는 최적화된 모델을 사용하는 방법을 보여줍니다. 이 예제는 텍스트 기반 텍스트, 텍스트 기반 텍스트와 이미지, 텍스트 기반 텍스트와 비디오, 텍스트 기반 텍스트와 audio 또는 video, 텍스트 기반 텍스트와 이미지, 텍스트 기반 텍스트와 audio, 텍스트 기반 텍스트와 video와 함께 이미지를 포함하는 텍스트입니다. (출처: https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/cloud-aws/bedrock-chat-model.html) 이 예제는 AWS Bedrock Cohere Chat Model을 사용하여 요청 특정 런타임 옵션을 사용하는 방법을 보여줍니다. 또한, 이 예제는 텍스트 기반 텍스트와 이미지, 텍스트 기반 텍스트와 audio 또는 video, 텍스트 기반 텍스트와 이미지, 텍스트 기반 텍스트와 audio, 텍스트 기반 텍스트와 video와 함께 이미지를 포함하는 텍스트입니다. (출처: https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/cloud-aws/bedrock-chat-model.html) 이 예제는 AWS Bedrock Cohere Chat Model을 사용하여 요청 특정 런타임 옵션을 사용하는 방법을 보여줍니다. 또한, 이 예제는 텍스트 기반 텍스트와 이미지, 텍스트 기반 텍스트와 audio 또는 video, 텍스트 기반 텍스트와 이미지, 텍스트 기반 텍스트와 audio, 텍스트 기반 텍스트와 video와 함께 이미지를 포함하는 텍스트입니다. (출처: https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/cloud-aws/bedrock-chat-model.html) 이'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "generate_ai_response(\"스프링 인증 서버가 지원하는 토큰 형식은 무엇인가요?\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T18:17:01.566805Z",
     "iopub.execute_input": "2024-10-03T18:17:01.567188Z",
     "iopub.status.idle": "2024-10-03T18:17:25.699030Z",
     "shell.execute_reply.started": "2024-10-03T18:17:01.567144Z",
     "shell.execute_reply": "2024-10-03T18:17:25.697780Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": [
    {
     "execution_count": 49,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'스프링 인증 서버는 JSON Web Token (JWT), JSON Web Signature (JWS), OAuth 2.0 Token Exchange, OAuth 2.0 Authorization Code Grant, OAuth 2.0 Client Credentials Grant, OAuth 2.0 Password Credentials Grant, OAuth 2.0 Client Credentials Refresh Grant, OAuth 2.0 Refresh Token Grant, OAuth 2.0 Token Exchange Token Request, OAuth 2.0 Token Exchange Token Refresh Token Request, OAuth 2.0 Token Exchange Token Refresh Token Refresh Token Request, OAuth 2.0 Token Exchange Token Refresh Token Refresh Token Refresh Token Request, OAuth 2.0 Bearer Token Request, OAuth 2.0 Refresh Token Request, OAuth 2.0 Access Code Request, OAuth 2.0 Bearer Assertion Request, OAuth 2.0 Client Credentials Request, OAuth 2.0 Client Credentials Refresh Request, OAuth 2.0 Client Credentials Refresh Refresh Request, OAuth 2.0 Bearer Token Request, OAuth 2.0 Token Request 및 OAuth 2.0 Refresh Token Request를 지원하며, 각각 JWT, JWS, OAuth 2.0 Client Credentials Grant, OAuth 2.0 Refresh Token Request, OAuth 2.0 Client Credentials Refresh Token Refresh Token Request를 지원합니다. (출처: https://docs.spring.io/spring-authorization-server/reference/1.3/index.html) 또한, OAuth 2.0 Authorization Code Grant, OAuth 2.0 Client Credentials Grant, OAuth 2.0 Refresh Token Request, OAuth 2.0 Bearer Token Request, OAuth 2.0 Refresh Token Request, OAuth 2.0 Refresh Token Request, OAuth 2.0 Token Exchange Token Refresh Token Refresh Token Request를 지원합니다. (출처: https://docs.spring.io/spring-authorization-server/reference/1.3/index.html) 공통적으로 지원되는 토큰 형식을 나타내는 세부 규칙은 OAuth 2.0 OAuth 2.0은 JWT, JWS, JWT를 포함합니다. (출처: https://docs.spring.io/spring-authorization-server/reference/1.3/index.html) 또한, OAuth 2.0은 OAuth 2.0 Client Credentials Grant, OAuth 2.0 Refresh Token Request, OAuth 2.0 Bearer Token Request, OAuth 2.0 Token Exchange Token Refresh Token Refresh Token Request를 지원'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "generate_ai_response(\"스프링 부트에서 메시징 프로토콜을 사용하려면 어떻게 해야 하나요?\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-03T18:18:33.017250Z",
     "iopub.execute_input": "2024-10-03T18:18:33.017619Z",
     "iopub.status.idle": "2024-10-03T18:18:52.251939Z",
     "shell.execute_reply.started": "2024-10-03T18:18:33.017581Z",
     "shell.execute_reply": "2024-10-03T18:18:52.250997Z"
    },
    "trusted": true
   },
   "execution_count": 51,
   "outputs": [
    {
     "execution_count": 51,
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"스프링 부트는 Spring Cloud Gateway에서 메시징 프로토콜을 사용할 수 있도록 제공합니다. 먼저, 'configureExternalTransportGateway' 메소드를 사용하여 HTTP 또는 TCP 메시징 프레임워크를 구성해야 합니다. 그런 다음, 'configureExternalTransportGateway' 메소드를 사용하여 메시징 프로토콜에 대한 구성을 추가해야 합니다. 이는 기본적으로 HTTP를 사용하며, TCP를 사용하려면 'tcp' 속성을 true로 설정해야 합니다. 이를 통해 메시징 프로토콜을 사용할 수 있습니다. (출처: https://docs.spring.io/spring-cloud-gateway/reference/html/spring-cloud-gateway-server-mvc.html) 자세한 내용은 다음 섹션에서 확인할 수 있습니다: https://docs.spring.io/spring-cloud-gateway/reference/html/spring-cloud-gateway-server-mvc.html#spring-cloud-gateway-server-mvc-supported-protocols-http-tcp-udp-tcp-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls-ssl-tls.html에서 확인할 수 있습니다. (출처: https://docs.spring.io/spring-cloud-gateway/reference/html/spring-cloud-gateway-server-mvc.html) (출처: https://docs.spring.io/spring-cloud-gateway/reference/html/spring-cloud-gateway-server-mvc.html) (출처: https://docs.spring.io/spring-cloud-gateway/reference/html/spring-cloud-gateway-server-mvc.html)\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
